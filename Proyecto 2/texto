cema Cómo leer y escribir archivos en Python
abril 26, 2019 Por Daniel Rodríguez Dejar un comentario
Tiempo de lectura: 4 minutos


Comparte
Tuitea
Pinear
Comparte
La importación y exportación de datos desde archivos son tareas que se realizan con bastante asiduidad. Por ello, en Python, librerías como pandas incorporan herramientas para el manejo de archivos CSV o en formato Microsoft Excel. Pero, si los datos no se encuentran en un formato estándar, estas herramientas pueden no ser las adecuadas. Por lo tanto, puede ser necesario manejar los archivos directamente. En esta entrada se va a mostrar cómo leer y escribir archivos en Python.

Abrir un archivo para leer o escribir en Python
Antes de leer o escribir archivos con Python es necesario es necesario abrir una conexión. Lo que se puede hacer con el comando open(), al que se le ha de indicar el nombre del archivo. Por defecto la conexión se abre en modo lectura, con lo que no es posible escribir en el archivo. Para poder escribir es necesario utilizar la opción "w" con la que se eliminará cualquier archivo existente y creará uno nuevo. Otra opción que se puede utilizar es "a", con la que se añadirá nuevo contenido al archivo existente. Las opciones se pueden ver en el siguiente código.

# Abre el archivo para escribir y elimina los archivos anteriores si existen
fic = open("text.txt", "w")
# Abre el archivo para agregar contenido
fic = open("text.txt", "a")
# Abre el archivo en modo lectura
fic = open("text.txt", "r")
En todos los casos, una vez finalizado las operaciones de lectura y escritura con los archivos, una buena práctica es cerrar el acceso. Para lo que se debe utilizar el método close().

Escribir archivos de texto en Python
Antes guardar un archivo es necesario disponer de un vector con las cadenas de texto que se desean guardar. Para ello se puede crear un vector al que se le puede llamar data.

data = ["Línea 1", "Línea 2", "Línea 3", "Línea 4", "Línea 5"]
Para escribir el contenido de este vector en un archivo se puede hacer de dos maneras: línea a línea o de una sola vez.

Escribir el archivo línea a línea
El método más fácil directo para volcar el vector en un archivo es escribir el contenido línea a línea. Para ello se puede iterar sobre el archivo y utilizar el método write de archivo. Este proceso es lo que se muestra en el siguiente ejemplo.

fic = open("text_1.txt", "w")
for line in data:
    fic.write(line)
    fic.write("\n")
    
fic.close()
Nótese que los elementos de vector no finalizan con el carácter salto de línea. Por lo tanto, es necesario añadir este después de escribir cada línea. Ya que, de lo contrario, todos los elementos se escribirían en una única línea en el archivo de salida.

Una forma de escribir el archivo línea a línea sin que sea necesario incluir el salto de línea es con la función print. Para lo es necesario incluir la opción file con la conexión al archivo. Esta opción se puede ver en el siguiente ejemplo.

fic = open("text_2.txt", "w")
for line in data:
    print(line, file=fic)
    
fic.close()
Escribir el archivo de una vez
Finalmente, en el caso de que los dato se encuentren en un objeto iterable se puede utilizar el método writelines para volcar este de golpe. Aunque es necesario tener en cuenta que este método no agrega el salto de línea, por lo que puede ser necesario agregarlo con antelación.

fic = open("text_3.txt", "w")
fic.writelines("%s\n" % s for s in data)
fic.close()
En el ejemplo se puede apreciar que se ha iterado sobre el vector para agregar el salto de línea para cada elemento.

Leer archivos de texto en Python
La lectura de los archivos, al igual que la escritura, se puede hacer de dos maneras: línea a línea o de una sola vez.

Leer el archivo de una vez
El procedimiento para leer los archivos de texto más sencillo es hacerlo de una vez con el método readlines. Una vez abierto el archivo solamente se ha de llamar a este método para obtener el contenido. Por ejemplo, se puede usar el siguiente código.

fic = open('text_1.txt', "r")
lines = fic.readlines()
fic.close()
En esta ocasión lines es un vector en el que cada elemento es una línea del archivo. Alternativamente, en lugar del método readlines se puede usar la función list para leer los datos.

fic = open('text_1.txt', "r")
lines = list(fic)
fic.close()
Leer el archivo línea a línea
En otras ocasiones puede ser necesario leer el archivo línea a línea. Esto se puede hacer simplemente iterando sobre el fichero una vez abierto. En casa iteración se podrá hacer con cada línea cualquier operación que sea necesaria. En el siguiente ejemplo cada una de las líneas se agrega a un vector.

fic = open('text_1.txt', "r")
lines = []
for line in fic:
    lines.append(line)
fic.close()
Eliminar los saltos de línea en el archivo importado
Los tres métodos que se han visto para leer los archivos importan el salto de línea. Por lo que puede ser necesario eliminarlo antes de trabajar con los datos. Esto se puede conseguir de forma sencilla con el método rstrip de las cadenas de texto de Python. Lo que se puede hacer iterando sobre el vector.

[s.rstrip('\n') for s in lines]
Conclusiones
Hoy se ha visto cómo leer y escribir archivos en Python utilizando solamente las funciones estándar del lenguaje. Explicando tres métodos tanto para escribir los archivos como para leerlos. Aunque normalmente para la lectura de archivos CSV en Python lo más fácil es utilizar pandas, puede ser que sea necesario procesar los datos de una forma no estándar. En estas situaciones es cuando los visto en esta entrada es bastante útil.
En este tutorial aprenderemos a utilizar diccionarios de datos en Python y algunos de sus métodos más importantes.

Python es un lenguaje de programación interpretado de alto nivel y orientado a objetos, con el cual podemos crear todo tipo de aplicaciones. Entre sus diversos tipos de estructuras de datos, se encuentra "Diccionarios de Datos". En este tutorial aprenderemos a utilizar esta estructura revisando sus méetodos más utilizados.

¿Qué es un Diccionario de datos?
Un Diccionario es una estructura de datos y un tipo de dato en Python con características especiales que nos permite almacenar cualquier tipo de valor como enteros, cadenas, listas e incluso otras funciones. Estos diccionarios nos permiten además identificar cada elemento por una clave (Key).

Para definir un diccionario, se encierra el listado de valores entre llaves. Las parejas de clave y valor se separan con comas, y la clave y el valor se separan con dos puntos.

diccionario = {'nombre' : 'Carlos', 'edad' : 22, 'cursos': ['Python','Django','JavaScript'] }
Podemos acceder al elemento de un Diccionario mediante la clave de este elemento, como veremos a continuación:

print diccionario['nombre'] #Carlos
print diccionario['edad']#22
print diccionario['cursos'] #['Python','Django','JavaScript']
También es posible insertar una lista dentro de un diccionario. Para acceder a cada uno de los cursos usamos los índices:

print diccionario['cursos'][0]#Python
print diccionario['cursos'][1]#Django
print diccionario['cursos'][2]#JavaScript
Para recorrer todo el Diccionario, podemos hacer uso de la estructura for:

for key in diccionario:
  print key, ":", diccionario[key]
Métodos de los Diccionarios
dict ()

Recibe como parámetro una representación de un diccionario y si es factible, devuelve un diccionario de datos.

dic =  dict(nombre='nestor', apellido='Plasencia', edad=22)

dic → {‘nombre’ : 'nestor', ‘apellido’ : 'Plasencia', ‘edad’ : 22}
zip()

Recibe como parámetro dos elementos iterables, ya sea una cadena, una lista o una tupla. Ambos parámetros deben tener el mismo número de elementos. Se devolverá un diccionario relacionando el elemento i-esimo de cada uno de los iterables.

dic = dict(zip('abcd',[1,2,3,4]))

dic →   {‘a’ : 1, ’b’ : 2, ‘c’ : 3 , ‘d’ : 4}
items()

Devuelve una lista de tuplas, cada tupla se compone de dos elementos: el primero será la clave y el segundo, su valor.

dic =   {‘a’ : 1, ’b’ : 2, ‘c’ : 3 , ‘d’ : 4}
items = dic.items()

items → [(‘a’,1),(‘b’,2),(‘c’,3),(‘d’,4)]
keys()

Retorna una lista de elementos, los cuales serán las claves de nuestro diccionario.

dic =  {‘a’ : 1, ’b’ : 2, ‘c’ : 3 , ‘d’ : 4}
keys= dic.keys()

keys→ [‘a’,’b’,’c’,’d’] 
values()

Retorna una lista de elementos, que serán los valores de nuestro diccionario.

dic =  {‘a’ : 1, ’b’ : 2, ‘c’ : 3 , ‘d’ : 4}
values= dic.values()

values→ [1,2,3,4] 
clear()

Elimina todos los ítems del diccionario dejándolo vacío.

dic 1 =  {‘a’ : 1, ’b’ : 2, ‘c’ : 3 , ‘d’ : 4}
dic1.clean()

dic1 → { }
copy()

Retorna una copia del diccionario original.

dic = {‘a’ : 1, ’b’ : 2, ‘c’ : 3 , ‘d’ : 4}
dic1 = dic.copy()

dic1 → {‘a’ : 1, ’b’ : 2, ‘c’ : 3 , ‘d’ : 4}
fromkeys()

Recibe como parámetros un iterable y un valor, devolviendo un diccionario que contiene como claves los elementos del iterable con el mismo valor ingresado. Si el valor no es ingresado, devolverá none para todas las claves.

dic = dict.fromkeys(['a','b','c','d'],1)

dic →  {‘a’ : 1, ’b’ : 1, ‘c’ : 1 , ‘d’ : 1}
get()

Recibe como parámetro una clave, devuelve el valor de la clave. Si no lo encuentra, devuelve un objeto none.

dic = {‘a’ : 1, ’b’ : 2, ‘c’ : 3 , ‘d’ : 4}
valor = dic.get(‘b’) 

valor → 2
pop()

Recibe como parámetro una clave, elimina esta y devuelve su valor. Si no lo encuentra, devuelve error.

dic = {‘a’ : 1, ’b’ : 2, ‘c’ : 3 , ‘d’ : 4}
valor = dic.pop(‘b’) 

valor → 2
dic → {‘a’ : 1, ‘c’ : 3 , ‘d’ : 4}
setdefault()

Funciona de dos formas. En la primera como get

dic = {‘a’ : 1, ’b’ : 2, ‘c’ : 3 , ‘d’ : 4}
valor = dic.setdefault(‘a’)

valor → 1
Y en la segunda forma, nos sirve para agregar un nuevo elemento a nuestro diccionario.

dic = {‘a’ : 1, ’b’ : 2, ‘c’ : 3 , ‘d’ : 4}
valor = dic.setdefault(‘e’,5)

dic → {‘a’ : 1, ’b’ : 2, ‘c’ : 3 , ‘d’ : 4 , ‘e’ : 5}
update()

Recibe como parámetro otro diccionario. Si se tienen claves iguales, actualiza el valor de la clave repetida; si no hay claves iguales, este par clave-valor es agregado al diccionario.

dic 1 = {‘a’ : 1, ’b’ : 2, ‘c’ : 3 , ‘d’ : 4}
dic 2 = {‘c’ : 6, ’b’ : 5, ‘e’ : 9 , ‘f’ : 10}
dic1.update(dic 2)

dic 1 → {‘a’ : 1, ’b’ : 5, ‘c’ : 6 , ‘d’ : 4 , ‘e’ : 9 , ‘f’ : 10}
Estos son algunos de los métodos más útiles y más utilizados en los Diccionarios. Python es un gran lenguaje de programación que nos permite programar de una manera realmente sencilla. Si deseas conocer mucho más y aprender a profundidad esta tecnología, ingresa al Curso de Python que tenemos en Devcode. ¡Te esperamos!

¿Te gusto el tutorial?
Ayúdanos a llegar a más personas

 
 
user
Carlos Eduardo Plasencia Prado
Backend Developer | Python / Django junior - Javascript / Node.js

@plasenciacar
ython: Agregar y eliminar elementos de un diccionario
Objetivo: mostrar cómo agregar y eliminar elementos de un diccionario en Python.

En la anterior entrega explicamos el concepto de diccionario en Python y vimos el modo de crearlos. Hoy presentaremos dos técnicas básicas: la agregación y eliminación de elementos.

Comencemos con la creación de un par de diccionarios de ejemplo sobre los que practicaremos desde el intérprete interactivo:

>>> gazpacho = {}
>>> menda = {'Nombre':'Javier', 'Apellido':'Montero'}
El primero, gazpacho, es un simple diccionario vacío que utilizaremos para almacenar la receta del gazpacho; el otro, menda, recoge algunos datos sobre mi persona. Más adelante veremos que podemos crear estructuras de datos más complejas, basadas en los diccionarios, que podremos emplear para mantener una agenda de nuestros contactos personales y sin necesidad de recurrir a un gestor de bases de datos.

Para agregar un par clave-valor a un diccionario, recurrimos a la siguiente sintaxis:

diccionario[clave] = valor

Probémoslo con gazpacho:

>>> gazpacho['Aceite'] = '300 ml'

Verificamos que gazpacho, que antes estaba vacío, ahora contiene ese par:

>>> gazpacho
{'Aceite': '300 ml'}
Continuemos con la receta secreta:

>>> gazpacho['Vinagre'] = '100 ml'
>>> gazpacho['Pepino'] = 1
>>> gazpacho['Pimiento'] = 1
Veamos cómo evoluciona nuestra creación:

>>> gazpacho
{'Pimiento': 1, 'Aceite': '300 ml', 'Vinagre': '100 ml', 'Pepino': 1}
Observa la salida de esta última instrucción y recuerda lo que dijimos sobre el orden de los diccionarios: no son una estructura ordenada, aunque veremos formas de hacerlos más presentables si lo deseamos.

Añadamos nuevos elemendos al menda:

>>> menda['URL'] = 'http://www.elclubdelautodidacta.es'
>>> menda['Twitter'] = '@pradery'
tras lo cual,

>>> menda
{'URL': 'http://www.elclubdelautodidacta.es', 'Nombre': 'Javier', 'Twitter': '@pradery', 'Apellido': 'Montero'}
Las claves han de ser únicas. Si tratamos de agregar otra ya existente, simplemente el valor nuevo sustituirá al antiguo:

>>> gazpacho['Pimiento'] = 2
>>> menda['URL'] = 'http://elclubdelautodidacta.es/wp/'
Observa cómo los valores correspondientes son actualizados:

>>> gazpacho
{'Pimiento': 2, 'Aceite': '300 ml', 'Vinagre': '100 ml', 'Pepino': 1}
>>> menda
{'URL': 'http://elclubdelautodidacta.es/wp/', 'Nombre': 'Javier', 'Twitter': '@pradery', 'Apellido': 'Montero'}
Para borrar un par clave-valor de un diccionario disponemos de la sentencia del, que emplearemos del siguiente modo:

del diccionario[clave]

Por ejemplo:

>>> del gazpacho['Aceite']
>>> gazpacho
{'Pimiento': 2, 'Vinagre': '100 ml', 'Pepino': 1}
>>> del menda['URL']
>>> menda
{'Nombre': 'Javier', 'Twitter': '@pradery', 'Apellido': 'Montero'}
Con del podríamos cargarnos incluso el objeto completo:

>>> del gazpacho

A partir de este momento el gazpacho ha dejado de existir y ha pasado a mejor vida:

>>> gazpacho
Traceback (most recent call last):
  File "<pyshell#28>", line 1, in <module>
    gazpacho
NameError: name 'gazpacho' is not defined
Y con menda mejor no lo hago, no sea que traiga mala suerte…

Javier Montero Gabarró

Python: Agregar y eliminar elementos de un diccionario

El texto de este artículo se encuentra sometido a una licencia Creative Commons del tipo CC-BY-NC-ND (reconocimiento, no comercial, sin obra derivada, 3.0 unported)

El Club del Autodidacta

Consulta el índice completo de artículos relacionados con Python.

Tal vez te interese...
Python – Buscando a Wally.txt
Python – Troceando desde el lado izquierdo
Comparando objetos en Python
Python – Sumando objetos
Python – Una tortuga de brocha fina
¿Conoces a alguien a quien le pueda interesar este artículo?
Haz clic para compartir en Facebook (Se abre en una ventana nueva)Haz clic para compartir en Twitter (Se abre en una ventana nueva)
Autor Javier MonteroPublicado el07/08/2012CategoríaspythonEtiquetasdel, diccionarios, python
16 opiniones en “Python: Agregar y eliminar elementos de un diccionario”
Isaac Lacoba (@IsaacLacoba)dice:
31/12/2012 a las 13:10
Gracias por tu post. Me ha ahorrado bastante tiempo. 😉

Responder
Javier Monterodice:
02/01/2013 a las 19:39
Me alegro de que te haya sido útil. Saludos. 🙂


About
Prensa
Legal
Política de privacidad
Términos de Servicio
Cookie Policy
STACK EXCHANGE
NETWORK
Technology
Life / Arts
Culture / Recreation
Science
Other
Blog
Facebook
Twitter
LinkedIn
Instagram
diseño del sitio / logo © 2021 Stack Exchange Inc; contribuciones de los usuarios bajo licencia cc by-sa. rev 2021.3.30.38947
Mujercitas (película de 2019)
Ir a la navegaciónIr a la búsqueda
Little Women
Mujercitas 2019.png
Título	Mujercitas
Ficha técnica
Dirección	
Greta Gerwig
Producción	Denise Di Novi
Amy Pascal
Robin Swicord
Arnon Milchan
Guion	Greta Gerwig
Basada en	Mujercitas, de Louisa May Alcott
Música	Alexandre Desplat
Fotografía	Yorick Le Saux
Protagonistas	Saoirse Ronan
Timothée Chalamet
Florence Pugh
Eliza Scanlen
James Norton
Laura Dern
Emma Watson
Louis Garrel
Meryl Streep
Bob Odenkirk
Chris Cooper
Ver todos los créditos (IMDb)
Datos y cifras
País	Estados Unidos
Año	2019
Estreno	
25 de diciembre de 2019 (Estados Unidos)
Género	Drama
Romance
Duración	134 minutos
Idioma(s)	Inglés
Compañías
Productora	Columbia Pictures
Di Novi Pictures
Pascal Pictures
New Regency Pictures
Sony Pictures Entertainment
Distribución	Sony Pictures Releasing
InterCom
Presupuesto	US$40.000.000
Recaudación	US$205.914.9761​
Ficha en IMDb
Ficha en FilmAffinity
[editar datos en Wikidata]
Mujercitas2​3​ (en inglés Little Women) es una película dramática estadounidense de 2019 escrita y dirigida por Greta Gerwig. Es la séptima adaptación cinematográfica de la novela de 1868 del mismo nombre de Louisa May Alcott. Narra las vidas de las hermanas March, Jo, Meg, Amy y Beth, en Concord, Massachusetts, durante el siglo XIX. Está protagonizada por un reparto coral compuesto por Saoirse Ronan, Emma Watson, Florence Pugh, Eliza Scanlen, Laura Dern, Timothée Chalamet, Meryl Streep, Tracy Letts, Bob Odenkirk, James Norton, Louis Garrel y Chris Cooper. Gerwig altera cronológicamente la trama de la obra y alterna pasajes de los primeros años de las niñas con su vida profesional, con la intención de hacer una nueva visión de la novela tras 150 años, haciendo hincapié en el subtexto feminista de la obra4​. En la nueva versión, además, se fusiona el texto original con la vida de Louisa May Alcott, de quien Jo era una especie de alter ego5​.

Sony Pictures inició el desarrollo de la película en 2013, Amy Pascal se incorporó para producir en 2015 y Gerwig fue contratada para escribir su guion al año siguiente. Usando otros escritos de Alcott como inspiración, Gerwig escribió el guion en 2018. Fue nombrada directora ese mismo año, tras su triunfo con Lady Bird, y es la segunda película dirigida por la directora, nominada a los Premios Óscar en 2018.6​ La filmación se llevó a cabo de octubre a diciembre de 2018 en el estado de Massachusetts, y la edición comenzó el día después del final de la filmación.

Mujercitas se estrenó en el Museo de Arte Moderno de la ciudad de Nueva York el 7 de diciembre de 2019 y se estrenó en cines en los Estados Unidos el 25 de diciembre de 2019 por Sony Pictures Releasing. La película recibió elogios de la crítica,7​ con elogios especiales por el guion y la dirección de Gerwig, así como por las actuaciones del elenco, y recaudó 216 millones de dólares en todo el mundo. Entre sus numerosos elogios, la película recibió seis nominaciones a los Premios de la Academia, incluyendo Mejor Película, Mejor Actriz (Ronan), Mejor Actriz de Reparto (Pugh) y Mejor Guion Adaptado, y ganó el Premio al Mejor Diseño de Vestuario. También recibió nominaciones para los Premios BAFTA y los Globos de Oro.


Índice
1	Argumento
2	Reparto
3	Producción
3.1	Desarrollo y casting
3.2	Escritura
3.3	Diseño de vestuario
3.4	Filmación y edición
4	Estreno
5	Recepción
5.1	Taquilla
5.2	Crítica
5.3	Premios y nominaciones
6	Referencias
7	Enlaces externos
Argumento
En 1868, Jo March, profesora de la ciudad de Nueva York, acude al Sr. Dashwood, un editor que acepta publicar una historia que ha escrito. Su hermana menor, Amy, que está en París con la tía March, asiste a una fiesta con su amigo de la escuela y vecino, Laurie. Amy se enoja por el comportamiento de Laurie, que se encuentra ebrio y se burla de ella por su intención de casarse con el rico empresario Fred Vaughn. En Nueva York, Jo se siente herida cuando Friedrich Bhaer, un profesor enamorado de ella, critica constructivamente sus escritos, lo que hace que dé por terminada su amistad. Después de enterarse por una carta de que la enfermedad de su hermana menor Beth ha empeorado, Jo regresa a su casa en Concord, Massachusetts.

Siete años antes, en una fiesta con su hermana mayor, Meg, Jo se hace amiga de Laurie. La mañana de Navidad, la madre de las niñas, "Marmee", las convence para que den su desayuno a su pobre vecina, la señora Hummel, y a sus hijos hambrientos. Cuando regresan a casa, encuentran una mesa llena de comida que les trajo su vecino y abuelo de Laurie, el Sr. Laurence. Marmee les a las hermanas una carta de su padre, que lucha en la Guerra Civil Estadounidense. Jo visita regularmente a la tía March para leerle, con la esperanza de que la tía March la invite a viajar por Europa.

Cuando Meg, Jo, Laurie y John Brooke -el tutor de Laurie y futuro esposo de Meg-, van al teatro, Amy celosa, quema los manuscritos de Jo. A la mañana siguiente, Amy, quiere reconciliarse con Jo, la persigue mientras Jo va con Laurie hasta un lago donde patinan. Laurie y Jo salvan a Amy tras hundirse en el agua fría de invierno. El Sr. Laurence invita a la pequeña Beth a tocar el piano de su difunta hija en su casa.

De vuelta al presente, Meg habla con su marido, John, tras comprar una tela tan cara que no podían pagar y le expresa su decepción por ser pobre. En Europa, Laurie visita a Amy -quien ha decidido abandonar su carrera como pintora- para disculparse por su comportamiento de la noche anterior y le ruega que no se case con Fred, y que acepte su propia petición de matrimonio. Aunque está enamorada de Laurie, Amy se niega, molesta por ser siempre segundo plato, tras la negativa que Jo le dio a Lauire ante la misma petición. Aun así, Amy rechaza la propuesta de Fred y acepta la de Laurie.

Jo recuerda cuando tuvo que cortarse el pelo para que su madre pudiese viajar a Nueva York a cuidar de su padre, que había sido herido en guerra. Por entonces el Sr. Laurence regaló su piano a Beth y descubrieron quela niña había contraído la escarlatina, posiblemente en casa de los Hummel. Para evitar contraer la enfermedad, Amy es enviada a quedarse con la tía March, quien le aconseja que mantenga a su familia formalizando un buen matrimonio. Beth se recupera a tiempo para la Navidad del pasado, durante la cual su padre también regresa a casa Meg y John Brooke se comprometen. Jo intenta convencerla de que huya, pero Meg expresa su alegría por casarse con John. La tía March anuncia su viaje por Europa, pero se lleva a Amy en lugar de Jo. Tras la boda, Laurie le propone matrimonio a Jo, quien lo rechaza y le explica que no quiere casarse.

En el presente, John pide a Meg que convierta la cara tela en un vestido si le hace feliz. Meg revela que la había vendido y le asegura que es feliz tal y como viven. Tras el empeoramiento, Beth fallece. Marmee informa a la familia de que Amy regresa de Europa con una tía March enferma. Jo se pregunta a sí misma si se apresuró a rechazar la proposición de matrimonio de Laurie y le escribe una carta. Amy se prepara para abandonar Europa y le dice a Laurie que ha rechazado la propuesta de Fred; se besan y luego se casan en el viaje de retorno a casa. Jo y Laurie acuerdan ser solo amigos, y se deshace de la carta que le escribió y no llegó a enviar. Jo comienza a escribir una novela basada en la vida de ella y sus hermanas y envía los primeros capítulos a un poco impresionado Sr. Dashwood. Bhaer sorprende a Jo al presentarse en la casa March camino a California.

En Nueva York, el Sr. Dashwood acepta publicar la novela de Jo pues sus propias hijas exigen saber cómo termina la historia, pero se niega a aceptar que la protagonista permanezca soltera al final. Para apaciguarlo, Jo termina su novela con la protagonista, ella misma, impidiendo que Bhaer se vaya a California. Negocia con éxito derechos de autor y regalías con el Sr. Dashwood. Tras el fallecimiento de la tía March, Jo hereda su casa y la abre como una escuela para niñas y niños, donde enseñan Meg, Amy, John y Bhaer. Jo observa la impresión de su novela, titulada Mujercitas.

Reparto
Saoirse Ronan como Josephine "Jo" March.
Emma Watson como Margaret "Meg" March.
Florence Pugh como Amy March.
Eliza Scanlen como Elizabeth "Beth" March.
Laura Dern como Marmee March.
Timothée Chalamet como Theodore "Laurie" Laurence.
Meryl Streep como Tía March.
Tracy Letts como el Señor Dashwood.
Bob Odenkirk como el padre de la familia March.
James Norton como John Brooke.
Louis Garrel como Friedrich Bhaer.
Chris Cooper como el Señor Laurence.
Jayne Houdyshell como Hannah.
Rafael Silva como el amigo de Friedrich.
Dash Barber como Fred Vaughn.
Hadley Robinson como Sallie Gardiner Moffat.
Abby Quinn como Annie Moffat.
Maryann Plunkett como la Señora Kirke.
Edward Fletcher como el sirviente del Señor Laurence.
Sasha Frolova como la Señora Hummel.
Producción
Desarrollo y casting
En octubre de 2013, se anunció que una nueva adaptación cinematográfica de la novela Mujercitas de Louisa May Alcott estaba en desarrollo en Sony Pictures, con Olivia Milch escribiendo el guion y Robin Swicord y Denise Di Novi como productoras.8​ En marzo de 2015, Amy Pascal se unió como productora de la nueva adaptación, y Sarah Polley fue contratada para escribir el guion y potencialmente dirigir.9​ En última instancia, la participación de Polley nunca fue más allá de las discusiones iniciales.10​ En agosto de 2016, Greta Gerwig fue contratada para escribir el guion.11​ En junio de 2018, Gerwig fue anunciado como el director de la película además de ser su guionista.12​ Se había enterado de los planes de Sony para adaptar el libro en 2015 e instó a su agente a que la pusiera en contacto con el estudio, admitiendo que si bien ella "no estaba en la lista de nadie para dirigir esta película", era algo a lo que aspiraba hacer, citando cómo el libro la inspiró a convertirse en escritora y directora.13​ Además de ser la primera película de estudio de Gerwig que dirigió, Mujercitas fue su segundo esfuerzo como directora en solitario.14​15​

También se anunció en junio de 2018 que Meryl Streep, Emma Watson, Saoirse Ronan, Timothée Chalamet y Florence Pugh habían sido elegidos para papeles no revelados.16​ Gerwig había trabajado con Ronan y Chalamet en su primera película como directora en solitario, Lady Bird,17​ mientras buscaba elegir a Pugh después de ver su actuación en la película Lady Macbeth.18​ Eliza Scanlen, a quien Gerwig vio protagonizar la miniserie Sharp Objects,19​ se unió al elenco al mes siguiente.20​ James Norton y Laura Dern fueron elegidos en agosto.21​22​ Ese mismo mes, Emma Watson se unió al elenco, reemplazando a Stone, quien tuvo que abandonar debido a conflictos de programación con la promoción de La Favorita.23​ En septiembre, Louis Garrel, Bob Odenkirk y Chris Cooper se unieron al elenco en papeles secundarios.24​25​26​ New Regency Pictures fue anunciado como un financiador adicional de la película en octubre.27​

Escritura
Gerwig comenzó a escribir el guion durante un viaje a Big Sur, California, luego de la ceremonia de los Premios de la Academia 2018, utilizando como inspiración las cartas y diarios de Alcott, así como "pinturas de mujeres jóvenes del siglo XIX".19​ También se inspiró en las otras historias de Alcott para los diálogos.28​ Gerwig escribió muchas líneas de diálogo superpuestas que se "leerían una encima de la otra".29​ Además, afirmó que un monólogo de la película se inspiró en una conversación que tuvo con Streep sobre "los desafíos que enfrentaban las mujeres en el Década de 1860".30​ Para "enfocar la película en [sus personajes] como adultos", Gerwig incorporó una línea de tiempo no lineal.31​ El final difiere del de la novela al describir "los placeres de un romance dentro de una historia sobre Alcott realizando sus ambiciones artísticas", que Gerwig creía que honra la verdadera visión de Alcott dado que Alcott tenía que "satisfacer las expectativas narrativas de la época".32​33​

Diseño de vestuario
La película requirió "aproximadamente 75 trajes de época principales", cada uno de los cuales tomó "aproximadamente 40 horas" para crear.19​ La diseñadora de vestuario, Jacqueline Durran, combinó "un espíritu de vestuario libre" y "la rigidez victoriana tradicional" al vestir a los personajes.34​ Queriendo hacer que "la ropa vintage pareciera codiciada para el espectador moderno", combinó "etiquetas de lana" con "faldas a cuadros de muy buen gusto", "capas largas de color carmesí" y "gorras de vendedor de periódicos".35​ Distinguió los guardarropas de la infancia y la edad adulta de los personajes teniendo en cuenta "la lógica interna de cada uno" y manteniendo "la conexión entre los dos", y a cada personaje se le asignó un "color central", incluido el rojo para el personaje de Ronan, el verde y el lavanda para Watson, marrón y rosa para Scanlen y azul claro para Pugh.36​ También hizo que los personajes compartieran y reutilizaran las mismas piezas de vestuario para reforzar sus relaciones entre ellos.37​ Además de diseñar el personaje de Ronan con "vestidos holgados de algodón" y "faldas de lana lisas",35​ incorporó "referencias modernas" y utilizó "un joven Bob Dylan", la subcultura Teddy Boy y la pintura del artista francés James Tissot. El círculo de la Rue Royale como inspiración para estilizar el de Chalamet.38​

Filmación y edición

La filmación tuvo lugar principalmente en Harvard, Massachusetts.
El elenco, con la excepción de Pugh debido a sus compromisos de filmación con la película Midsommar, comenzó los ensayos de la película dos semanas antes de la filmación.29​ La fotografía principal comenzó en Boston en octubre de 2018, 39​ con Harvard, Massachusetts, como ubicación principal.40​ Las ubicaciones adicionales incluyeron Lancaster, la Universidad de Harvard en Cambridge, Crane Beach en Ipswich y Concord, todas en el estado de Massachusetts.41​42​ La casa de la familia March se construyó desde cero en un terreno en Concord,19​ mientras que el Arnold Arboretum de Harvard se utilizó para rodar una escena ambientada en un parque de París del siglo XIX con Pugh, Chalamet y Streep.43​ Castle Hill en Ipswich también se utilizó para duplicar las escenas europeas.44​ El director de fotografía Yorick Le Saux filmó la película en formato de 35 mm.45​

Gerwig descubrió que estaba embarazada durante la producción y lo mantuvo en privado durante todo el proceso.14​ También impuso la prohibición de los teléfonos móviles en el set durante el rodaje.46​ Después de terminar la fotografía principal el 16 de diciembre de 2018, Gerwig comenzó a editar la película junto con el editor Nick Houy al día siguiente y luego la proyectó para los ejecutivos de Sony Pictures en la ciudad de Nueva York el 10 de marzo de 2019, tres días antes de dar a luz a un hijo.33​

Estreno
El 19 de junio de 2019, Vanity Fair lanzó las primeras imágenes fijas,47​ y el avance oficial se lanzó el 13 de agosto.48​ Mujercitas tuvo su estreno mundial en el Museo de Arte Moderno de la ciudad de Nueva York el 7 de diciembre de 2019,49​ y también se proyectó para inaugurar el Festival Internacional de Cine de Río de Janeiro el 9 de diciembre.50​ Fue estrenada en cines en los Estados Unidos el 25 de diciembre de 2019 por Sony Pictures Releasing.51​52​ Deadline Hollywood informó que Sony gastó aproximadamente $70 millones en la promoción de la película.53​

Mujercitas estaba programada originalmente para un estreno en cines en China el 14 de febrero de 2020, pero esto se descartó debido a la pandemia de COVID-19.54​ La película fue lanzada digitalmente el 10 de marzo de 2020 y en DVD y Blu-ray el 7 de abril.55​56​ En mayo, Variety informó que una vez más estaba destinado a ser lanzado en China en una fecha no especificada después de la pandemia.57​ La película se estrenó en Dinamarca y Japón en junio después de que ambos países reabrieran sus salas de cine luego de cierres pandémicos.58​ Finalmente fue lanzada en China el 25 de agosto.59​

Recepción
Taquilla
Mujercitas recaudó $108.1 millones en los Estados Unidos y Canadá, y $108.5 millones en otros países, para un total mundial de $216.6 millones, contra un presupuesto de producción de $40 millones.60​61​ En abril de 2020, Deadline Hollywood calculó su beneficio neto en 56 millones de dólares.53​

En los Estados Unidos y Canadá, la película se estrenó junto con Spies in Disguise y la expansión de Diamantes en Bruto, y se proyectaba que recaudaría entre 18 y 22 millones de dólares en 3308 salas durante su fin de semana de estreno de cinco días. Ganó $6.4 millones el día de Navidad y $6 millones en su segundo día,62​ y pasó a debutar con $16.8 millones (un total de $29.2 millones durante el período de cinco días de Navidad), terminando en cuarto lugar.63​64​ En su segundo fin de semana, la película recaudó $13.6 millones, terminando tercero.65​ Luego ganó $7.8 millones y $6.4 millones, respectivamente, los siguientes fines de semana.66​67​

En junio de 2020, la película recaudó 495000 dólares y 255000 dólares durante su primer fin de semana en Japón y su segundo fin de semana en Dinamarca, respectivamente.68​ Ese mismo mes, superó los $100 millones en la taquilla internacional luego de lanzamientos en otros 12 mercados.69​ La película ganó $4.7 millones durante los primeros seis días de su lanzamiento en agosto de 2020 en China.70​

Crítica


Saoirse Ronan (arriba) y Florence Pugh fueron nominadas al Premio de la Academia a Mejor Actriz y Mejor Actriz de Reparto, respectivamente.
En el sitio web del agregador de reseñas Rotten Tomatoes, la película tiene una calificación de aprobación del 95% basada en 403 reseñas, con una calificación promedio de 8.54/10. El consenso de los críticos del sitio web dice: "Con un elenco estelar y un recuento inteligente y sensible de su material original clásico, Mujercitas de Greta Gerwig demuestra que algunas historias son verdaderamente atemporales".71​ En Metacritic, tiene una puntuación media ponderada de 91 sobre de 100, sobre la base de 57 críticas, lo que indica "aclamación universal".72​ Las audiencias encuestadas por CinemaScore le dieron a la película una calificación promedio de "A–" en una escala de A+ a F, y los espectadores encuestados por PostTrak le dieron un promedio de cinco de cinco.63​

Escribiendo para IndieWire, Kate Erbland destacó la "ambiciosa narración elíptica" de Gerwig y elogió su dirección por no ser ni "torpe" ni "sermoneadora".73​ Anthony Lane de The New Yorker dijo que "puede que sea la mejor película hecha hasta ahora por una mujer estadounidense".74​ Lindsey Bahr, de Associated Press, también elogió la dirección de Gerwig, considerándola un "logro asombroso" y una "declaración de artista".75​ Al premiar la película con tres y medio de cuatro, Brian Truitt de USA Today elogió la escritura de Gerwig como "magnífica" y dijo que "hace que el tiempo y el lenguaje de Alcott se sientan efervescentemente modernos y auténticamente nostálgicos".76​ Mick LaSalle, que escribe para el San Francisco Chronicle, le dio a la película una crítica mixta, en la que elogió la dirección de Gerwig pero criticó la línea de tiempo no lineal y los personajes "presumidos".77​

Los críticos elogiaron las actuaciones del elenco, con David Rooney de The Hollywood Reporter destacando su "encantador trabajo de conjunto", y Alonso Duralde de TheWrap diciendo que no hubo "un solo momento artificial" de ninguno de los actores.78​79​ Caryn James de BBC Online calificó la actuación de Ronan de "luminosa",80​ y Leah Greenblatt de Entertainment Weekly sugirió que "lleva casi todas las escenas en las que se encuentra".81​ David Sims de The Atlantic destacó la actuación de Pugh, escribiendo que convirtió a su personaje en "una heroína tan rica y convincente como [Ronan]",82​ mientras que Clarisse Loughrey de The Independent declaró que Pugh "se las arregla para robar el show".83​ En su reseña para NPR, Justin Chang elogió las actuaciones de Ronan y Pugh como "increíblemente buenas".84​ Chalamet también fue elogiado por Peter Travers de Rolling Stone y Ann Hornaday de The Washington Post por el "encanto innato y la vulnerabilidad conmovedora", así como la "fisicalidad lúdica" en su actuación.85​86​

Si bien la película en general recibió seis nominaciones al Premio de la Academia, Gerwig no fue nominada a Mejor Director, lo que se consideró un desaire.87​88​ Allison Pearson de The Daily Telegraph calificó esto como un "estándar completamente nuevo de idiotez", y opinó que "menosprecia la experiencia de las mujeres",89​ mientras que Dana Stevens de Slate teorizó que los miembros de la Academia creen que "las mujeres solo pueden tener un pequeño reconocimiento, como un regalo "y que Gerwig" ahora puede ser ignorada con seguridad "ya que anteriormente había sido nominada para Lady Bird.90​ Escribiendo para Los Angeles Times, los psicólogos sociales Devon Proudfoot y Aaron Kay concluyeron que el desaire se debió a una "tendencia psicológica general a ver sin saberlo el trabajo de las mujeres como menos creativo que el de los hombres".91​

Premios y nominaciones
En la 92ª edición de los Premios de la Academia, Mujercitas recibió seis nominaciones, incluyendo Mejor Película, Mejor Actriz (Ronan), Mejor Actriz de Reparto (Pugh) y Mejor Guion Adaptado,92​93​ y ganó el Premio al Mejor Diseño de Vestuario.7​94​ La película también recibió nueve nominaciones en los Premios de la Crítica Cinematográfica 2019, ganando como Mejor Guion Adaptado,95​96​ cinco nominaciones en los Premios BAFTA 2019,97​ y dos en los Premios Globo de Oro 2019.98​ Fue elegida por el American Film Institute como una de las diez mejores películas del año.99​

Referencias
 «'Little Women' (2019)». Box Office Mojo. Consultado el 18 de marzo de 2020.
 https://www.ecartelera.com/películas/mujercitas-2019/
 https://www.elle.com/es/living/ocio-cultura/a19594863/nuevo-mujercitas-reparto/
 «Entrevista a Greta Gerwig».
 «La nueva “Mujercitas”: una mirada real sobre el deseo y las ambiciones del universo femenino».
 ««Pensaba que solo los hombres podían dirigir»: Greta Gerwig, la realizadora de ‘Mujercitas’ explica su pasión por las mujeres que escriben».
 Carras, Christi (9 de febrero de 2020). «The only Oscar 'Little Women' won was for costume design». Los Angeles Times. Archivado desde el original el 10 de febrero de 2020. Consultado el 15 de junio de 2020. Parámetro desconocido |url-status= ignorado (ayuda)
 Kroll, Justin (18 de octubre de 2013). «Sony Sets Up 'Little Women' Adaptation with Olivia Milch Writing (EXCLUSIVE)». Variety. Archivado desde el original el 12 de enero de 2020. Consultado el 8 de enero de 2020. Parámetro desconocido |url-status= ignorado (ayuda)
 Sneider, Jeff (18 de marzo de 2015). «Amy Pascal, Sarah Polley Team on 'Little Women' Remake at Sony». TheWrap. Archivado desde el original el 5 de octubre de 2018. Consultado el 29 de junio de 2018. Parámetro desconocido |url-status= ignorado (ayuda)
 Whipp, Glenn (5 de julio de 2018). «Why it's a perfect time for Greta Gerwig's version of 'Little Women'». Los Angeles Times. Archivado desde el original el 9 de noviembre de 2019. Consultado el 10 de noviembre de 2019. Parámetro desconocido |url-status= ignorado (ayuda)
 Calvario, Liz (6 de agosto de 2016). «'Little Women': Greta Gerwig Will Rewrite Sony's Remake of Louisa May Alcott Novel». IndieWire. Archivado desde el original el 11 de abril de 2019. Consultado el 15 de junio de 2020. Parámetro desconocido |url-status= ignorado (ayuda)
 Kroll, Justin (29 de junio de 2018). «Greta Gerwig Eyes 'Little Women' With Meryl Streep, Emma Watson, Saoirse Ronan, Timothee Chalamet Circling». Variety. Archivado desde el original el 29 de junio de 2018. Consultado el 29 de junio de 2018. Parámetro desconocido |url-status= ignorado (ayuda)
 Salisbury, Mark (17 de enero de 2020). «Greta Gerwig on fighting to make 'Little Women': "I was not on anybody's list to direct this film"». Screendaily.com. Screen International. Archivado desde el original el 29 de mayo de 2020. Consultado el 16 de junio de 2020. Parámetro desconocido |url-status= ignorado (ayuda)
 Kaufman, Amy (24 de diciembre de 2019). «'Little Women' director Greta Gerwig didn't just make a 'women's movie'». Los Angeles Times. Archivado desde el original el 24 de diciembre de 2019. Consultado el 31 de julio de 2020. Parámetro desconocido |url-status= ignorado (ayuda)
 Miller, Jenni (25 de diciembre de 2019). «Greta Gerwig's 'Little Women' is the adaptation every Jo March always needed». NBC News. Archivado desde el original el 27 de diciembre de 2019. Consultado el 1 de agosto de 2020. Parámetro desconocido |url-status= ignorado (ayuda)
 N'Duka, Amanda (29 de junio de 2018). «Greta Gerwig To Helm 'Little Women' At Sony; Meryl Streep, Emma Stone, Timothée Chalamet, Saoirse Ronan In Talks». Deadline Hollywood. Archivado desde el original el 10 de julio de 2018. Consultado el 29 de junio de 2018. Parámetro desconocido |url-status= ignorado (ayuda)
 Canfield, David (17 de octubre de 2019). «Little Women: Timothee Chalamet and Saoirse Ronan talk reunion». Entertainment Weekly. Archivado desde el original el 18 de octubre de 2019. Consultado el 15 de junio de 2020. Parámetro desconocido |url-status= ignorado (ayuda)
 Keegan, Rebecca (5 de enero de 2020). «The Season of Florence Pugh». The Hollywood Reporter. Archivado desde el original el 5 de enero de 2020. Consultado el 15 de junio de 2020. Parámetro desconocido |url-status= ignorado (ayuda)
 Sandberg, Bryn Elise (29 de noviembre de 2019). «Making of 'Little Women': Greta Gerwig Gives Modern Take on 1868 Novel for Big Screen». The Hollywood Reporter. Archivado desde el original el 30 de noviembre de 2019. Consultado el 15 de junio de 2020. Parámetro desconocido |url-status= ignorado (ayuda)
 N'Duka, Amanda (24 de julio de 2018). «'Little Women': 'Sharp Objects' Actress In Talks For The Role Of Beth March». Deadline Hollywood. Archivado desde el original el 27 de julio de 2018. Consultado el 24 de julio de 2018. Parámetro desconocido |url-status= ignorado (ayuda)
 N'Duka, Amanda (2 de agosto de 2018). «Sony's 'Little Women' Adaptation Adds 'Flatliners' Actor James Norton». Deadline Hollywood. Archivado desde el original el 17 de junio de 2019. Consultado el 15 de junio de 2020. Parámetro desconocido |url-status= ignorado (ayuda)
 Galuppo, Mia (14 de agosto de 2018). «Laura Dern in Talks to Join Meryl Streep in 'Little Women'». The Hollywood Reporter. Archivado desde el original el 15 de agosto de 2018. Consultado el 14 de agosto de 2018. Parámetro desconocido |url-status= ignorado (ayuda)
 Kroll, Justin (24 de agosto de 2018). «Emma Watson Joins Greta Gerwig's Adaptation of 'Little Women'». Variety. Archivado desde el original el 24 de agosto de 2018. Consultado el 24 de agosto de 2018. Parámetro desconocido |url-status= ignorado (ayuda)
 N'Duka, Amanda (5 de septiembre de 2018). «Louis Garrel Cast In 'Little Women' Movie At Sony». Deadline Hollywood. Archivado desde el original el 6 de septiembre de 2018. Consultado el 5 de septiembre de 2018. Parámetro desconocido |url-status= ignorado (ayuda)
 N'Duka, Amanda (24 de septiembre de 2018). «'Better Call Saul's Bob Odenkirk Joins Greta Gerwig's 'Little Women' Remake». Deadline Hollywood. Archivado desde el original el 25 de septiembre de 2018. Consultado el 24 de septiembre de 2018. Parámetro desconocido |url-status= ignorado (ayuda)
 N'Duka, Amanda (28 de septiembre de 2018). «Oscar Winner Chris Cooper Boards Greta Gerwig's 'Little Women' Adaptation». Deadline Hollywood. Archivado desde el original el 29 de septiembre de 2018. Consultado el 28 de septiembre de 2018. Parámetro desconocido |url-status= ignorado (ayuda)
 Fleming Jr, Mike (2 de octubre de 2018). «New Regency Co-Finances Two Sony Films: 'Little Women' & 'Girl In The Spider's Web'». Deadline Hollywood. Archivado desde el original el 3 de octubre de 2018. Consultado el 2 de octubre de 2018. Parámetro desconocido |url-status= ignorado (ayuda)
 White, Abbey (23 de diciembre de 2019). «Greta Gerwig on How Her 'Little Women' Adaptation Became "A Movie About Making Movies"». The Hollywood Reporter. Archivado desde el original el 23 de diciembre de 2019. Consultado el 15 de junio de 2020. Parámetro desconocido |url-status= ignorado (ayuda)
 Kaufman, Amy (31 de octubre de 2019). «How Saoirse Ronan and Florence Pugh updated 'Little Women' for modern feminists». Los Angeles Times. Archivado desde el original el 9 de diciembre de 2019. Consultado el 9 de diciembre de 2019. Parámetro desconocido |url-status= ignorado (ayuda)
 Weinberg, Lindsay (2 de noviembre de 2019). «Greta Gerwig Says Meryl Streep Inspired a Powerful Scene in 'Little Women'». The Hollywood Reporter. Archivado desde el original el 4 de noviembre de 2019. Consultado el 15 de junio de 2020. Parámetro desconocido |url-status= ignorado (ayuda)
 Topel, Fred (30 de diciembre de 2019). «[WATCH] Greta Gerwig And Cast Discuss Focusing On 'Little Women' As Adults». Deadline Hollywood. Archivado desde el original el 15 de junio de 2020. Consultado el 15 de junio de 2020. Parámetro desconocido |url-status= ignorado (ayuda)
 Nicolaou, Elena (27 de diciembre de 2019). «Why Greta Gerwig's Little Women Movie Radically Changed the Book's Ending». O, The Oprah Magazine. Archivado desde el original el 15 de junio de 2020. Consultado el 15 de junio de 2020. Parámetro desconocido |url-status= ignorado (ayuda)
 Whipp, Glenn (16 de diciembre de 2019). «Why Greta Gerwig kept her perfect 'Little Women' ending a secret». Los Angeles Times. Archivado desde el original el 20 de diciembre de 2019. Consultado el 15 de junio de 2020. Parámetro desconocido |url-status= ignorado (ayuda)
 Kinosian, Janet (25 de diciembre de 2019). «Designing 'Women' lets Jacqueline Durran get a little freer with Victorian costumes». Los Angeles Times. Archivado desde el original el 25 de diciembre de 2019. Consultado el 15 de junio de 2020. Parámetro desconocido |url-status= ignorado (ayuda)
 Syme, Rachel (13 de enero de 2020). «How Jacqueline Durran, the "Little Women" Costume Designer, Remixes Styles and Eras». The New Yorker. Archivado desde el original el 13 de enero de 2020. Consultado el 4 de agosto de 2020. Parámetro desconocido |url-status= ignorado (ayuda)
 Grobar, Matt (2 de enero de 2020). «Costume Designer Jacqueline Durran Talks 'Little Women' Timelines, '1917' Military Attire & Entering Domain Of Superheroes With 'The Batman'». Deadline Hollywood. Archivado desde el original el 2 de enero de 2020. Consultado el 15 de junio de 2020. Parámetro desconocido |url-status= ignorado (ayuda)
 Gonzales, Erica (9 de febrero de 2020). «Jo and Laurie Shared Clothes on Purpose in Little Women». Harper's Bazaar. Archivado desde el original el 29 de febrero de 2020. Consultado el 15 de junio de 2020. Parámetro desconocido |url-status= ignorado (ayuda)
 Ivie, Devon (18 de diciembre de 2019). «Timothée Chalamet Little Women Outfits and Fashion Interview». Vulture. Archivado desde el original el 18 de diciembre de 2019. Consultado el 4 de agosto de 2020. Parámetro desconocido |url-status= ignorado (ayuda)
 Nechamkin, Sarah (9 de octubre de 2018). «Everything We Know About Greta Gerwig's Little Women Adaptation». The Cut. Archivado desde el original el 14 de agosto de 2019. Consultado el 15 de junio de 2020. Parámetro desconocido |url-status= ignorado (ayuda)
 Schaffstall, Katherine (8 de febrero de 2020). «Oscars: 10 Things to Know About Best Picture Nominee 'Little Women'». The Hollywood Reporter. Archivado desde el original el 15 de junio de 2020. Consultado el 15 de junio de 2020. Parámetro desconocido |url-status= ignorado (ayuda)
 Slane, Kevin (24 de diciembre de 2019). «'Little Women' was filmed entirely in Massachusetts. Here are the historic, picturesque locations from the movie.». Boston.com. Archivado desde el original el 26 de diciembre de 2019. Consultado el 15 de junio de 2020. Parámetro desconocido |url-status= ignorado (ayuda)
 Goldstein, Meredith (21 de diciembre de 2019). «A big stage for Concord in 'Little Women'». The Boston Globe. Archivado desde el original el 22 de diciembre de 2019. Consultado el 15 de junio de 2020. Parámetro desconocido |url-status= ignorado (ayuda)
 Blackwell, Deborah (1 de noviembre de 2018). «Harvard's Arnold Arboretum attracts 'Little Women' with Meryl Streep». The Harvard Gazette. Archivado desde el original el 18 de noviembre de 2018. Consultado el 17 de noviembre de 2018. Parámetro desconocido |url-status= ignorado (ayuda)
 Gale, Natalie (19 de diciembre de 2019). «Inside the Filming of Greta Gerwig's Little Women». Northshore Magazine. Archivado desde el original el 17 de junio de 2020. Consultado el 16 de junio de 2020. Parámetro desconocido |url-status= ignorado (ayuda)
 Thompson, Anne (23 de diciembre de 2019). «Little Women: 10 Decisions That Turned It Into a Modern Movie Classic». IndieWire. Archivado desde el original el 23 de diciembre de 2019. Consultado el 4 de agosto de 2020. Parámetro desconocido |url-status= ignorado (ayuda)
 Ford, Rebecca (3 de enero de 2020). «Why Quentin Tarantino and More Directors Are Banning Cellphones on Set». The Hollywood Reporter. Archivado desde el original el 4 de enero de 2020. Consultado el 15 de junio de 2020. Parámetro desconocido |url-status= ignorado (ayuda)
 Saraiya, Sonia (19 de junio de 2019). «Exclusive First Look: Greta Gerwig and Saoirse Ronan's 'Little Women'». Vanity Fair. Archivado desde el original el 19 de junio de 2019. Consultado el 19 de junio de 2019. Parámetro desconocido |url-status= ignorado (ayuda)
 Beresford, Trilby (13 de agosto de 2019). «Greta Gerwig's 'Little Women' Releases First Trailer». The Hollywood Reporter. Consultado el 13 de agosto de 2019.
 Bell, Keaton (9 de diciembre de 2019). «All the Photos From Inside the New York Premiere of Little Women». Vogue. Archivado desde el original el 10 de diciembre de 2019. Consultado el 15 de junio de 2020. Parámetro desconocido |url-status= ignorado (ayuda)
 Cajueiro, Marcelo (7 de diciembre de 2019). «Rio Fest's Compact Edition Opens Amidst Sectorial Crisis». Variety. Archivado desde el original el 7 de diciembre de 2019. Consultado el 15 de junio de 2020. Parámetro desconocido |url-status= ignorado (ayuda)
 McClintock, Pamela (18 de julio de 2018). «Quentin Tarantino's Manson Movie Shifts Off Sharon Tate Murder Anniversary Date». The Hollywood Reporter. Archivado desde el original el 19 de julio de 2018. Consultado el 18 de julio de 2018. Parámetro desconocido |url-status= ignorado (ayuda)
 Eldredge, Kristy (27 de diciembre de 2019). «Opinion – Men Are Dismissing 'Little Women.' What a Surprise.». The New York Times. Archivado desde el original el 27 de diciembre de 2019. Consultado el 27 de diciembre de 2019. Parámetro desconocido |url-status= ignorado (ayuda)
 D'Alessandro, Anthony (7 de abril de 2019). «'Little Women,' Big Profits: Remake Lands At No. 24 In Deadline's 2019 Most Valuable Blockbuster Tournament». Deadline Hollywood. Archivado desde el original el 7 de abril de 2020. Consultado el 7 de abril de 2019. Parámetro desconocido |url-status= ignorado (ayuda)
 Brzeski, Patrick (3 de febrero de 2020). «China Releases for 'Dolittle,' '1917,' 'Jojo Rabbit' Canceled Amid Coronavirus Crisis». The Hollywood Reporter. Archivado desde el original el 5 de febrero de 2020. Consultado el 15 de junio de 2020. Parámetro desconocido |url-status= ignorado (ayuda)
 Russian, Ale (9 de marzo de 2020). «See Saoirse Ronan, Timothée Chalamet and Emma Watson Get Silly Behind-the-Scenes on Little Women». People. Archivado desde el original el 10 de marzo de 2020. Consultado el 15 de junio de 2020. Parámetro desconocido |url-status= ignorado (ayuda)
 West, Amy (2 de abril de 2020). «Little Women made a Game of Thrones mistake that was just spotted by fans». Digital Spy. Archivado desde el original el 5 de abril de 2020. Consultado el 15 de junio de 2020. Parámetro desconocido |url-status= ignorado (ayuda)
 Davis, Rebecca (13 de mayo de 2020). «'Little Women,' '1917' Likely Among First Films to Hit Reopened Chinese Theaters». Variety. Archivado desde el original el 14 de mayo de 2020. Consultado el 15 de junio de 2020. Parámetro desconocido |url-status= ignorado (ayuda)
 Roxborough, Scott (15 de junio de 2020). «Little Women: Denmark, Japan Give Cinemas Hope for Post-Virus Recovery». The Hollywood Reporter. Archivado desde el original el 16 de junio de 2020. Consultado el 16 de junio de 2020. Parámetro desconocido |url-status= ignorado (ayuda)
 Grater, Tom (6 de agosto de 2020). «'Little Women' Locks China Release For August 25». Deadline Hollywood. Archivado desde el original el 7 de agosto de 2020. Consultado el 7 de agosto de 2020. Parámetro desconocido |url-status= ignorado (ayuda)
 «Little Women (2019)». Box Office Mojo. Amazon. Archivado desde el original el 13 de abril de 2020. Consultado el 31 de agosto de 2020. Parámetro desconocido |url-status= ignorado (ayuda)
 «Little Women (2019)». The Numbers. Archivado desde el original el 3 de enero de 2020. Consultado el 31 de agosto de 2020. Parámetro desconocido |url-status= ignorado (ayuda)
 McClintock, Pamela (25 de diciembre de 2019). «Box Office: 'Star Wars: Rise of Skywalker' Unwraps Huge $32M on Christmas Day». The Hollywood Reporter. Archivado desde el original el 26 de diciembre de 2019. Consultado el 26 de diciembre de 2019. Parámetro desconocido |url-status= ignorado (ayuda)
 D'Alessandro, Anthony (28 de diciembre de 2019). «'Star Wars: Rise Of Skywalker' Chasing 'Last Jedi' With $76M 2nd Weekend; 'Little Women' Not So Tiny With $29M 5-Day». Deadline Hollywood. Archivado desde el original el 29 de diciembre de 2019. Consultado el 29 de diciembre de 2019. Parámetro desconocido |url-status= ignorado (ayuda)
 «Domestic 2019 Weekend 52». Box Office Mojo. Amazon. Archivado desde el original el 30 de diciembre de 2019. Consultado el 3 de enero de 2020. Parámetro desconocido |url-status= ignorado (ayuda)
 D'Alessandro, Anthony (5 de enero de 2020). «'Star Wars: Rise Of Skywalker' Dips To $34M+ Third Weekend; 'Grudge' Doesn't Scream With $11M+ & 'F' CinemaScore». Deadline Hollywood. Archivado desde el original el 5 de enero de 2020. Consultado el 5 de enero de 2020. Parámetro desconocido |url-status= ignorado (ayuda)
 D'Alessandro, Anthony (12 de enero de 2020). «'1917' Strong With $36M+, But 'Like A Boss' & 'Just Mercy' Fighting Over 4th With $10M; Why Kristen Stewart's 'Underwater' Went Kerplunk With $6M+». Deadline Hollywood. Archivado desde el original el 12 de enero de 2020. Consultado el 12 de enero de 2020. Parámetro desconocido |url-status= ignorado (ayuda)
 D'Alessandro, Anthony (19 de enero de 2020). «'Bad Boys For Life' So Great With $100M+ Worldwide; 'Dolittle' Still A Dud With $57M+ Global – Box Office Update». Deadline Hollywood. Archivado desde el original el 20 de enero de 2020. Consultado el 19 de enero de 2020. Parámetro desconocido |url-status= ignorado (ayuda)
 Tartaglione, Nancy (14 de junio de 2020). «Little Women Marches Towards $100M Overseas – International Box Office». Deadline Hollywood. Archivado desde el original el 14 de junio de 2020. Consultado el 15 de junio de 2020. Parámetro desconocido |url-status= ignorado (ayuda)
 Moreau, Jordan (21 de junio de 2020). «'Little Women' Crosses $100 Million at the International Box Office». Variety. Archivado desde el original el 22 de junio de 2020. Consultado el 22 de junio de 2020. Parámetro desconocido |url-status= ignorado (ayuda)
 Tartaglione, Nancy (30 de agosto de 2020). «'Tenet' Triumphs With $53M Worldwide Launch From 40 Offshore Markets & Canada – International Box Office». Deadline Hollywood. Archivado desde el original el 27 de agosto de 2020. Consultado el 30 de agosto de 2020. Parámetro desconocido |url-status= ignorado (ayuda)
 «Little Women (2019)». Rotten Tomatoes. Fandango Media. Archivado desde el original el 18 de diciembre de 2019. Consultado el 19 de agosto de 2020. Parámetro desconocido |url-status= ignorado (ayuda)
 «'Little Women' (2019) Reviews». Metacritic. CBS Interactive. Archivado desde el original el 18 de diciembre de 2019. Consultado el 25 de diciembre de 2019. Parámetro desconocido |url-status= ignorado (ayuda)
 Erbland, Kate (25 de noviembre de 2019). «'Little Women' Review: Greta Gerwig Marries Tradition With Meta Modernity in Stunning Adaptation». IndieWire. Archivado desde el original el 29 de febrero de 2020. Consultado el 29 de febrero de 2020. Parámetro desconocido |url-status= ignorado (ayuda)
 Lane, Anthony (6 de enero de 2020). «Greta Gerwig's "Little Women," Reviewed». The New Yorker. Archivado desde el original el 25 de diciembre de 2019. Consultado el 15 de junio de 2020. Parámetro desconocido |url-status= ignorado (ayuda)
 Bahr, Lindsey (16 de diciembre de 2019). «Review: Greta Gerwig's 'Little Women' is a new classic». Associated Press. Archivado desde el original el 15 de junio de 2020. Consultado el 15 de junio de 2020. Parámetro desconocido |url-status= ignorado (ayuda)
 Truitt, Brian (25 de noviembre de 2019). «Review: Greta Gerwig's all-star 'Little Women' adapts a classic with modern wit, resonance». USA Today. Archivado desde el original el 11 de febrero de 2020. Consultado el 29 de febrero de 2020. Parámetro desconocido |url-status= ignorado (ayuda)
 LaSalle, Mick (19 de diciembre de 2019). «Gerwig's 'Little Women' are snootier than we remember». San Francisco Chronicle. Archivado desde el original el 20 de diciembre de 2019. Consultado el 6 de julio de 2020. Parámetro desconocido |url-status= ignorado (ayuda)
 Rooney, David (25 de noviembre de 2019). «'Little Women': Film Review». The Hollywood Reporter. Archivado desde el original el 26 de noviembre de 2019. Consultado el 15 de junio de 2020. Parámetro desconocido |url-status= ignorado (ayuda)
 Duralde, Alonso (25 de noviembre de 2019). «'Little Women' Film Review: Greta Gerwig's New Spin on a Beloved Tale». TheWrap. Archivado desde el original el 28 de noviembre de 2019. Consultado el 15 de junio de 2020. Parámetro desconocido |url-status= ignorado (ayuda)
 James, Caryn (16 de diciembre de 2019). «Why Little Women is a triumph». BBC Online. Archivado desde el original el 15 de junio de 2020. Consultado el 15 de junio de 2020. Parámetro desconocido |url-status= ignorado (ayuda)
 Greenblatt, Leah (25 de noviembre de 2019). «Little Women review: Greta Gerwig's remake is a warm blanket in a cold world». Entertainment Weekly. Archivado desde el original el 26 de enero de 2020. Consultado el 15 de junio de 2020. Parámetro desconocido |url-status= ignorado (ayuda)
 Sims, David (25 de diciembre de 2019). «Greta Gerwig Captures the Poignancy of 'Little Women'». The Atlantic. Archivado desde el original el 26 de diciembre de 2019. Consultado el 15 de junio de 2020. Parámetro desconocido |url-status= ignorado (ayuda)
 Loughrey, Clarisse (27 de diciembre de 2019). «Little Women review: Greta Gerwig's loving adaptation waltzes with a literary ghost». The Independent. Archivado desde el original el 25 de diciembre de 2019. Consultado el 15 de junio de 2020. Parámetro desconocido |url-status= ignorado (ayuda)
 Chang, Justin (20 de diciembre de 2019). «'Little Women' Again? Greta Gerwig's Adaptation Is Both Faithful And Radical». NPR. Archivado desde el original el 20 de diciembre de 2019. Consultado el 15 de junio de 2020. Parámetro desconocido |url-status= ignorado (ayuda)
 Travers, Peter (23 de diciembre de 2019). «Greta Gerwig Delivers a 'Little Women' for a New Generation». Rolling Stone. Archivado desde el original el 8 de abril de 2020. Consultado el 14 de junio de 2020. Parámetro desconocido |url-status= ignorado (ayuda)
 Hornaday, Ann (17 de diciembre de 2019). «Part Alcott, part Gerwig, 'Little Women' is a very nearly perfect film». The Washington Post. Archivado desde el original el 18 de diciembre de 2019. Consultado el 14 de junio de 2020. Parámetro desconocido |url-status= ignorado (ayuda)
 Aurthur, Kate (4 de febrero de 2020). «Greta Gerwig on 'Little Women's' Oscar Nominations — and That One Big Snub». Variety. Archivado desde el original el 9 de febrero de 2020. Consultado el 24 de marzo de 2020. Parámetro desconocido |url-status= ignorado (ayuda)
 Butler, Bethonie (8 de febrero de 2020). «The biggest female director Oscar snubs of the past decade». The Washington Post. Archivado desde el original el 8 de febrero de 2020. Consultado el 24 de marzo de 2020. Parámetro desconocido |url-status= ignorado (ayuda)
 Pearson, Allison (14 de enero de 2020). «Greta Gerwig's Oscars snub proves Hollywood is still pale, male and stale». The Daily Telegraph. Archivado desde el original el 15 de enero de 2020. Parámetro desconocido |url-status= ignorado (ayuda)
 Stevens, Dana (13 de enero de 2020). «2020 Oscar nominations snub Greta Gerwig for Best Director: Does the Academy think Little Women directed itself?». Slate. Archivado desde el original el 14 de enero de 2020. Consultado el 16 de junio de 2020. Parámetro desconocido |url-status= ignorado (ayuda)
 Proudfoot, Devon; Kay, Aaron (8 de febrero de 2020). «Op-Ed: A scientific reason for Greta Gerwig's Oscar snub: The creativity of women is judged more harshly». Los Angeles Times. Archivado desde el original el 8 de febrero de 2020. Parámetro desconocido |url-status= ignorado (ayuda)
 Nordyke, Kimberly; Konerman, Jennifer; Strause, Jackie; Howard, Annie (13 de enero de 2020). «Oscar Nominations 2020: The Complete List of Nominees». The Hollywood Reporter. Archivado desde el original el 13 de enero de 2020. Consultado el 15 de enero de 2020. Parámetro desconocido |url-status= ignorado (ayuda)
 Wilson, Jordan (13 de enero de 2020). «Oscars: Greta Gerwig's Adaptation Brings 'Little Women' Noms Tally to 14». The Hollywood Reporter. Archivado desde el original el 13 de enero de 2020. Consultado el 13 de enero de 2020. Parámetro desconocido |url-status= ignorado (ayuda)
 N'Duka, Amanda (9 de febrero de 2020). «Jacqueline Durran Nabs Second Career Oscar Award For Costume Design For 'Little Women'». Deadline Hollywood. Archivado desde el original el 10 de febrero de 2020. Consultado el 10 de febrero de 2020. Parámetro desconocido |url-status= ignorado (ayuda)
 Malkin, Marc (8 de diciembre de 2019). «Critics' Choice: 'The Irishman,' 'Once Upon a Time in Hollywood' Lead Movie Nominations». Variety. Archivado desde el original el 9 de diciembre de 2019. Consultado el 8 de diciembre de 2019. Parámetro desconocido |url-status= ignorado (ayuda)
 Ramos, Dino-Ray (12 de enero de 2020). «Critics' Choice Awards: 'Once Upon A Time In Hollywood' Wins Best Picture, Netflix And HBO Among Top Honorees – Full Winners List». Deadline Hollywood. Archivado desde el original el 14 de enero de 2020. Consultado el 15 de enero de 2020. Parámetro desconocido |url-status= ignorado (ayuda)
 Tartaglione, Nancy (7 de enero de 2020). «BAFTA Film Awards Nominations: 'Joker', 'The Irishman', 'Once Upon A Time In Hollywood' Lead – Full List». Deadline Hollywood. Archivado desde el original el 8 de enero de 2020. Consultado el 7 de enero de 2020. Parámetro desconocido |url-status= ignorado (ayuda)
 Nordyke, Kimberly; Konerman, Jennifer; Howard, Annie (9 de diciembre de 2019). «Golden Globes: Full List of Nominations». The Hollywood Reporter. Archivado desde el original el 10 de diciembre de 2019. Consultado el 9 de diciembre de 2019. Parámetro desconocido |url-status= ignorado (ayuda)
 Hipes, Patrick (4 de diciembre de 2019). «AFI Awards Film: 'The Irishman', '1917', 'Little Women' Among Top 10». Deadline Hollywood. Archivado desde el original el 4 de diciembre de 2019. Consultado el 4 de diciembre de 2019. Parámetro desconocido |url-status= ignorado (ayuda)
Enlaces externos
Página web oficial
Mujercitas en Internet Movie Database (en inglés).
Little Women en Metacritic (en inglés).
Little Women en Rotten Tomatoes (en inglés).
Little Women en Box Office Mojo (en inglés)
Control de autoridades	
Proyectos WikimediaWd Datos: Q56881140IdentificadoresWorldCatVIAF: 69155707017022410579LCCN: no2019055758
Categorías: Películas en inglésPelículas de Estados UnidosPelículas de 2019Películas de Columbia PicturesPelículas dramáticasPelículas románticasPelículas de Regency EnterprisesPelículas sobre hermanasPelículas sobre la guerra de SecesiónPelículas dramáticas de Estados UnidosPelículas dramáticas de los años 2010Películas candidatas al premio Óscar a la mejor películaAdaptaciones cinematográficas de Mujercitas
Menú de navegación
No has accedido
Discusión
Contribuciones
Crear una cuenta
Acceder
ArtículoDiscusión
LeerEditarVer historialBuscar
Buscar en Wikipedia
Portada
Portal de la comunidad
Actualidad
Cambios recientes
Páginas nuevas
Página aleatoria
Ayuda
Donaciones
Notificar un error
Herramientas
Lo que enlaza aquí
Cambios en enlazadas
Subir archivo
Páginas especiales
Enlace permanente
Información de la página
Citar esta página
Elemento de Wikidata
Imprimir/exportar
Crear un libro
Descargar como PDF
Versión para imprimir

En otros idiomas
العربية
Deutsch
English
Français
Bahasa Indonesia
Português
Русский
Tiếng Việt
中文	!"#$%&%()(=()/=)=/)?/=?/=ÑÑ))=Ñ/)99
21 más
Editar enlaces
Esta página se editó por última vez el °22 mar 2021 a las 18:14.
El texto está disponible bajo la Lic	|encia Creative Commons Atribución Compartir Igual 3.0; pueden aplicarse cláusulas adicionales. Al usar este sitio, usted acepta nuestros términos de uso y nuestra política de privacidad.
Wikipedia® es una marca registrada de la Fundación Wikimedia, Inc., una organización sin ánimo de lucro.
	"#import string as st
import pmli as pm 
import sys
import operator as op

class crearAyudante():
    def __init__(self):
        self.MAX = 27
        self.lista1 = [i for i in st.ascii_lowercase]
        self.lista1.insert(14,"ñ")
        
        self.dicc = [pm.crearPMLI(i) for i in self.lista1]

    def esPalabraValida(self, s):
        assert(type(s) == str)
        for letra in s:
            if not (letra in self.lista1):
                return False
        return True
        
    def distance(self, s1, s2):
        d=dict()
        for i in range(len(s1)+1):
           d[i]=dict()
           d[i][0]=i
        for i in range(len(s2)+1):
           d[0][i] = i
        for i in range(1, len(s1)+1):
            for j in range(1, len(s2)+1):
                d[i][j] = min(d[i][j-1]+1, d[i-1][j]+1, d[i-1][j-1]+(not s1[i-1] == s2[j-1]))
        return d[len(s1)][len(s2)]    

    def cargarDiccionario(self, fname):
        with open(fname) as archivo:
            for linea in archivo:
                linea = linea[:-1].lower()
                # print(linea)
                if self.esPalabraValida(linea) is True:
                    for i in range(self.MAX):
                        if self.dicc[i].l == linea[0] and  not linea.isspace():
                            #print(i, self.dicc[i].l, len(self.dicc))
                            #resolver las lineas en blamco
                            self.dicc[i].agregarPalabra(linea)
                            break              
                else:
                    return

    def borrarPalabra(self, p):
        assert(self.esPalabraValida(p) is True)
        for i in range(self.MAX):
            if self.dicc[i].l == p[0] and self.dicc[i].eliminarPalabra(p) is True:
                return           

    def buscarPalabra(self, p):
        assert(self.esPalabraValida(p) is True)
        for i in range(self.MAX):
            if self.dicc[i].l == p[0] and self.dicc[i].buscarPalabra(p) is True:
                return True

    def mostrar(self):
        for i in range(self.MAX):
            print(self.dicc[i].mostrar(), "\n")

    def corregirTexto(self, finput):
        with open(finput) as archivo:
            for linea in archivo:
                lista = [i for i in linea.split()
                         if (self.esPalabraValida(i) is True) and (self.buscarPalabra(i) is not True)]
                
                for palabra in lista:
                    # print(palabra)
                    dlev = {}
                    for i in range(self.MAX):
                        aux = self.dicc[i].pal.tabla
                        dlev1 = {}
                        for j in range(len(aux)):
                            dlev1 = {}
                            if aux[j] is not None:
                                if len(dlev) < 4:
                                    dlev[aux[j]] = self.distance(aux[j], palabra)

                                else:    
                                    # print(dlev, palabra, dlev1,"\n")
                                    pal_dicc = (aux[j], self.distance(aux[j], palabra))

                                    dlev = dict(sorted([i for i in dlev.items()], key=lambda x: x[1]))
                                    for i in dlev.items():

                                        if i[1] <= pal_dicc[1] and len(dlev1) < 4:
                                            dlev1[i[0]] = i[1]  

                                        elif i[1] > pal_dicc[1] and len(dlev1) < 4:       
                                            dlev1[pal_dicc[0]] = pal_dicc[1]    
                                            pal_dicc = i

                                    dlev = dlev1
                    
                    with open("foutput.txt", "a") as archivo:
                        salida = dlev
                        salida = [clave for clave in salida.keys()]
                        salida.insert(0, palabra)
                        salida = ",".join(salida)
                        archivo.write(salida)
                        archivo.write("\n")



                                
                        
                        

                
                # for i in d.items():
                # print("*", lista,"\n")


helpo = crearAyudante()
helpo.cargarDiccionario(sys.argv[1])

# print(helpo.mostrar())

# elpo.borrarPalabra(sys.argv[2])
# helpo.buscarPalabra(sys.argv[3])
helpo.corregirTexto(sys.argv[4])
helpo.mostrar()




# with open(sys.argv[4]) as archivo:
#    for linea in archivo:
#        lista = linea.split()
#        print(lista)
Series Editors
Gerhard Goos
Universitat Karlsruhe
Postfach 69 80
Vincenz-Priessnitz-StraBe 1
D-76131 Karlsruhe, Germany
Juris Hartmanis
Cornell University
Department of Computer Science
4130 Upson Hall
Ithaca, NY 14853, USA
Author
Gerhard Reinelt
Institut fur Angewandte Mathematik, Universitat Heidelberg
Im Neuenheimer Feld 294, D-69120 Heidelberg, Germany
CR Subject Classification (1991): G.2,1.3.5, G.4,1.2.8, J.l-2
ISBN 3-540-58334-3 Springer-Verlag Berlin Heidelberg New York
ISBN 0-387-58334-3 Springer-Verlag New York Berlin Heidelberg
CIP data applied for
This work is subject to copyright. All rights are reserved, whether the whole or part
of the material is concerned, specifically the rights of translation, reprinting, re-use
of illustrations, recitation, broadcasting, reproduction on microfilms or in any other
way, and storage in data banks. Duplication of this publication or parts thereof is
permitted only under the provisions of the German Copyright Law of September 9,
1965, in its current version, and permission for use must always be obtained from
Springer-Verlag. Violations are liable for prosecution under the German Copyright
Law.
© Springer-Verlag Berlin Heidelberg 1994
Printed in Germany
Typesetting: Camera-ready by author
SPIN: 10475419
45/3140-543210 - Printed on acid-free paperPreface to the Online Edition
Still today I am receiving requests for reprints of the book, but unfortunately it
is out of print. Therefore, since the book still seems to receive some attention, I pro-
posed to Springer Verlag to provide a free online edition. I am very happy that Springer
agreed. Except for the correction of some typographical errors, the online edition is just
a copy of the printed version, no updates have been made. In particular, Table 13.1
gives the status of TSPLIB at the time of publishing the book. For accessing TSPLIB the
link http://www.iwr.uni-heidelberg.de/iwr/comopt/software/TSPLIB95/ should be
used instead of following the procedure described in Chapter 13.
Heidelberg, January 2001
Gerhard ReineltPreface
More than fifteen years ago, I was faced with the following problem in an assignment
for a class in computer science. A brewery had to deliver beer to five stores, and the task
was to write a computer program for determining the shortest route for the truck driver to
visit all stores and return to the brewery. All my attemps to find a reasonable algorithm
failed, I could not help enumerating all possible routes and then select the best one.
Frustrated at that point, I learnt later that there was no fast algorithm for solving this
problem. Moreover, I found that this problem was well known as the traveling salesman
problem and that there existed a host of published work on finding solutions. Though
no efficient algorithm was developed, there was a tremendous progress in designing fast
approximate solutions and even in solving ever larger problem instances to optimality. I
started some work on the traveling salesman problem several years ago, first just writing
demos for student classes, but then trying to find good and better solutions more effec-
tively. I experienced the fascination of problem solving that, I think, everyone studying
the traveling salesman problem will experience. In addition, I found that the problem has
relevance in practice and that there is need for fast algorithms.
The present monograph documents my experiments with algorithms for finding good
approximate solutions to practical traveling salesman problems. The work presented here
profited from discussions and meetings with several people, among them Thomas Christof,
Meinrad Funke, Martin Grötschel, Michael Jünger, Manfred Padberg, Giovanni Rinaldi,
and Stefan Thienel, not naming dozens of further international researchers.
It is the aim of this text to serve as a guide for practitioners, but also to show that
the work on the traveling salesman problem is not at all finished. The TSP will stimulate
further efforts and continue to serve as the classical benchmark problem for algorithmic
ideas.
Heidelberg, June 1994
Gerhard ReineltContents
1 Introduction
2 Basic Concepts
2.1
2.2
2.3
2.4
2.5
. . . . . . . . . . . . . . . . . . . . . . . . . . 1
. . . . . . . . . . . . . . . . . . . . . . . . . 4
Graph Theory . . . . . . . .
Complexity Theory . . . . . .
Linear and Integer Programming
Data Structures
. . . . . . .
Some Fundamental Algorithms .
.
.
.
.
. 4
7
12
14
25
. . . . . . . . . . . . . . . 31
Some Related Problems . . . . . . . . . . . . . . . . . . . .
Practical Applications of the TSP . . . . . . . . . . . . . . . .
The Test Problem Instances . . . . . . . . . . . . . . . . . . 31
35
40
3 Related Problems and Applications
3.1
3.2
3.3
4 Geometric Concepts
4.1
4.2
4.3
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
42
Voronoi Diagrams . . . . . . . . . . . . . . . . . . . . . . .
Delaunay Triangulations . . . . . . . . . . . . . . . . . . . .
Convex Hulls . . . . . . . . . . . . . . . . . . . . . . . . . 42
48
54
. . . . . . . . . . . . . . . . . . . . . . . . . 64
Nearest Neighbors
. . . . . . . . . . . . . . . . . . . . . .
Candidates Based on the Delaunay Graph . . . . . . . . . . . .
Other Candidate Sets . . . . . . . . . . . . . . . . . . . . . 64
67
70
6 Construction Heuristics
6.1
6.2
6.3
6.4
6.5
.
.
.
.
.
. . . . . . . . . . . . . . . . . . . . . .
5 Candidate Sets
5.1
5.2
5.3
.
.
.
.
.
. . . . . . . . . . . . . . . . . . . . .
Nearest Neighbor Heuristics . . . . .
Insertion Heuristics . . . . . . . . .
Heuristics Using Spanning Trees . . .
Savings Methods and Greedy Algorithm
Comparison of Construction Heuristics
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
73
73
82
89
94
97VIII
7 Improving Solutions
7.1
7.2
7.3
7.4
7.5
7.6
. . . . . . . . . . . . . . . . . . . . . . . 100
Node and Edge Insertion . . . . . .
2-Opt Exchange
. . . . . . . . .
Crossing Elimination . . . . . . .
The 3-Opt Heuristic and Variants . .
Lin-Kernighan Type Heuristics . . .
Comparison of Improvement Heuristics
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
8 Fast Heuristics for Large Geometric Problems
8.1
8.2
8.3
8.4
Space Filling Curves
. .
Strip Heuristics . . . . .
Partial Representation . .
Decomposition Approaches
9 Further Heuristic Approaches
9.1
9.2
9.3
9.4
Simulated Annealing
Evolutionary Strategies
Tabu Search . . . .
Neural Networks . .
10 Lower Bounds
10.1
10.2
10.3
10.4
. .
and
. .
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
100
105
115
117
123
130
. . . . . . . . . 133
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
133
137
139
145
. . . . . . . . . . . . . . . . . . 153
. . . .
Genetic
. . . .
. . . .
. . . . . . . . .
Algorithms
. . .
. . . . . . . . .
. . . . . . . . .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
153
157
158
159
. . . . . . . . . . . . . . . . . . . . . . . . . . 161
Bounds from Linear Programming
. .
Simple Lower Bounds . . . . . . . .
Lagrangean Relaxation . . . . . . .
Comparison of Lower Bounds . . . . .
11 A Case Study: TSPs in PCB Production
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
161
163
172
184
. . . . . . . . . . . . 187
11.1 Drilling of Printed Circuit Boards . . . . . . . . . . . . . . . . 187
11.2 Plotting of PCB Production Masks . . . . . . . . . . . . . . . 193
12 Practical TSP Solving
. . . . . . . . . . . . . . . . . . . . . . 200
12.1 Determining Optimal Solutions . . . . . . . . . . . . . . . . . 200
12.2 An Implementation Concept . . . . . . . . . . . . . . . . . . 204
12.3 Interdependence of Algorithms . . . . . . . . . . . . . . . . . 207
Appendix: TSPLIB
References
Index
. . . . . . . . . . . . . . . . . . . . . . . 211
. . . . . . . . . . . . . . . . . . . . . . . . . . . 214
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 222Chapter 1
Introduction
The most prominent member of the rich set of combinatorial optimization problems is
undoubtly the traveling salesman problem (TSP), the task of finding a route through
a given set of cities with shortest possible length. It is one of the few mathematical
problems that frequently appear in the popular scientific press (Cipra (1993)) or even
in newspapers (Kolata (1991)). It has a long history, dating back to the 19th century
(Hoffman & Wolfe (1985)).
The study of this problem has attracted many researchers from different fields, e.g.,
Mathematics, Operations Research, Physics, Biology, or Artificial Intelligence, and there
is a vast amount of literature on it. This is due to the fact that, although it is easily
formulated, it exhibits all aspects of combinatorial optimization and has served and
continues to serve as the benchmark problem for new algorithmic ideas like simulated
annealing, tabu search, neural networks, simulated tunneling or evolutionary methods
(to name only a few of them).
On the other hand, the TSP is interesting not only from a theoretical point of view.
Many practical applications can be modeled as a traveling salesman problem or as
variants of it. Therefore, there is a tremendous need for algorithms. The number of
cities in practical applications ranges from some dozens up to even millions (in VLSI
design). Due to this manifold area of applications there also has to be a broad collection
of algorithms to treat the various special cases.
In the last two decades an enormous progress has been made with respect to solving
traveling salesman problems to optimality which, of course, is the ultimate goal of every
researcher. Landmarks in the search for optimal solutions are the solution of a 48-city
problem (Dantzig, Fulkerson & Johnson (1954)), a 120-city problem (Grötschel
(1980)), a 318-city problem (Crowder & Padberg (1980)), a 532-city problem (Pad-
berg & Rinaldi (1987)), a 666-city problem (Grötschel & Holland (1991)), a
2392-city problem (Padberg & Rinaldi (1991)), a 3038-city problem (Applegate,
Bixby, Chvàtal & Cook (1991)), and of a 4461-city problem (Applegate, Bixby,
Chvàtal & Cook (1993)). This progress is only partly due to the increasing hardware
power of computers. Above all, it was made possible by the development of mathemat-
ical theory (in particular polyhedral combinatorics) and of efficient algorithms. But,
despite of these achievements, the traveling salesman problem is far from being solved.
Many aspects of the problem still need to be considered and questions are still left to
be answered satisfactorily.
First, the algorithms that are able to solve the largest (with respect to the number of
cities) problems to optimality are not stable in the following sense: solution times vary
strongly for different problems with the same number of cities and there is no function
G. Reinelt: The Traveling Salesman, LNCS 840, pp. 1-3, 1994.
 Springer-Verlag Berlin Heidelberg 19942
Chapter 1. Introduction
depending on the number of cities that only gives a slight idea of the time necessary
to solve a particular problem. Already problems with some hundred nodes can be very
hard for these algorithms and require hours of CPU time on supercomputers. And,
there is a lot of theoretical knowledge that has not yet gone into implementations.
Second, problems arising in practice may have a number of cities that is far beyond
the capabilities of any exact algorithm available today. There are very good heuristics
yielding solutions which are only a few percent above optimality. However, they still
can be improved with respect to running time or quality of the computed solutions.
Third, requirements in the production environment may make many algorithms or
heuristics unsuitable. Possible reasons are that not enough real time or CPU time is
available to apply certain algorithms, that the problem instances are simply too large,
or that not enough real time or man power is at hand to code a method one would like
to apply.
These arguments visualize the potential that is still inherent in the traveling salesman
problem.
The present monograph is meant to be a contribution to practical traveling salesman
problem solving. Main emphasis will be laid on the question of how to find good or
acceptable tours for large problem instances in short time. We will discuss variants
and extensions of known approaches and discuss some new ideas that have proved to
be useful. Furthermore we will indicate some directions of future research. Literature
will be reviewed to some extent, but a complete coverage of the knowledge about the
TSP is beyond the purpose and available space of this tract. For an introduction we
recommend the book Lawler, Lenstra, Rinnooy Kan & Shmoys (1985) and the
survey article Jünger, Reinelt & Rinaldi (1994).
Nevertheless, even without consulting further references the present text is meant to
be a guide for readers who are concerned with applications of the TSP and aims at
providing sufficient information for their successful treatment.
We give a short survey of the topics that will be addressed. Chapter 2 covers basic con-
cepts that we need throughout this monograph. This chapter contains an introduction
to complexity theory and describes some fundamental data structures and algorithms.
The many possible applications of the TSP are indicated in Chapter 3. Of particular
importance are Euclidean instances. To exploit the underlying geometric structure we
use Voronoi diagrams and convex hulls which are discussed in Chapter 4. A basic ingre-
dient of fast heuristics will be a limitation of the scope of search for good tours. This is
accomplished by candidate sets which restrict algorithms to certain subsets of promising
connections. The construction of reasonable candidate sets is the topic of Chapter 5.
Construction heuristics to find starting tours are given in Chapter 6. Emphasis is laid
on improving standard approaches and making them applicable for larger problems.
Many of these heuristics are also useful in later chapters. Chapter 7 is concerned with
the improvement of given tours. It is shown how data structures can be successfully
employed to come up with very efficient implementations. An important issue is cov-
ered in Chapter 8: the treatment of very large problem instances in short time. Several
types of approaches are presented. A short survey of recent heuristic methods is con-
tained in Chapter 9. Lower bounds are the topic of Chapter 10. Besides variants of
known approaches we comment on heuristics for computing Lagrange multipliers. The
algorithms described in this text have been successfully applied in an industry project.Chapter 1. Introduction
3
We discuss this project in depth in Chapter 11. Chapter 12 addresses the question of
computing optimal solutions as well as solutions with quality guarantee and discusses
some lines of future research. In particular, a proposal for a hardware and software
setup for the effective treatment of traveling salesman problems in practice is presented.
The appendix gives information of how getting access to TSPLIB, a publicly available
collection of TSP instances, and lists the current status of these problem instances.
In this monograph we will not describe the approaches down to the implementation
level. But, we will give enough information to facilitate implementations and point
out possible problems. Algorithms are presented on a level that is sufficient for their
understanding and for guiding practical realizations.
Extensive room is spent for computational experiments. Implementations were done
carefully, however, due to limited time for coding the software, not always the absolutely
fastest algorithm could be used. The main point is the discussion of various algorithmic
ideas and their comparison using reasonable implementations. We have not restricted
ourselves to only tell “success stories”, but we rather point out that sometimes even
elaborate approaches fail in practice.
Summarizing, it is the aim of this monograph to give a comprehensive survey on heuristic
approaches to traveling salesman problem solving and to motivate the development and
implementation of further and possibly better algorithms.Chapter 2
Basic Concepts
The purpose of this chapter is to survey some basic knowledge from computer science
and mathematics that we need in this monograph. It is intended to provide the reader
with some fundamental concepts and results. For a more detailed representation of the
various subjects we shall refer to appropriate textbooks.
2.1 Graph Theory
Many combinatorial optimization problems can be formulated as problems in graphs.
We will therefore review some basic definitions from graph theory.
An undirected graph (or graph) G = (V, E) consists of a finite set of nodes V and a
finite set of edges E. Each edge e has two endnodes u, v and is denoted by e = uv or
e = {u, v}. We call such a graph undirected because we do not distinguish between the
edges uv and vu. However, we will sometimes speak about head and tail of an edge.
If e = uv then e is incident to v and to u. The set of edges incident to a node v is
denoted by δ(v). The number |δ(v)| is the degree of v.
A graph G  = (V  , E  ) is called a subgraph of G = (V, E) if V  ⊆ V and E  ⊆ E. For an
edge set E ⊆ E we define V (E) := {u, v ∈ V |uv ∈ E}. Conversely, for a node set V ⊆ V
we define E(V ) := {uv ∈ E | u ∈ V and v ∈ V }. We call the subgraph G  = (V (E), E)
edge induced (by E) and the subgraph G  = (V , E(V )) node induced (by V ).
A graph G = (V, E) is said to be complete if for all u, v ∈ V it contains edge uv. We
denote the complete graph on n nodes by K n = (V n , E n ) and assume unless otherwise
stated that V n = {1, 2, . . . , n}.
Two graphs G  = (V  , E  ) and G  = (V  , E  ) are isomorphic if there exists a bijective
mapping f : V  → V  such that uv ∈ E  if and only if f (u)f (v) ∈ E  , e.g., the complete
graph K n is unique up to isomorphism.
A graph G = (V, E) is called bipartite if its node set V can be partitioned into two
nonempty disjoint sets V 1 , V 2 with V 1 ∪V 2 = V such that no two nodes in V 1 and no two
nodes in V 2 are connected by an edge. If |V 1 | = m, |V 2 | = n and E = {ij | i ∈ V 1 , j ∈ V 2 }
then we call G the complete bipartite graph K m,n .
An edge set P = {v 1 v 2 , v 2 v 3 , . . . , v k−1 v k } is called a walk or more precisely a [v 1 , v k ]–
walk. If v i  = v j for all i  = j then P is called path or [v 1 , v k ]–path. The length of a
walk or path is the number of its edges and is denoted by |P |. If in a walk v 1 = v k we
speak of a closed walk.
A set of edges C = {v 1 v 2 , v 2 v 3 , . . . , v k−1 v k , v k v 1 } with v i  = v j for i  = j is called a cycle
(or k-cycle). An edge v i v j , 1 ≤ i  = j ≤ k, not in C is called chord of C. The length of
G. Reinelt: The Traveling Salesman, LNCS 840, pp. 4-30, 1994.
 Springer-Verlag Berlin Heidelberg 19942.1. Graph Theory
5
a cycle C is denoted by |C|. For convenience we shall sometimes abbreviate the cycle
{v 1 v 2 , v 2 v 3 , . . . , v k v 1 } by (v 1 , v 2 , . . . v k ) and also say that a graph G is a cycle if its edge
set forms a cycle. A graph or edge set is called acyclic if it contains no cycle. An
acyclic graph is also called forest.
A graph G = (V, E) is said to be connected if it contains for every pair of nodes a
path connecting them; otherwise G is called disconnected. A spanning tree is a
connected forest containing all nodes of the graph.
A nonempty edge set F ⊆ E is said to be a cut of the graph G = (V, E) if V can be
partitioned into two nonempty disjoint subsets V 1 , V 2 with V 1 ∪ V 2 = V such that the
following holds: F = {uv ∈ E | u ∈ V 1 , v ∈ V 2 }. Equivalently, F is a cut if there exists
a node set W ⊆ V such that F = δ(W ).
Sometimes it is useful to associate a direction with the edges of a graph. A directed
graph (or digraph) D = (V, A) consists of a finite set of nodes V and a set of arcs
A ⊆ V × V \ {(v, v) | v ∈ V } (we do not consider loops or multiple arcs). If e = (u, v) is
an arc of D with endnodes u and v then we call u its tail and v its head. The arc e is
said to be directed from u to v, incident from u and incident to v. The number
of arcs incident to a node v is called the indegree of v and the number of arcs incident
from v is called the outdegree of v. The degree of v is the sum of its indegree and
outdegree. For a node v the sets of arcs incident from v, incident to v, and incident from
or to v are denoted by δ + (v), δ − (v), and δ(v), respectively. Two nodes are adjacent if
there is an arc connecting them.
Most of the definitions for undirected graphs carry over in a straightforward way to
directed graphs. For example, diwalks, dipaths and dicycles are defined analogously
to walks, paths, and cycles with the additional requirement that the arcs are directed
in the same direction.
A digraph D = (V, A) is said to be complete if for all u, v ∈ V it contains both arcs
(u, v) and (v, u). We denote the complete digraph on n nodes by D n = (V n , A n ).
For each digraph D = (V, A) we can construct its underlying graph G = (V, E) by
setting E = {uv | u and v are adjacent in D}.
A digraph D = (V, A) is called connected (disconnected) if its underlying graph is
connected (disconnected). D is called diconnected if for each pair u, v of its nodes
there are a [u, v]– and a [v, u]–dipath in D. A node v ∈ V is called articulation node
or cutnode of a digraph (graph) if the removal of v and all arcs (edges) having v as an
endnode disconnects the digraph (graph). A connected digraph (graph) is said to be
2-connected if it contains no articulation node.
To avoid degenerate situations we assume that, unless otherwise noted, all graphs and
digraphs contain at least one edge, respectively arc.
A walk (diwalk) that traverses every edge (arc) of a graph (digraph) exactly once is
called Eulerian trail (Eulerian ditrail). If such a walk (diwalk) is closed we speak of
a Eulerian tour. A graph (digraph) is Eulerian if its edge (arc) set can be traversed
by a Eulerian tour.
A cycle (dicycle) of length n in a graph (digraph) on n nodes is called Hamiltonian
cycle (Hamiltonian dicycle) or Hamiltonian tour. A path (dipath) of length n is
called Hamiltonian path (Hamiltonian dipath). A graph (digraph) containing a
Hamiltonian tour is called Hamiltonian.6
Chapter 2. Basic Concepts
Often we have to deal with graphs where a rational number (edge weight) is associated
with each edge. We call a function c : E → Q (where Q denotes the set of rational
numbers) a weight function defining a weight c(e) (or c e , or c uv ) for every edge e =
uv ∈ E. (In the context of practical computations it makes no sense to admit arbitrary
real-valued functions since only rational numbers are representable on a computer.) The
weight of a set of edges F ⊆ E is defined as

c uv .
c(F ) :=
uv∈F
The weight of a tour is usually called its length, a tour of smallest weight is called
shortest tour. The problem in the focus of this monograph is the so-called (symmetric)
traveling salesman problem.
(Symmetric) Traveling Salesman Problem
Given the complete graph K n with edge weights c uv find a shortest Hamiltonian tour
in K n .
A symmetric TSP is said to satisfy the triangle inequality, if c uv ≤ c uw + c wv for
all distinct nodes u, v, w ∈ V . Of particular interest are metric traveling salesman
problems. These are problems where the nodes correspond to points in some space and
where the edge weights are given by evaluating some metric distance between corre-
sponding points. For example, a Euclidean TSP is defined by a set of points in the
plane. The corresponding graph contains a node for every point and edge weights are
given by the Euclidean distance of the points associated with the end nodes.
We list some problems on graphs related to the traveling salesman problem which will
be referred to at some places.
Asymmetric Traveling Salesman Problem
Given the complete digraph D n with arc weights c uv find a shortest Hamiltonian tour
in D n .
Chinese Postman Problem
Given a graph G = (V, E) with edge weights c uv for uv ∈ E find a shortest closed walk
in G containing all edges at least once.
Hamiltonian Cycle Problem
Given a graph G = (V, E) decide if G contains a Hamiltonian cycle.
Eulerian Tour Problem
Given a graph G = (V, E) decide if G is Eulerian.
Though quite similar, these problems are very different with respect to their hardness.
It is a topic of the next section to give a short introduction into complexity theory and
its impact on the traveling salesman problem.2.2. Complexity Theory
7
2.2 Complexity Theory
When dealing with combinatorial problems or algorithms one is often interested in
comparing problems with respect to their hardness and algorithms with respect to their
efficiency. Often it is intuitively clear that some problem is more difficult to solve than
another problem and that one algorithm takes longer than another algorithm. The work
of Cook (Cook (1971)) laid the foundation for putting these questions into an exact
mathematical framework. Based on the notion of deterministic and nondeterministic
Turing machines it makes the classification of problems as “hard” or “easy” possible and
allows the measurement of efficiency of algorithms. Although being only a theoretical
model this concept had a great impact on the design and the analysis of algorithms.
For our purposes it is sufficient to introduce the concepts of complexity theory in a more
informal manner. If we omit certain subtleties we can take a real-world computer as our
computational model and think of an algorithm as a procedure written in some high-
level programming language. For a thorough study of complexity issues we recommend
Garey & Johnson (1979).
For reasons of exactness we distinguish in this section between “problems” and “in-
stances of problems”. A problem or problem class is a question defined on several
formal parameters, e.g., determine whether a graph contains a Hamiltonian cycle or
compute a shortest Hamiltonian tour in a weighted graph. If we associate concrete
values to these formal parameters we create a particular instance of the problem. A
particular graph defines an instance of the Hamiltonian cycle problem and a particular
weight function c : E n → Q gives an instance of the traveling salesman problem for the
complete graph K n = (V n , E n ).
These two examples show in addition that we have to distinguish between two types
of problems. One type is the so-called decision problem which requires a “yes” or
“no” answer and the other type is the optimization problem demanding to exhibit a
solution which optimizes some objective function. We first give the basic terminology
which has been defined for decision problems and then show how optimization problems
can also be handled within this concept.
The performance of an algorithm has to be measured in some way depending on the
“sizes” of the problem instances to be solved. Therefore we associate with each instance
I of a certain problem class S a size or encoding length l(I) which is defined as
the number of bits required to represent the actual parameters in the usual binary
encoding scheme. If A is an algorithm for the solution of problem S then we define
its running time (for instance I) as the number of elementary operations (addition,
multiplication, etc.) which have to be executed on a computer to solve instance I. The
time complexity or running time of an algorithm A for a problem class S is then
defined as a function t A : N → N giving for each natural number n the number t A (n)
of elementary operations that the algorithm has to execute at most to solve an instance
of size n.
Note, that we assume that arithmetic operations are executed in constant time, i.e.,
independent of the size of the numbers involved. This is not correct in general, but is
feasible in our context.
Of course, in the usual case we will not be able to derive an explicit formula for evaluating
t A (n). On the other hand, we are not interested in concrete values of t A but rather in8
Chapter 2. Basic Concepts
the rate of growth of t A with increasing n. If it is not possible to give the exact rate
of growth we are interested in lower and upper bounds for this rate. In the case of a
problem we are also interested in bounds for the running time of algorithms that are
able to solve the problem. We introduce some notations to express knowledge about
the rate of growth or bounds on this rate.
Definition 2.1 Let f : N → N and g : N → N be given.
(i) We say that f is O(g) if there exist positive constants c and n 0 such that 0 ≤
f (n) ≤ c · g(n) for all n ≥ n 0 .
(ii) We say that f is Ω(g) if there exist positive constants c and n 0 such that 0 ≤
c · g(n) ≤ f (n) for all n ≥ n 0 .
(iii) We say that f is Θ(g) if there exist positive constants c 1 , c 2 , and n 0 such that
0 ≤ c 1 · g(n) ≤ f (n) ≤ c 2 · g(n) for all n ≥ n 0 .
The three notations define asymptotic upper, lower, and tight bounds, respectively, on
the rate of growth of f . An alternate definition of an asymptotic lower bound is obtained
by replacing “for all n ≥ n 0 ” in 2.1 (iii) by “for infinitely many n”. Asymptotic upper
bounds are of practical interest since they give a worst case running time of an
algorithm. It is usually harder to derive nontrivial asymptotic lower bounds, but we
will occasionally be able to give such bounds.
We also use the Ω- and Θ-notation for problems. With the first notation we indicate
lower bounds on the running time of any algorithm that solves the problem, with the
second notation we indicate that an algorithm with best possible time complexity exists
to solve the problem.
An algorithm A is said to have polynomial time complexity if there exists a polyno-
mial p such that t A (n) = O(p(n)). All other algorithms are said to be of exponential
time complexity (although there are superpolynomial functions not being exponen-
tial). Edmonds (1965) was the first to emphasize the difference between polynomial and
nonpolynomial algorithms. It is now commonly accepted that only algorithms having a
polynomial worst case time complexity should be termed efficient algorithms.
We denote by P the class of decision problems which can be solved by polynomial time
algorithms.
The Eulerian tour problem can easily be solved in polynomial time. Using a result from
graph theory the following algorithm tests whether a connected graph G = (V, E) is
Eulerian.
procedure eulerian(G)
(1) For every v ∈ V compute its degree |δ(v)|.
(2) If all node degrees are even then the graph is Eulerian. If the degree of at least
one node is odd then the graph is not Eulerian.
end of eulerian2.2. Complexity Theory
9
This algorithm runs in time Θ(n + m). Surprisingly, also the Chinese postman problem
can be solved in polynomial time (Edmonds & Johnson (1973)).
However, many problems (in fact, most of the interesting problems in combinatorial
optimization) can up to date not be solved (and probably are not solvable) by polynomial
time algorithms. From a theoretical viewpoint they could be solved in the following way.
If the answer to an instance I (of a decision problem) is “yes”, then in a first step some
string s whose length is polynomial in the input size is guessed nondeterministically.
In a second step it is verified that s proves that the problem has a “yes” answer. The
verification step is performed (deterministically) in time polynomial both in the length
of s and in the size of I. If the answer to I is “no” then there exists no such string
and the algorithm is assumed to run forever. E.g., in the Hamiltonian cycle problem
the string s could be the encoding of a Hamiltonian cycle (if the graph contains such
a cycle); the length of s is polynomial in the input length, and it can be easily verified
whether s is indeed the encoding of a Hamiltonian cycle.
Obviously this procedure cannot be realized in practice. The formal model enabling
such computations is the so-called nondeterministic Turing machine. For our pur-
poses we can think of the instruction set of an ordinary computer enhanced by the
instruction “Investigate the following two branches in parallel”. The time complexity of
a nondeterministic algorithm is the maximal number of elementary steps that is required
to solve a decision problem if it has a “yes” answer.
The class of decision problems that can be solved in polynomial time using such non-
deterministic algorithms is called NP.
Note the important point that there is an asymmetry between “yes” and “no” answers
here. The question of how to show that a decision problem has a “no” answer is not
considered in this concept.
An important subclass of NP consists of the NP-complete problems. These are the
hardest problems in NP in the sense that if one of them is shown to be in P then
P=NP. Let A be an algorithm for the solution of problem B. We say that a problem
C is polynomially reducible to problem B if it can be solved in polynomial time by
an algorithm that uses A as a subroutine provided that each subroutine call of A only
counts as one step. A problem is then called NP-complete if every problem in NP is
polynomially reducible to it.
The Hamiltonian cycle problem is one member of the broad collection of NP-complete
problems (for a derivation of this result see Johnson & Papadimitriou (1985)).
The question P=NP? is one of the most famous unsolved questions in complexity theory.
Since this question has now been attacked for two decades and since NP-complete
problems proved to be substantially hard in practice it is commonly accepted that
P = NP should be the probable answer to this question (if it can be decided at all).
We want to emphasize again that our representation is kept on an informal level, and it
is intended to give just an idea of the concepts of complexity theory. Especially we have
not considered space complexity which measures the amount of storage an algorithm
requires.
We now discuss how optimization problems like the traveling salesman problem can be
dealt with. With the TSP we associate the following decision problem which can be
analyzed using the above concepts.10
Chapter 2. Basic Concepts
Traveling Salesman Decision Problem
Given the complete graph K n with edge weights c uv and a number b decide if there
exists a Hamiltonian tour in K n with length at most b.
This decision problem is NP-complete (Johnson & Papadimitriou (1985)).
If the traveling salesman problem is in P then obviously also the corresponding decision
problem is in P. An optimization problem having the property that the existence of
a polynomial time algorithm for the solution of an associated decision problem implies
the polynomial solvability of an NP-complete problem, is said to be NP-hard.
On the other hand, assume there exists a polynomial time algorithm for the solution
of the TSP decision problem. If all edge weights are integral and the largest weight in
absolute value of an edge is c then clearly the optimal solution of the traveling salesman
problem is not smaller than −c · n and not larger than c · n. Using the algorithm to solve
the decision problem we can find the shortest tour length using the following approach.
procedure tsplength(G)
(1) Set L = −c · n and U = c · n.
(2) As long as L < U perform the following steps.
(2.1) Set b =  L+U
2 .
(2.2) If there exists a Hamiltonian tour of length at most b then set U = b, otherwise
set L = b + 1.
end of tsplength
Applying this binary search technique we can find the length of the shortest tour by
at most log(c · n) + 1 calls of the solution algorithm for the TSP decision problem.
(Throughout this text we will use log to denote the logarithm with base 2).
To completely solve the optimization problem we have to exhibit an optimal solution.
This is now easily done once the shortest length is known.
procedure tsptour(G)
(1) Let U be the optimal tour length found by algorithm tsplength.
(2) For all u = 1, 2, . . . , n and all v = 1, 2, . . . , n perform the following steps.
(2.1) Set s uv = c uv and c uv = c · n + 1.
(2.2) If there does not exist a Hamiltonian tour of length U in the modified graph
then restore c uv = s uv .
end of tsptour2.2. Complexity Theory
11
After execution of this procedure the edges whose weights have not been altered give
the edges of an optimal tour.
The procedures tsplength and tsptour call a polynomial number (in n and log c) of times
the algorithm for the solution of the traveling salesman decision problem. Optimization
problems with the property that they can be polynomially reduced to a decision problem
in NP are called NP-easy. Problems which are both NP-easy and NP-hard (like the
traveling salesman problem) are called NP-equivalent. If P  = NP then no NP-hard
problem can be solved in polynomial time, if P=NP then every NP-easy problem is in
P.
So far we have considered the general traveling salesman problem. One might hope
that there are special cases where the problem can be solved in polynomial time. Un-
fortunately, such cases rarely have practical importance (Burkard (1990), van Dal
(1992), van der Veen (1992), Warren (1993)). For most practical situations, namely
for symmetric distances with triangle inequality, for Euclidean instances, for bipartite
planar graphs, or even for grid graphs, the traveling salesman problem remains NP-hard.
A different important issue is the question of whether algorithms can be designed which
deliver solutions with requested or at least guaranteed quality in polynomial time (poly-
nomial in the problem size and in the desired accuracy). Whereas for other NP-hard
problems such possibilities do exist, there are only discouraging results for the general
TSP. For a particular problem instance let c opt denote the length of a shortest tour and
c H denote the length of a tour computed by heuristic H. There are two basic results
relating these two values.
Theorem 2.2 Unless P=NP there does not exist for any constant r ≥ 1 a polynomial
time heuristic H such that c H ≤ r · c opt for all problem instances.
A proof is given in Sahni & Gonzales (1976).
A fully polynomial approximation scheme for a minimization problem is a heuristic
H which computes for a given problem instance and any ε > 0 a feasible solution
satisfying c H ≤ (1 + ε) · c opt in time polynomial in the size of the instance and in ε −1 .
It is an easy exercise to prove that to require polynomiality also in the encoding length
of ε is equivalent to require a polynomial algorithm for the exact solution. It is very
unlikely that fully polynomial approximation schemes exist for the traveling salesman
problem since the following result holds.
Theorem 2.3 Unless P=NP there does not exist a fully polynomial approximation
scheme for the Euclidean traveling salesman problem.
A proof can be found in Johnson & Papadimitriou (1985). The result holds in
general for TSPs with triangle inequality.
Despite these theoretical results we can nevertheless design heuristics that determine
good or very good tours in practice. The theorems tell us that for every heuristic there
are however problem instances where it fails badly. There are a few approximation
results for problems with triangle inequality which will be addressed in Chapter 6.
It should be pointed out that the complexity of an algorithm derived by theoretical
analysis might be insufficient to predict its behaviour when applied to real-world in-
stances of a problem. This is mainly due to the fact that only worst case analysis is12
Chapter 2. Basic Concepts
performed which may be different from average behaviour, and, in addition, polynomial
algorithms can cause a large amount of CPU time if the polynomial is not of low degree.
In fact, for practical applications only algorithms having running time at most O(n 3 )
would be rated efficient, but even algorithms with running times as low as O(n 2 ) may
not be applicable in certain situations.
Another point is, that the proof of NP-hardness of a problem does not imply the nonex-
istence of a reasonable algorithm for the solution of problem instances arising in practice.
It is the aim of this study to show that in the case of the traveling salesman problem
algorithms can be designed which are capable of finding good approximate solutions to
even large sized real-world instances within moderate time limits.
2.3 Linear and Integer Programming
Linear and integer programming is not a central topic of this tract. However, at some
points we will make references to concepts and results of linear and integer programming.
We give a short survey on these. Highly recommendable references in this area are the
prize-winning books Schrijver (1986) and Nemhauser & Wolsey (1988).
Let A be an m × n-matrix (constraint matrix), b be an m-vector (right hand side) and c
be an n-vector (objective function), where all entries of A, b, and c are rational numbers.
Given these data the linear programming problem is defined as follows.
Linear Programming Problem
Find a vector x ∗ maximizing the objective function c T x over the set {x ∈ Q | Ax ≤ b}.
A linear program may be given in various forms which can all be transformed to the
above. For example we may have equality constraints, nonnegativity conditions for
some variables, or the objective function is to be minimized.
In its general form a linear programming problem is given as
max c T x + d T y
Ax + By ≤ a
Cx + Dy = b
x ≥ 0
with appropriately dimensioned matrices and vectors.
A fundamental concept of linear programming is duality. The dual linear program
to the program given above (which is then called the primal linear program) is
defined as
min u T a + v T b
u T A + v T C ≥ c T
u T B + v T D = d T
u ≥ 0.
It is easily verified that the dual of the dual problem is again the primal problem.
One important aspect of the duality concept is stated in the following theorem.2.3. Linear and Integer Programming
13
Theorem 2.4 Let P and D be a pair of dual linear programs as defined above.
Suppose there exist vectors (x ∗ , y ∗ ) and (u ∗ , v ∗ ) satisfying the constraints of P , resp.,
D. Then we have
(i) The objective function value of (x ∗ , y ∗ ) (in problem P ) is less than or equal to
the objective function value of (u ∗ , v ∗ ) (in problem D).
(ii) Both problems have optimal solutions and their objective function values are
equal.
Duality exhibits further relations between primal and dual problem. But since they
are not important in the sequel we omit them here. Note in particular, that the dual
problem can be used to give bounds for the optimal value of the primal problem (and
vice versa).
The first algorithm for solving linear programming problems was the Simplex method
invented by Dantzig (1963). Since then implementations of this method have been
considerably improved. Today, even very large sized linear programming problems with
several ten thousands of variables and constraints can be solved routinely in moderate
CPU time (Bixby (1994)). The running time of the Simplex method cannot be bounded
by a polynomial, in fact there are examples where exponential running time is necessary
to solve a problem.
However, the linear programming problem, i.e., the problem of maximizing a linear ob-
jective function subject to linear constraints is in P. This was proved in the famous
papers Khachian (1979) (using the Ellipsoid method) and Karmarkar (1984) (us-
ing an Interior-point method). Though both of these algorithms are polynomial, only
interior point methods are competitive with the Simplex method (Lustig, Marsten
& Shanno (1994)).
These facts illustrate again that complexity analysis is in the first place only a theoretical
tool to assess hardness of problems and running time of algorithms.
A step beyond polynomial solvability is taken if we require feasible solutions to have
integral entries. The integer linear programming problem is defined as follows.
Integer Linear Programming Problem
Let A, b, and c be appropriately dimensioned with rational entries. Find a vector x ∗
maximizing the objective function c T x over the set {x ∈ Q | Ax ≤ b, x integer}.
This problem is NP-complete and no duality results are available. We show that the
traveling salesman problem can be formulated as an integer linear program.
To be able to apply methods of linear algebra to graph theory we associate vectors to
edge sets in the following way. Let G = (V, E) be a graph. If |E| = m then we denote
by Q E the m-dimensional rational vector space where the components of the vectors
x ∈ Q E are indexed by the edges uv ∈ E. We denote a component by x uv or x e if
e = uv.
The incidence vector x F ∈ Q E of an edge set F ⊆ E is defined by setting x F
uv = 1 if
F
uv ∈ F and by setting x uv = 0 otherwise. Similarly, if we associate a variable x uv to
each edge uv we denote by x(F ) the formal sum of the variables belonging to the edges
of F .14
Chapter 2. Basic Concepts
Now consider the TSP for the complete graph K n with edge weights c uv . With the
interpretation that x uv = 1 if edge uv is contained in a tour and x uv = 0 otherwise, the
following is a formulation of the TSP as an integer linear program.

min
c uv x uv
uv∈E
x(δ(v)) = 2,
x(C) ≤ |C| − 1,
x uv ∈ {0, 1},
for all u ∈ V,
for all cycles C ⊆ E n , |C| < n,
for all u, v ∈ V.
The TSP can be successfully attacked within the framework of linear and integer pro-
gramming for surprisingly large problem sizes. We will comment on this issue in Chap-
ter 12.
2.4 Data Structures
In this section we discuss some data structures that are useful for implementing traveling
salesman problem algorithms. They are all used in the software package TSPX (Reinelt
(1991b)) with which all experiments in this monograph were conducted. The exposition
is based on the books Tarjan (1983) and Cormen, Leiserson & Rivest (1989) on
algorithms and data structures. A further reference on the foundations of algorithms is
Knuth (1973).
2.4.1 Binary Search Trees
A rooted tree is a connected acyclic graph with one distinguished node, the so-called
root or root node of the tree. Therefore, if the graph has n nodes a tree consists of
n − 1 edges. The depth of the tree is the length (number of edges) of the longest path
from the root to any other node. Every node v is connected to the root by a unique
path. The length of this path is said to be the depth or the level of v. We say that
a tree is binary if at most two edges are incident to the root node and at most three
edges are incident to every other node.
If node u is the first node encountered when traversing the unique path from v to the
root then u is called father of v, denoted by f [v] in the following. By definition f [r] = 0
if r is the root node. If v is a node in a binary tree then there are at most two nodes
with v as their father. These nodes are called sons of v, and we define one of them to
be the right son and the other one to be the left son of v. Either of them may be
missing. A node without sons is called leaf of the tree. To represent a binary tree we
store for every node v its father f [v], its right son r[v] and its left son l[v]. If one of
them is missing we assign 0 to the respective entry.
Figure 2.1 shows a binary tree with root 5 and leaves 2, 7, 3, and 9.15
2.4. Data Structures
5
9
8
10
4
2
1
6
7
3
Figure 2.1 A binary tree on 10 nodes
By assigning to each node v a number k[v], the key of the node, we can store information
in such a tree.
Let a 1 , a 2 , . . . , a n be a sequence of keys assigned to a set of n nodes.
Definition 2.5 A binary tree on these nodes is called search tree if it satisfies the
following conditions.
(i) If u = r[v] then k[u] ≥ k[v].
(ii) If w = l[v] then k[w] ≤ k[v].
If the following procedure is called with the root of the search tree as parameter then
it prints the stored numbers in increasing order.
procedure inorder(v)
(1) If v = 0 then return.
(2) Call inorder(l[v]).
(3) Print k[v].
(4) Call inorder(r[v]).
end of inorder
Algorithms to find the smallest key stored in a binary search tree or to check if some
key is present are obvious.
The depth of an arbitrary binary search tree can be as large as n − 1 in which case the
tree is a path. Hence checking if a key is present in a binary search tree can take time
O(n) in the worst case. This is also the worst case time for inserting a new key into the
tree.
On the other hand, if we build the tree in a clever way, we could realize it with depth
log n. In such a tree searching can be performed much faster.
This observation leads us to the concept of balanced binary search trees which allow
searching in worst case time O(log n). There are many possibilities for implementing
balanced trees. We describe the so-called red-black trees.16
Chapter 2. Basic Concepts
For the proper definition we have to change our notion of binary search trees. Now we
distinguish between internal nodes (having a key associated with them) and external
nodes (leafs of the tree). Every internal node is required to have two sons, no internal
node is a leaf of the tree.
Definition 2.6 A binary tree is called red-black tree if it satisfies the following
conditions.
(i) Every node is either red or black.
(ii) Every external node (leaf) is black.
(iii) If a node is red then its sons are black.
(iv) Every path from a node down to a leaf contains the same number of black nodes.
It can be shown that a red-black tree with n non-leaf nodes has depth at most 2 log(n+1).
Therefore searching in a red-black tree takes time O(log n).
Insertion of a new key can also be performed in time O(log n) because of the small
depth of the tree. But we have to ensure that the red-black property still holds after
having inserted a key. To do this we have to perform some additional fixing operations.
Basically, we color the newly inserted node red and then reinstall the red-black condition
on the path from the new node to the root. At every node on this path we spend constant
time to check correctness and to fix node colors and keys if necessary. So the overall time
spent for inserting a new node and reestablishing the red-black condition is O(log n).
2.4.2 Disjoint Sets Representation
Very frequently we need to manage a partition of some ground set V = {1, 2, . . . , n} into
disjoint subsets S 1 , S 2 , . . . , S k , i.e., sets satisfying ∪ ki=1 S i = V and S i ∩ S j = ∅ for all
i  = j. Operations to be performed are merging two subsets and identifying the subset
containing a given element.
The data structure to store such a partition consists of a collection of trees each repre-
senting one subset. The nodes of each tree correspond to those elements of the ground
set which belong to the respective subset, and the root node of each tree is said to be
the representative of the respective set. For simplification of algorithms we define
here the father of the root to be the root itself.
Suppose we have an initialized data structure representing some partition. Identification
of the subset, i.e., the representative of the subset, to which an element v ∈ V belongs
is achieved by the following function.
function find(v)
(1) While f [v]  = v set v = f [v].
(2) Return v.
end of find
Joining two subsets S and T where we are given elements x ∈ S and y ∈ T is accom-
plished by the following procedure.2.4. Data Structures
17
procedure union(x, y)
(1) Set u = find(x).
(2) Set v = find(y).
(3) Set f [u] = v.
end of union
The representative of the new set is v.
We will exclusively use the disjoint set representation in the following context. The
ground set V is the node set of some graph with edge set E. Initially, the ground set
is partitioned into n sets containing one element each. We then scan the edges of E in
some order depending on the application. If the endnodes of the current edge satisfy
some condition then the sets containing these endnodes are merged. Usually n − 1
merge operations are performed so that the final partition consists just of the set V .
The number of find operations is bounded by 2|E|. An application of this principle is
used for implementing Kruskal’s spanning tree algorithm to be discussed in section 2.5.
Without further modifications the above implementation can result in trees which are
paths. This is the worst case for performing find operations. Ideally, we would like to
have trees of depth 1 for set representation. But to achieve this we have to traverse one
of the trees participating in a union operation. On the other hand, this can result in a
running time of O(m 2 ) for m union operations if the wrong trees are chosen.
Fortunately, there are two improvements to overcome these problems. We implement
the find operations with additional path compression and union operations as union
by rank. In addition we now store for each node r the number of nodes n[r] in the tree
rooted at r. The modified procedures are now.
function find and compress(v)
(1) If v  = f [v] then set f [v] = find and compress(f [v]).
(2) Return f [v].
end of find and compress
After execution of this procedure for a node v all nodes on the path from v to the root
(including v) in the previous tree now have the root node as their father.
procedure union by rank(x, y)
(1) Set u = find and compress(x).
(2) Set v = find and compress(y).
(3) If n[u] < n[v] then set f [u] = v and n[v] = n[v] + n[u]. Otherwise set f [v] = u
and n[u] = n[u] + n[v].
end of union by rank18
Chapter 2. Basic Concepts
This procedure makes the root of the larger tree the father of the root of the smaller
tree after the union operation. Looking more closely at this principle one realizes that
it is not necessary to know the exact number of nodes in the trees. It suffices to store
a rank at the root node which is incremented by one if trees of equal rank are merged.
This way one can avoid additions of integers.
Initialization of a single element set is simply done by the following code.
procedure make set(v)
(1) Set f [v] = v and n[v] = 1.
end of make set
The modified implementation of union/find turns out to perform very efficiently for our
purposes.
Theorem 2.7 If m operations are performed using disjoint sets representation by
trees where n of them are make set operations and the other ones are union operations
(by rank) for disjoint sets and find operations (with path compression) then this can be
performed in time O(m log ∗ n).
A proof of this instructive theorem can be found in Tarjan (1983) or Cormen, Leis-
erson & Rivest (1989). The number log ∗ n is defined via log (i) n as follows.

n
if i = 0,
(i)
(i−1)
n) if i > 0 and log (i−1) n > 0,
log n = log(log
undefined
otherwise,
and then
log ∗ n = min{i ≥ 0 | log (i) n ≤ 1}.
In fact, in the above theorem a slightly better bound of O(mα(m, n)) where α denotes
the inverse of the Ackermann function can be proved. But this is of no importance for
practical computations since already log ∗ n ≤ 5 for n ≤ 2 65536 . So we can speak of
linear running time of the fast union-find algorithm in practice (for our applications).
2.4.3 Heaps and Priority Queues
A heap is a data structure to store special binary trees satisfying an additional heap
condition. These binary trees have the property that except for the deepest level all
levels contain the maximal possible number of nodes. The deepest level is filled from
“left to right” if we imagine the tree drawn in the plane. To every tree node there is an
associated key, a real number. These keys are stored in a linear array A according to a
special numbering that is assumed for the tree nodes. The root receives number 1 and if
a node has number i then its left son has number 2i and its right son has number 2i + 1.
If a node has number i then its key is stored in A[i]. Therefore, if such a binary tree
has k nodes then the corresponding keys are stored in A[1] through A[k]. The special
property that array A has to have is the following.2.4. Data Structures
19
Definition 2.8 An array A of length n satisfies the heap property if for all 1 < i ≤ n
we have A[ 2 i ] ≤ A[i].
Stated in terms of binary trees this condition means that the key of the father of a node
is not larger than the key of the node. The heap property implies that the root has the
smallest key among all tree nodes, or equivalently, A[1] is the smallest element of the
array.
Alternatively, we can define the heap property as “A[ 2 i ] ≥ A[i]”. This does not make
an essential difference for the following, since only the relative order of the keys is
reversed.
As a first basic operation we have to be able to turn an array filled with arbitrary keys
into a heap. To do this we have to use the subroutine heapify with argument i which
fixes the heap property for the subtree rooted at node i (where it is assumed that the
two subtrees rooted at the left, resp. right, son of i are already heaps).
procedure heapify(i)
(1) Let n be the number of elements in heap A (nodes in the binary tree). If 2i > n
or 2i + 1 > n the array entries A[2i], resp. A[2i + 1] are assumed to be +∞.
(2) Let k be the index of i, 2i, and 2i + 1 whose array entry is the smallest.
(3) If k  = i then exchange A[i] and A[k] and perform heapify(k).
end of heapify
It is easy to see that heapify(i) takes time O(h) if h is the length of the longest path
from node i down to a leaf in the search tree. Therefore heapify(1) takes time O(log n).
Suppose we are given an array A of length n to be turned into a heap. Since all leaves
represent 1-element heaps the following procedure does the job.
procedure build heap(A)
(1) For i =  n 2  downto 1 perform heapify(i).
end of build heap
A careful analysis of this procedure shows that an arbitrary array of length n can be
turned into a heap in time O(n). Note that the binary tree represented by the heap is
not necessarily a search tree.
Except for sorting (see section 2.5) we use the heap data structure for implementing
priority queues. As the name suggests such a structure is a queue of elements such
that elements can be accessed one after the other according to their priority. The
first element of such a queue will always be the element with highest priority. For the
following we assume that an element has higher priority than another element if its key
is smaller.
The top element of a heap is therefore the element of highest priority. The most fre-
quently applied operation on a priority queue is to extract the top-priority element and
assure that afterwards the element of the remaining ones with highest priority is in the
first position. This operation is implemented using the heap data structure. We assume
that the current size of the heap is n.20
Chapter 2. Basic Concepts
function extract top(A)
(1) Set t = A[1].
(2) Set A[1] = A[n] and decrease the heap size by 1.
(3) Call heapify(1).
(4) Return t.
end of extract top
Because of the call of heapify(1), extracting the top element and fixing the heap property
needs time O(log n). Inserting a new element is accomplished as follows.
procedure insert key(k)
(1) Increase the heap size by 1 and set i to the new size.
(2) While i > 1 and A[ 2 i ] > k
(2.1) Set A[i] = A[ 2 i ] and i =  2 i .
(3) Set A[i] = k.
end of insert key
Since in the worst case we have to scan the path from the new leaf to the root, a call of
insert key takes time O(log n).
2.4.4 Graph Data Structures
Very frequently we have to store undirected graphs G = (V, E) with |V | = n nodes and
|E| = m edges where n is large, say in the range of 1,000 to 100,000. The number of
edges to be stored depends on the application.
Matrix type data structures are the (node-edge) incidence matrix and the (node-node)
adjacency matrix. The incidence matrix A is an n × m-matrix whose entries a ie are
defined by

1 if i is an endnode of edge e,
a ie =
0 otherwise.
The adjacency matrix is an n × n-matrix B whose entries b ij are defined by

1 if ij is an edge of G,
b ij =
0 otherwise.
Since we need O(nm) or O(n 2 ) storage for these matrices they cannot be used for large
graphs. This also limits the use of distance matrices for the definition of edge weights
in large problem instances. Fortunately, for large TSPs, distances between nodes are
usually given by a distance function.
If we just want to store a graph we can use an edge list consisting of two arrays tail
and head such that tail[e] and head[e] give the two endnodes of the edge e. This is21
2.4. Data Structures
appropriate if some graph is generated edge by edge and no specific operation has to be
performed on it.
Another possibility is to use a system of adjacency lists. Here we store for each node
a list of its adjacent nodes. This is done by an array adj of length 2m containing the
adjacent nodes and an array ap of length n. These arrays are initialized such that the
neighbors of node i are given in adj[ap[i]], adj[ap[i] + 1], through adj[ap[i + 1] − 1]. This
data structure is suitable if we have to scan neighbors of a node and if the graph remains
unchanged. Adding edges is time consuming since large parts of the array may have to
be moved.
If we have to add edges and want to avoid moving parts of arrays, then we have to
use linked lists. Since this is our most frequent operation on graphs we have used the
following data structure to store an undirected graph. The two arrays tail and head
contain the endnodes of the edges. The arrays nxtt and nxth are initialized such that
nxtt[e] gives the number of a further edge in this structure having tail[e] as one endnode
and nxth[e] gives the number of a further edge having head[e] as an endnode. An entry 0
terminates a linked list of edges for a node. For each node v the array entry first[v]
gives the number of the first edge having v as an endnode.
To get used to this form of storing a graph we give an example here. Suppose we
have a subgraph of the complete graph on six nodes consisting of the edges {1, 2},
{1, 5}, {2, 5}, {2, 3}, {3, 5}, {4, 5} and {4, 6}. This graph could be stored e.g., by the
assignment shown in Table 2.2.
Index first Index head tail nxth nxtt
1
2
3
4
5
6 6
5
5
7
7
4 1
2
3
4
5
6
7 1
3
2
4
2
1
4 2
5
5
6
3
5
5 0
0
1
0
3
1
4 0
0
2
0
2
3
6
Table 2.2 Example for subgraph data structure
Suppose the current graph has m edges and is stored using this data structure. Adding
a new edge can then be performed in constant time using the following piece of code.
procedure add edge(i, j)
(1) Set tail[m + 1]=i, nxtt[m + 1]=first[i], and first[i] = m + 1.
(2) Set head[m + 1]=j, nxth[m + 1]=first[j], and first[j] = m + 1.
(3) Set m = m + 1.
end of add edge
Of course, if we add more edges than dynamic memory space has been allocated for we
have to allocate additional space.22
Chapter 2. Basic Concepts
2.4.5 Representing Tours
An easy way to store a tour is to use an array t of length n and let t[k] be the k-th node
visited in the tour. However, this is not sufficient as we shall see below. We also have
to impose a direction on the tour. We therefore store with each node i its predecessor
pred[i] and its successor succ[i] in the tour with respect to the chosen orientation.
When using heuristics to find short tours one has to perform a sequence of local modi-
fications of the current tour to improve its length. We explain our method to perform
modifications in an efficient way using the example of 2-opt moves. A 2-opt move
consists of removing two edges from the current tour and reconnecting the resulting
paths in the best possible way. This operation is depicted in Figure 2.3 where broken
arcs correspond to directed paths.
j l j l
k i k i
Figure 2.3 A 2-opt move
Note that we have to have an imposed direction of the tour to be able to decide which
pair of new edges can be added to form a new tour. Adding edges jk and il would result
in an invalid subgraph consisting of two subtours. Furthermore, the direction on one of
the paths has to be reversed which takes time O(n).
Since we have to make a sequence of such 2-opt moves we do not update the tour
structure as in Figure 2.3 but rather store the current tour as a sequence of unchanged
intervals of the starting tour. This is of particular importance for some heuristics where
2-opt moves are only tentative and might not be realized to form the next tour. For
each interval we store the direction in which it is traversed in the current tour. The
result of the 2-opt move in our example would be a doubly linked sequence of intervals
as in Figure 2.4 where we also indicate that the path represented by the interval [k, j]
has been reversed.
[ l , i ]
[ k , j ]
Figure 2.4 Result of the 2-opt move
To make the approach clearer we use concrete node numbers l = 4, i = 7, k = 3, j = 10
and perform another 2-opt move involving nodes 11 and 5 on the path from 4 to 7 and
nodes 8 and 16 on the path from 3 to 10 as in Figure 2.5.23
2.4. Data Structures
10
4
10
4
16 11 16 11
8 5 8 5
3
3
7
7
Figure 2.5 A second 2-opt move
After execution of this move we have the interval sequence shown in Figure 2.6. Note
that the segment between nodes 3 and 8 was reversed before the 2-opt move, so it is
not reversed any more after the move.
[ 4 , 11 ]
[ 8 , 3 ]
[ 7 , 5 ]
[ 16 , 10 ]
Figure 2.6 Result of the second 2-opt move
The new interval sequence was obtained by splitting two intervals and reconnecting the
intervals in an appropriate way. Note that also in the interval representation of a tour
we have to reorient paths of the sequence. But, if we limit the number of moves in such
a sequence by k this can be performed in O(k).
One difficulty has been omitted so far. If we want to perform a 2-opt move involving
four nodes we have to know the intervals in which they are contained to be able to
choose the correct links. We cannot do this efficiently without additional information.
We store with each node its rank in the starting tour. This rank is defined as follows:
An arbitrary node gets rank 1, its successor gets rank 2, etc., until the predecessor of the
rank 1 node receives rank n. Since we know for each interval the ranks of its endnodes
and whether it has been reversed with respect to the starting tour or not, we can check
which interval contains a given node if we store the intervals in a balanced binary search
tree.
Applying this technique the interval containing a given node can be identified in time
O(log k) if we have k intervals.
This way of maintaining tour modifications is extensively used in the implementation
of a Lin-Kernighan type improvement method (see Chapter 7). Experience with this
data structure was also reported in Applegate, Chvátal & Cook (1990). They used
splay trees (Tarjan (1983)) instead of red-black trees in their implementation.
Of course, the number of intervals should not become too large because the savings in
execution time decreases with the number of intervals.
Finally, or if we have too many intervals, we have to clear the interval structure and
generate correct successor and predecessor pointers to represent the current tour. This24
Chapter 2. Basic Concepts
is done in the obvious way. Note that the longest path represented as an interval can
remain unchanged, i.e., for its interior nodes successors, predecessors, and ranks do not
have to be altered.
[11] [12] [1] [2]
1 3 5 8
[10] 6 10 [3]
[9] 9 11 [4]
1 (12)
[ 5 , 3 ]
2 7 12 4
[8] [7] [6] [5]
1 3 5 8
6 10
9 11
2 7 12 4
1 3 5 8
9 11
12
4
[ 3 , 12 ]
3 (3)
10
7
6 (7)
[ 5 , 4 ]
6
2
1 (5)
1 (2)
6 (2)
8 (5)
[ 5 , 8 ]
[ 2 , 3 ]
[ 4 , 10 ]
[ 7 , 12 ]
Figure 2.7 An example for representing a tour
To visualize this approach we give a sequence of 2-opt moves starting from a basic tour
together with the resulting interval sequences and search trees in Figure 2.7. Ranks of
nodes are listed in brackets. Each node of the search tree represents an interval. We give
for each node the rank of the endnode with lower rank and in parentheses the number
of nodes in the interval.2.5. Some Fundamental Algorithms
25
2.5 Some Fundamental Algorithms
In this chapter we review some basic algorithms which are not traveling salesman prob-
lem specific but are used as building blocks in many heuristics. For an extensive dis-
cussion we refer again to Cormen, Leiserson & Rivest (1989).
2.5.1 Sorting
Given a set A = {a 1 , a 2 , . . . , a n } of n integer or rational numbers, the sorting problem
consists of finding the sequence of these numbers in increasing (or decreasing) order. In
many cases the numbers will correspond to the weights of the edges of a subgraph.
There is a variety of algorithms we cannot discuss here. One example of a very sim-
ple sorting algorithm shall be given first. This algorithm recursively subdivides a set
into two halves, sorts the subsets and then merges the sorted sequences. Suppose the
numbers are stored in B[1], B[2], through B[n].
procedure mergesort(B, l, u)
(1) If l ≥ u − 1 sort B[l] through B[u] by comparisons and return.
).
(2) Perform mergesort(B, l,  l+u
2
 + 1, u).
(3) Perform mergesort(B,  l+u
2
(4) Rearrange B to represent the sorted sequence.
end of mergesort
The call mergesort(B, 1, n) sorts the array in time O(n log n). This will follow from
considerations in section 2.5.3 since Step (1) is executed in constant time and Step (4)
can be performed in time O(u − l).
Another sorting algorithm makes use of the heap data structure presented in the pre-
vious chapter. Since in a heap the top element is always the smallest one we can
successively generate the sorted sequence of the elements of A using the heap.
procedure heapsort(B)
(1) Call build heap(B).
(2) For i = n downto 1 perform the following steps.
(2.1) Exchange B[1] and B[i].
(2.2) Decrement the heap size by 1.
(2.3) Call heapify(1).
end of heapsort26
Chapter 2. Basic Concepts
After execution of heapsort(A) we have the elements of A sorted in increasing order in
A[n], A[n − 1], through A[1].
The running time is easily derived from the discussion in the section on heaps. Step (1)
takes time O(n),  and, since Step (2.3) takes time O(log i), we obtain the overall running
n
time as O(n) + i=1 O(log i) = O(n log n).
Therefore both merge sort and heap sort seem to be more or less equivalent. However,
heap sort has an important advantage. It is able to generate the sorted sequence as
long as needed. If for some reason the remaining sequence is not of interest any more
at some point we can exit from heapsort prematurely.
A final remark is in order. It can be shown that sorting based on the comparison of two
elements cannot be performed faster than in O(n log n) time. So the above discussion
shows that the sorting problem has time complexity Θ(n log n) (in this computational
model).
Faster sorting algorithms can only be achieved if some assumptions on the input can
be made, e.g., that all numbers are integers between 1 and n. Expected linear running
time of some sorting algorithms can be shown for specific input distributions.
2.5.2 Median Finding
We could also have implemented a sorting algorithm by recursively doing the following
for a set A = {a 1 , a 2 , . . . , a n }. First identify a median of A, i.e., a value a such that half
of the elements of A are below a and half of the elements are above a. More precisely,
we identify a number a such that we can partition A into two sets A 1 and A 2 satisfying
A 1 ⊆ {a i | a i ≤ a}, A 2 ⊆ {a i | a i ≥ a}, |A 1 | =  n 2 , and |A 2 | =  n 2 . We then sort A 1
and A 2 separately. The concatenation of the respective sorted sequences give a sorting
of A.
In particular for geometric problems defined on points in the plane we often need to
compute horizontal or vertical lines separating the point set into two (approximately
equally sized) parts. For this we need medians with respect to the x- or y-coordinates
of the points.
A natural way to find a median is to sort the n points. The element at position  n 2 
gives a median. However, one can do better as is shown in the following sophisticated
algorithm which is also very instructive. The algorithm requires a subroutine that for a
given input b rearranges an array such that in the first part all elements are at most as
large as b while in the second part all elements are at least as large as b. Assume again
that array B contains n numbers in B[1] through B[n].
function partition(B, b)
(1) Set i = 1 and j = n.
(2) Repeat the following steps until i ≥ j.
(2.1) Decrement j by 1 until B[j] ≤ b.
(2.2) Increment i by 1 until B[i] ≥ b.
(2.3) If i < j exchange B[i] and B[j].27
2.5. Some Fundamental Algorithms
(3) Return j.
end of partition
After execution of this algorithm we have B[l] ≤ b for all l = 1, 2, . . . , i and B[l] ≥ b for
all l = j, j + 1, . . . , n.
The procedure for finding a median can now be given. In fact, the procedure does a
little bit more. It finds the i-th smallest element of a set.
procedure find ith element(B, i)
(1) Partition B into  n 5  groups of 5 elements each and one last group containing the
remaining elements.
(2) Sort each set to find its “middle” element. If the last set has even cardinality l
we take the element at position 2 l + 1.
(3) Apply find ith element to find the median b of the set of medians found in Step (2).
(4) Let k = partition(B, b).
(5) If i ≤ k then find the i-th smallest element of the lower side of the partition,
otherwise find the (i − k)-th smallest element of the higher side of the partition.
end of find ith element
The call find ith element(B,  n+1
2 ) now determines a median of B.
A running time analysis shows that this algorithm runs in linear time which is best
possible since every element of B has to be considered in a median finding procedure.
Hence medians can be found in time Θ(n).
2.5.3 Divide and Conquer
We have already applied the divide and conquer principle without having named it
explicitly. The basic idea is to divide a problem into two (or more) subproblems, solve
the subproblems, and then construct a solution of the original problem by combining
the solutions of the subproblems in an appropriate way.
Since this is a very important and powerful principle, we want to cite the main result for
deriving the running time of a divide and conquer algorithm of Cormen, Leiserson
& Rivest (1989). Such an algorithm has the following structure where we assume that
it is applied to some set S, |S| = n, to solve some (unspecified) problem.
procedure divide and conquer(S)
(1) Partition S into subproblems S 1 , S 2 , . . . , S a of size less than
n
.
b
(2) For each subproblem S i perform divide and conquer(S i ).
(3) Combine the subproblem solutions to solve the problem for S.
end of divide and conquer28
Chapter 2. Basic Concepts
In the following we assume that f is the running time function for performing Steps (1)
and (3).
Theorem 2.9 gives a formula to compute the running time of divide and conquer algo-
rithms. (It does not matter that nb is not necessarily integral, we can both substitute
 nb  or  nb ).
Theorem 2.9 Let a ≥ 1 and b > 1 be constants and let T (n) be defined by the
recurrence T (n) = aT ( nb )+f (n). Then T (n) can be bounded asymptotically (depending
on f ) as follows.
(i) If f (n) = Θ(n log b a−ε ) for some constant ε > 0, then T (n) = Θ(n log b a ).
(ii) If f (n) = O(n log b a ), then T (n) = Θ(n log b a log n).
(iii) If f (n) = Ω(n log b a+ε ) for some constant ε > 0 and if a · f ( nb ) ≤ c · f (n) for some
constant c < 1 and all sufficiently large n, then T (n) = Θ(f (n)).
This theorem provides a very useful tool. Take as an example the merge sort algorithm.
Step (1) is performed in constant time and Step (4) is performed in time O(u − l), hence
f (n) = O(n). For the theorem we have a = 2 and b = 2. Therefore case (ii) applies and
we obtain a running time of Θ(n log n) for merge sort.
2.5.4 Minimum Spanning Trees
Let G = (V, E), |V | = n, |E| = m, be a connected graph with edge weights c uv for all
uv ∈ E. A minimum spanning tree of G is an acyclic connected subset T ⊆ E such
that c(T ) is minimal among these edge sets. Clearly a spanning tree has n − 1 edges.
The following algorithm computes a minimum length spanning tree of G (Prim (1957)).
procedure prim(G)
(1) Set T = ∅ and Q = {1}.
(2) For all i = 2, 3, . . . , n set d[i] = c 1i and p[i] = 1 if {1, i} ∈ E, resp., d[i] = ∞ and
p[i] = 0 if {1, i} ∈
/ E.
(3) As long as |T | < n − 1 perform the following.
(3.1) Let d[j] = min{d[l] | l ∈ V \ Q}.
(3.2) Add edge {j, p[j]} to T and set Q = Q ∪ {j}.
(3.3) For all l ∈ V \ Q check if c jl < d[l]. If this is the case set d[l] = c jl and
p[l] = j.
(4) T is a minimum spanning tree of G.
end of prim2.5. Some Fundamental Algorithms
29
The running time of this algorithm is Θ(n 2 ). It computes in each iteration a mini-
mum spanning tree for the subgraph G Q = (Q, E(Q)), and so upon termination of the
algorithm we have a minimum spanning tree of G.
The implementation of Prim’s algorithm given here is best possible if we have complete
or almost complete graphs G. If we want to compute minimum spanning trees for graphs
with fewer edges then other implementations are superior. The idea is to maintain the
nodes not yet connected to the tree in a binary heap (where the keys are the shortest
distances to the tree). Since keys are only decreased, every update in Step (3.3) can
be performed in time O(log n). Because we have to scan the adjacency list of j to see
which distances might be updated, Step (3.3) takes altogether time O(m log n). Finding
the minimum of a heap and updating it has to be performed n − 1 times requiring
time O(log n) each time. Therefore we obtain running time O(m log n) for the heap
implementation of Prim’s algorithm.
Using more advanced techniques (as e.g., binomial heaps or Fibonacci heaps) one can
even achieve time O(m + n log n).
A different approach to finding a minimum spanning tree was given in Kruskal (1956).
It is particularly suited for sparse graphs.
procedure kruskal(G)
(1) Build a heap of the edges of G (with respect to smaller weights).
(2) Set T = ∅.
(3) As long as |T | < n − 1 perform the following.
(3.1) Get the top edge {u, v} from the heap and update the heap.
(3.2) If u and v belong to different connected components of the graph (V, T ) then
add edge {u, v} to T .
(4) T is a minimum spanning tree of G.
end of kruskal
The idea of this algorithm is to generate minimum weight acyclic edge sets with in-
creasing cardinality until such an edge set of cardinality n − 1 is found (which is then a
minimum spanning tree of G).
This algorithm is implemented using fast union-find techniques. Combining results for
maintaining heaps and for applying fast union-find operations we obtain a running time
of O(m log m) for this algorithm.
Instead of using a heap we could sort in a first step all edges with respect to increasing
length and then use only union-find to identify different components. Using a heap we
can stop as soon as |T | = n − 1 and do not necessarily have to sort all edges.
If G is not connected a slight modification of the above computes the minimum weight
acyclic subgraph of cardinality n − k where k is the number of connected components
of G.30
Chapter 2. Basic Concepts
2.5.5 Greedy Algorithms
The greedy algorithm is a general algorithmic principle for finding feasible solutions of
combinatorial optimization problems. It is characterized by a myopic view, performing
the construction of a feasible solution step by step based only on local knowledge of the
problem.
To state it in general, we need a suitable definition of a combinatorial optimization
problem. Let E = {e 1 , e 2 , . . . , e m } be a finite set where each element has an associated
the set of so-called feasible solutions. For a set
weight c i , 1 ≤ i ≤ m, and I ⊆ 2 E be 
I ⊆ E, its weight is given as c(I) = e i ∈I c i . The optimization problem consists of
finding a feasible solution I ∗ ∈ I such that c(I ∗ ) = min{c(I) | I ∈ I}.
The greedy algorithm works as follows.
procedure greedy(I, c)
(1) Sort E such that c 1 ≤ c 2 ≤ . . . , c m .
(2) Set I = ∅.
(3) For i = 1, 2, . . . , m:
(3.1) If I ∪ {e i } ∈ I, then set I = I ∪ {e i }.
end of greedy
Due to the sorting in Step (1), the algorithm needs time Ω(m log m). It does not need
more time, if the test “I ∪ {e i } ∈ I?” can be performed in time O(log m).
Inspite of its simplicity, the greedy principle has some important aspects, in particular
in the context of matroids and independence systems, but we do not elaborate on this.
In general, however, it delivers feasible solutions of only moderate quality. Note, that
Kruskal’s algorithm for computing minimum spanning trees is nothing but the greedy
algorithm. Because the spanning tree problem is essentially an optimization problem
over a matroid, the greedy algorithm is guaranteed to find an optimal solution in this
case.Chapter 3
Related Problems and Applications
For several practical problems it is immediately seen that the TSP provides the suit-
able optimization model. In many cases, however, this is either not straightforward
or the pure TSP has to be augmented by further constraints. In this chapter we first
discuss some optimization problems that are related to the TSP. Some of them can
be transformed to a pure TSP in a reasonable way, others are at least related in the
sense that algorithms developed for the TSP can be adapted to their solution. Then
we survey some application areas where the TSP or its relatives can be used to treat
practical problems. Further aspects are found in Garfinkel (1985). Finally, we intro-
duce the collection of sample problem instances that we will use in the sequel for testing
algorithms.
3.1 Some Related Problems
Note that we can assume without loss of generality that a symmetric TSP is always a
minimization problem and that all distances c ij are positive. First, if we are looking
for the longest Hamiltonian cycle we can multiply all edge weights by −1 and solve
a minimization problem. Second, we can add a constant to all edge weights without
affecting the ranking of tours with respect to their lengths. Hence all edge weights can
be made positive. In the sequel we will not explicitly mention this fact. Without loss
of generality we can also assume that all edge lengths are integer numbers.
Observe, however, that approximation results may be no longer valid after having mod-
ified edge weights. For example, if a very large constant is added to each edge weight,
then every tour is near optimal.
3.1.1 Traveling Salesman Problems in General Graphs
There may be situations where we want to find shortest Hamiltonian tours in arbitrary
graphs G = (V, E), in particular in graphs which are not complete. If it is required that
each node is visited exactly once and that only edges of the given graph must be used
then we can do the following.
Add all missing edges giving them a sufficiently large

c
weight M (e.g., M =
ij∈E ij ) and apply an algorithm for the symmetric TSP in
complete graphs. If this algorithm terminates with an optimal tour containing none of
the edges with weight M , then this tour is also optimal for the original problem. If an
edge with weight M is contained in the optimal tour then the original graph does not
G. Reinelt: The Traveling Salesman, LNCS 840, pp. 31-41, 1994.
 Springer-Verlag Berlin Heidelberg 199432
Chapter 3. Related Problems and Applications
contain a Hamiltonian cycle. Note however, that heuristics cannot guarantee to find a
tour in G even if one exists.
The second way to treat such problems is to allow that nodes may be visited more than
once and edges be traversed more than once. If the given graph is connected we can
always find a feasible roundtrip under this relaxation. This leads us to the so-called
graphical traveling salesman problem.
3.1.2 The Graphical Traveling Salesman Problem
For an arbitrary connected graph G with edge weights, the graphical traveling sales-
man problem (GTSP) consists of finding a closed walk in G for the salesman to visit
every city requiring the least possible total distance. The salesman may only use edges
of G, but is allowed to visit a city or to traverse an edge more than once. This is
sometimes a more practical definition of the TSP because we may have cases where the
underlying graph of connections does not even contain a Hamiltonian cycle and where
some direct transitions from a city i to a city j are not possible. In the formulation as
a GTSP we explicitly stick to the given graph.
To avoid degenerate situations we have to have nonnegative edge weights. Otherwise
we could use an edge as often as we like in both directions to achieve an arbitrarily
small length of the solution. We transform a GTSP to a symmetric TSP as follows.
Let K n = (V n , E n ) be the complete graph on n nodes. For every pair i, j of nodes we
compute the shortest path from i to j in the graph G. The length d ij of this path is
taken as the weight of edge ij in K n . Now the shortest Hamiltonian tour in K n can be
transformed to a shortest closed walk in G visiting all nodes.
Naddef & Rinaldi (1993) discuss relation between the TSP and the GTSP in detail.
3.1.3 The Shortest Hamiltonian Path Problem
We are given a graph G = (V, E) with edge weights c ij . Two special nodes, say v s and
v t , of V are also given. The task is to find a path from v s to v t visiting each node of V
exactly once with minimal length, i.e., to find the shortest Hamiltonian path in G from
v s to v t .
This problem can be solved as a standard TSP in two ways.
a) Choose M sufficiently large and assign weight −M to the edge from v s to v t .
Then compute the optimal traveling salesman tour in this graph. This tour has
to contain edge v s v t and thus solves the Hamiltonian path problem.
b) Add a new node 0 to V and edges from 0 to v s and to v t with weight 0. Each
Hamiltonian tour in this new graph corresponds to a Hamiltonian path from v s
to v t in the original graph with the same length.
If the terminating point of the Hamiltonian path is not fixed, then we can solve the
problem by introducing a new node 0 and adding edges from all nodes v ∈ V \ {v s } to 0
with zero length. Now we can solve the Hamiltonian path problem with starting point
v s and terminating point v t = 0 which solves the original problem.
If neither starting point nor terminating point are specified, then we just add node 0
and connect all other nodes to 0 with edges of length zero. In this new graph we solve
the standard TSP.3.1. Some Related Problems
33
3.1.4 Hamiltonian Path and Cycle Problems
Sometimes it has to be checked if a given graph G = (V, E) contains a Hamiltonian
cycle or path at all. This question can be answered by solving a symmetric TSP in the
complete graph K n (with n = |V |) where all edges of the original graph obtain weight
1 and all other edges obtain weight 2. Then G contains a Hamiltonian cycle if and only
if the shortest Hamiltonian cycle in K n has length n. If this shortest cycle has length
n + 1, then G is not Hamiltonian, but contains a Hamiltonian path.
3.1.5 The Asymmetric Traveling Salesman Problem
If the cost of traveling from city i to city j is not necessarily the same as of traveling
from city j to city i, then an asymmetric traveling salesman problem has to be solved.
Let D = (W, A), W = {1, 2, . . . , n}, A ⊆ W × W , and d ij be the arc weight of (i, j) ∈ A.
We define a graph G = (V, E) by
V = W ∪ {n + 1, n + 2, . . . , 2n},
E = {(i, n + i) | i = 1, 2, . . . , n}
∪ {(n + i, j) | (i, j) ∈ A}.
Edge weights are assigned as follows
c i,n+i = − M for i = 1, 2, . . . , n,
c n+i,j = d ij for (i, j) ∈ A,

where M is a sufficiently large number, e.g., M = (i,j)∈A d ij . It is easy to see that
for each directed Hamiltonian cycle in D with length d D there is a Hamiltonian cycle
in G with length c G = d D − nM . In addition, since an optimal tour in G contains all
edges with weight −M , it induces a directed Hamiltonian cycle in D. Hence we can
solve asymmetric TSPs as symmetric TSPs.
3.1.6 The Multisalesmen Problem
We give the asymmetric version of this problem. Instead of just one salesman now there
are m salesmen available who are all located in a city n + 1 and have to visit cities
1, 2, . . . , n. The task is to select some (or all) of these salesmen and assign tours to
them such that in the collection of all these tours together each city is visited exactly
once. The activation of salesman j incurs a fixed cost w j . The cost of the tour of
salesman j is the sum of the intercity distances of his tour (starting at and returning
to city n + 1). The m-salesmen problem (m-TSP) now consists of selecting a subset
of the salesmen and assigning a tour to each of them such that each city is visited by
exactly one salesman and such that the total cost of visiting all cities this way is as
small as possible.
In Bellmore & Hong (1974) it was observed that this problem can be transformed
to an asymmetric TSP involving only one salesman. We give their construction.34
Chapter 3. Related Problems and Applications
Let D = (V, A) be the digraph where V = {1, 2, . . . , n, n + 1} and A ⊆ V × V gives the
possible transitions between cities. Let d ij be the distance from city i to city j.
We construct a new digraph D = (V  , A  ) as follows.
V  = V ∪ {n + 2, n + 3, . . . , n + m},
A  = A
∪ {(n + i, j) | 2 ≤ i ≤ m, (n + 1, j) ∈ A}
∪ {(j, n + i) | 2 ≤ i ≤ m, (j, n + 1) ∈ A}
∪ {(n + i, n + i − 1) | 2 ≤ i ≤ m}.
The weights d  ij for the arcs (i, j) in A  are defined via
d  ij =
d  n+i,j
d  j,n+i
= d n+1,j +
= d j,n+1 +
d ij , for 1 ≤ i ≤ n, 1 ≤ j ≤ n, (i, j) ∈ A,
1
w ,
2 i
1
2 w i ,
for 1 ≤ i ≤ m, 1 ≤ j ≤ n, (n + 1, j) ∈ A,
for 1 ≤ i ≤ m, 1 ≤ j ≤ n, (j, n + 1) ∈ A,
d  n+i,n+i−1 = 12 w i−1 − 12 w i , for 2 ≤ i ≤ m.
It is not difficult to verify that the shortest tour in D  relates to an optimal solution of
the corresponding m-salesmen problem for D.
Observe in addition, that with an easy modification we can require that every salesman
is to be activated. We simply eliminate all edges (n + i, n + i − 1) for 2 ≤ i ≤ m. Of
course, the fixed costs w j can now be ignored.
A different transformation is given in Jonker & Volgenant (1988). Solution algo-
rithms are discussed in Gavish & Srikanth (1986).
3.1.7 The Rural Postman Problem
We are given a graph G = (V, E) with edge weights c ij and a subset F ⊆ E. The Rural
Postman Problem consists of finding a shortest closed walk in G containing all edges
in F . If F = E we have the special case of a Chinese postman problem which can be
solved in polynomial time using matching techniques (Edmonds & Johnson (1973)).
The standard symmetric TSP can easily be transformed to a rural postman problem.
Therefore, in general the rural postman problem is NP-hard. In Chapter 11 we will en-
counter a special version of the rural postman problem and find approximative solutions
for it using TSP methods.
3.1.8 The Bottleneck Traveling Salesman Problem
Instead of tours with minimal total length one searches in this problem for tours whose
longest edge is as short as possible. This bottleneck traveling salesman problem
can be solved by a sequence of TSPs. To see this observe that the absolute values of the
distances are not of interest under this objective function. We may reduce distances as
long as they compare exactly as before. Hence we may assume that we have at most
1
1
2 n(n − 1) different distances and that the largest of them is not greater than 2 n(n − 1).
We now solve problems of the following kind for some parameter b.3.2. Practical Applications of the TSP
35
“Is there a Hamiltonian cycle of the original graph consisting only of edges with length
at most b?”
This problem can be transformed to a standard TSP. By performing a binary search
on the parameter b (starting with b = 14 n(n − 1)) we can identify the smallest such b
leading to a “yes” answer by solving at most O(log n) TSPs.
3.1.9 The Prize Collecting Traveling Salesman Problem
We are given a graph G = (V, E) with edge weights c ij , node weights u i (representing
benefits received when visiting the respective city), and a special base node v 0 (with
c v 0 = 0). The Prize Collecting Traveling Salesman Problem consists of finding
a cycle in G containing the node v 0 such that the sum of the edge weights of the cycle
minus the sum of the benefits of the nodes of the cycle is minimized. We can get rid of
the node weights if we substitute the edge weights c ij by c ij − 12 v i − 12 v j . Now the prize
collection TSP amounts to finding a shortest cycle in G containing v 0 . More details are
given in Balas (1989) and Ramesh, Yoon & Karwan (1992).
We have seen that a variety of problems can be transformed to symmetric TSPs or are
at least related to it. However, each such transformation has to be considered with
some care before actually trying to use it for practical problem solving. E. g., the
shortest path computations necessary to treat a GTSP as a TSP take time O(n 3 ) which
might not be acceptable in practice. Most transformations require the introduction of
a large number M . This can lead to numerical problems or may even prevent finding
feasible solutions at all using only heuristics. In particular, for LP-based approaches,
the usage of the “big M ” cannot be recommended in general. But, in any case, these
transformations provide a basic means for using a TSP code to treat related problems.
3.2 Practical Applications of the TSP
Since we are aiming at the development of algorithms and heuristics for practical travel-
ing salesman problem solving we give a survey on some of the possible applications. The
list is not complete but covers the most important cases. In addition we have included
problems which cannot be transformed to pure TSPs, but which can be attacked using
variants of the methods to be described later.
3.2.1 Drilling of Printed Circuit Boards
The drilling problem for printed circuit boards (PCBs) is a standard application of the
symmetric traveling salesman problem. To connect a conductor on one layer with a
conductor on another layer or to position (in a later stage of the PCB production) the
pins of integrated circuits, holes have to be drilled through the board. The holes may
be of different diameters. To drill two holes of different diameters consecutively, the
head of the machine has to move to a tool box and change the drilling equipment. This36
Chapter 3. Related Problems and Applications
is quite time consuming. Thus it is clear at the outset that one has to choose some
diameter, drill all holes of the same diameter, change the drill, drill the holes of the
next diameter etc.
Thus, this drilling problem can be viewed as a sequence of symmetric traveling salesman
problems, one for each diameter resp. drill, where the “cities” are the initial position
and the set of all holes that can be drilled with one and the same drill. The “distance”
between two cities is the time it takes to move the head from one position to the other.
The goal is to minimize the travel time for the head of the machine.
We will discuss an application of the drilling problem in depth in Chapter 11.
3.2.2 X-Ray Crystallography
A further direct application of the TSP occurs in the analysis of the structure of crystals
(Bland & Shallcross (1989), Dreissig & Uebach (1990)). Here an X-ray diffrac-
tometer is used to obtain information about the structure of crystalline material. To
this end a detector measures the intensity of X-ray reflections of the crystal from various
positions. Whereas the measurement itself can be accomplished quite fast there is a con-
siderable overhead in positioning time since up to 30,000 positions have to be realized
for some experiments. In the two examples that we refer to, the positioning involves
moving four motors. The time needed to move from one position to the other can be
computed very accurately. For the experiment the sequence in which the measurements
at the various positions are taken is irrelevant. Therefore, in order to minimize the total
positioning time the best sequence for the measurements has to be determined. This
problem can be modeled as a symmetric TSP.
3.2.3 Overhauling Gas Turbine Engines
This application was reported by Plante, Lowe & Chandrasekaran (1987) and
occurs when gas turbine engines of aircrafts have to be overhauled. To guarantee a
uniform gas flow through the turbines there are so-called nozzle-guide vane assemblies
located at each turbine stage. Such an assembly basically consists of a number of
nozzle guide vanes affixed about its circumference. All these vanes have individual
characteristics and the correct placement of the vanes can result in substantial benefits
(reducing vibration, increasing uniformity of flow, reducing fuel consumption). The
problem of placing the vanes in the best possible way can be modeled as a symmetric
TSP.
3.2.4 The Order-Picking Problem in Warehouses
This problem is associated with material handling in a warehouse (Ratliff & Rosen-
thal (1981)). Assume that at a warehouse an order arrives for a certain subset of the
items stored in the warehouse. Some vehicle has to collect all items of this order to
ship them to the customer. The relation to the TSP is immediately seen. The storage
locations of the items correspond to the nodes of the graph. The distance between two
nodes is given by the time needed to move the vehicle from one location to the other.3.2. Practical Applications of the TSP
37
The problem of finding a shortest route for the vehicle with minimal pickup time can
now be solved as a TSP. In special cases this problem can be solved easily (see van Dal
(1992) for an extensive discussion).
3.2.5 Computer Wiring
A special case of connecting components on a computer board is reported in Lenstra
& Rinnooy Kan (1974). Modules are located on a computer board and a given subset
of pins has to be connected. In contrast to the usual case where a Steiner tree connection
is desired, here the requirement is that no more than two wires are attached to each
pin. Hence we have the problem of finding shortest Hamiltonian paths with unspecified
starting and terminating points.
A similar situation occurs for the so-called testbus wiring. To test the manufactured
board one has to realize a connection which enters the board at some specified point,
runs through all the modules, and terminates at some specified point. For each module
we also have a specified entering and leaving point for this test wiring. This problem also
amounts to solving a Hamiltonian path problem with the difference that the distances
are not symmetric and that starting and terminating point are specified.
3.2.6 Clustering of a Data Array
This application is also reported in Lenstra & Rinnooy Kan (1974). An (r, s)-
matrix A = (a ij ) is given representing relationships between two finite sets of elements
R = {R 1 , R 2 , . . . , R r } and S = {S 1 , S 2 , . . . , S s }. The entry a ij gives the strength of
the relationship between R i ∈ R and S j ∈ S. The task is to identify clusters of highly
related elements.
To this end, a permutation of the rows and columns of A has to be found which max-
imizes the sum of all products of horizontally or vertically adjacent pairs of entries of
A. We transform this problem as follows.
If ρ and σ are permutations of R and S, respectively, then the corresponding measure
of effectiveness is
ME(ρ, σ) =
s−1 
r

a i,σ(j) a i,σ(j+1) +
j=1 i=1
r−1 
s

a ρ(i),j a ρ(i+1),j .
i=1 j=1
The two terms can be evaluated separately. We only consider the first one. Let V =
{1, 2, . . . , s} and E = V × V . We define weights for the edges of G = (V, E) by
c ij = −
r

a ki a kj .
k=1
Then the problem of maximizing the first term amounts to finding a shortest Hamilto-
nian path in G with arbitrary starting and terminating node. The clustering problem
is then solved as two separate such problems, one for the rows and one for the columns.
The special cases where A is symmetric or where A is a square matrix and where we
only allow simultaneous permutations of rows and columns lead to a single Hamiltonian
path problem.38
Chapter 3. Related Problems and Applications
3.2.7 Seriation in Archeology
Suppose archeologists have discovered a graveyard and would like to determine the
chronological sequence of the various gravesites. To this end each gravesite is classi-
fied according to the types of items contained in it. A distance measure between two
gravesites is introduced reflecting the diversity between their respective contents. A very
likely chronological sequence can be found by computing the shortest Hamiltonian path
in the graph whose nodes correspond to the gravesites and where distances are given
due to the criterion above. In fact, this was one of the earliest applications mentioned
for the TSP.
3.2.8 Vehicle Routing
Suppose that in a city n mail boxes have to be emptied every day within a certain
period of time, say 1 hour. The problem is to find the minimal number of trucks to do
this and the shortest time to do the collections using this number of trucks. As another
example, suppose that customers require certain amounts of some commodities and a
supplier has to satisfy all demands with a fleet of trucks. Here we have the additional
problem to assign customers to trucks and to find a delivery schedule for each truck so
that its capacity is not exceeded and the total travel cost is minimized.
The vehicle routing problem is solvable as a TSP if there is no time constraint or if
the number of trucks is fixed (say m). In this case we obtain an m-salesmen problem.
Nevertheless, one can apply methods for the TSP to find good feasible solutions for this
problem (see Lenstra & Rinnooy Kan (1974)).
3.2.9 Scheduling
We are given n jobs that have to be performed on some machine. The time to process
job j is t ij if i is the job performed immediately before j (if j is the first job then its
processing time is t 0j ). The task is to find an execution sequence for the jobs such that
the total processing time is as short as possible.
We define the directed graph D = (V, A) with node set V = {0, 1, 2, . . . , n} and arc
set A = {1, 2, . . . , n} × {1, 2, . . . , n} ∪ {(0, i) | i = 1, 2, . . . , n}. Arc weights are t ij for
(i, j) ∈ A. The scheduling problem can now be solved by finding a shortest (directed)
Hamiltonian path starting at node 0 with arbitrary terminating node.
Sometimes it is requested that the machine returns to its initial state after having
performed the last job. In this case we add arcs (i, 0) for every i = 1, 2 . . . , n where t i0
is the time needed to return to the initial state if job i was performed last. Now the
scheduling problem amounts to solving the asymmetric TSP in the new digraph.
Suppose the machine in question is an assembly line and that the jobs correspond to
operations which have to be performed on some product at the workstations of the line.
In such a case the primary interest would lie in balancing the line. Therefore instead of
the shortest possible time to perform all operations on a product the longest individual
processing time needed on a workstation is important. To model this requirement a
bottleneck TSP is more appropriate.3.2. Practical Applications of the TSP
39
In Lenstra & Rinnooy Kan (1974) it is shown that the following job-shop scheduling
problem can be transformed to an asymmetric TSP. We are given n jobs that have to
be processed on m machines. Each job consists of a sequence of operations (possibly
more than m) where each operation has to be performed on one of the machines. The
operations have to be performed one after the other in a sequence which is given in ad-
vance. As a restriction we have that no passing is allowed (we have the same processing
order of jobs on every machine) and that each job visits each machine at least once.
The problem is to find a schedule for the jobs that minimizes the total processing time.
3.2.10 Mask Plotting in PCB Production
For the production of each layer of a printed circuit board, as well as for layers of
integrated semiconductor devices, a photographic mask has to be produced. In our case
for printed circuit boards this is done by a mechanical plotting device. The plotter
moves a lens over a photosensitive coated glass plate. The shutter may be opened or
closed to expose specific parts of the plate. There are different apertures available to
be able to generate different structures on the board. Two types of structures have
to be considered. A line is exposed on the plate by moving the closed shutter to one
endpoint of the line, then opening the shutter and moving it to the other endpoint of
the line. Then the shutter is closed. A point type structure is generated by moving the
appropriate aperture to the position of that point then opening the shutter just to make
a short flash, and then closing it again. Exact modeling of the plotter control problem
leads to a problem more complicated than the TSP and also more complicated than the
rural postman problem.
We will discuss an application of the plotting problem in Chapter 11.
3.2.11 Control of Robots
In order to manufacture some workpiece a robot has to perform a sequence of operations
on it (drilling of holes of different diameters, cutting of slots, planishing, etc.). The
task is to determine a sequence to perform the necessary operations that leads to the
shortest overall processing time. A difficulty in this application arises because there
are precedence constraints that have to be observed. So here we have the problem of
finding the shortest Hamiltonian path (where distances correspond to times needed for
positioning and possible tool changes) that satisfies certain precedence relations between
the operations. This problem cannot be formulated as a TSP in a straightforward way,
but can be treated by applying methods similar to those presented in the forthcoming
chapters.40
Chapter 3. Related Problems and Applications
3.3 The Test Problem Instances
Throughout this tract we will use a set of sample problems compiled from various sources
to compare the different approaches and to examine the behaviour of algorithms with
respect to problem sizes.
To choose a suitable test set one has to decide between contradicting goals.
– Too many computational results might bore the reader.
– Too few results may not exhibit too much insight.
– Problems from different sources might not be comparable in a fair way because
distance computations may be more complicated and time consuming in one case.
– Not all problems are suitable for every approach.
To overcome these difficulties we have chosen to proceed as follows.
– We have selected a set of sample problem instances which can always be treated
by our methods and which are of the same type. This set consists of twenty-four
Euclidean problems in the plane of sizes from 198 to 5934 nodes and is given
in Table 3.1. This table also gives the currently best known upper and lower
bounds for the respective problems. A number in boldface indicates that an
optimal solution is known (and proved!).
Problem
d198
lin318
fl417
pcb442
u574
p654
rat783
pr1002
u1060
pcb1173
d1291
rl1323
fl1400
u1432
fl1577
d1655
vm1748
rl1889
u2152
pr2392
pcb3038
fl3795
fnl4461
rl5934
Size
198
318
417
442
574
654
783
1002
1060
1173
1291
1323
1400
1432
1577
1655
1748
1889
2152
2392
3038
3795
4461
5934
Bounds
15780
42029
11861
50778
36905
34643
8806
259045
224094
56892
50801
270199
[19849,20127]
152970
[22137,22249]
62128
336556
316536
[64163,64294]
378032
137694
[28594,28772]
182566
[554070,556146]
Table 3.1 Bounds for sample problems
– In addition we sometimes report about results on other problems. Most of these
problem instances are contained in the library TSPLIB of traveling salesman3.3. The Test Problem Instances
41
problem instances (see Reinelt (1991a)) and are therefore at the disposal of the
reader to conduct own experiments.
In our experiments, we have almost completely dispensed of random problem instances.
Only at some points we have included a few random problems for extrapolating CPU
times. For these problem instances, the points are located in a square and are drawn
independently from a uniform distribution. To our opinion, this should be the primary
reason for considering random problems. With respect to the assessment of algorithms
one should prefer the treatment of real instances (if available) because these are the
problems whose solution is of interest. Moreover, real problems have properties that
cannot be modeled by random distributions in an appropriate way.
Concerning the CPU times that are either explicitly given or presented in a graphical
display the following remarks apply.
– All CPU times are given in seconds on the SUN SPARCstation 10/20. All software
(unless otherwise noted) has been written in C and was compiled using cc with
option -O4 under the operating system SUNOS 4.1.2. The function get rusage
was used for measuring running times.
– Distance matrices are not used. Except for precomputed distances for candidate
sets, distances are always computed by evaluating the Euclidean distance function
and not by a matrix-lookup.
– If a figure displays CPU times for problems up to size 6,000 then these are times
for the 24 standard problem instances listed in Table 3.1.
– If a figure displays CPU times for problems up to size 20,000 then this includes
in addition the times for further problem instances, in particular for the real
problems rl11849, brd14051, and d18512, and for random problems of size 8,000
to 20,000.
Usually, we will not give the explicit length of tours produced by the various heuristics.
Rather we give their quality with respect to the lower bounds of Table 3.1. More
precisely, if c H is the length of a tour computed by heuristic H and if c L is the lower
bound of Table 3.1 for the respective problem instance, we say that the heuristic tour
has quality 100 · (c H /c L − 1) percent.Chapter 4
Geometric Concepts
Many traveling salesman problem instances arising in practice have the property that
they are defined by point sets in the 2-dimensional plane (e.g., drilling and plotting
problems to be discussed in Chapter 11). Though true distances can usually be not
given exactly or only by very complicated formulae, they can very well be approxi-
mated by metric distances. To speed up computations we can therefore make use of
geometric properties of the point set. In this chapter we introduce concepts for deriv-
ing informations about the structure of point sets in the plane and review some basic
algorithms. A textbook on computational geometry is Edelsbrunner (1987).
4.1 Voronoi Diagrams
Although known since quite some time (Voronoi (1908)) the Voronoi diagram has
only recently received the attention of many researchers in the field of computational
geometry. It has several attractive features and puts particular emphasis on proximity
relations between points.
Let S = {P 1 , P 2 , . . . , P n } be a finite subset of R m and let d : R m × R m −→ R be a
metric. We define the Voronoi region VR(P i ) of a point P i via
VR(P i ) = {P ∈ R m | d(P, P i ) ≤ d(P, P j ) for all j = 1, 2, . . . , n, j = i},
i.e., VR(P i ) is the set of all points that are at least as close to P i as to any other point
of S. The set of all n Voronoi regions is called the Voronoi diagram VD(S) of S.
Other names are Dirichlet tessellation or Thiessen tessellation. In the following
we call the elements of S generators.
We consider only the 2-dimensional case. With each generator P i we associate its
Cartesian coordinates (x i , y i ). For the first part we assume that d is the Euclidean
metric (L 2 ), i.e.,

d((x 1 , y 1 ), (x 2 , y 2 )) = (x 1 − x 2 ) 2 + (y 1 − y 2 ) 2 .
For two generators P i and P j we define the perpendicular bisector B(P i , P j ) = {P ∈
R 2 | d(P, P i ) = d(P, P j )}. If we define the half space B ij = {x ∈ R 2 | d(P i , x) ≤
d(P j , x)} then we see that
n

VR(P i ) =
B ij .
j=1
j = i
G. Reinelt: The Traveling Salesman, LNCS 840, pp. 42-63, 1994.
 Springer-Verlag Berlin Heidelberg 19944.1. Voronoi Diagrams
43
This implies that in the case of the Euclidean metric the Voronoi regions are convex
polygons with at most n − 1 vertices. Figure 4.1 shows the Voronoi diagram for the
points defining the traveling salesman problem instance rd100, a random problem on
100 points.
Figure 4.1 Voronoi diagram for rd100 (L 2 -metric)
We call nonempty intersections of two or more Voronoi regions Voronoi points if they
have cardinality 1 and Voronoi edges otherwise. Note that every Voronoi point is
the center of a circle through (at least) three generators and every Voronoi edge is a
segment of a perpendicular bisector of two generators.
A generator configuration is called degenerate if there are four generators lying on a
common circle. We say that two Voronoi regions are adjacent if they intersect. Note
that only in degenerate cases two regions can intersect in just one point. Degeneracy
does not occur in Figure 4.1 as is usually the case for randomly generated points.
Some observations are important.
Proposition 4.1 The Voronoi diagram of a generator P i is unbounded if and only if
P i lies on the boundary of the convex hull of S.
Proof. If VR(P i ) is bounded then P i is contained in the interior of the convex hull of
those generators whose regions are adjacent to VR(P i ). On the other hand, if VR(P i )
is unbounded it cannot be situated in the interior of conv(S).44
Chapter 4. Geometric Concepts
An unbounded region does not necessarily imply that a generator is a vertex of the
convex hull. Consider for example the case where all generators are located on a line.
Then all regions are unbounded, but only two generators define the convex hull of the
set.
Proposition 4.2 If d(P i , P j ) ≤ d(P i , P k ) for all k = i, j then VR(P i ) and VR(P j )
intersect in a Voronoi edge.
Proof. If P j is a nearest neighbor of P i then VR(P j ) and VR(P i ) intersect by definition.
If they intersect in only one point then this Voronoi point is the center of a circle through
P i , P j , and at least two more generators. At least two of these generators must be nearer
to P i than P j .
Proposition 4.3 The Voronoi diagram of n generators has at most 2n − 4 Voronoi
points and at most 3n − 6 Voronoi edges.
Proof. This property follows easily from Euler’s formula stating the relation between
the number of vertices, edges and facets of a convex polygon in the 3-dimensional
Euclidean space. Namely, if n v , n e , and n f denote the respective numbers we have
n f + n v = n e + 2.
We can apply this formula to a Voronoi diagram if we connect all Voronoi edges ex-
tending to infinity to a common imaginary Voronoi point (edges extending to infinity
in both directions are ignored here). We have n f = n Voronoi regions. Every Voronoi
point is the endnode of at least three Voronoi edges. Hence we obtain n v ≤ 23 n e and
therefore n e ≤ 3n − 6. This implies n v ≤ 2n − 4.
The Voronoi diagram gives a concise description of proximity relations between the
generators and also exhibits the information about which generators lie on the boundary
of the convex hull of the set S.
Before talking about time complexity of Voronoi diagram computations we have to
specify what the result of such a computation has to be. As the result of a Voronoi
diagram algorithm we require a data structure that allows for an easy access to the
Voronoi edges forming a specific region as well as to all Voronoi edges containing a given
Voronoi point. This can, e.g., be achieved by a data structure proposed in Ottmann
& Widmayer (1990). We store a doubly linked list of the Voronoi edges together with
the following information for every edge:
– tail and head nodes, u and v, of the edge (the number −1 indicates that the edge
extends to infinity),
– the Voronoi regions V l and V r to the left and to the right of the edge (where left
and right are with respect to some arbitrarily imposed direction of the edge),
– pointers to the next Voronoi edge of region V l (resp. V r ) having u (resp. v) as one
endnode.
A straightforward algorithm for computing the Voronoi diagram takes time O(n 2 ). We
just compute each VR(P i ) as the intersection of the halfspaces B ij for j = i.
A lower bound on the running time of every Voronoi diagram algorithm is established
as follows. Consider the situation that all generators are located on the x-axis, i.e.,
their y-coordinates are 0. If we have the Voronoi diagram of this set we easily obtain
the sorted sequence of the generators with respect to their x-coordinates. Since sorting4.1. Voronoi Diagrams
45
cannot be performed in less than O(n log n) time we have a lower bound on the running
time for Voronoi diagram computations. Note that sorting and computing the Voronoi
diagram are essentially equivalent in this 1-dimensional case. The following section will
describe an algorithm that achieves this best possible running time.
In the present and in the following sections which also deals with geometric problems
we will not discuss the algorithms in full detail. When solving geometric problems
very often certain “degenerate” situations can occur that may have the consequence
that some object or some operation is not uniquely specified. Explaining the idea of a
geometric algorithm is often easy but when it comes to implementing the algorithm one
will usually face severe difficulties. It is a commonly observed fact that, when coding
algorithms for geometric problems, about 10–20% of the code account for the “real”
algorithm while the remaining part is only necessary to deal with degenerate situations.
Almost the same is true when describing such algorithms in detail. We will therefore not
discuss degenerate situations explicitly, but will assume that points defining a geometric
problem are in general position, i.e., no degeneracy occurs. The reader should have in
mind, however, that an implementation must take care of these cases.
The O(n log n) algorithm to be described now was given in Shamos and Hoey (1975).
It is a divide and conquer approach that divides the set of generators recursively into
two halves, computes the respective Voronoi diagram for the two parts, and merges
them to obtain the Voronoi diagram of the whole set S.
procedure divide and conquer
(1) If the current set has at most three elements compute its Voronoi diagram and
return.
(2) Partition S into two sets S 1 = {P i 1 , P i 2 , . . . P i l } and S 2 = {P i l+1 , P i l+2 , . . . , P i n }
where l = n 2 such that there is a vertical line separating S 1 and S 2 .
(3) Perform the divide and conquer algorithm recursively to compute VD(S 1 ) and
VD(S 2 ).
(4) Obtain VD(S) from VD(S 1 ) and VD(S 2 ).
end of divide and conquer
The partition in Step (2) can be found in linear time if we have sorted the elements of
S in a preprocessing step with respect to their x-coordinates. There is also a more com-
plicated algorithm achieving linear running time without preprocessing (see Chapter 2
for median finding).
The critical step is Step (4). If this step can be performed in linear time (linear in
|S 1 | + |S 2 |) then the basic recurrence relation for divide and conquer algorithms gives a
time bound of O(n log n) for the overall Voronoi computation.
Merging two Voronoi diagrams (where one set of generators is to the left of the other
set of generators) basically consists of identifying the thick “merge line” in the center
of Figure 4.2.
We cannot go into detail here and only discuss the principle of the construction of
this merge line. The line is constructed from the bottom to the top. One first has to
identify the respective generators in VD(S 1 ) and VD(S 2 ) with minimal y-coordinates46
Chapter 4. Geometric Concepts
(and therefore unbounded Voronoi regions). The perpendicular bisector between these
two generators then gives the lower part of the merge line. The process of merging the
two diagrams can now be visualized as extending the merge line upwards until an edge
of either of the two Voronoi diagrams is hit. At such a point the merge line may change
direction because it will now be part of the perpendicular bisector between two other
generators. This process is continued until the final part of the merge line is part of the
bisector of the two generators in the respective diagrams with maximal y-coordinates.
Figure 4.2 The merge line for merging two Voronoi diagrams
Implementing these steps carefully results in a running time of O(|S 1 | + |S 2 |) for the
merge process. This gives the desired optimal running time of the divide and conquer
algorithm.
The principle of the next algorithm (see Green & Sibson (1978)) is to start out with
the Voronoi diagram for three generators and then to successively take into account the
remaining generators and update the current Voronoi diagram accordingly.
procedure incremental algorithm
(1) Compute the Voronoi diagram VD({P 1 , P 2 , P 3 }).
(2) For t = 4, 5, . . . , n compute the diagram VD({P 1 , P 2 , . . . , P t }) as follows.
(2.1) Find P s , 1 ≤ s ≤ t − 1, such that P t ∈ VR(P s ), i.e., find the nearest neighbor
of P t among the generators already considered.47
4.1. Voronoi Diagrams
(2.2) Start with the perpendicular bisector of P t and P s and find its intersection
with a boundary edge of VR(P s ). Suppose this edge is also the boundary
edge of VR(P i ). Find the other intersection of the bisector between P t and
P i with the boundary of VR(P i ). Proceed this way by entering neighboring
regions and computing intersections between the current bisector B(P t , P j )
and the boundary of VR(P j ) until the starting region VR(P s ) is reached
again. Eliminate everything within the resulting closed walk. (In the case of
an unbounded region VR(P t ) some further details have to be observed.)
end of incremental algorithm
Figure 4.3 shows a typical step of the incremental algorithm. The broken lines indicate
the bisectors that are followed in Step (2.2). The new diagram is obtained by deleting
everything inside the convex polygon determined by the broken lines.
P
v
P
r
P u
P t
P w
P s
Figure 4.3 A step of the incremental algorithm
The running time of this algorithm can only be bounded above by O(n 2 ). In Ohya,
Iri & Murota (1984) the algorithm is examined for application in practice. It turns
out that an enormous speed up can be obtained if the generators are considered in a
clever sequence. Ohya et al. use a bucketing approach which allows the nearest neighbor
in Step (2.1) to be guessed with high probability in constant time. Step (2.2) is not
that critical because usually only very local updates have to be performed to obtain the
new Voronoi diagram. In practical experiments linear running time on several classes of
randomly generated point sets was observed. In particular, the incremental algorithm
outperformed the divide and conquer method.
It is interesting to note, that selecting the next generator at random results in observed
3
O(n 2 ) running time.48
Chapter 4. Geometric Concepts
Expected linear running time is not proven in Ohya, Iri & Murota (1984), but some
evidence is given for it. For generators that are independently drawn from a uniform
distribution on the unit square, the expected number of generators to be examined
in Step (2.1) to find the nearest neighbor is bounded by a constant. In addition, the
expected number of Voronoi edges of a Voronoi region in any intermediate diagram is
bounded by a constant. These are two basic results that suggest that also a rigorous
mathematical proof of linear expected running time for this algorithm can be obtained.
A further algorithm for Voronoi diagram construction has been given in Fortune
(1987). This algorithm uses a sweep-line principle to compute the diagram in time
O(n log n).
Cronin (1990) and Ruján, Evertsz and Lyklema (1988) employ the Voronoi dia-
gram for generating traveling salesman tours.
4.2 Delaunay Triangulations
For most applications it is not the Voronoi diagram itself that is of interest. More
important are the proximity relations that it exhibits and not the concrete specification
of the Voronoi points and edges. To represent the topology of the diagram it suffices to
consider the “dual” of the diagram.
Given the Voronoi diagram of S, its dual D(S) is the undirected graph G(S) = (S, D)
where D = {{P 1 , P 2 } | VR(P 1 ) ∩ VR(P 2 ) = ∅}. It is easy to observe that G(S) is a
triangulated graph, i.e., every cycle of length at least four contains a chord. This graph
is called Delaunay triangulation (Delaunay (1934)).
An alternative definition excludes those edges {P 1 , P 2 } for which |VR(P 1 )∩VR(P 2 )| = 1.
In this case the name is misleading, because we do not necessarily have a triangulation
anymore, but the resulting graph is planar (implying |D| = O(|S|)). We will use this
definition in the sequel and speak about the Delaunay graph or the straight line
dual of the Voronoi diagram.
Note that in the case of nondegeneracy (no four generators lie on a common circle) both
definitions coincide and D(S) is a planar graph.
Besides planarity D(S) (according to the modified definition) has additional important
properties.
Proposition 4.4
(i) If P i and P j are generators such that d(P i , P j ) ≤ d(P i , P k ) for all k = i, j then
{P i , P j } is an edge of D(S).
(ii) D(S) has at most 3n − 6 edges.
(iii) D(S) contains a minimum spanning tree of the complete graph on n nodes where
the nodes correspond to the generators and the edge weights are respective Eu-
clidean distances.
Proof. Part (i) is clear because of Proposition 4.2 and part (ii) follows from Proposi-
tion 4.3.
For part (iii) consider Prim’s algorithm to compute a minimum spanning tree. In each
step we have a set V of nodes that are already connected by a spanning tree and the4.2. Delaunay Triangulations
49
set S \ V (consisting of isolated nodes). The next edge to be added to the tree is the
shortest edge connecting a node in V to a node in S \ V . This edge must be contained
in D(S) since it connects two generators whose Voronoi regions intersect in a Voronoi
edge.
Figure 4.4 shows the Delaunay triangulation corresponding to the diagram of Figure 4.1.
Figure 4.4 The Delaunay triangulation for rd100
Note that Proposition 4.4 (ii) does not hold for the general Delaunay triangulation but
only for the Delaunay graph. For example, if all generators are located on a circle then
all Voronoi regions intersect in a common Voronoi point and the Delaunay triangulation
is the complete graph on n nodes. The Delaunay graph is only a cycle of length n.
Straightforward implementations of algorithms for computing the Voronoi diagram (or
the Delaunay triangulation), in which all numerical computations are carried out in
floating point arithmetic, run into numerical problems.
Voronoi points are given as intersection points of bisectors. Due to insufficient accuracy
it may not be possible to safely decide whether two lines are parallel or almost parallel.
Moreover, the intersection points may be located “far away” from the generators leading
to imprecise computations of Voronoi points.
The consequence is that due to incorrect decisions the algorithm may not work because
computed data is contradictory.
Consider the following example (Figure 4.5). We have three generators that are located
at three corners of a square. Depending on whether the fourth generator is at the fourth50
Chapter 4. Geometric Concepts
corner, or inside, or outside the square different Voronoi diagrams arise. If it cannot
be exactly differentiated between these three cases, then the correct computation of the
Voronoi diagram and hence of the Delaunay triangulation fails.
Figure 4.5 Inconsistent decisions due to round-off errors
The question of how to obtain correct results and avoid numerical difficulties is consid-
ered in Sugihara & Iri (1988), Sugihara (1988), and Jünger, Reinelt & Zepf
(1991). The principle idea is to not compute an explicit representation of the Voronoi
diagram, but to base the computation of the Delaunay graph on different logical tests.
Details are given in Jünger, Reinelt & Zepf (1991), we review the main results.
If all generators have integral coordinates between 0 and M , then one can compute
the Delaunay graph using integer numbers of value at most 6M 4 . On a computer
representing integers with binary 1-complement numbers having b bits, integers in the
interval [−2 b−1 , 2 b−1 − 1] are available. The inequality 6M 4 ≤ 2 b−1 − 1 implies
 

b−1
− 1
4 2
M ≤
.
6
For the usual word length of real-world computers that means that we can allow

8 if b = 16
M ≤
137 if b = 32
35, 211 if b = 64 .
So, only 32-bit integer arithmetic is not enough for computing correct Delaunay trian-
gulations in practical applications. Only by using at least 64-bit arithmetic we can treat
reasonable inputs.
Of special interest are also the two metrics
– Manhattan metric (L 1 ): d((x 1 , y 1 ), (x 2 , y 2 )) = |x 1 − x 2 | + |y 1 − y 2 |, and
– Maximum metric (L ∞ ): d((x 1 , y 1 ), (x 2 , y 2 )) = max{|x 1 − x 2 |, |y 1 − y 2 |}.
This is due to the fact that very often distances correspond to the time a mechanical
device needs to travel from one point to the other. If this movement is performed
first in the horizontal direction and then in the vertical direction the L 1 -metric should
be chosen to approximate travel times. If the movement is performed by two motors
working simultaneously in horizontal and vertical directions then the L ∞ -metric is the
appropriate choice for modeling the movement times.4.2. Delaunay Triangulations
51
For these metrics, bisectors are no longer necessarily straight lines. They may consist of
three line segments and we can also have degenerate situations as shown in Figure 4.6.
Figure 4.6 Bisectors for L 1 - and L ∞ -metric
Here the bisectors include the shaded regions. In the L 1 -metric (left) this situation
occurs when the coordinate differences in both coordinates are the same. In the L ∞ -
metric (right picture) we have this situation if both points coincide in one of the two
coordinates. It is convenient to restrict our definition of bisectors to the bold line
segments in Figure 4.6 (the definition of Voronoi regions is changed accordingly). The
L 1 -metric Voronoi diagram for problem rd100 is shown in Figure 4.7.
Figure 4.7 Voronoi diagram for rd100 (L 1 -metric)52
Chapter 4. Geometric Concepts
The numerical analysis of the L 1 -case shows that we can compute an explicit representa-
tion of the Voronoi diagram itself using only one additional bit of accuracy than needed
to input the data. It follows that we can carry out all computations with numbers of
size at most 2M , and depending on the word length b we have the following constraints
for M

16, 383 if b = 16
1, 073, 741, 823 if b = 32
M ≤
4, 611, 686, 018, 427, 387, 903 if b = 64 .
Observe that also in diagrams for the Manhattan as well as for the maximum metric
the vertices of the convex hull of the generators have unbounded Voronoi regions. But
there may be further points with unbounded regions lying in the interior of the convex
hull. This is also reflected by the shape of the Delaunay graph in Figure 4.8 which
corresponds to the the Voronoi diagram of Figure 4.7.
Figure 4.8 Delaunay graph for rd100 (L 1 -metric)
Finally, we want to give an indication that Delaunay graphs can indeed be computed
very fast. We have used an implementation of the incremental Voronoi diagram algo-
rithm described in Ohya, Iri & Murota (1984) by M. Jünger and D. Zepf for the
L 2 -metric. This implementation uses floating point arithmetic, but was able to solve all
sample problem instances. Details for implementing an algorithm to compute Voronoi
diagrams for further metrics are discussed in Kaibel (1993).53
4.2. Delaunay Triangulations
Figure 4.9 shows the running time of this implementation on our set of sample problems.
CPU times are given in seconds on a SUN SPARCstation 10/20.
1.5
1.0
0.5
0.0
0
1000
2000
3000
4000
5000
6000
Figure 4.9 CPU times for computing Delaunay graphs
The figure seems to suggest indeed a linear increase of the running times with the
problem size. But, as we shall see throughout this monograph, we have to accept that
real-world problems do not behave well in the sense that smooth running time functions
can be obtained. Just the number of nodes of a problem is not sufficient to characterize
it. Real problems have certain structural properties that cannot be modeled by random
problem instances and can lead to quite different running times of the same algorithm
for different problem instances of the same size.
The number of edges of the respective Delaunay graphs is shown in Figure 4.10.
For random problems the expected number of edges forming a Voronoi region is six,
which is hence also the expected degree of a node in the Delaunay graph. Therefore we
would expect about 3n edges in a Delaunay graph which is quite closely achieved for
our sample problems.
We can conclude that Delaunay graphs can be computed very efficiently. For practical
purposes it is also important that running times are quite stable even though we have
problem instances from various sources and with different structural properties.54
Chapter 4. Geometric Concepts
20000
15000
10000
5000
0
0
1000
2000
3000
4000
5000
6000
Figure 4.10 Number of edges of the Delaunay graphs
CPU times are very well predictable (in our case as approximately n/5000 seconds on the
SPARCstation). Computing the Delaunay graph for problem d18512 took 19.4 seconds,
the number of edges of this graph was 61,126.
4.3 Convex Hulls
The convex hull of a set of points is the smallest convex set containing these points. It is
a convenient means for representing point sets. If the point set is dense then the convex
hull may very well reflect its shape. Large instances of traveling salesman problems
in the plane usually exhibit several clusters. Building the convex hull of these clusters
can result in a concise representation of the whole point set still exhibiting many of its
geometric properties.
A short description of complicated objects is also important in other areas, for example
in computer graphics or control of robots. Here movements of objects in space have
to be traced in order to avoid collisions. In such cases convex hulls can be applied to
represent the objects approximately.
We define the problem to compute the convex hull as follows. We are given a finite set
A = {a 1 , a 2 , . . . , a n } of n points in the plane where a i = (x i , y i ). The task is to identify4.3. Convex Hulls
55
those points that constitute the vertices of the convex hull conv(A) of the n points,
i.e., which are not representable as a convex combination of other points. Moreover,
we require that the computation also delivers the sequence of these vertices in correct
order. This means that if the computation outputs the vertices v 1 , v 2 , . . . , v t then the
convex hull is the (convex) polygon that is obtained by drawing edges from v 1 to v 2 ,
from v 2 to v 3 , etc., and finally by drawing the edges from v t−1 to v t and from v t to v 1 .
We will review some algorithms for computing the convex hull that also visualize some
fundamental principles in the design of geometric algorithms.
Before starting to describe algorithms we want to make some considerations concerning
the time that is at least necessary to find convex hulls. Of course, since all n input
points have to be taken into account we must spend at least time of linear order in n.
It is possible to establish a better lower bound. Let B = {b 1 , b 2 , . . . , b n } be a set of
distinct positive real numbers. Consider the set A ⊆ R 2 defined by A = {(b i , b 2 i ) | 1 ≤
i ≤ n}. Since the function f : R → R with f (x) = x 2 is strictly convex none of the
points of A is a convex combination of other points. Hence computing the convex hull
of A also sorts the numbers b i . It is known that in many computational models sorting
of n numbers needs worst case time Ω(n log n).
A second way to derive a lower bound is based on the notion of maxima of vectors. Let
A = {a 1 , a 2 , . . . , a n } be a finite subset of R 2 . We define a partial ordering “” on A by
a i = (x i , y i )  a j = (x j , y j ) if and only if x i ≥ x j and y i ≥ y j .
The maxima with respect to this ordering are called maximal vectors. In Kung,
Luccio & Preparata (1975) the worst case time lower bound of Ω(n log n) for identi-
fying the maximal vectors of A is proved. This observation is exploited in Preparata
& Hong (1977) for the convex hull problem. Suppose A is such that every a i is a vertex
of the convex hull. Identify the (w.l.o.g.) four points with maximal, resp. minimal x- or
y-coordinate. Let a j be the vertex with maximal y-coordinate and a k be the vertex with
maximal x-coordinate. The number of vertices between a j and a k may be of order n.
They are all maximal elements. Since convex hull computations can identify maximal
vectors it cannot be faster in the worst case than O(n log n). Note that this lower bound
is also valid if we do not require that the vertices of the convex hull are output in their
correct sequence.
Though the lower bound derived here may seem to be weak there are many algorithms
that compute the convex hull in worst case time O(n log n).
According to Toussaint (1985) the first efficient convex hull algorithm has been out-
lined in Bass & Schubert (1967). Their algorithm was designed to be the first step
for computing the smallest circle containing a given set of points in the plane. Though
the algorithm is not completely correct it already exhibits some of the powerful ideas
used in convex hull algorithms. It consists of an elimination step as in the throw-away
algorithm of section 4.3.3 and afterwards basically performs a scan similar to Graham’s
scan (to be described next). When corrected appropriately a worst case running time
of O(n log n) can be shown. Therefore, this algorithm can be considered as the first
O(n log n) convex hull algorithm.56
Chapter 4. Geometric Concepts
4.3.1 Graham’s Scan
This algorithm was given by Graham (1972). Its main step consists of computing a
suitable ordering of the points. Then the convex hull is built by successively scanning
the points in this order. The algorithm works as follows.
procedure graham scan
(1) Identify an interior point of conv(A), say P 0 . This can be done by finding three
points of A that are not collinear and by taking their center of gravity as P 0 .
(2) Compute polar coordinates for the points of A with respect to the center P 0 and
some arbitrary direction representing the angle zero.
(3) Sort the points with respect to their angles.
(4) If there are points with the same angle then eliminate all of them but the one
with largest radius. Let a i 1 , a i 2 , . . . , a i t be the sorted sequence of the remaining
points.
(5) Start with three consecutive points P r , P m , and P l , i.e., P r = a i k , P m = a i k+1 ,
P l = a i k+2 for some index k where indices are taken modulo t.
(6) Perform the following step for the current three points until the same triple of
points occurs for the second time.
a) If P m lies on the same side of the segment [P l , P r ] as P 0 or lies on the segment
then delete P m and set P m = P r and P r to its predecessor in the current sorted
list.
b) If P m lies on the side of the segment [P l , P r ] opposite to P 0 then set P r = P m ,
P m = P l , and P l to its successor in the current sorted list.
end of graham scan
P l
P r
P m
P 0
Figure 4.11 Graham’s scan4.3. Convex Hulls
57
Correctness of this algorithm is easily verified. Step (6) has to be performed at most
2t times for scanning the necessary triples of nodes. In Step (6a) a point is discarded
so it cannot be performed more than t − 3 times. If Step (6b) is executed t times we
have scanned the points “once around the clock” and no further changes are possible.
Therefore the worst case running time is dominated by the sorting in Step (3) and we
obtain the worst case running time O(n log n).
This way we have established the worst case time complexity Θ(n log n) for computing
convex hulls in the plane.
4.3.2 Divide and Conquer
The divide and conquer principle also applies in the case of convex hull computations
(Bentley & Shamos (1978)). In this case, the basic step consists of partitioning a
point set according to some rule into two sets of about equal size, computing their
respective convex hulls and merging them to obtain the convex hull of the whole set.
procedure divide and conquer
(1) If the current set has at most three elements compute its convex hull and return.
(2) Partition A into two sets A 1 = {a i 1 , a i 2 , . . . , a i l } and A 2 = {a i l+1 , a i l+2 , . . . , a i n }
where l = n 2 such that there is a vertical line separating A 1 and A 2 .
(3) Perform the divide and conquer algorithm recursively to compute conv(A 1 ) and
conv(A 2 ).
(4) Merge the two convex hulls to obtain the convex hull of A.
end of divide and conquer
Figure 4.12 The divide and conquer algorithm58
Chapter 4. Geometric Concepts
The partition required in Step (2) can be easily computed if the points are presorted
(which takes time O(n log n)). So, the only critical step to be examined is Step (4).
When merging two hulls we have to add two edges (the so-called upper and lower
bridges) and eliminate all edges of conv(A 1 ) and conv(A 2 ) that are not edges of
conv(A). To find these bridges we can exploit the fact that due to the sorting step
we know the leftmost point of A 1 and the rightmost point of A 2 (w.l.o.g., A 1 is left of
A 2 ). Starting at these points it is fairly simple to see that the edges to be eliminated
can be readily identified and that no other edges are considered for finding the bridges.
Therefore, the overall time needed to merge convex hulls during the algorithm is linear
in n. Due to Theorem 2.9, this establishes the O(n log n) worst case time bound for the
divide and conquer approach.
Kirkpatrick & Seidel (1986) describe a refinement of the divide and conquer ap-
proach to derive a convex hull algorithm which has worst case running time O(n log v)
where v is the number of vertices of the convex hull.
4.3.3 Throw-Away Principles
It is intuitively clear that when computing the convex hull of a set A not all points
are equally important. With high probability, points in the “interior” of A will not
contribute to the convex hull whereas points near the “boundary” of A are very likely
vertices of the convex hull. Several approaches make use of this observation in that they
eliminate points before starting the true convex hull computation.
If we consider a convex polygon whose vertices are contained in the set A then all points
inside this polygon can be discarded since they cannot contribute to the convex hull. In
Akl & Toussaint (1978) an algorithm is given that makes use of this fact.
procedure throw away
(1) Compute the points a xmax , a xmin , a ymax , and a ymin with maximal (minimal) x-,
resp. y-coordinate.
(2) Discard all points inside the convex polygon given by these four points and identify
four regions of points to be considered. The regions are associated with the four
edges of the polygon.
(3) For each subregion, determine the convex hull of points contained in it.
(4) Construct conv(A) from these four convex hulls.
end of throw away
Any of the convex hull algorithms could be used in Step (3). Akl and Toussaint basically
use Graham’s scan modified in a way that no angles have to be computed.
In a refined version Devroye & Toussaint (1981) compute four additional points,
namely those with maximal (minimal) coordinate sum x i +y i , resp. coordinate difference
x i − y i . Elimination is now performed using the convex polygon given by these eight
points.59
4.3. Convex Hulls
a ymax
a xmin
a xmax
a ymin
Figure 4.13 A throw-away principle
4.3.4 Convex Hulls from Maximal Vectors
Every point in R 2 can be viewed as the origin of a coordinate system with axes parallel
to the x- and y-directions. This coordinate system induces four quadrants. A point is
called maximal with respect to the set A if at least one of these quadrants (including
the axes) does not contain any other point of A. It is easily seen that every vertex of
conv(A) is maximal. Namely, let x be a vertex of conv(A) and assume that each of the
corresponding quadrants contains a point of A. Then x is contained in the convex hull of
these points and therefore cannot be a vertex of the convex hull of A. Kung, Luccio
& Preparata (1975) give an algorithm to compute the maximal vectors of a point
set in the plane in worst case time O(n log n). This leads to the following O(n log n)
algorithm for computing the convex hull.
procedure maximal vector hull
(1) Compute the set S of maximal vectors with respect to A.
(2) Let A  = S.
(3) Compute the convex hull of A  using any of the O(n log n) worst case time algo-
rithms.
(4) conv(A) = conv(A  ).
end of maximal vector hull
The expectation is that very many points can be discarded in Step (1).60
Chapter 4. Geometric Concepts
Figure 4.14 Maximal vectors
4.3.5 A New Elimination Type Algorithm
We are now going to discuss a further elimination type algorithm that uses a particularly
simple discarding mechanism. This algorithm is best suited for large dense point sets
distributed uniformly in a rectangle. It is discussed in full detail in Borgwardt,
Gaffke, Jünger & Reinelt (1991).
Assume that the points of A are contained in the unit sqaure, i.e., their coordinates are
between 0 and 1. The function h : [0, 1] × [0, 1] → R is defined by h(x) = h(x 1 , x 2 ) =
min{x 1 , 1 − x 1 } · min{x 2 , 1 − x 2 }.
The basic idea is to compute the convex hull of a small subset S of A such that conv(S) =
conv(A) with high probability. The value h(x) will express whether x is likely to be an
interior point of conv(A). With increasing h(x) the probability that x can be eliminated
as a candidate for a vertex of the convex hull increases.
We will discard points based on this function in a first phase and compute the convex
hull for the remaining points. It will turn out that we cannot guarantee that we have
obtained conv(A) this way. Therefore, in a second phase we have to check correctness
and possibly correct the results.
The following is a sketch of our elimination type algorithm where CH is some algorithm
to compute the convex hull of a set of points in the plane.
procedure elim hull
Phase1
(1) Choose a suitable parameter α.
(2) Let S α = {a i ∈ A | h(a i ) ≤ α}.
(3) Apply CH to compute the convex hull of S α .4.3. Convex Hulls
61
(4) Compute the minimum γ such that conv(S α ) ⊇ {x ∈ [0, 1] × [0, 1] | h(x) > γ}.
(5) If α ≥ γ then STOP (conv(S α ) = conv(A)), otherwise perform Phase 2.
Phase2
(6) Compute conv(S γ ) where S γ = {a i ∈ A | h(a i ) ≤ γ}. Now conv(S γ ) = conv(A).
end of elim hull
Figure 4.15 gives an illustration of the algorithm. The set S α is given by the solid
points (‘•’) and its convex hull by solid lines. The broken curve defines the set S γ and
the additional points to be considered in Phase 2 are shown as small circles (‘◦’). The
extreme points of the correct convex hull resulting from Phase 2 are obtained by adding
one point in the north-east corner.
A detailed analysis shows that Step (4) can be performed in linear time. Therefore the
worst case running time of this algorithm is given by the worst case running time of CH
(independent of α).
Figure 4.15 Illustration of the algorithm
The analysis of the average time complexity of this algorithm exhibits some interesting
consequences. If α is chosen carefully, then in certain random models only very few
points are contained in S α and with very high probability Phase 2 is not needed. In
particular, one can obtain a speed-up theorem for convex hull algorithms in the following
sense.62
Chapter 4. Geometric Concepts
Theorem 4.5 Let A be a set of n random points generated independently from the
uniform distribution on the unit square [0, 1]×[0, 1]. For any algorithm CH with polyno-
mial worst-case running time the two-phase method has linear expected running time.
For a detailed analysis of the algorithm and the proper choice of α as well as for
a discussion of the computation of convex hulls for random points in the unit disk
D(0, 1) = {x = (x 1 , x 2 ) ∈ R 2 | x 21 + x 22 ≤ 1} we refer to Borgwardt, Gaffke,
Jünger & Reinelt (1991). Our approach can be generalized to higher dimensions. A
complete coverage of the 3-dimensional case can be found in Thienel (1991).
Assuming a uniform distribution of the n independent points over the unit square, linear
expected time algorithms have been given by Bentley & Shamos (1978), Akl &
Toussaint (1978), Devroye (1980), Devroye & Toussaint (1981), Kirkpatrick
& Seidel (1986), and Golin & Sedgewick (1988). For a survey on these and related
subjects see Lee & Preparata (1984).
We have compared the practical behaviour of five linear expected time algorithms,
namely
[1] the divide and conquer algorithm (4.3.2),
[2] the maximal vectors approach (4.3.4),
[3] the throw-away principle based on eight points (4.3.3),
[4] the throw-away principle based on four points (4.3.3),
[5] the new algorithm.
For algorithms [2], [3], [4], and [5] we apply Graham’s scan to the selected points.
All five algorithms have been implemented as Pascal programs on a SUN SPARCsta-
tion SLC which is about four times slower than the SPARCstation 10/20. We have tried
to put the same efforts into all five programs to make the comparison as fair as possible.
For instance, in all throw-away type algorithms we found that the following trick helped
to reduce the computation time. As soon as the elimination area has been determined
(a closed polygon in case [3] and [4] and the curve defined by the function h in [5]) we
inscribe the biggest possible rectangle with vertical and horizontal sides into this area.
Assume that this rectangle (always a square in case [5]) has vertices (x 1 , y 1 ), (x 2 , y 1 ),
(x 2 , y 2 ) and (x 1 , y 2 ). The elimination criterion is satisfied for a point with coordinates
(x, y) if x 1 ≤ x ≤ x 2 and y 1 ≤ y ≤ y 2 which takes only four comparisons to check. Only
if this criterion fails, we have to check the actual elimination criterion which, e.g., in
case [5] amounts to checking which quadrant (x, y) lies in, and depending on this, up
to two additions, one multiplication, and one comparison.
Figure 4.16 shows the computation times of the algorithms for computing the convex
hull of point sets in the unit square drawn independently form a uniform distribution.
The curves are based on 10 sample problems for each problem size n = 1 000, 2 000, . . .,
10 000, 20 000, . . . , 100 000, 200 000, . . . , 1 000 000. In our opinion, these curves should
be interpreted as follows. When doing practical computations, the throw-away principle
is superior compared to the divide and conquer algorithms. The four point method is
slightly better than the eight point method.
For our experiments with practical traveling salesman problems in the plane we have
coded the algorithm as follows. The points are mapped to the unit square by horizontal63
4.3. Convex Hulls
20
15
10
5
0
0
200000
400000
[1]
[2]
600000
[3]
[4]
800000
1000000
[5]
Figure 4.16 Comparison of convex hull algorithms
and vertical scaling and appropriate offsets. This is done in such a way that each of
the four sides of the square contains at least one problem point. Then the elimination
algorithm is performed using α = 4 log n/n. However, in the range of problem sizes
we had available, the new algorithm does not pay off since the convex hull of some
ten thousands of points can be computed in so little time that it is negligibly small
compared to the other algorithms employed for problem solving. Therefore we do not
report computing times for TSPLIB problems here.Chapter 5
Candidate Sets
In many practical applications it is required to find reasonably good tours for a traveling
salesman problem in short time. When designing fast heuristics, one is faced with the
problem that, in principle, very many connections need to be considered. For example,
in the traveling salesman problem fnl4461, tours have to be constructed by selecting
4461 out of 9,948,030 possible connections. Standard implementations of heuristics
consider all these connections which leads to substantial running times.
On the other hand, it is intuitively clear, that most of the possible connections will not
occur in short tours because they are too long. It is therefore a reasonable idea, which
we will exploit extensively in the sequel, to restrict attention to “promising” edges and
to avoid considering long edges too frequently. To this end we employ several types of
candidate sets from which edges are taken with priority in the computations.
In geometric problem instances one has immediate access to long edges because their
length is related to the location of the points. In general, for problems given by a
distance matrix, already time Ω(n 2 ) has to be spent to scan all edge lengths. We will
discuss three types of candidate sets in this chapter. The first one is applicable in
general, but can be computed very fast for geometric instances. The other two sets can
only be computed for geometric instances.
5.1 Nearest Neighbors
It can be observed that most of the edges in good or optimal tours connect nodes to near
neighbors. For a TSP on n nodes and k ≥ 1, we define the corresponding k nearest
neighbor subgraph G k = (V, E) by setting
V ={1, 2 . . . , n},
E ={uv | v is among the k nearest neighbors of u}.
For example, an optimal solution for the problem pr2392 can be found within the
8 nearest neighbor subgraph and for pcb442 even within the subgraph of the 6 nearest
neighbors.
Figure 5.1 shows the 10 nearest neighbor subgraph for the problem u159. This subgraph
contains an optimal tour.
G. Reinelt: The Traveling Salesman, LNCS 840, pp. 64-72, 1994.
 Springer-Verlag Berlin Heidelberg 19945.1. Nearest Neighbors
65
Figure 5.1 The 10 nearest neighbor subgraph for u159
A straightforward computation of the k nearest neighbors by enumeration takes time
Ω(n 2 ) for fixed k. The following proposition shows that for Euclidean problem instances
we can exploit the Delaunay graph for nearest neighbor computations. The discussion
applies to other metrics as well.
Proposition 5.1 Let P i and P j be two generators.
(i) If the straight line connecting P i and P j intersects the interior of the Voronoi
region of a generator P l different from P i and P j then d(P i , P l ) < d(P i , P j ).
(ii) If the smallest number of edges on a path from P i to P j in the Delaunay graph
is k then there exist at least k − 1 generators P l different from P i and P j with
d(P i , P l ) < d(P i , P j ).
Proof. For part (i) suppose that the line intersects the boundary of VR(P l ) in the
points T 1 and T 2 where w. l. o. g. d(P i , T 1 ) < d(P i , T 2 ). By definition we have d(P l , T 1 ) ≤
d(P i , T 1 ) and d(P l , T 2 ) ≤ d(P j , T 2 ). Since P i , P l , and T 1 are distinct we obtain
d(P i , P l ) ≤ d(P i , T 1 ) + d(P l , T 1 )
< d(P i , T 1 ) + d(T 1 , P j )
= d(P i , P j ).
Part (ii) is an immediate corollary.
Therefore, to compute the k nearest neighbors of some generator P i we only have to
examine generators which are connected to P i in the Delaunay graph by a path of length66
Chapter 5. Candidate Sets
at most k − 1. Since, for random instances, the expected degree of a node in this graph
is six, we can expect linear running time of this procedure for fixed k.
The fast algorithm for Euclidean problem instances is the following.
procedure nearest neighbors
(1) Compute the Delaunay graph and initialize the empty candidate list.
(2) For i = 1, 2, . . . , n compute the k nearest neighbors of node i by breadth-first
search in the Delaunay graph starting at node i. Add the corresponding edges to
the candidate set.
end of nearest neighbors
5.0
4.0
3.0
2.0
1.0
0.0
0
1000
2000
3000
4000
5000
6000
Figure 5.2 CPU times for computing 10 nearest neighbor graph
Figures 5.2 and 5.3 show the running times for the 10 nearest neighbor computations
for our set of sample problems as well as the number of edges in the resulting candidate
sets. A good approximation for the cardinality of the 10 nearest neighbor candidate set
is 6n.
It is interesting to note that computation of the 10 nearest neighbor set for problem
pr2392 takes 22.3 seconds if the trivial algorithm is used. If the complete distance
matrix is stored this time reduces to 18.8 seconds which is still substantially more than
the 0.5 seconds needed with the Delaunay graph. It should be kept in mind that we
do not use complete distance matrices in this tract, but that much CPU time can be
gained if they are available.67
5.2. Candidates Based on the Delaunay Graph
70000
60000
50000
40000
30000
20000
10000
0
0
1000
2000
3000
4000
5000
6000
Figure 5.3 Number of edges of the 10 nearest neighbor graphs
There are further approaches for an efficient computation of nearest neighbors for geo-
metric problems instances, e.g., probabilistic algorithms and algorithms based on k-d-
trees (Bentley (1990)).
5.2 Candidates Based on the Delaunay Graph
In particular if point sets exhibit several clusters, the k nearest neighbor subgraph is not
connected and many edges to form good tours are missing. Here the Delaunay graph
should help since it contains important connections between the clusters.
Though it may seem to be true at a first glance, the Delaunay graph itself does not
necessarily contain a Hamiltonian tour. For example, in the case where all points
are on a line the Delaunay graph is just a path. There are also examples where the
Delaunay graph is a triangulation, but does not contain a tour or even a Hamiltonian
path (Dillencourt (1987a,1987b)).
First experiments have indicated that the Delaunay graph provides a candidate set too
small. We therefore decided to augment it using transitive relations in the following
way. If the edges {i, j} and {j, k} are contained in the Delaunay graph then we also
add edge {i, k} to the candidate set. We call this set Delaunay candidate set. The
cardinality of this set can be quite large. For example, if n − 1 generators are located
on a circle and one generator is at the center of this circle then the Delaunay candidate
set is the complete graph on n nodes. Figure 5.4 shows the Delaunay candidate set for
problem u159.68
Chapter 5. Candidate Sets
Figure 5.4 The Delaunay candidate set for u159
The following procedure computes the Delaunay candidate set.
procedure Delaunay candidates
(1) Compute the Delaunay graph and initialize the candidate set with the edges of
the Delaunay graph.
(2) For every node i = 1, 2, . . . , n do
(2.1) For every two nodes j and k adjacent to i in the Delaunay graph add edge
{j, k} to the candidate set if it was not a candidate edge before.
end of Delaunay candidates
For random points in the plane, empirical observations show that we can expect about
9n to 10n edges in this candidate set.
Figure 5.4 illustrates that the candidate set is rather dense. Therefore we have to expect
more than 9n edges in this candidate set for practical problem instances. Furthermore,
due to long edges in the Delaunay graph, the candidate set may contain many long
edges. To avoid long edges, we usually first run a fast heuristic to compute an initial
tour (as the space filling curves heuristic described in Chapter 8) and then eliminate
all edges from the candidate set that are longer than the longest edge in this tour. For
dense point sets most long edges will be eliminated, for clustered point sets the edges
connecting the clusters will be kept. In general, however, elimination of long edges is
not critical for the performance of our heuristics.69
5.2. Candidates Based on the Delaunay Graph
5.0
4.0
3.0
2.0
1.0
0.0
0
1000
2000
3000
4000
5000
6000
Figure 5.5 CPU times for computing Delaunay candidate sets
70000
60000
50000
40000
30000
20000
10000
0
0
1000
2000
3000
4000
5000
6000
Figure 5.6 Number of edges of Delaunay candidate sets
Figures 5.5 and 5.6 give the CPU times necessary for computing the Delaunay candidate
set and the cardinality of this set. CPU time and size of the candidate set depend highly
on the point configuration and not only on the problem size. For a random problem on70
Chapter 5. Candidate Sets
15,000 nodes we obtained 44,971 (i.e., 2.99 · n) edges in the Delaunay graph and 147,845
(i.e., 9.86 · n) edges in the Delaunay candidate set.
5.3 Other Candidate Sets
We discuss a further candidate set that is easily computed and does not require such
sophisticated computations as the Delaunay graph.
The idea is to obtain near neighbors without too much effort just based on coordinates.
We outline the procedure for horizontal coordinates. We sort the points with respect to
their x-coordinates. For every point i we consider the points that appear right before or
after i in this sorted sequence. To limit the amount of work we only take those points
into account which appear at most w positions before or at most w positions after i.
Among these points we compute the k points nearest to i and choose the corresponding
edges as candidate edges.
The complete heuristic also takes the vertical coordinates into account. This is accom-
plished as follows. The parameter w specifies a search width as sketched above, the
parameter k gives the number of candidate edges that are selected from each node.
procedure candidate heuristic
(1) Initialize two sorted lists of the points by sorting them with respect to their x-
coordinates and with respect to their y-coordinates. For every point i let i x and
i y be its respective positions in the lists.
(2) Initialize the empty candidate list.
(3) For every node i = 1, 2, . . . , n do
(3.1) Let Q 1 = {j|j x ∈ {i x + 1, . . . , i x + w}, j y ∈ {i y + 1, . . . , i y + w}}, Q 2 = {j|j x ∈
{i x + 1, . . . , i x + w}, j y ∈ {i y − 1, . . . , i y − w}}, Q 3 = {j|j x ∈ {i x − 1, . . . , i x −
w}, j y ∈ {i y − 1, . . . , i y − w}}, and Q 4 = {j|j x ∈ {i x − 1, . . . , i x − w}, j y ∈
{i y + 1, . . . , i y + w}}.
(3.2) Add edges from node i to its two nearest neighbors in every set Q j , j =
1, 2, 3, 4 (or less, if Q j contains fewer than two elements) to the candidate set
and remove the corresponding nodes from the sets. (We incorporate this step
to have candidate edges connecting i to each of the sets Q j .)
(3.3) Compute the k − l nearest neighbors of i in the reduced set Q 1 ∪ Q 2 ∪ Q 3 ∪ Q 4
(where l is the number of edges selected in (3.2)) and add the corresponding
edges to the candidate set.
end of candidate heuristic
Figure 5.7 shows the candidate set obtained with this heuristic for problem instance u159
(parameters were k = 10 and w = 20). It contains 94% of the edges of the 10 nearest
neighbor graph. Because of pathological conditions at the border of point sets we may
also incur a number of long edges in this heuristic. These could be eliminated as above,
but keeping them has no significant effects as our experiments showed.71
5.3. Other Candidate Sets
Figure 5.7 Result of candidate heuristic for u159
5.0
4.0
3.0
2.0
1.0
0.0
0
1000
2000
3000
4000
5000
Figure 5.8 CPU times for candidate heuristic
600072
Chapter 5. Candidate Sets
70000
60000
50000
40000
30000
20000
10000
0
0
1000
2000
3000
4000
5000
6000
Figure 5.9 Number of edges of heuristic candidate sets
Figures 5.8 and 5.9 display the CPU times for computing this candidate set and the
number of its edges, respectively. Both, sizes of the candidate sets and the times to
compute them, are very well predictable depending on the size of the problem instance.
An interesting further candidate set can be obtained by taking the union of the Delaunay
graph and an appropriate nearest neighbor graph. Here we have the short connections
as well as the important connections between distant clusters.
We will investigate the usefulness of our candidate sets in the subsequent chapters.Chapter 6
Construction Heuristics
Starting with this chapter we will now consider computational aspects of the traveling
salesman problem. For the beginning we shall consider pure construction procedures,
i.e., heuristics that determine a tour according to some construction rule, but do not
try to improve upon this tour. In other words, a tour is successively built and parts
already built remain in a certain sense unchanged throughout the algorithm.
Many of the construction heuristics presented here are known and computational results
are available (Golden & Stewart (1985), Arthur & Frendeway (1985), Johnson
(1990), Bentley (1992)) We include them for the sake of completeness of this tract
and for having a reference to be compared with other algorithms on our sample problem
instances. Moreover, most evaluations of heuristics performed in the literature lack
from the fact that either fairly small problem instances or only random instances were
examined.
The following types of algorithms will be discussed:
– nearest neighbor heuristics,
– insertion heuristics,
– heuristics based on spanning trees, and
– savings heuristics.
This does not cover by far all the approaches that have been proposed. But we think
that the ideas presented here provide the reader with the basic principles that can also
be adapted to other combinatorial optimization problems.
For the following, we will always assume that we are given the complete undirected
graph K n with edge weights c uv for every pair u and v of nodes. For ease of notation
we will denote the node set by V and assume that V = {1, 2, . . . , n}. The question to
be addressed is to find good Hamiltonian tours in this graph.
6.1 Nearest Neighbor Heuristics
This heuristic for constructing a traveling salesman tour is near at hand. The salesman
starts at some city and then visits the city nearest to the starting city. From there he
visits the nearest city that was not visited so far, etc., until all cities are visited, and
the salesman returns to the start,
6.1.1 The Standard Version
Formulated as an algorithm we obtain the following procedure.
G. Reinelt: The Traveling Salesman, LNCS 840, pp. 73-99, 1994.
 Springer-Verlag Berlin Heidelberg 199474
Chapter 6. Construction Heuristics
procedure nearest neighbor
(1) Select an arbitrary node j, set l = j and T = {1, 2, . . . , n} \ {j}.
(2) As long as T  = ∅ do the following.
(2.1) Let j ∈ T such that c lj = min{c li | i ∈ T }.
(2.2) Connect l to j and set T = T \ {j} and l = j.
(3) Connect l to the first node (selected in Step (1)) to form a tour.
end of nearest neighbor
This procedure runs in time Ω(n 2 ). A possible variation of the standard nearest neighbor
heuristic is the double-sided nearest neighbor heuristic where the current path can
be extended from both of its endnodes.
No constant worst case performance guarantee can be given, since the following theorem
due to Rosenkrantz, Stearns & Lewis (1977) holds.
Theorem 6.1 For every r > 1 and arbitrarily large n there exists a TSP instance on
n cities such that the nearest neighbor tour is at least r times as long as an optimal
tour.
In addition, Rosenkrantz, Stearns and Lewis (1977) show that for arbitrarily
large n there exist TSP instances on n nodes such that the nearest neighbor solution is
Θ(log n) times as long as an optimal Hamiltonian cycle. This results still holds if the
triangle inequality is satisfied. Therefore it also applies to metric problem instances.
Figure 6.1 A nearest neighbor tour for rd1006.1. Nearest Neighbor Heuristics
75
If one displays nearest neighbor tours one realizes the reason for their poor performance.
The procedure proceeds very well and produces connections with short edges in the
beginning. But, as can be seen from a graphics display, several cities are “forgotten”
during the course of the algorithm. They have to be inserted at high cost in the end.
Figure 6.1 shows a typical nearest neighbor tour.
Though usually rather bad, nearest neighbor tours have the advantage that they only
contain a few severe mistakes, but there are long segments connecting nodes with short
edges. Therefore, such tours can serve as good starting tours for subsequently performed
improvement methods and it is reasonable to put some effort in designing heuristics that
are based on the nearest neighbor principle. We will comment on improvement methods
in the next chapter. The standard procedure itself is easily implemented with a few lines
of code. But, since running time is quadratic, we describe some variants to speed up
and/or improve the standard nearest neighbor search.
6.1.2 Exploiting the Delaunay Graph
We have seen in Chapter 5 that the Delaunay graph can be used to speed up nearest
neighbor computations. We can apply these results here, too. Namely, when searching
the nearest neighbor of node l in step (2.1) among the nodes which are not yet contained
in the partial tour, we can use the principle of section 5.1 to generate the k-th nearest
neighbor of l for k = 1, 2, . . . , n until a node is found that is not yet connected. Due to
the properties of the Delaunay graph we should find this neighbor examining only a few
edges of the graph in the neighborhood of l. Since in the last steps of the algorithm we
have to collect the forgotten nodes (which are far away from the current node) it makes
no sense to use the Delaunay graph any further. So, for connecting the final nodes we
just use a simple enumeration procedure.
We have conducted several experiments to see how many neighbors of the current node
are examined and to what depth the breadth-first search to find the nearest neighbor is
performed.
Figure 6.2 shows the average depth in the breadth-first search tree needed to find the
next neighbor. The average depth varies between 3 and 5 for real-world problems and
is about 3.5 for random problems.
Figure 6.3 displays the average number of nodes examined to find the next neighbor,
which is then also the number of necessary distance evaluations per node. Here real-
world problems behave better than random problems.
Furthermore, we examined how search depth and number of neighbors to be examined
develop during the heuristic. Figures 6.4 and 6.5 depict the search depth and the number
of examined nodes, respectively, obtained during the execution of the nearest neighbor
heuristic on the problem pr2392.
We see that at the majority of nodes next neighbors can indeed be found in the local
neighborhood with a search depth below five in most cases. Only sometimes a large
part of the Delaunay graph has to be explored to find the next node. Note that we do
not use the Delaunay strategy for inserting the final 100 nodes, since the overhead for
identifying the nearest neighbor increases significantly at the end of the procedure.76
Chapter 6. Construction Heuristics
6.0
5.0
4.0
3.0
2.0
1.0
0.0
0
5000
10000
15000
20000
Figure 6.2 Average search depth
50
45
40
35
30
25
20
15
10
0
5000
10000
15000
20000
Figure 6.3 Average number of examined nodes
The worst case running time of this implementation is still quadratic. The running
times for the sample problems will be given below together with the running times for
all other variants.77
6.1. Nearest Neighbor Heuristics
35
30
25
20
15
10
5
0
0
500
1000
1500
2000
2500
Figure 6.4 Search depth for pr2392
2000
1500
1000
500
0
0
500
1000
1500
2000
Figure 6.5 Number of examined nodes for pr2392
250078
Chapter 6. Construction Heuristics
6.1.3 Precomputed Neighbors
Suppose we have computed a candidate subgraph representing “reasonable” connections,
e.g., the k nearest neighbor subgraph. A speed up of the nearest neighbor procedure
is then possible if we first look for nearest neighbors of a node within its adjacent
nodes in the subgraph. This way we reduce the exhaustive neighbor search and the
necessary costly distance computations. If all nodes adjacent in the subgraph are already
contained in the partial tour then we compute the nearest neighbor among all free nodes.
This modification does not improve worst case time complexity but should be faster
when run on practical problems.
Note, that even if the subgraph is obtained by computing the k nearest neighbors this
modified routine and the standard routine will usually come up with different results.
This is due to the fact that we may proceed from the current node l to a node j which
is not its nearest neighbor among all free nodes. This can occur, if edge {l, j} is in the
candidate set and l is among the k nearest neighbors of j, but j is not among the k
nearest neighbors of l.
6.1.4 Neighbors of Predecessors
In this modification we also use a precomputed set of candidate edges but apply the
following variant for the neighbor search. If all nodes adjacent to the current node in
the subgraph are already contained in the partial tour then we look for free neighbors
(in the candidate subgraph) of the predecessor of the current node. If this fails too,
we go back to the predecessor of the predecessor, etc. The backtrack to a predecessor
is only done a limited number of times, say 20 times, because then free neighbors are
usually already far away from the current node and it should be preferrable to look for
the exact nearest neighbor. Again, worst case time complexity is not affected, but a
significant speed up should be possible.
6.1.5 Insertion of Forgotten Nodes
As the main problem with nearest neighbor heuristics is that in the end nodes have to
be connected at high cost, we try to avoid that nodes become isolated. To do this we
first compute the degree of each node in the chosen candidate subgraph. (Without this
subgraph the procedure would not make sense.)
Whenever a node is connected to the current partial tour we decrease the degrees of its
adjacent nodes (in the subgraph) by 1. If the degree of a free node is decreased below
a specified limit (e.g., 2 or 3) this way, we insert that node immediately into the path.
To this end we look for the best insertion point before or after one of its neighbors in
the candidate subgraph. This way more nodes of degree less than or equal to the given
limit may occur which are also inserted rightaway. The selection of the next nodes in
the algorithm is accomplished as in variant 6.1.3. The worst case time complexity is
still quadratic.6.1. Nearest Neighbor Heuristics
79
6.1.6 Using Rotation Operations
The idea of this heuristic is to try to grow the tour within the candidate subgraph. If
at the current node the tour cannot be extended using candidate edges it is tried to
perform a sequence of rotation operations. Such an operation introduces an unused
subgraph edge from the current node. Since this results in a cycle with the partial tour,
another edge has to be eliminated and we obtain a new last node from which we can
try to extend the path.
k
i
l
Figure 6.6 A rotation operation
Figure 6.6 depicts a rotation operation. If at the current last node l the path cannot
be extended within the subgraph we try to use the subgraph edge {l, k}. To break the
resulting cycle we delete the edge {k, i} and try to extend the path now starting at i.
A sequence of rotations can be performed if also the extension from i fails. If the tour
cannot be extended this way using only subgraph edges a neighbor search like in one of
the previous variants can be performed.
6.1.7 Comparison of Variants
We compare the variants with the standard heuristic implemented according to 6.1.2.
It is clear that the tours produced by the algorithms heavily depend on the choice of
the starting point. Since we cannot look for the best starting point we always chose
to start with node  n 2 , thus giving an unbiased starting node to the heuristics. The
chosen candidate subgraph was the 10 nearest neighbor subgraph for variants 6.1.3
through 6.1.6.
In variant 6.1.4 we examined at most 20 predecessors to extend the path. In variant 6.1.5
forgotten nodes were inserted as soon as they were merely connected to at most three
free nodes. In variant 6.1.6 a sequence of rotations was limited to be composed of at
most five single rotations.
Table 6.7 shows the tour lengths (given as deviation in percent from the best known
lower bounds) obtained by applying the different procedures to our set of test problems.
Variants 6.1.2 through 6.1.6 are denoted by Variant 1 through Variant 5 in this table.
The best solution found for every problem instance is marked with a ‘*’.
The results strongly support variant 6.1.5 which avoids adding too many isolated nodes
in the end. Usually, this decreases the tour length considerably. The quality of the
solutions can be expected to be in the range of 15% to 25% above optimality. In
Johnson (1990) an average excess of 24% over an approximation of the Held-Karp
lower bound (see Chapter 10) is reported for randomly generated problems.80
Chapter 6. Construction Heuristics
Problem
d198
lin318
fl417
pcb442
u574
p654
rat783
pr1002
u1060
pcb1173
d1291
rl1323
fl1400
u1432
fl1577
d1655
vm1748
rl1889
u2152
pr2392
pcb3038
fl3795
fnl4461
rl5934
Average
Variant 1
25.79
26.85
21.28
21.36
29.60
31.02
27.13
24.35
30.43
28.18
22.97
22.30
42.42
25.50
27.65
25.99
25.67
28.37
25.80
24.96
23.63
24.44
25.31
22.93
26.27
Variant 2
16.51*
22.52
17.84*
22.91
21.11
18.75*
24.76
25.18
27.14
27.69
15.44*
21.94
42.96
28.04
20.34*
25.06
26.78
25.54
25.85
27.23
23.53
25.92
23.24
23.41
26.58
Variant 3
25.86
33.06
36.42
18.63
25.90
25.80
27.98
28.96
27.33
27.30
25.54
25.51
30.64
31.55
21.97
20.82
31.90
23.85*
23.22
26.09
28.39
32.85
26.98
24.77
27.04
Variant 4
20.88
15.90*
25.36
13.51*
18.67*
25.98
18.86*
18.16*
24.14*
18.09*
17.33
16.81*
30.23*
19.21*
23.30
19.81*
19.11*
25.62
18.97*
22.68*
19.16*
20.09*
19.53*
18.75*
21.45
Variant 5
28.09
27.84
31.80
27.15
29.16
24.43
27.02
27.97
29.42
29.85
16.09
25.10
48.39
25.71
21.55
23.38
31.89
25.41
23.14
26.20
26.92
35.24
24.99
24.38
28.28
Table 6.7 Results of nearest neighbor variants
1.2
1.0
0.8
0.6
0.4
0.2
0.0
0
1000
2000
[S]
[1]
3000
[2]
4000
[3]
[4]
5000
[5]
Figure 6.8 CPU times for nearest neighbor variants
600081
6.1. Nearest Neighbor Heuristics
CPU times for the complete set of instances are shown in Figure 6.8. The running
times for the variants do not include the time to set up the Delaunay graph or the 10
nearest neighbor subgraph. These times were given in Chapters 4 and 5. To document
the speed up obtained by using the different variants we have also included the running
time for the standard implementation. Variant i is indicated by the number [i], the
standard implementation is indicated by [S].
Figure 6.8 clearly visualizes that already simple algorithms of quadratic time complexity
take quite some time when applied to large problems. The comparison of the variants
gives a clear picture. All the variants are much faster than the standard nearest neighbor
algorithm (even if the preprocessing times would be included). When considering larger
random problems (results are not displayed here), variants 2, 3, and 4 seem to exhibit
a quadratic component in their running time while variants 1 and 5 seem to have
subquadratic running times.
6.1.8 Stability of Nearest Neighbor Heuristics
Since we have performed only one run of each heuristic for every sample problem (start-
ing with node  n 2 ) we cannot be absolutely sure that Table 6.7 gives a correct assessment
of the five heuristics. We have therefore examined the average quality of each variant for
three sample problems. To this end we have performed each heuristic for every starting
node l = 1, 2, . . . , n.
Table 6.9 shows the results. Each line corresponds to one variant and gives (in that
sequence) the length of the best, resp. worst tour, the average tour length obtained, the
span between best and worst tour (i.e., worst quality − best quality), and the standard
deviation.
Variant
lin318
1
2
3
4
5
pcb442
1
2
3
4
5
u1060
1
2
3
4
5
Minimum Maximum Average Span Deviation
16.89
18.58
20.86
12.96
21.54 39.82
35.96
37.66
29.24
36.94 24.89
25.88
27.12
20.25
29.37 22.92
17.38
16.80
16.28
15.40 2.99
2.96
3.65
3.00
3.07
18.01
16.60
17.19
11.77
15.67 38.15
32.23
34.31
30.03
36.30 29.66
22.11
26.06
16.63
25.81 20.14
15.63
17.12
18.27
20.63 3.63
2.84
3.44
2.74
3.26
22.33
21.25
24.18
19.04
24.98 36.51
39.19
37.99
28.46
37.44 26.11
30.05
28.89
22.95
30.03 14.17
17.94
13.81
9.42
12.46 1.93
3.45
2.21
1.70
2.29
Table 6.9 Sensitivity analysis for nearest neighbor variants
The results verify that insertion of forgotten neighbors leads to the best results. The
average quality of the tours obtained this way is substantially better than for the other82
Chapter 6. Construction Heuristics
four variants. The other variants perform more or less the same. The span is consider-
able, the quality of the tours strongly depends on the choice of the starting node.
6.2 Insertion Heuristics
A further intuitive approach is to start with tours on small subsets (including trivial
“tours” on one or two nodes) and then extend these tours by inserting the remaining
nodes. This principle is realized by the following procedure.
procedure insertion
(1) Select a starting tour through k nodes v 1 , v 2 , . . . , v k (k ≥ 1) and set W = V \
{v 1 , v 2 , . . . , v k }.
(2) As long as W  = ∅ do the following.
(2.1) Select a node j ∈ W according to some criterion.
(2.2) Insert j at some position in the tour and set W = W \ {j}.
end of insertion
Using this principle a tour is built containing more and more nodes of the problem until
all nodes are inserted and the final Hamiltonian tour is found.
6.2.1 Standard Versions
Of course, there are several possibilities for implementing such an insertion scheme. The
main difference is the determination of the order in which the nodes are inserted. The
starting tour is usually just some tour on three nodes or, an edge (k = 2), or even a
loop containing only one node (k = 1). We will consider also another type of starting
tour below. The selected node to be inserted is usually inserted into the tour at the
point causing shortest increase in the length of the tour.
We say that a node is a tour node if it is already contained in the partial tour. For
j ∈ W 
we define d min (j) = min{c ij | i ∈ V \ W }, d max (j) = max{c ij | i ∈ V \ W }, and
s(j) = i∈V \W c ij .
The following possibilities for extending the current tour are considered.
6.2.1.1 Nearest Insertion
Insert the node that has the shortest distance to a tour node, i.e., select j with d min (j) =
min{d min (l) | l ∈ W }.
6.2.1.2 Farthest Insertion 1
Insert the node whose minimal distance to a tour node is maximal, i.e., select j with
d min (j) = max{d min (l) | l ∈ W }.
6.2.1.3 Farthest Insertion 2
Insert the node that has the farthest distance to a tour node, i.e., select j with d max (j) =
max{d max (l) | l ∈ W }.6.2. Insertion Heuristics
83
6.2.1.4 Farthest Insertion 3
Insert the node whose maximal distance to a tour node is minimal, i.e., select j with
d max (j) = min{d max (l) | l ∈ W }.
6.2.1.5 Cheapest Insertion 1
Among all nodes not inserted so far, choose a node whose insertion causes the lowest
increase in the length of the tour. I.e., among all nodes not inserted so far, choose a
node which can be inserted causing the lowest increase in the length of the tour.
6.2.1.6 Cheapest Insertion 2
In the cheapest insertion heuristic, we have to know for every node not in the tour its
cheapest insertion point. Update of this information is expensive (see below). In this
variant, we only perform a partial update of the best insertion points in the following
sense. Suppose node j has just been inserted into the partial tour. This may have the
effect that the best insertion point changes for a non-tour node, say l. Now, we do not
consider all possibilities to insert l, but only insertions before or after j and a limited
number k of j’s successors and predecessors. This has the consequence, that, for some
nodes, not necessarily the best insertion point is determined.
6.2.1.7 Random Insertion
Select the node to be inserted at random.
6.2.1.8 Largest Sum Insertion
Insert the node whose sum of distances to tour nodes is maximal, i.e., select j with
s(j) = max{s(l) | l ∈ W }. This is equivalent to choosing the node with maximal
average distance to tour nodes.
6.2.1.9 Smallest Sum Insertion
Insert the node whose sum of distances to tour nodes is minimal, i.e., select j with
s(j) = min{s(l) | l ∈ W }. This is equivalent to choosing the node with minimal average
distance to tour nodes.
There are also variants of these ideas where the node selected is not inserted at cheapest
insertion cost but as a neighbor of that tour node that is nearest to it. These variants
are usually named “addition” instead of insertion. Bentley (1992) reports that the
results are slightly inferior.
All heuristics except for cheapest Insertion have running time O(n 2 ). Cheapest Insertion
can be implemented to be executed in time O(n 2 log n) by storing for each external
node a heap based on the insertion cost at the possible insertion points. Because this
procedure requires O(n 2 ) space it cannot be used for large problem instances. The fast
version of cheapest insertion runs in time O(n 2 ) because of the limited update.
We give an illustration of insertion principles in Figure 6.10 for a Euclidean problem
instance. In the next step nearest insertion adds node i, farthest insertion adds node j,
and cheapest insertion adds node k to the tour.84
Chapter 6. Construction Heuristics
j
i
k
Figure 6.10 Illustration of insertion heuristics
Nearest insertion and cheapest insertion tours are less than twice as long as an optimal
tour if the triangle inequality holds (Rosenkrantz, Stearns & Lewis (1977)). It
can also be shown that there exist classes of problem instances for which the length of
the heuristic solution is 2 − n 2 times longer than the optimal tour, thus proving that
these approximation results are tight.
Recently it was shown (Hurkens (1991)) that for random or farthest insertion there
exist examples where these heuristics yield tours that are 13/2 times longer than an
optimal tour (although the triangle inequality is satisfied).
We have compared the nine insertion heuristics for our set of sample problems. Each
heuristic was started with the cycle ( n 2 ,  n 3 ,  n 4 ) to get unbiased starting conditions.
For the variant of cheapest insertion described in 6.2.1.6 we have set k = 30.
Table 6.11 displays the results (headings 1 through 9 corresponding to the insertion
heuristics 6.2.1.1 through 6.2.1.9). The best solution in each row is marked with a ‘*’.
Farthest insertion 1 performs best for our set of problems followed closely by random
insertion. The fast version of cheapest insertion performs as well as the full version, the
time for doing correct cheapest insertion does not pay off. In fact, the results were the
same except for two cases. However, though reasonable at first sight, cheapest insertion
performs significantly worse that farthest insertion. The relatively good performance of
farthest insertion can be explained when observing the development of the generated
tour: after few steps already, a good global outline of the final tour is obtained. Almost
the same is true for random insertion. An average excess over the Held-Karp bound
of 27% for the nearest insertion and of 13.5% for the farthest insertion procedure is
reported in Johnson (1990) for random problem instances.85
6.2. Insertion Heuristics
Problem
d198
lin318
fl417
pcb442
u574
p654
rat783
pr1002
u1060
pcb1173
d1291
rl1323
fl1400
u1432
fl1577
d1655
vm1748
rl1889
u2152
pr2392
pcb3038
fl3795
fnl4461
rl5934
Average
1
13.19
21.62
12.50
20.89
22.33
10.81
23.04
18.57
21.39
25.84
22.90
31.01
20.28
15.26
21.61
20.18
21.26
23.82
21.09
24.70
23.12
19.61
21.10
27.40
20.98
2
3.85*
10.87
5.48
13.83
11.39*
6.89
12.09*
10.85*
12.68
14.22*
23.78
18.89*
8.45*
12.59*
15.17*
17.09*
13.54*
19.10
19.55
14.32*
14.89*
21.97
12.03*
22.17
13.99
3
7.57
18.41
13.37
16.99
22.68
11.33
22.52
24.81
21.52
26.82
27.29
29.30
14.56
20.43
20.04
22.22
25.37
27.74
28.64
28.26
24.54
19.58
27.69
30.12
22.16
4
14.80
24.30
13.37
29.06
26.32
5.94
28.72
27.24
27.55
32.67
29.50
27.80
24.78
20.08
25.21
27.80
33.59
32.70
32.84
33.55
27.84
29.45
28.90
33.42
26.56
5
11.08
18.39
12.39
21.15
19.12
5.79*
16.02
16.61
18.67
21.50
17.01*
24.81
17.98
12.65
17.08
18.83
18.86
21.24
16.12*
20.50
17.08
12.79*
15.97
21.84*
17.23
6
11.08
18.39
12.39
21.15
19.12
5.79*
16.02
16.61
18.67
21.50
17.01*
24.81
17.76
12.65
17.08
18.77
18.86
21.24
16.12*
20.50
17.08
12.79*
15.97
21.84*
17.22
7
8.17
9.18*
3.29*
12.23*
11.64
9.87
13.37
12.50
11.43*
16.58
22.13
20.64
8.47
12.63
18.70
17.69
13.87
17.30*
19.76
16.65
16.69
19.77
12.99
22.71
14.51
8
8.15
20.02
7.84
27.07
23.32
11.30
26.37
23.98
23.94
29.56
31.06
29.30
16.30
23.84
26.66
28.20
29.52
29.99
28.26
31.75
27.57
21.62
28.99
33.56
24.51
9
7.78
16.27
9.04
20.43
22.21
12.64
25.02
25.42
21.58
28.80
18.70
26.56
16.44
20.54
17.97
23.95
24.26
27.53
28.98
28.32
27.28
25.62
28.03
30.36
22.24
Table 6.11 Results of insertion heuristics
Note that the quality of the solutions of the different heuristics is highly problem de-
pendent. Running time will be addressed in the next section.
6.2.2 Fast Versions of Insertion Heuristics
As in the case of the nearest neighbor heuristic we want to give priority to edges from
a candidate set to speed up the insertion heuristics.
To this end we base all our calculations on the edges contained in the candidate set.
E.g., now the distance of a non-tour node v to the current partial tour is infinite if there
is no candidate edge joining v to the tour, otherwise it is the length of the shortest such
edge joining v to the tour. Using this principle the heuristics of the previous chapter
are modified as follows.
6.2.2.1 Nearest Insertion
If there are nodes connected to the current tour by a subgraph edge then insert the node
connected to the tour by the shortest edge. Otherwise insert an arbitrary node.
6.2.2.2 Farthest Insertion 1
Among the nodes that are connected to the tour insert the one whose minimal distance
to the tour is maximal. If all external nodes are not connected to the tour insert an
arbitrary node.86
Chapter 6. Construction Heuristics
6.2.2.3 Farthest Insertion 2
Among the nodes that are connected to the tour insert the one whose distance to the
tour is maximal. If all external nodes are not connected to the tour insert an arbitrary
node.
6.2.2.4 Farthest Insertion 3
Among the nodes that are connected to the tour insert the one whose maximal distance
to the tour is minimal. If all external nodes are not connected to the tour insert an
arbitrary node.
6.2.2.5 Cheapest Insertion 1
Insert a node connected to the current tour by a subgraph edge whose insertion yields
minimal additional length. If no such node exists then compute the cheapest insertion
possibility. Insertion information is only updated for those nodes that are connected to
the node inserted last by a subgraph edge. This way insertion information may become
incorrect for some nodes since it may not be updated.
6.2.2.6 Cheapest Insertion 2
Insert a node connected to the current tour by a subgraph edge whose insertion yields
minimal additional length. If no such node exists then insert an arbitrary node. Update
of insertion information is further simplified as in 6.2.1.6.
6.2.2.7 Random Insertion
Select the node to be inserted at random where priority is given to nodes connected to
the current tour by a subgraph edge.
6.2.2.8 Largest Sum Insertion
For each node compute the sum of lengths of the subgraph edges connecting this node
to the current tour. Insert the node whose sum is maximal. If all external nodes are
not connected to the tour insert an arbitrary node.
6.2.2.9 Smallest Sum Insertion
For each node compute the sum of lengths of the subgraph edges connecting this node
to the current tour. Insert the node whose sum is minimal. If all external nodes are not
connected to the tour insert an arbitrary node.
We have performed the same experiment as for the heuristics in the complete graph.
Results are shown in Table 6.12. Now, the advantages of farthest or random insertion are
lost due to the restricted view. They still perform best but tour quality is significantly
inferior than before. The cheapest insertion variants give some very bad solutions which
is caused by the incomplete update of insertion information.
To visualize the CPU time for insertion heuristics we have compared five variants in
Figure 6.13: Farthest insertion 6.2.1.2 ([1]), Cheapest insertion 6.2.1.5 ([2]), Cheapest
insertion 6.2.1.6 ([3]), Farthest insertion 6.2.2.2 ([4]), Cheapest insertion 6.2.2.5 ([5]).
The diagram shows that standard farthest insertion compares favorably with all cheapest
insertion variants. Speed up using candidate graphs is considerable, but due to inferior
quality there seems to be no point in using these heuristics. This will be further justified
in Chapter 7.87
6.2. Insertion Heuristics
Problem
d198
lin318
fl417
pcb442
u574
p654
rat783
pr1002
u1060
pcb1173
d1291
rl1323
fl1400
u1432
fl1577
d1655
vm1748
rl1889
u2152
pr2392
pcb3038
fl3795
fnl4461
rl5934
Average
1
15.31
25.69
36.20
28.85
22.54
48.59
26.07
19.89*
25.39
28.93
31.24
37.34
30.83
21.61
34.75
28.95
26.05
35.45
28.99
27.01
25.19
35.77
23.47
44.63
29.53
2
7.84*
20.03
31.08
18.59
17.34*
46.22
15.35*
21.30
17.54*
19.28*
25.33
22.46*
31.66
17.81
27.27
23.22
21.07*
25.60*
24.68
23.14*
18.48*
24.96*
16.88*
31.26*
22.85
3
9.41
18.85
33.92
20.16
19.97
36.84*
18.31
29.43
20.42
21.60
26.61
26.82
28.69
20.29
28.95
23.74
21.82
29.58
28.89
27.88
21.47
29.32
17.23
29.81
24.58
4
13.67
23.78
44.57
19.93
18.60
42.62
20.00
20.37
20.78
21.87
27.29
32.97
29.67
20.27
28.19
26.05
23.34
30.32
24.46
28.22
19.67
30.18
20.27
35.55
25.94
5
13.47
42.95
24.36*
29.66
25.28
78.21
24.90
26.54
22.95
34.27
20.91
31.19
85.17
28.08
31.09
33.35
22.90
42.91
21.34*
35.15
25.61
40.31
31.74
51.60
34.33
6
13.47
42.95
24.36*
29.66
25.28
78.81
24.90
26.50
24.07
34.27
20.73*
31.43
94.98
29.89
31.09
35.48
22.90
42.39
21.34*
32.68
25.72
40.62
36.16
48.17
34.91
7
10.42
18.09
26.82
20.07
17.53
49.36
17.47
22.52
18.52
21.84
24.78
26.04
19.07*
20.25
23.67*
22.40*
22.27
31.51
25.03
24.56
20.05
25.80
17.64
32.91
23.28
8
13.80
17.63*
38.90
14.08*
19.02
44.91
16.11
20.74
19.97
22.42
26.81
31.16
27.06
16.51*
29.51
24.38
21.20
28.60
25.06
24.28
20.00
33.85
18.11
33.31
24.48
9
11.13
24.41
31.99
27.33
26.72
54.21
29.58
28.17
25.55
28.86
28.16
35.37
30.59
25.52
36.09
29.23
29.31
35.12
30.82
31.41
28.57
32.41
28.51
37.97
30.29
Table 6.12 Results of fast insertion heuristics
300
250
200
150
100
50
0
0
1000
2000
[1]
3000
[2]
[3]
4000
[4]
5000
[5]
Figure 6.13 CPU times for some insertion heuristics
600088
Chapter 6. Construction Heuristics
6.2.3 Convex Hull Start
The following observation suggests to use a specific starting tour for Euclidean problems.
Let v 1 , v 2 , . . . , v k be located on the boundary of the convex hull of the given points (in
this order). Then, in any optimal tour, this sequence is respected (otherwise the tour
would contain crossing edges and hence could not be optimal). Therefore it is reasonable
to use (v 1 , v 2 , . . . , v k ) as starting tour for the insertion heuristics.
From the results of Chapter 4 we know that convex hulls can be computed very quickly
(in time Θ(n log n)). Therefore, only negligible additional CPU time is necessary to
compute this type of starting tour for the insertion heuristics in the Euclidean case.
Results with the convex hull start using the standard versions of the insertion heuristics
are displayed in Table 6.14.
Problem
d198
lin318
fl417
pcb442
u574
p654
rat783
pr1002
u1060
pcb1173
d1291
rl1323
fl1400
u1432
fl1577
d1655
vm1748
rl1889
u2152
pr2392
pcb3038
fl3795
fnl4461
rl5934
Average
1
12.86
15.08
14.24
16.52
17.24
17.07
16.90
20.05
22.78
21.61
25.58
25.86
14.04
15.34
20.30
20.94
19.83
25.74
19.03
21.26
22.41
24.06
22.21
26.54
19.90
2
6.73
10.82
5.28
10.37*
9.95*
3.05*
12.72
11.10*
10.69
15.44*
21.80
15.10*
5.79*
12.65
15.18*
15.05
10.77*
17.98*
18.26
15.24*
14.31*
21.60
11.94*
20.28*
13.00
3
6.41
19.70
14.95
17.54
23.55
12.40
24.68
25.65
24.71
26.14
25.52
28.57
14.62
21.06
18.72
21.55
25.31
29.40
29.05
28.88
25.57
16.23
29.49
30.27
22.50
4
6.58
16.82
5.65
18.89
18.47
6.38
23.89
21.66
22.79
26.62
26.22
25.74
12.05
18.73
28.25
27.26
24.04
31.63
27.32
27.07
24.62
27.65
27.94
31.89
22.01
5
8.51
11.42
7.61
11.83
14.67
8.15
15.16
14.23
16.65
19.18
14.69*
20.30
9.73
11.73*
18.09
13.23*
16.94
18.72
13.98*
17.52
16.47
13.81*
15.42
21.24
14.55
6
8.51
11.42
7.61
11.83
14.67
8.15
15.16
14.23
16.65
19.18
14.69*
20.30
9.73
11.73*
18.09
13.23*
16.94
18.72
13.98*
17.52
16.47
13.81*
15.42
21.24
14.55
7
4.37*
7.97*
2.77 *
13.62
10.73
6.49
11.90*
13.27
10.39*
18.25
21.03
20.18
8.35
13.19
15.58
15.99
12.03
18.15
19.73
15.83
15.44
18.35
13.07
21.71
13.68
8
7.03
18.01
6.83
19.17
19.83
13.16
22.31
26.71
23.99
28.45
22.06
27.73
13.69
22.48
37.73
26.67
26.52
30.07
27.73
29.87
25.81
25.40
29.14
29.66
23.34
9
7.34
16.39
8.36
22.71
20.62
10.85
23.80
21.76
22.65
26.35
22.37
27.59
17.90
21.66
24.71
24.03
25.48
27.26
27.05
26.18
27.12
19.04
27.67
29.58
22.02
Table 6.14 Results of insertion heuristics with convex hull start
There is a slight improvement in the quality of tours with respect to the starting tour
( n 2 ,  n 3 ,  n 4 ). Farthest and random insertion do not profit very much from the convex
hull start since they generate good globals tours themselves. For the other heuristics,
this starting variant is more important, but still leading to poor final tours.89
6.3. Heuristics Using Spanning Trees
6.2.4 Stability of Insertion Heuristics
As in the case of the nearest neighbor heuristic we also investigated how strongly our
results depend on the choice of the starting tour.
To get an impression of this, we performed experiments for problems d198, lin318, and
pcb442. Each heuristic was started with all possible tours consisting of just two nodes.
Numbers displayed in Table 6.15 have the same meaning as in Table 6.9 for the nearest
neighbor methods.
Variant
d198
1
2
3
4
5
6
7
8
9
lin318
1
2
3
4
5
6
7
8
9
pcb442
1
2
3
4
5
6
7
8
9
Minimum Maximum Average Span Deviation
8.52
1.53
4.75
6.95
7.57
8.25
2.63
5.87
3.83 17.78
10.72
16.90
17.00
15.65
14.18
8.86
19.99
13.24 12.59
6.08
7.70
12.14
10.95
11.01
5.53
11.55
7.75 9.25
9.18
12.15
10.05
8.08
5.93
6.23
14.13
9.41 1.70
2.11
2.07
1.77
1.40
1.30
1.10
3.56
1.73
17.18
5.47
12.93
20.13
13.81
13.81
6.67
16.48
11.99 25.97
13.24
22.38
29.30
22.15
22.15
14.18
26.60
23.34 22.00
9.00
18.72
24.92
18.61
18.69
10.89
21.78
18.92 8.79
7.77
9.45
9.17
8.35
8.35
7.51
10.12
11.35 1.82
1.50
1.30
1.73
1.46
1.47
1.65
1.69
1.86
13.89
9.05
14.93
21.78
12.73
12.73
10.22
20.13
14.22 23.17
18.03
23.99
33.13
21.38
21.55
17.71
32.93
23.64 18.50
13.00
18.45
27.50
17.76
17.80
14.00
25.30
18.56 9.29
8.97
9.06
11.36
8.66
8.82
7.48
12.80
9.41 1.85
1.49
1.10
1.99
1.47
1.54
1.44
2.44
1.52
Table 6.15 Sensitivity analysis for insertion heuristics
Farthest insertion and random insertion also performed best here. Stability of insertion
heuristics is much better than for the nearest neighbor variants. The table also shows
that performance and stability are highly problem dependent.
6.3 Heuristics Using Spanning Trees
The heuristics considered so far construct in their standard versions the tours “from
scratch” in the sense that they do not exploit any additional knowledge about the90
Chapter 6. Construction Heuristics
problem instance. In their fast variants they used the presence of a candidate subgraph
to guide the tour construction.
The two heuristics to be described next use a minimum spanning tree as a basis for
generating tours. They are particularly suited for problem instances obeying the triangle
inequality. In this case performance guarantees are possible. Nevertheless, in principle
they can also be applied to general instances.
Before describing these heuristics, consider the following observation. Suppose we are
given some Eulerian tour containing all nodes of the given problem instance. If the
triangle inequality is satisfied we can derive a Hamiltonian tour which is not longer
than the Eulerian tour.
Let v i 0 , v i 1 , . . . , v i k be the sequence in which the nodes (including repetitions) are visited
when traversing the Eulerian tour starting at v i 0 and returning to v i k = v i 0 . The
following procedure obtains a Hamiltonian tour.
procedure obtain tour
(1) Set Q = {v i 0 }, T = ∅, v = v i 0 , and l = 1.
(2) As long as |Q| < n perform the following steps.
(2.1) If v i l ∈ Q then set Q = Q ∪ {v i l }, T = T ∪ {vv i l }, and v = v i l .
(2.2) Set l = l + 1.
(3) Set T = T ∪ {vv i 0 }.
(4) T is a Hamiltonian tour.
end of obtain tour
Every connection made in this procedure is either an edge of the Eulerian tour or is
a shortcut replacing a subpath of the Eulerian tour by an edge connecting its two
endnodes. This shortcut reduces the length of the tour if the triangle inequality is
satisfied. Hence the resulting Hamiltonian tour cannot be longer than the Eulerian
tour.
Both heuristics start with a minimum spanning tree and differ only in how a Eulerian
graph is generated from the tree.
procedure doubletree
(1) Compute a minimum spanning tree.
(2) Double all edges of the tree to obtain a Eulerian graph.
(3) Compute a Eulerian tour in this graph.
(4) Call obtain tour to get a Hamiltonian tour.
end of doubletree6.3. Heuristics Using Spanning Trees
91
Note that we get multiple edges in Step (2) which we have not allowed in our definition
of graphs in Chapter 2. But it is clear that this does not create any problems. Our graph
data structure is able to handle multiple edges. The running time of the algorithm is
dominated by the time needed to obtain a minimum spanning tree. Therefore we have
time complexity Θ(n 2 ) for the general TSP and Θ(n log n) for Euclidean problems.
If we compute the minimum spanning tree with Prim’s algorithm (Prim (1957)), we
could as well construct a Hamiltonian cycle along with the tree computation. We always
keep a cycle on the nodes already in the tree (starting with the loop consisting of only
one node) and insert the node into the current cycle which is added to the spanning
tree. If this node is inserted at the best possible position this algorithm is identical
to the nearest insertion heuristic. If it is inserted before or after its nearest neighbor
among the cycle nodes, then we obtain the nearest addition heuristic.
Christofides (1976) suggested a better method to make spanning trees Eulerian.
Namely, it is sufficient to add a perfect matching on the odd-degree nodes of the tree.
(A perfect matching of a node set W , |W | = 2k, is a set of k edges such that each
node of W is incident to exactly one of these edges.) After addition of all edges of this
perfect matching, all node degrees are even and hence the graph is Eulerian.
Figure 6.16 illustrates this idea. The solid edges form a spanning tree and the broken
edges form a perfect matching on the odd-degree nodes of the spanning tree.
Figure 6.16 Illustration of spanning tree heuristic
The cheapest way (with respect to edge weights) to obtain a Eulerian graph is to add
a minimum weight perfect matching.92
Chapter 6. Construction Heuristics
procedure christofides
(1) Compute a minimum spanning tree.
(2) Compute a minimum weight perfect matching on the odd-degree nodes of the
tree and add it to the tree to obtain a Eulerian graph.
(3) Compute a Eulerian tour in this graph.
(4) Call obtain tour to get a Hamiltonian tour.
end of christofides
This procedure takes considerably more time than the previous one. Computation of
a minimum weight perfect matching on k nodes can be performed in time O(k 3 ) (Ed-
monds (1965)). Since a spanning tree may have O(n) odd-degree nodes, Christofides’
heuristic has cubic worst case time.
The sequence of the edges in the Eulerian tour is not unique. So one can try to find
better solutions by determining different Eulerian tours. We do not elaborate on this
since the gain to be expected is small.
Since a minimum spanning tree is not longer than a shortest Hamiltonian tour and since
the matching computed in Step (2) of Christofides’ heuristic has weight at most half of
the length of an optimal tour the following theorem holds.
Theorem 6.2 Let an instance of the TSP obeying the triangle inequality be given.
(i) The double tree heuristic produces a tour which is at most twice as long as an
optimal tour.
(ii) Christofides’ heuristic produces a tour which is at most 1.5 times as long as an
optimal tour.
There are classes of instances (Cornuejols & Nemhauser (1978)) where the Chri-
stofides heuristic yields a tour that is (3n − 1)/(2n) times longer than the optimal tour,
thus proving that the above bound is tight.
Because of the cubic worst case running time, we avoid the computation of minimum
weight matchings and simplify the heuristic as follows. First we double all edges to
leaves, and then we compute a farthest insertion tour on the remaining (and newly
introduced) odd-degree nodes. This tour induces two perfect matchings and we add the
shorter one to our subgraph which is then Eulerian. Time complexity of this procedure
is O(n 2 ).
Table 6.17 compares the two heuristics with respect to CPU time and length of the
generated tours.
The double tree heuristic has a very poor performance. But, also the performance
of the Christofides variant is disappointing (coinciding with the findings in Johnson
(1990)), in particular when taking into account that it has the best worst case bound
among the heuristics. This is not due to our simplification, but it was observed in
many experiments that it does not pay off to compute exact minimum weight perfect
matchings in Step (2).93
6.3. Heuristics Using Spanning Trees
Problem Double tree
d198
22.62
lin318
41.32
fl417
36.04
pcb442
39.64
u574
36.29
p654
36.89
rat783
36.75
pr1002
37.29
u1060
34.30
pcb1173
42.29
d1291
48.16
rl1323
39.04
fl1400
39.40
u1432
45.78
fl1577
42.75
d1655
37.47
vm1748
31.68
rl1889
40.50
u2152
48.11
pr2392
37.22
pcb3038
43.23
fl3795
41.38
fnl4461
39.47
rl5934
48.18
Average
39.41
Christofides
15.67*
18.42*
24.52*
18.59*
20.08*
21.73*
21.34*
20.67*
18.97*
18.77*
24.31*
14.05*
22.10*
24.05*
13.27*
18.92*
21.73*
14.00*
22.73*
18.70*
20.58*
17.25*
21.92*
15.17*
19.48
Table 6.17 Comparison of tree heuristics
2.0
1.5
1.0
0.5
0.0
0
1000
2000
3000
[1]
4000
5000
[2]
Figure 6.18 CPU times for tree heuristics
Universidad Sim ́on Bol ́ıvar
Departamento de Computaci ́on y Tecnolog ́ıa de la Informaci ́on
CI-2692 - Laboratorio de Algoritmos y Estructuras II
Trimestre enero-marzo 2021

Proyecto Ayudante Ortogr ́afico

El objetivo del proyecto es la implementaci ́on de un ayudante ortogr ́afico. Esta herramienta carga un
diccionario desde un archivo y adem ́as el ayudante debe ser capaz de revisar un texto y detectar las palabras
que no se encuentren en el diccionario. A cada una de las estas palabras que no est ́an en el diccionario, se les
va a buscar las palabras que les sean m ́as cercanas para presentarlas al usuario como recomendaciones.
1. Preliminares
1.1. Funci ́on palabra v ́alida

Consideramos que un elemento de tipo String es una palabra v ́alida si est ́a formado por caracteres al-
fab ́eticos en min ́usculas, esto es los caracteres entre ‘a’y ‘z’, y agregando el car ́acter e ̃ne (‘ ̃n’). Para determinar

si un String es una palabra v ́alida definimos la funci ́on esPalabraValida como sigue:
esPalabraValida(s) ≡ (∀ i : 0 6 i < len(s) : ‘a’ 6 s[i] 6 ‘z’ ∨ s[i] = ‘~n’)
Se tiene que len una la funci ́on que retorna la longitud del elemento de tipo String y que s[i] denota el
i- ́esimo car ́acter del String s.
1.2. Distancia entre palabras
Para poder determinar la similitud entre dos palabras en necesario contar con un algoritmo que mida
la distancia entre ellas. Para medir la distancia entre dos elementos de tipo String se debe utilizar una
m ́etrica de comparaci ́on de cadenas de caracteres. En espec ́ıfico, debe usar la m ́etrica conocida como la
distancia Levenshtein. En la secci ́on 3.3.3 del libro [2], se encuentra una descripci ́on y el algoritmo de la
distancia de Levenshtein. El libro [2] se encuentra disponible en http://nlp.stanford.edu/IR-book/pdf/
irbookonlinereading.pdf
2. Tipos de datos
2.1. TAD Palabras con la misma letra inicial (PMLI)
Este tipo de dato contiene a un conjunto de palabras, las cuales comienzan por una misma letra. A
continuaci ́on se presenta la especificaci ́on del TAD.

1

Especificaci ́on del TAD PMLI
Modelo de Representaci ́on
var letra : Char
var palabras : Conjunto de Strings
Invariante de Representaci ́on
(letra = ‘a’ ∨ letra = ‘b’ ∨ . . . ∨ letra = ‘z’ ∨ letra = ‘~n’) ∧
(∀ p)(p ∈ palabras ⇒ (p[0] = letra ∧ esPalabraValida(p)))
Operaciones
fun crearPMLI (in l : Char ) → PMLI
{ Pre: True }
{ Post: letra = l ∧ palabras = ∅ }
proc agregarPalabra (in p : String )
{ Pre: p[0] = letra }
{ Post: palabras = palabras0 ∪ {p} }
proc eliminarPalabra (in p : String )
{ Pre: p[0] = letra }
{ Post: palabras = palabras0 − {p} }
proc mostrarPalabras ( )
{ Pre: True }
{ Post: Muestra por la salida est ́andar los elementos en palabras en orden lexicogr ́afico. }
fun buscarPalabra (in p : String ) → Boolean
{ Pre: p[0] = letra }
{ Post: buscarPalabra ≡ (p ∈ palabras) }
Fin TAD
2.2. TAD Ayudante Ortogr ́afico
Este TAD posee como principal estructura de datos un arreglo de PMLI de tama ̃no 27. Cada casilla del
arreglo corresponde a una letra del alfabeto latino, incluyendo al car ́acter ’ ̃n’. Esta estructura de datos es
donde usted debe almacenar las palabras del diccionario o diccionarios que va a usar el TAD. A continuaci ́on
se presenta el modelo de representaci ́on y el invariante del TAD.
Especificaci ́on del TAD Ayudante Ortogr ́afico
Modelo de Representaci ́on
const MAX : int
var dicc : arreglo de PMLI
Invariante de Representaci ́on
MAX = 27 ∧ #dicc = 27 ∧
dicc[0].letra = ‘a’ ∧ dicc[1].letra = ‘b’ ∧ . . . ∧ dicc[26].letra = ‘z’
Operaciones
. . .
Fin TAD
A continuaci ́on se describen las operaciones operaciones del TAD Ayudante Ortogr ́afico.

2

2.2.1. crearAyudante
Crea un nuevo TAD Ayudante Ortogr ́afico en donde la estructura de datos dicc se inicializa, generando
un arreglo, donde se crean 27 instancias del TAD PMLI, una por cada letra del alfabeto.

fun crearAyudante () → AyudanteOrtografico
{ Pre: True }
{ Post: Queda inicializa la estructura dicc
creando las 27 instancias de TAD PMLI, una para cada letra del alfabeto. }

2.2.2. cargarDiccionario
Este procedimiento lee un archivo de texto que contiene las palabras de un diccionario, las cuales van a
ser almacenadas en la estructura de datos dicc. La entrada del procedimiento es el nombre del archivo con
las palabras del diccionario. El formato del archivo de diccionario es el siguiente: solo debe contener una
palabra por l ́ınea y todas las palabras deben cumplir la definici ́on de esPalabraValida. Despu ́es de finalizar
la lectura del archivo y de haber verificado que el mismo tiene un formato v ́alido, las palabras son cargadas
en el estructura dicc, siguiendo las especificaciones del TAD PMLI. Observe que con este procedimiento es
posible cargar varios archivos con diccionarios.
proc cargarDiccionario (in fname : String )
{ Pre: El archivo fname debe cumplir con el formato preestablecido. }
{ Post: Quedan agregadas en dicc las palabras de fname
cumpliendo la especificaci ́on del TAD PMLI. }

2.2.3. borrarPalabra
Este procedimiento recibe como entrada una palabra, si la misma se encuentra en dicc la elimina. Si la
palabra no se encuentra en dicc la estructura no sufre ninguna modificaci ́on.

proc borrarPalabra (in p : String )
{ Pre: esPalabraValida(p) }
{ Post: dicc[x ].palabras = dicc0

[x ].palabras, donde dicc0

[x ].palabras es el conjunto
resultante de aplicar dicc0[x ].eliminarPalabra(p) en la instancia dicc0[x ] del TAD PMLI,
que cumple el invariante de representaci ́on dicc[x ]0.letra = p[0], para 0 ≤ x ≤ 26 }

2.2.4. corregirTexto
Este procedimiento recibe como entrada un archivo de texto con palabras a revisar. Este archivo adem ́as de
palabras v ́alidas, puede contener palabras inv ́alidas, espacios en blanco, saltos de l ́ınea, espacio de tabulaciones
y signos de puntuaci ́on [1]. El procedimiento corregirTexto debe procesar el archivo de entrada
para extraer  ́unicamente las palabras v ́alidas que contenga, ignorando todos los dem ́as elementos
mencionados anteriormente. Luego debe determinar cuales son las palabras v ́alidas que no es encuentran en
el diccionario. Despu ́es para cada una de estas palabras, debe calcular las cuatro palabras en el diccionario
de palabras (estructura dicc), con menor valor de distancia Levenshtein. Las cuatro palabras con menor
distancia, ser ́an las palabras va a sugerir el corrector ortogr ́afico. Finalmente debe imprimir los resultados
en el archivo de salida. El archivo de salida debe tener el siguiente formato. Cada palabra v ́alida que no
se encuentre en el diccionario debe comenzar una l ́ınea y luego le siguen las cuatros palabras con menor
distancia, separadas por un espacio una coma.

3

proc corregirTexto (in finput : String ; in-out foutput : String)
{ Pre: finput es un archivo de texto v ́alido }
{ Post: Imprime en el archivo foutput cada una de las palabras v ́alidas contenidas
en el archivo finput que no se encuentren en dicc, seguidas de las cuatro palabras
con menor distancia. }
2.2.5. imprimirDiccionario
Imprime por la salida est ́andar la estructura dicc, en un formato que permita entender f ́acilmente el
contendido de la misma.

proc imprimirDiccionario ()
{ Pre: True }
{ Post: Imprime dicc por la salida est ́andar mostrando las palabras en orden lexicogr ́afico }

3. Requerimientos de la implementaci ́on
El conjunto de Strings del TAD PMLI, llamado palabras, debe ser implementado como una tabla de hash
basada en direccionamiento abierto usando como funci ́on de hash sondeo lineal. Para el sondeo lineal se debe
usar como funci ́on de hash interna (h1) la funci ́on hash de Python. Cuando al insertar un elemento en la tabla
de hash, se tiene un factor de carga igual o mayor a 0.7, entonces se aplica la operaci ́on rehashing, en donde
se duplica el tama ̃no de la tabla. La estructura de datos tabla de hash debe ser creada en una clase llamada
OpenHtable con sus operaciones. El m ́odulo que contiene esta tabla de hash debe llamarse open htable.py.
La estructura dicc del TAD Ayudante Ortogr ́afico, debe ser implementada como una lista de Python.
Una vez creada la estructura dicc, se deben agregar las 27 instancias del TAD PMLI. Luego esta estructura
no cambia de tama ̃no.
Debe implementar una aplicaci ́on cliente llamada prueba ortografia.py, que al ejecutarla, proporciona
al usuario de un men ́u simple que permite llevar a cabo todas las operaciones del TAD Ayudante Ortogr ́afico.
Al iniciar el cliente, entrar ́a en una iteraci ́on de men ́u con las siguiente 6 opciones:
1. Crear un nuevo ayudante ortogr ́afico.
2. Cargar un diccionario.
3. Eliminar palabra.
4. Corregir texto.
5. Mostrar diccionario.
6. Salir de la aplicaci ́on.
Este men ́u se mostrar ́a por la salida est ́andar y el usuario terminar ́a la ejecuci ́on del cliente cuando
seleccione la opci ́on 6 “Salir de la aplicaci ́on”. Si alguna de las precondiciones de los procedimientos de
los TADs no se cumple, entonces se le debe indicar al usuario que la operaci ́on no pudo ser efectuada porque
no se cumple la precondici ́on, indicando cual es la precondici ́on.
Cada uno de los TADs debe ser implementado como una clase de Python. Debe hacer entrega de por lo
menos cuatro archivos:
pmli.py: Implementaci ́on del TAD PMLI.
ayudante ortografico.py: Implementaci ́on del TAD Ayudante Ortogr ́afico.
open htable.py: Tabla se hash basada en direccionamiento abierto y con sondeo lineal.

4

prueba ortografia.py: Cliente que permite interactuar con el TAD Ayudante Ortogr ́afico.
Como los TADs se deben a implementar como clases de Python, la funci ́on crearPMLI del TAD PMLI
va a corresponder al constructor de la clase, por lo no debe ser implementado como un m ́etodo aparte. De
la misma forma la funci ́on crearAyudante corresponde al constructor de la clase que implementa al TAD
Ayudante Ortogr ́afico, por lo que no debe ser creada como un m ́etodo nuevo.
El c ́odigo debe estar debidamente documentado y cada uno de los m ́etodos de los TADs deben tener los

siguientes elementos: descripci ́on, descripci ́on de los par ́ametros, efectos secundarios, precondici ́on y postcon-
dici ́on. Adem ́as debe hacer uso de la gu ́ıa de estilo de Python.

4. Condiciones de entrega
Debe entregar los c ́odigos fuentes en un archivo comprimido llamado Proy2ci2692em21-X.tar.xz donde
X es el n ́umero de carn ́e del estudiante autor del proyecto. La entrega se realizar ́a por medio de la plataforma
Classroom antes de las 8:00 am del viernes 16 de abril de 2021.
Referencias
[1] Colaboradores de Wikipedia. Signos de puntuaci ́on — Wikipedia, la enciclopedia libre, 2021. [Online;
revisado el 25-marzo-2021].
[2] Manning, C., Raghavan, P., and Schutze, H.  ̈ Introduction to Information Retrieval. Cambridge
University Press, 2009.

Guillermo Palma / gvpalma@usb.ve / Marzo 2021
Sea S una superficie regular orientada y φ : U ⊂ R

2 → S una parametrizaci ́on en
la orientaci ́on de S. A cada punto de φ(U) le podemos asignar un triedro formado
por los vectores φu, φv y N. Expresando las derivadas de estos vectores en la base
{φu, φv, N} obtenemos:
φuu = Γ1
11 φu + Γ2
11 φv + L1 N

φuv = Γ1
12 φu + Γ2
12 φv + L2 N
Definici ́on. Sea ω un campo vectorial diferenciable sobre un abierto U ⊂ S y
p ∈ U. Sea y ∈ TpS. Consideremos una curva parametrizada α : (−, ) → U
con α(0) = p y α
0
(0) = y, y sea ω(t) con t ∈ (−, ) la restricci ́on del campo
vectorial ω a la curva α. El vector que se obtiene tomando la proyecci ́on normal
del vector
dω
dt

(0) sobre el plano TpS se llama derivada covariante en p del
campo vectorial ω relativa al vector y. Denotaremos esta derivada covariante por
Dω
dt

(0) o
Dyω

(p).

Veamos que la definici ́on anterior no depende de la escogencia de la curva α.
Sea φ(u, v) una parametrizaci ́on de S en p; escribamos φ
u(t), v(t)

= α(t) y

ω(t) = a
u(t), v(t)

φu + b
u(t), v(t)

φv = a(t) φu + b(t) φv. Entonces,

dω
dt
= a
φuu u
0 + φuv v
0

+ b
φvu u
0 + φvv v
0

= a
0 φu + b
0 φv.

Como Dω
dt
es la componente de dω
dt
sobre el plano tangente, usamos las expresiones
que aparecen en (??) para φuu, φuv y φvv, y simplemente quitamos la componente
que cae sobre N. As ́ı,
Dω
dt
=
a
0 + Γ1
11 au0 + Γ1
12 av0 + Γ1
12 bu0 + Γ1
22 bv0

φu

+
b
0 + Γ2
11 au0 + Γ2
12 av0 + Γ2
12 bu0 + Γ2
22 bv0

φv.

Notemos que esta expresi ́on depende s ́olo del vector (u
0
, v0
) = y y no de la curva α.

Observaci ́on. Si S es un plano, sabemos que es posible encontrar una parametri-
zaci ́on tal que E = G = 1 y F = 0. En tal caso, todos los s ́ımbolos de Christoffel,

Γ
k
ij , son iguales a cero. Notemos que, en tal caso, la derivada covariante coincide
con la derivada usual para vectores en el plano.
1

2 Transporte paralelo y Geod ́esicas
Definici ́on. Una curva parametrizada α : [0, l] → S es la restricci ́on a [0, l] de un
mapa diferenciable de (0 − , l + ) en S ( > 0). Si α(0) = p y α(l) = q, diremos
que α une a p con q. Diremos que α es regular si α
0
(t) 6= 0 para t ∈ [0, l].

(Denotemos al intervalo [0, l] por I.)
Definici ́on. Sea α : I → S una curva parametrizada en S. Un campo vectorial
ω a lo largo de α es una correspondencia que le asigna a cada t ∈ I un vector
ω(t) ∈ Tα(t)S. Diremos que el campo vectorial ω es diferenciable en t0 ∈ I si para
alguna parametrizaci ́on φ(u, v) en α(t0) las componentes a(t) y b(t) de ω(t) =
a φu + b φv son funciones diferenciables de t en t0. Diremos que ω es diferenciable
en I si lo es para cada t ∈ I.
Definici ́on. Sea ω un campo diferenciable a lo largo de α : I → S. La expresi ́on

a
0 + Γ1
11 au0 + Γ1
12 av0 + Γ1
12 bu0 + Γ1
22 bv0

φu

+
b
0 + Γ2
11 au0 + Γ2
12 av0 + Γ2
12 bu0 + Γ2
22 bv0

φv

de
Dω
dt

(t), t∈I, est ́a bien definida y se llama la derivada covariante de ω en t.
Si α(t) es una curva sobre la superficie S, pensamos en ella como la trayectoria de
un punto movi ́endose sobre S, α
0
(t) es entonces la velocidad y α

00 la aceleraci ́on

de α. La derivada covariante Dα
0
dt
, del campo α
0
(t), es la componente tangencial

de la aceleraci ́on α
00(t).

Definici ́on. Diremos que un campo vectorial ω es paralelo a lo largo de una
curva parametrizada α : I → S si Dω

dt = 0 ∀ t ∈ I.

Proposici ́on. Sean w y v campos vectoriales paralelos a lo largo de α : I → S.
Entonces, hw(t), v(t)i es constante. En particular, |w(t)|, |v(t)| y el  ́angulo entre
w(t) y v(t) tambi ́en son constantes.
Demostraci ́on. Como w es paralelo a lo largo de α entonces dw
dt
es normal a

Tα(t)S. Luego, hv(t), w0

(t)i = 0, ∀ t ∈ I, ya que v(t) ∈ Tα(t)S. An ́alogamente,

hv
0
(t), w(t)i = 0 ∀ t ∈ I. As ́ı,
hv(t), w(t)i
0 = hv(t), w0
(t)i + hv
0
(t), w(t)i = 0

para todo t ∈ I, por lo que hv(t), w(t)i es constante. Como |w(t)|, |v(t)| y el
 ́angulo entre w(t) y v(t) se expresan en t ́erminos de hw(t), w(t)i y hv(t), v(t)i y
hv(t), w(t)i (solamente) entonces ellos tambi ́en son constantes.

Transporte paralelo y Geod ́esicas 3
Definici ́on. Sea ω un campo diferenciable de vectores unitarios a lo largo de la
curva parametrizada α : I → S sobre una superficie orientada S. Como ω(t),
t ∈ I, es un campo vectorial unitario,
dω
dt

(t) es normal a ω(t) y, por lo tanto,

Dω
dt
= λ
N ∧ ω(t)

.
El n ́umero real λ = λ(t), denotado por  Dω
dt

, se llama valor algebraico de la

derivada covariante de ω en t.
Notemos que el signo de  Dω
dt

depende de la orientaci ́on de S y
 Dω
dt

=
dω
dt
, N ∧ω
.

Sean v y w dos campos vectoriales diferenciables a lo largo de la curva parame-
trizada α : I → S con |v(t)| = |w(t)| = 1, t ∈ I. Queremos definir una funci ́on

diferenciable Θ : I → R tal que Θ(t) es la determinaci ́on del  ́angulo de v(t) a w(t)
en la orientaci ́on de S. Consideremos el campo vectorial diferenciable  ̄v a lo largo
de α de manera tal que {v(t), v ̄(t)} es una base ortonormal positiva para cada
t ∈ I. As ́ı, podemos expresar

w(t) = a(t) v(t) + b(t)  ̄v(t)
donde a y b son funciones diferenciables en I y a
2 + b
2 = 1.
Lema. Sean a y b funciones diferenciables en I tales que a
2 + b
2 = 1, Θ0 tal que

a(t0) = cos(Θ0) y b(t0) = sen(Θ0). Entonces, la funci ́on diferenciable

Θ = Θ0 +
Z t
t0
(ab0 − ba0
) dt

es tal que cos
Θ(t)

= a(t), sen
Θ(t)

= b(t), para t ∈ I, y Θ(t0) = Θ0.

Demostraci ́on. Queremos ver que
0 ≡
a − cos(Θ)2
+
b − sen(Θ)2
= 2 − 2
a cos(Θ) + b sen(Θ)
o equivalentemente, que A = a cos(Θ) + b sen(Θ) = 1. Luego,
A
0 = a
0
cos(Θ) − a sen(Θ) Θ0 + b
0
sen(Θ) + b cos(Θ) Θ0

= a
0
cos(Θ) − b
0
sen(Θ)
a
2 + b
2

+ b
0
sen(Θ) − a
0
cos(Θ)
a
2 + b
2


= 0
ya que a
2 + b
2 = 1 ⇒ aa0 = −bb0

. Entonces A(t) es constante y como A(t0) = 1

entonces A ≡ 1.

4 Transporte paralelo y Geod ́esicas
Lema. Sean v y w dos campos vectoriales diferenciables a lo largo de la curva
α : I → S con |w(t)| = |v(t)| = 1, t ∈ I. Entonces,


Dw
dt

−

Dv
dt

=
dΘ
dt

donde Θ es una de las determinaciones diferenciables del  ́angulo desde v hasta w
como se tiene en el lema anterior.
Demostraci ́on. Sean  ̄v = N ∧ v y  ̄w = N ∧ w. Entonces

w = cos(Θ) v + sen(Θ)  ̄v (∗)

y
w ̄ = N ∧ w = cos(Θ) N ∧ v + sen(Θ) N ∧ v ̄ = cos(Θ)  ̄v − sen(Θ) v. (∗∗)
Derivando a (∗) respecto de t obtenemos,

w
0 = −sen(Θ) Θ0

v + cos(Θ) v

0 + cos(Θ) Θ0

v ̄ + sen(Θ)  ̄v
0
.
Tomando el producto interno de esta  ́ultima expresi ́on con (∗∗) obtenemos,
w
0
, w ̄
= sen2
(Θ) Θ0 + cos2
(Θ)
v
0
, v ̄
+ cos2
(Θ) Θ0 − sen2
(Θ)
v ̄
0
, v

= Θ0 + cos2
(Θ)
v
0
, v ̄
− sen2
(Θ)
v ̄
0
, v

= Θ0 +

cos2
(Θ) + sen2
(Θ)
v
0
, v ̄

= Θ0 +
v
0
, v ̄
ya que hv, v ̄i = 0 
⇒
v
0
, v ̄
= −
v, v ̄
0

y
v, v0
= 0. Luego,


Dw
dt

=
w
0
, w ̄
= Θ0 +
v
0
, v ̄
=
dΘ
dt
+

Dv
dt


ya que
w
0
, w ̄
=
dw
dt
, w ̄
=
 Dw
dt

hN ∧ w, w ̄i =
 Dw
dt

.

Proposici ́on. Sea φ(u, v) una parametrizaci ́on ortogonal (es decir, F = 0) de un
entorno de una superficie orientada S, y sea ω(t) un campo vectorial diferenciable
de vectores unitarios a lo largo de la curva φ
u(t), v(t)

. Entonces


Dω
dt

=
1
2
√
EG 
Gu
dv
dt
− Ev
du
dt

+
dΘ
dt

donde Θ(t) es el  ́angulo formado desde φu hasta ω(t) en la orientaci ́on dada.

Transporte paralelo y Geod ́esicas 5
Demostraci ́on. Sean e1 = √
1
E
φu y e2 = √
1
G
φv los vectores unitarios tangentes
a las curvas coordenadas. Notemos que e1 ∧ e2 = N, donde N es la orientaci ́on
dada para S. Por segundo lema, tenemos que


Dω
dt

=

De1
dt

+
dΘ
dt

donde e1(t) = e1
u(t), v(t)

es el campo vectorial e1 restringido a la curva

φ
u(t), v(t)

. Luego

De1
dt

=

de1
dt
, N ∧ e1

=

de1
dt
, e2

=
(e1)u, e2
du
dt
+
(e1)v, e2
dv
dt
.

Como F = 0 entonces hφuu, φvi = 1 1

2Ev (recordemos que hφuu, φvi = Fu1
1
2Ev y

hφuv, φvi =
1
2Gu). As ́ı,
(e1)u, e2
=
 φu √
E

u
,
φv √
G

=
*
φuu
√
E −
φuEu
2
√
E
E
,
φv √
G
+
= −
1
2
Ev √
EG
ya que φ es una parametrizaci ́on ortogonal (0 = F = hφu, φvi). An ́alogamente,
(e1)v, e2
=
1
2
Gu √
EG
.

Combinando lo anterior se obtiene el resultado.
Proposici ́on. Sea α : I → S una curva parametrizada en S y ω0 ∈ Tα(t0)S,
t0 ∈ I. Entonces existe un  ́unico campo vectorial paralelo ω(t) a lo largo de α(t)
con ω(t0) = ω0.
Demostraci ́on. Supongamos que la curva parametrizada α : I → S est ́a contenida
en un entorno coordenado de una parametrizaci ́on ortogonal φ(u, v). Siguiendo la
notaci ́on de la proposici ́on anterior, la condici ́on de paralelismo para el campo ω
es

dΘ
dt
= −
1
2
√
EG 
Gu
dv
dt
− Ev
du
dt

= B(t).

Denotando por Θ0 la determinaci ́on del  ́angulo orientado de φu a ω0, el campo ω
queda determinado por

Θ = Θ0 +
Z t
t0
B(t) dt

lo que demuestra la existencia y unicidad de ω para este caso. Si α(I) no est ́a
contenido en un entorno coordenado, usamos la compacidad de I para dividir a

6 Transporte paralelo y Geod ́esicas
α(I) en un n ́umero finito de partes tal que cada una est ́a contenida en un entorno
coordenado. Como la soluci ́on es  ́unica en las intersecciones de estas partes, el
resultado se puede extender.
Definici ́on. Sea α : I → S una curva parametrizada y ω0 ∈ Tα(t0)S, t0 ∈ I. Sea
ω el campo vectorial paralelo a lo largo de α con ω(t0) = ω0. El vector ω(t1),
t1 ∈ I, se llama el transporte paralelo de ω0 a lo largo de α en el punto t1.
Observaci ́on. Si α : I → S es regular entonces el transporte paralelo no depende
de la parametrizaci ́on de α(I). Si β : J → S (t ∈ I, σ ∈ J) es otra parametrizaci ́on
regular para α(I) entonces Dω
dσ =
Dω
dt
dt
dσ
. Como dt
dσ
6= 0, ω(t) es paralelo si, y s ́olo

si, ω(σ) es paralelo.
Observaci ́on. Dados los puntos p, q, ∈ S y una curva parametrizada α: [0, 1]→S
con α(0) = p y α(1) = q, denotemos por Pα : TpS → TqS el mapa que le asigna
a cada v ∈ TpS su transporte paralelo a lo largo de α en q. Como el producto
interno es constante a lo largo de campos paralelos (ver primera proposici ́on en
esta secci ́on), este mapa es una isometr ́ıa.
Observaci ́on. Si dos superficies, S y S ̄, son tangentes a lo largo de una curva
parametrizada α y ω0 es un vector de Tα(t0)S = Tα(t0)S ̄, entonces ω(t) es el
transporte paralelo de ω0 relativo a la superficie S si, y s ́olo si, ω(t) es el transporte
paralelo de ω0 relativo a S ̄. (La derivada covariante Dω
dt
de ω es igual para ambas

superficies y el transporte paralelo es  ́unico.)
Definici ́on. Diremos que una curva parametrizada (y no constante) γ : I → S es
geod ́esica en t ∈ I si su campo de vectores tangentes γ
0
(t) es paralelo a lo largo

de γ en t. Es decir, si Dγ
0
(t)
d( t) = 0. Diremos que γ es una geod ́esica parametrizada

si es geod ́esica para todo t ∈ I.
Notemos que la definici ́on anterior equivale a decir que α

00(s) = κn es normal
al plano tangente (es decir, paralelo a la normal de la superficie). Entonces, una
curva regular C ⊂ S (κ 6= 0) es una geod ́esica si, y s ́olo si, su normal principal en
cada punto p ∈ C es paralela a la normal de S en p.
Ejemplo. Los c ́ırculos maximales de una esfera son geod ́esicas.

Transporte paralelo y Geod ́esicas 7
Ejemplo. Consideremos el cilindro recto sobre la circunferencia x
2 +y
2 = 1. Los
c ́ırculos que se obtienen intersectanto al cilindro con planos perpendiculares al eje
de rotaci ́on son geod ́esicas. Por otra parte, las rectas (generadoras) del cilindro
tambi ́en son geod ́esicas. Para determinar las otras geod ́esicas del cilindro, C,
consideremos la parametrizaci ́on φ(u, v) =

cos(u),sen(u), v

en el punto p ∈ C,
con φ(0, 0) = p. En esta parametrizaci ́on, un entorno de p en C es expresado por
φ
u(s), v(s)

, donde s es la longitud de arco de C. Recordemos que este mapa φ
es una isometr ́ıa local. Como la condici ́on de ser geod ́esica es local e invariante
bajo isometr ́ıas, la curva
u(s), v(s)

debe ser una geod ́esica en U que pase por
(0, 0). Pero las geod ́esicas del plano son l ́ıneas rectas. As ́ı, adem ́as de los casos
anteriores,
u(s) = as, v(s) = ba, a2 + b

2 = 1 (para que est ́e parametrizada
por longitud de arco).
Luego, para que una curva regular C (que no sea ni un c ́ırculo ni una l ́ınea) sea
geod ́esica del cilindro debe ser localmente de la forma

cos(as),sen(as), bs

que resulta ser una h ́elice.
Observaci ́on. Dados dos puntos sobre el cilindro, que no

est ́en en un c ́ırculo paralelo al plano xy, pueden existir infi-
nitas geod ́esicas que los una (es necesario que las geod ́esicas

den al menos una vuelta completa alrededor del cilindro).

q

p

Definici ́on. Sea C una curva regular orientada contenida en una superficie orien-
tada S, y sea α(s) una parametrizaci ́on de C en un entorno de p ∈ S parametrizada

por longitud de arco s. El valor algebraico de la derivada covariante de α
0
(s) en

p,
h
Dα
0
(s)
ds
i
= κg, se llama la curvatura geod ́esica de C en p.
Observaci ́on. Geod ́esicas que son curvas regulares est ́an caracterizadas como
curvas cuya curvatura geod ́esica es cero.
Observaci ́on. El valor absoluto de la curvatura geod ́esica, Kg, de C en p es igual
al valor absoluto de la componente tangencial del vector α

00(s) = κn, donde κ es
la curvatura de C en p y n es el vector normal de C en p. Recordando que el valor

8 Transporte paralelo y Geod ́esicas
absoluto de la componente normal del vector κn es igual al valor absoluto de la
curvatura normal κn de C ⊂ S en p, tenemos que K2 = (κg)
2 + (κn)
2
.
Observaci ́on. La curvatura geod ́esica de C ⊂ S cambia de signo cuando se
cambia la orientaci ́on o bien de C o bien de S.
Proposici ́on (F ́ormula de Liouville). Sea α(s) una parametrizaci ́on por longitud
de arco, de un entorno de un punto p ∈ S, de una curva regular orientada C sobre
una superficie orientada S. Sea φ(u, v) una parametrizaci ́on ortogonal de S en p
y Θ(s) el  ́angulo que hace φu con α
0
(s) en la orientaci ́on dada. Entonces,

κg =
κg

1
cos(Θ) +
κg

2
sen(Θ) + dΘ
ds

donde
κg

1
y
κg

2
son las curvaturas geod ́esicas de las curvas coordenadas

v = constante y u = constante, respectivamente.
Demostraci ́on. Por una proposici ́on anterior (considerando ω = α
0
(s)) tenemos

que

κg =
1
2
√
EG 
Gu
dv
ds
− Ev
du
ds

+
dΘ
ds
.
A lo largo de la curva v = constante u = u(s), tenemos que dv
ds = 0 y du
ds = √
1
E
;

luego,
κg

1
= −
Ev
2E
√
G
. An ́alogamente,
κg

2
= −
Gu
2G
√
E
. As ́ı,

κg =
κg

1
√
E
du
ds
+
κg

2
√
G
dv
ds
+
dΘ
ds
.

Como √
E
du
ds =
D
α
0
(s), √
φu
E
E
= cos(Θ) y √
G
dv
ds =
D
α
0
(s), √
φv
G
E
= sen(Θ), el

resultado sigue.
Sea γ : I → S una curva parametrizada de S y φ(u, v) una parametrizaci ́on de
S en un entorno V de γ(t0), t0 ∈ I. Sea J ⊂ I un intervalo abierto que contenga
a t0 tal que γ(J) ⊂ v. Sea φ
u(t), v(t)

, t ∈ J, la expresi ́on de γ : J → S en

la parametrizaci ́on φ. El campo de vectores tangentes, γ
0
(t), t ∈ J es dado por

ω = u
0
(t) φu + v
0
(t) φv. As ́ı, el hecho que ω es paralelo es equivalente al sistema

de ecuaciones diferenciales:
u
00 + Γ1
11(u
0
)
2 + 2Γ1
12u
0v
0 + Γ1
22(v
0
)
2 = 0

v
00 + Γ2
11(u
0
)
2 + 2Γ2
12u
0v
0 + Γ2
22(v
0
)
2 = 0

(∗)

obtenido de la definici ́on de Dω
dt
(con a = u
0 y b = v
0
) e igualando a cero.

Transporte paralelo y Geod ́esicas 9
Es decir, γ : I → S es una geod ́esica si, y s ́olo si, el sistema (∗) se satisface para
cada intervalo J ⊂ I tal que γ(J) est ́e contenido en un entorno coordenado. El
sistema (∗) se conoce como las ecuaciones diferenciales de las geod ́esicas
de S.
Al caracterizar las geod ́esicas mediante el sistema (∗) tenemos la siguiente:
Proposici ́on. Dado un punto p ∈ S y un vector ω ∈ TpS, ω 6= 0, existe un  > 0
y una  ́unica geod ́esica parametrizada γ : (−, ) → S tal que γ(0) = p y γ
0
(0) = ω.
Observaci ́on. Pedimos ω 6= 0 ya que las curvas constantes est ́an excluidas de la
definici ́on de geod ́esica parametrizada.
Ejemplo. Para un s ́olido de revoluci ́on, parametrizado por

φ(u, v) =
f(v) cos(u), f(v) sen(u), g(v)


con f(v) 6= 0, como
Γ
1
11 = 0 Γ1
12 =
ff0
f
2
Γ
1
22 = 0

Γ
2
11 = −
ff0
(f
0)
2 + (g
0)
2
Γ
2
12 = 0 Γ2
22 =
f
0f
00 + g
0
g
00
(f
0)
2 + (g
0)
2

el sistema (∗) se reduce a

u
00 +
2ff0
f
2
u
0
v
0 = 0 (i)

v
00 −
ff0
(f
0)
2 + (g
0)
2
(u
0
)
2 +
f
0f
00 + g
0
g
00
(f
0)
2 + (g
0)
2
(v
0
)
2 = 0 (ii)
Notemos que los meridianos u = const. y v = v(s), parametrizados por longitud
de arco, son geod ́esicas:
(i) se satisface trivialmente
(ii) se convierte en v
00 +
f
0
f
00+g
0
g
00
(f
0)
2+(g
0)
2 (v
0
)
2 = 0 pero la primera forma fundamental

(a lo largo de un meridiano) nos dice que
(f
0
)
2 + (g
0
)
2

(v
0
)
2 = 1. Derivando y

usando que v

0 6= 0 se tiene que se satisface la ecuaci ́on.

10 Transporte paralelo y Geod ́esicas
Miremos ahora los paralelos u = u(s) y v = const., parametrizados por longitud
de arco s. La ecuaci ́on (i) nos dice que u

0 = const. y la ecuaci ́on (ii) se convierte en

f f0
(f
0)
2+(g
0)
2 (u
0
)
2 = 0. Como u

0 6= 0 (para que pueda ser geod ́esica), (f
0
)
2+(g
0
)
2 6= 0

y f 6= 0 entonces, necesariamente, f

0 = 0. Es decir, para que un paralelo sea una
geod ́esica es necesario que sea generado por la rotaci ́on de un punto de la curva
generadora donde la recta tangente es paralela al eje de rotaci ́on de la superficie.
Observaci ́on. Notemos que la ecuaci ́on (i) se puede reescribir como (f
2u
0
)
0
f
2 = 0

y, por lo tanto, f
2u
0 = const. = c. Sea θ ∈ [0, 2π] el  ́angulo entre una geod ́esica y

el paralelo con el cual se est ́a intersectando. Luego,

cos(θ) =
φu, φu u
0 + φv v
0
|φu|
=
fu0
.

Como f = r es el radio del paralelo en el punto de intersecci ́on, tenemos la
relaci ́on de Clairaut: r cos(θ) = const. = |c|.
Volviendo al ejemplo, sea u = u(s) y v = v(s) una geod ́esica, parametrizada por
longitud de arco, que no sea ni un meridiano ni un paralelo de la superficie. Ya
vimos que la ecuaci ́on (i) es equivalente a f
2u
0 = const. = c 6= 0. La primera

forma fundamental, a lo largo de
u(s), v(s)

Las ecuaciones (iii) y f
2u
0 = c 6= 0 producen la ecuaci ́on (ii) ya que al reemplazar

y podemos asumir que v

0 6= 0 porque φ
u(s), v(s)

no es un paralelo y si la
geod ́esica llegase a ser tangente a un paralelo que no es una geod ́esica (v
0 = 0),
la relaci ́on de Clairaut establece que esto s ́olo ocurre en puntos aislados. Como
c 6= 0 (pues la geod ́esica no es un meridiano) tenemos que u
0
(s) 6= 0. As ́ı, podemos

invertir u = u(s) y obtener s = s(u), de donde v = v
s(u)

. Multiplicando a (iii)


Transporte paralelo y Geod ́esicas 11
de donde,
dv
du
=
φvv = Γ1
22 φu + Γ2
22 φv + L3 N
Nu = a11 φu + a21 φv
Nv = a12 φu + a22 φv.

(∗)

Los coeficientes aij se determinan igual que antes. Los coeficientes Γk
ij se llaman
los s ́ımbolos de Christoffel de S en la parametrizaci ́on φ. Como φuv = φvu,
tenemos que Γ1
12 = Γ1
21 y Γ2
12 = Γ2
21. Tomando el producto interno con N en

las primeras cuatro ecuaciones obtenemos que L1 = e, L2 = L ̄

2 = f y L3 = g,
donde e, f y g son los coeficientes de la segunda forma fundamental sobre S. Por
otra parte, tomando el producto interno con los vectores φu y φv, en las primeras
cuatro ecuaciones, obtenemos:


2 El Teorema de Gauss y las ecuaciones de compatibilidad
Notemos que el determinante del sistema para cada par de ecuaciones, como han
sido agrupadas, es EG − F

2 6= 0; por lo que el sistema admite soluci ́on. As ́ı,
podemos calcular los s ́ımbolos de Christoffel en t ́erminos de los coeficientes de la
primera forma fundamental (E, F y G) y sus derivadas.
Observaci ́on. Todos los conceptos y propiedades geom ́etricas expresadas en
t ́erminos de los s ́ımbolos de Christoffel son invariantes bajo isometr ́ıas.
Ejemplo. Consideremos la superficie de revoluci ́on parametrizada por

φ(u, v) =
f(v) cos(u), f(v) sen(u), g(v)

, f(v) 6= 0.



Notemos que
(φuu)v − (φuv)u = 0 , (φvv)u − (φvu)v = 0 y Nuv − Nvu = 0. (∗∗)
Introduciendo los valores de (∗) en (∗∗) obtenemos
A1 φu + B1 φv + C1 N = 0
A2 φu + B2 φv + C2 N = 0
A3 φu + B3 φv + C3 N = 0

(∗∗∗)

donde Ai
, Bi y Ci (i = 1, 2, 3) son funciones de E, F, G, e, f, g y sus derivadas.
Como los vectores φu, φv y N son linealmente independientes, entonces
Ai = 0 , Bi = 0 y Ci = 0 para i = 1, 2, 3.

Usando los valores de (∗) en la primera relaci ́on de (∗∗), volviendo a usar (∗),
mirando el coeficiente de φv y reemplazando los valores de aij , obtenemos
Γ
2
conocida como la f ́ormula de Gauss.

El Teorema de Gauss y las ecuaciones de compatibilidad 3
Teorema (Egregium de Gauss). La curvatura Gaussiana, K, de una superficie
es invariante bajo isometr ́ıas locales.
De hecho, si φ : U ⊂ R

2 → S es una parametrizaci ́on en p ∈ S y si ψ : V ⊂ S → S,
donde V ⊂ φ(U) es un entorno de p, es una isometr ́ıa local en p, entonces ψ ◦ φ
es una parametrizaci ́on en ψ(p). Como ψ es una isometr ́ıa, los coeficientes de la
primera forma fundamental en las parametrizaciones φ y ψ ◦ φ coinciden en los

puntos correspondientes q y ψ(q) (q ∈ V ). Luego, los s ́ımbolos de Christoffel co-
rrespondientes tambi ́en coinciden. Usando la f ́ormula de Gauss, podemos calcular

a K en t ́erminos de los s ́ımbolos de Christoffel en una parametrizaci ́on dada. As ́ı,
K(q) = K
ψ(q)

pata todo q ∈ V .

Ejemplo. Recordemos que el catenoide es localmente isom ́etrico al helicoide.
Por el Teorema de Gauss, las curvaturas Gaussianas son iguales en los puntos
correspondientes.
Observaci ́on. La curvatura Gaussiana no depende de la posici ́on en el espacio
de la superficie sino de la estructura m ́etrica de la superficie (primera forma
fundamental).
Siguiendo el procedimiento anterior, si miramos el coeficiente de φu obtenemos

Mirando el coeficiente de N obtenemos
el procedimiento ahora con la segunda expresi ́on en (∗∗) y mirando el
coeficiente de N obtenemos

Los dem ́as coeficientes (A2 = 0, B2 = 0, A3 = 0, B3 = 0 y C3 = 0) no producen

ecuaciones nuevas. Las ecuaciones (MC.1) y (MC.2) se conocen como las ecua-
ciones de Mainardi-Codazzi.

La f ́ormula de Gauss y las ecuaciones de Mainardi-Codazzi se conocen como las
ecuaciones de compatibilidad de la teor ́ıa de superficies.
arras de progreso, hélices y contadores




Las barras de progreso, las hélices y los contadores son elementos que se utilizan en un programa para mostrar gráficamente el avance de un proceso. Generalmente, sirven para informar a los usuarios sobre su evolución cuando duran más de unos pocos segundos. Por ello, tienen por objeto dar certidumbre a la persona que espera sobre que un proceso sigue en ejecución informando, a veces, del tiempo transcurrido y del que resta para su finalización. Es un recurso apropiado para mostrar el progreso de unos cálculos prolongados; de una transferencia de archivos; de la instalación de una aplicación, etc.

A continuación, mostraremos algunas de las posibilidades que existen para incorporar este práctico y vistoso elemento a un programa Python. Nos centraremos en los paquetes Progress y TQDM, ambos disponibles en nuestro repositorio favorito. 


Progress

El paquete Progress desarrollado por el griego Georgios Verigakis (Verigak) consta de varios módulos que agrupan un conjunto de clases que permiten declarar barras de progreso, hélices, contadores, etc. en programas para la consola. Son objetos de distintos tipos que tienen la misma finalidad y de los cuales existen algunas variantes.


Instalación de Progress

Para instalar Progress con PIP desde la línea de comandos:

$ pip install progress


Objetos Bar, ChargingBar, Spynner y Countdown de Progress

Normalmente los objetos se declaran antes del inicio de un bucle y, después, en cada ciclo del bucle se utiliza el método Next() para hacer que avance.

A continuación, varios ejemplos que muestran cómo declarar y usar los objetos: dos tipos de barras, una hélice y un contador. La función sleep() de time se utiliza a veces para introducir un pequeño retardo de tiempo para que se pueda observar el progreso de los distintos elementos usados.




from progress.bar import Bar, ChargingBar
import os, time, random

# Declara un objeto de la clase Bar(). En cada ciclo la barra
# muestra una porción hasta llegar a su máxima longitud en el 
# ciclo 20. La barra se representa con el carácter #

bar1 = Bar('Procesando:', max=20)
for num in range(20):
    time.sleep(0.2)
    bar1.next()
bar1.finish()


# Declara un objeto de la clase ChargingBar(). Cuando comienza
# el bucle aparece una barra punteada y durante los ciclos los 
# puntos "∙" son sustituidos por el carácter "█" hasta alcazar
# el 100%.

bar2 = ChargingBar('Instalando:', max=100)
for num in range(100):
    time.sleep(random.uniform(0, 0.2))
    bar2.next()
bar2.finish()


# Declara un objeto de la clase Bar(). En cada ciclo la barra
# muestra el carácter del atributo "fill" (·) hasta alcanzar 
# el 100%.
# Durante el proceso se escribe un archivo de texto con 
# 5000 caracteres ('X' y 'O') que son generados por una función 
# aleatoria. En este caso el retardo de tiempo es real.
# En el módulo hay otras clases para declarar objetos similares:
# FillingSquaresBar, FillingCirclesBar, IncrementalBar, PixelBar
# y ShadyBar

bar3 = Bar('Escribiendo:', fill='·', suffix='%(percent)d%%')
caracteres = ['X', 'O']
datos = os.getcwd()+os.sep+"datos.txt"
archivo = open(datos, "w")
for i in range(100):
    cadena = ""
    longitud = 5000
    for num in range(longitud):
        cadena += caracteres[random.randint(0, 1)]
    
    archivo.write(cadena + "\n")
    bar3.next()
    
bar3.finish()
archivo.close


# Declara un objeto de la clase Spinner(). En cada ciclo una hélice
# gira hasta que se completa la lectura del archivo creado en el
# ejemplo anterior. Cuando se completa la lectura se muestra el 
# número total de caracteres encontrados de cada tipo ('X'/'O').
# Una hélice muestra que un proceso se está ejecutando pero no 
# no ayuda a prever su final.
# El módulo tienes otras clase para declarar objetos similares:
# PieSpinner, MoonSpinner, LineSpinner y PixelSpinner.

from progress.spinner import Spinner
import time

spinner = Spinner('Leyendo: ')
cuenta_X = 0
cuenta_O = 0
archivo = open(datos, "r")
while True: 
    linea = archivo.readline()
    if linea:
        for caracter in linea:
            if caracter == 'X':
                cuenta_X+=1
            elif caracter == 'O':
                cuenta_O+=1
    else:
        break
    time.sleep(0.1)
    spinner.next()
    
print(' X=', cuenta_X, 'O=', cuenta_O)
archivo.close


# Declara un objeto de la clase Countdown(). En cada ciclo un 
# contador que comienza en 100 va disminuyendo su valor hasta
# alcanzar 0, que marca el fin del bucle.
# El módulo tiene otras clases para declarar objetos 
# similares: Counter, Pie y Stack

from progress.counter import Countdown
import time

contador = Countdown("Contador: ")
for num in range(100):
    contador.next()
    time.sleep(0.05)

print()

TQDM

El módulo TQDM de Noam Yorav-Raphael y un grupo de colaboradores a diferencia de progress sólo permite implementar barras de progreso, pero más sofisticadas porque son capaces de ofrecer más información mientras se ejecuta un proceso: el porcentaje ejecutado, el tiempo trascurrido, una estimación del tiempo que resta para su finalización y el número de iteraciones por segundo.

Algunas características interesantes de TQDM son que se puede utilizar en el entorno interactivo IPython, dentro de un cuaderno Jupyter y desde la línea de comandos.


Instalación de TQDM

Para instalar TQDM con PIP desde la línea de comandos:

$ pip install tqdm


Barras de progreso con TQDM

A continuación, varios ejemplos que muestran el uso de las barras de progreso TQDM:




# Declara una barra de progreso tqdm. En cada ciclo la barra
# muestra una porción hasta llegar a 100.
# La barra es representada con el carácter "█": a su izquierda
# aparece el porcentaje procesado y a su derecha el número de ciclo
# actual, el total de ciclos, el tiempo transcurrido de proceso,
# una estimación del tiempo pendiente y el número de ciclos o 
# iteraciones por segundo.

from tqdm import tqdm, trange
import time

for num in tqdm(range(100)):
    time.sleep(0.01)


# Declara una barra de progreso con trange() que es equivalente a
# tqdm(range()) del ejemplo anterior

for num in trange(100):
    time.sleep(0.01)


# Declara una barra de progreso tqdm utilizando una lista,
# correspondiéndose en este caso el número de ciclos con el 
# número de elementos de la lista.

texto = ""
for caracter in tqdm(["a", "b", "c", "d"]):
    texto = texto + caracter


# Declara una barra de progreso tqdm con una lista y
# se establece una descripción para mostrar el elemento
# en curso a la izquierda de la barra.

barra1 = tqdm(["a", "b", "c", "d"])
for caracter in barra1:
    barra1.set_description("Procesando %s" % caracter)
    time.sleep(0.5)


# Declara una barra de progreso tqdm para un bucle de
# 100 ciclos que se actualiza cada 10 ciclos.
# El carácter utilizado para construir la barra es '#'
# y lo establece el atributo ascii con el valor True.

with tqdm(total=100, ascii=True) as barra2:
    for num in range(10):
        barra2.update(10)
        time.sleep(0.5)


# Declara una barra de progreso tqdm en un bucle para
# recorrer y acumular con accumulate() los valores de una lista

from itertools import *
barra3 = tqdm(accumulate([0, 1, 2, 3, 4, 5]))
for acumulado in barra3:
    barra3.set_description("Acumulado %i" % acumulado)
    time.sleep(0.5)


# Declara una barra de progreso tqdm en un bucle para
# recorrer y acumular con accumulate() los valores de una lista.
# En ese ejemplo el atributo disable se establece con el valor
# True para no mostrar la barra de progreso

desactivado = True
barra4 = tqdm(accumulate([0, 1, 2, 3, 4, 5]), disable=desactivado)
for acumulado in barra4:
    barra4.set_description("Acumulado %i" % acumulado)
    if desactivado:
        print(acumulado, end=' ')
    time.sleep(0.5)


# Ejecutado desde la línea de comandos lista los archivos y carpetas
# del directorio /usr/ redirigiendo la salida a un archivo de texto,
# mostrando el número de elementos encontrados y el número de
# iteraciones por segundo.

$ ls /usr/ -R | tqdm >>usr.txt


Ir al índice del tutorial de Python

Publicado por Pherkad en 14:09 
Enviar por correo electrónico
Escribe un blog
Compartir con Twitter
Compartir con Facebook
Compartir en Pinterest
Etiquetas: IPython, Jupyter, Python3
Entrada más recienteEntrada antiguaInicio
Buscar
Python para impacientes
Python
IPython
EasyGUI
Tkinter
JupyterLab
Numpy
Anexos
Guía urgente de MySQL
Guía rápida de SQLite3
Entradas + populares
Instalación de Python, paso a paso
Instalación de Python 3.6 A finales de 2016 se produjo el lanzamiento de Python 3.6 . El propósito de esta entrada es mostrar, pas...
Gráficos en IPython
Unos de los motivos que inspiraron el desarrollo de IPython fue contar con una herramienta que uniera la posibilidad de realizar cálcu...
Operaciones con fechas y horas. Calendarios
Los módulos datetime y calendar amplían las posibilidades del módulo time que provee funciones para manipular expresiones de ti...
Cálculo estadístico
El módulo statistics agrupa un conjunto de funciones para cálculo estadístico. Las funciones están organizadas en dos grupos: las...
Tkinter: interfaces gráficas en Python
Introducción Con Python hay muchas posibilidades para programar una interfaz gráfica de usuario ( GUI ) pero Tkinter es fácil d...
Cálculo con arrays Numpy
Numpy ofrece todo lo necesario para obtener un buen rendimiento cuando se trata de hacer cálculos con arrays. Por como está concebid...
Dar color a las salidas en la consola
En Python para dar color a las salidas en la consola (o en la terminal de texto) existen varias posibilidades. Hay un método basado ...
Añadir, consultar, modificar y suprimir elementos en Numpy
Acceder a los elementos de un array. [], [,], ... Acceder a un elemento de un array. Para acceder a un elemento se utiliz...
Variables de control en Tkinter
Variables de control Las variables de control son objetos especiales que se asocian a los widgets para almacenar sus valore...
Operaciones con archivos CSV
Un archivo CSV (de Valores Separados por Comas) es un tipo de documento que representa los datos de forma parecida a una tabla, es d...
Archivo

abril 2018 (2)
python.org
python.org
pypi.org
pypi.org
Sitios
ActivePython
Anaconda
Bpython
Django
Flask
Ipython
IronPython
Matplotlib
MicroPython
Numpy
Pandas
Pillow
PortablePython
PyBrain
PyCharm
PyDev
PyGame
Pypi
PyPy
Pyramid
Python.org
PyTorch
SciPy.org
Spyder
Tensorflow
TurboGears
2014-2020 | Alejandro Suárez Lamadrid y Antonio Suárez Jiménez, Andalucía - España
. Tema Sencillo. Con la tecnología de Blogger.
Después de que el pasado 3 de abril Caly et al. publicaran los resultados de un experimento in vitro sobre ivermectina y SARS-CoV-2 , el mundo de la investigación y el público en general se vieron arrastrados a dos puntos de vista extremos y opuestos sobre este tema. Un grupo pedía que se descartase el uso de este fármaco contra la COVID-19 de manera temprana, ya que las concentraciones efectivas informadas por Caly et al. eran demasiado elevadas para poder alcanzarlas in vivo y, por tanto, cualquier esfuerzo invertido en conseguirlo sería un desperdicio y generaría una falsa sensación de esperanza. El otro grupo, siguiendo un legítimo sentido de urgencia, se lanzó a promover el uso generalizado de la ivermectina, incluso sin pruebas adecuadas de eficacia y, lo que es más importante, de seguridad para este uso específico. Ambos extremos son igualmente erróneos. Aquí proporcionamos respuestas a preguntas comunes sobre este tema y mostramos la participación de investigadores de ISGlobal para encontrar respuestas a algunas de ellas.

¿Qué es la ivermectina?
La ivermectina es un fármaco antiparasitario desarrollado durante la década de 1970 tras una asociación entre el instituto Kitasato en Japón y Merck & Co. El proyecto fue dirigido por Satoshi Omura y William Campbell, respectivamente. Dado su amplio espectro contra parásitos internos y externos que mejoró la salud de los animales y aumentó la productividad, la ivermectina se convirtió rápidamente en un fármaco de gran éxito en el ámbito veterinario.

Durante la década los 70 del siglo XX, el mundo estaba librando una guerra contra la ceguera de los ríos, también conocida como oncocercosis, una enfermedad causada por el parásito Onchocerca volvulus en áreas rurales y que paralizaba comunidades enteras. Ya se habían logrado algunos éxitos gracias a la participación del Banco Mundial. Pero una vez que se comercializó la ivermectina, se observó que Onchocerca cervicalis, el agente de la oncocercosis en caballos, prácticamente desaparecía en las áreas donde se usaba el nuevo fármaco.

The burden of onchocerciasis: children leading blind adults in Africa.
La carga de oncocercosis: jóvenes guían a personas adultas invidentes en África.
[Imagen: Otis Historical Archives National Museum of Health & Medicine]

Esto condujo a esfuerzos acelerados para probar la ivermectina en humanos, que terminaron con la aprobación de las autoridades reguladoras francesas a principios de la década de 1980 y fueron seguidos por la decisión sin precedentes de Merck & Co. de donar tanta ivermectina como fuera necesaria, durante el tiempo que fuese necesario, para erradicar la ceguera de los ríos. Esto dio origen al programa de donación de Mectizan. Este programa ha distribuido más de 3.000 millones de tratamientos durante los últimos 30 años y ha contribuido a salvar innumerables vidas durante este período. Posteriormente, el programa se amplió para incluir la filariasis linfática, otra enfermedad debilitante causada por gusanos filarias.

Elephantiasis of leg due to filariasis, in Luzon, Philippines.
Elefantiasis en piernas debido a la filariasis, en Luzon, Philippines. [Imagen: CDC]

Omura y Campbell fueron galardonados con el premio Nobel de Medicina en 2015 por este descubrimiento, un honor compartido con Tu Youyou, la investigadora china responsable del descubrimiento del antimalárico artemisinina.

Más información sobre el origen y los usos de la ivermectina, aquí.

¿Para qué se usa la ivermectina en humanos?
Aquí describimos algunos de los usos aprobados en el mundo:

1. En los EE. UU., la ivermectina se comercializa en dosis de hasta 200 mcg / kg una vez al año para las siguientes indicaciones:

a. Strongyloides stercoralis, un parásito intestinal capaz de causar una enfermedad sistémica grave.
Una niña de dos años con estrongiloidiasis diseminada curada con ivermectina (A) antes, (B) muestra fecal, (C) muestra de esputo y (D) seis semanas después del tratamiento.
Una niña de dos años con estrongiloidiasis diseminada curada con ivermectina (A) antes, (B) muestra fecal, (C) muestra de esputo y (D) seis semanas después del tratamiento. [Imagen: Chaccour y Del Pozo, NEJM, 2012]

b. Oncocercosis o ceguera de los ríos
2. En Europa, la ivermectina también se comercializa contra la filariasis linfática y la sarna en dosis únicas de hasta 400 mcg / kg.

3. En Australia, se recomiendan 3 o más dosis de 200 mcg / kg en un mes para el tratamiento de la sarna costrosa severa.

Una línea de investigación muy prometedora, en la que ISGlobal ha tenido un papel primordial, es la distribución de ivermectina a nivel poblacional para matar a los mosquitos y así disminuir la transmisión de la malaria

¿Algún otro uso probado o potencial?
Aunque no se comercializa para estas indicaciones, la ivermectina tiene una eficacia parcial contra otros parásitos intestinales comunes en humanos, como Ascaris lumbricoides y Trichuris trichura.

A veces también se usa fuera de etiqueta contra ectoparásitos como piojos y Tunga penetrans, entre muchos otros parásitos internos o externos.

Una línea de investigación muy prometedora, en la que personal investigador de ISGlobal ha tenido un papel primordial, es la distribución de ivermectina a nivel poblacional para matar a los mosquitos que se alimentan de humanos o de animales tratados y así disminuir la transmisión de la malaria.

¿Es segura la ivermectina?
Cuando se utiliza para las indicaciones actuales y en las dosis aprobadas, la ivermectina es un fármaco muy seguro. Hasta la fecha, se han distribuido más de 3.000 millones de tratamientos solo en el contexto del programa de donación Mectizan con un excelente perfil de seguridad. La mayoría de las reacciones adversas son leves, transitorias y se asocian con la muerte del parásito más que con el fármaco en sí.

La ivermectina tiene afinidad por los canales de cloro glutamato dependientes, que solo están presentes en invertebrados. Los mamíferos solo expresan un canal similar que podría reaccionar de forma cruzada con la ivermectina (los canales de cloro GABA dependientes), pero estos solo se expresan en el sistema nervioso central y están protegidos por la barrera hematoencefálica, un sistema de bombeo que mantiene potenciales tóxicos fuera de nuestro sistema nervioso. A pesar de eso, Rebecca Chandler describió una serie de 28 casos con reacciones neurológicas severas después del tratamiento con ivermectina fuera de las áreas endémicas de oncocercosis.

Cuando se utiliza para las indicaciones actuales y en las dosis aprobadas, la ivermectina es un fármaco muy seguro

En individuos infectados con una carga alta (> 30.000 mf / ml) del parásito conocido como Loa loa, el tratamiento con ivermectina puede provocar encefalopatía grave y la muerte. Esto ha impedido la administración de ivermectina en varios países de África central, donde la estrategia reciente de testar y no tratar podría permitir el uso del fármaco.

No hay pruebas sólidas que apoyen el uso de ivermectina en niños de menos de 15 kg de peso. Tampoco hay evidencia que respalde el uso de ivermectina durante el embarazo.

¿Es segura la ivermectina en dosis superiores a las aprobadas?
Guzzo et al. realizaron un estudio con dosis crecientes de ivermectina en el que algunos voluntarios recibieron de manera segura dosis de hasta 2.000 mcg / kg, es decir, diez veces la dosis aprobada para la oncocercosis.

Aunque no se observó en el estudio de Guzzo, los pacientes que recibieron ivermectina en dosis de 800 mcg / kg o equivalentes en otros estudios han descrito en ocasiones alteraciones visuales transitorias.

Smit et al. administraron de forma segura 600 mcg / kg diarios durante tres días.

¿Tiene la ivermectina propiedades antivirales?
Sí. Se ha demostrado que la ivermectina inhibe la replicación de varios virus de ARN como:

Dengue
Zika
Fiebre amarilla
Virus del Nilo Occidental
Chikunguña
Encefalitis equina venezolana
Virus del bosque de Semliki
Virus Sindbis
Virus del síndrome reproductivo y respiratorio porcino y, más recientemente,
SARS-CoV-2
¿Podría la ivermectina tener un papel en el tratamiento o la prevención de la COVID-19?
Quizá, pero la respuesta no es sencilla.

Los experimentos in vitro de Caly et al. se realizaron agregando ivermectina en un cultivo celular infectado con el virus en una placa de Petri. Las concentraciones demostradas para reducir la replicación viral entre un 50% y un 99% en estos experimentos fueron 2,8 y 5 micromolar, respectivamente.

Para poner esto en perspectiva, la concentración máxima alcanzada en la sangre después de una dosis oral única de 200 mcg / kg (la dosis habitual para la ceguera de los ríos) es de 40 ng / ml. Por otro lado, 2,8 micromolar es el equivalente a 2.450 ng / ml, es decir, una concentración 60 veces mayor que la máxima concentración alcanzada tras las dosis habituales o diez veces la concentración máxima observada en el estudio de dosis altas de Guzzo.

Existe incertidumbre o equipoise —término que se utiliza en bioética para definir una situación en la que existe una duda razonable de si un medicamento puede ser útil o no

Entonces, ¿por qué molestarse en investigar esto?
La COVID-19 es una emergencia de salud pública de importancia internacional y no existe un tratamiento específico para ella. Este hecho, junto con el excelente perfil de seguridad de la ivermectina, se combinan para justificar la investigación sobre su uso potencial. Además, la extrapolación directa de una placa de Petri a un organismo vivo no es correcta. Varios factores pueden contribuir a hacer que la ivermectina sea eficaz in vivo en dosis más bajas que las descritas por Caly et al. Aquí están algunos de ellos:

Sistema inmunitario. Un cultivo celular es solo una capa de células en una placa de Petri y no contiene un sistema inmunológico para combatir el virus "codo con codo" con la ivermectina.
La carga viral. La proporción de virus y células en una placa de Petri puede estar muy lejos de lo que se puede esperar en un organismo, es decir, puede haber un exceso de la relación virus a células en el cultivo que desequilibraría la lucha a favor del virus y que llevaría a subestimar el verdadero efecto de la ivermectina en los resultados in vitro.
Efecto inmunomodulador. La ivermectina tiene la capacidad de modular la respuesta inmune. Una respuesta inmune exacerbada es en parte responsable de la fisiopatología de COVID-19.
Mayor penetración en los pulmones. Aunque existen opiniones contradictorias al respecto, la ivermectina, dada su lipofilicidad, se acumula en compartimentos profundos, incluidos los pulmones.
Desajuste previo in vitro - in vivo. Algunos efectos in vivo pueden ser posibles incluso si no se pueden lograr concentraciones eficaces in vitro. Un reciente ensayo clínico de fase III en pacientes con dengue en Tailandia, en el que se encontró que una dosis de 400 μg / kg una vez al día durante tres días era segura pero no produjo ningún beneficio clínico, mostró un efecto in vivo modesto e indirecto contra el dengue al reducir la circulación de ciertas proteínas virales.
¿Se utilizó la línea celular correcta? Las células VERO son células renales de mono verde africano, aunque se utilizan ampliamente en cultivos virales y experimentos in vitro, es posible que no sean la mejor plataforma para el SARS-CoV-2. Después de mostrar resultados prometedores en las células VERO, la hidroxicloroquina no tuvo el mismo efecto en los cultivos de células pulmonares humanas. En el caso de la ivermectina, este efecto podría o bien favorecer a la ivermectina o bien reducir su eficacia.
Otros posibles mecanismos de acción incluyen: inhibición de la enzima viral utilizada para desenrollar su ARN, la helicasa, para la que parece que la ivermectina puede ser eficaz en concentraciones mucho más bajas. Interacción con el receptor nicotínico de acetilcolina que puede causar inmunomodulación o reducir la expresión de ACE-II, el receptor utilizado por el virus para ingresar a las células.
¿Qué pasa con la importina alfa / beta? Las importinas son proteínas de transporte intracelular que a veces utilizan los virus para ingresar al núcleo y replicar su material genético. La ivermectina inhibe la importina y esto se ha citado ampliamente como el mecanismo de acción potencial contra el SARS-CoV-2. Esto puede tener un efecto en la COVID-19 a través de un mecanismo directo o indirecto1.
En resumen, hay incertidumbre o equipoise. Este es un término que se utiliza en bioética para definir una situación en la que existe una duda razonable de si un medicamento puede ser útil o no. Esto justifica que se pruebe la ivermectina contra el SARS-CoV-2 en ensayos clínicos.

Entonces, ¿por qué no administramos ivermectina a las personas que la necesitan hoy? ¡La gente se está muriendo!
El proceso para lograr un uso generalizado de un medicamento está muy regulado. Esto se debe a buenas razones. Se han aprendido lecciones después de algunas tragedias graves.

El principal requisito para aprobar un medicamento para su uso a nivel poblacional es su eficacia y seguridad probadas. Para la ivermectina hay pruebas sólidas de seguridad cuando se usa para la indicación aprobada en las dosis aprobadas. Existe evidencia muy limitada sobre la seguridad de la ivermectina en dosis más altas (ver enlace de la tabla anterior) e incluso menos evidencia de su seguridad cuando se usa en pacientes con COVID-19 que tienden a tener estados proinflamatorios. Estos estados proinflamatorios pueden aumentar la penetración de la ivermectina en el sistema nervioso central con consecuencias desconocidas (ver seguridad arriba). Otros problemas potenciales incluyen interacciones medicamentosas con algunos antivirales administrados a pacientes con COVID-19, como el ritonavir, que pueden aumentar los niveles de ivermectina.

Dada la ausencia de evidencia razonable de que la ivermectina tenga alguna eficacia contra el SARS-CoV-2, el análisis de riesgo-beneficio dicta que debemos ser prudentes

Dada la ausencia de evidencia razonable de que la ivermectina tenga alguna eficacia contra el SARS-CoV-2, el análisis de riesgo-beneficio dicta que debemos ser prudentes, es decir, evaluar la eficacia (y la seguridad en este contexto) antes de adoptar ivermectina a nivel poblacional.

La evidencia de la eficacia debe provenir de ensayos clínicos aleatorizados controlados (preferiblemente doble ciego), que son estudios en los que los pacientes se asignan al azar para recibir ivermectina o un placebo y en los que ni el paciente ni el médico saben qué producto se administra. Esto se hace para prevenir sesgos comunes que pueden afectar a los resultados de los ensayos no aleatorizados, como que el fármaco en cuestión se administre solo a pacientes más graves (o menos graves), la administración de fármacos adicionales que podrían cambiar el resultado, la retirada de pacientes que no "funcionan bien ”después del tratamiento, etc.

Los ensayos clínicos llevan mucho tiempo, ¿por qué esperar? Si es seguro, ¿qué se puede perder?
El uso generalizado de la ivermectina puede (¡y ya lo ha hecho!) provocar un uso indebido. La ivermectina está ampliamente disponible como medicamento veterinario. Si existe la percepción de que la ivermectina es beneficiosa para los pacientes con COVID-19, es previsible que las formulaciones veterinarias se utilicen a gran escala y este uso sin supervisión puede provocar sobredosis y otras prácticas nocivas. La Agencia Federal del Medicamento de EE. UU. ha emitido una advertencia sobre el uso de productos veterinarios con ivermectina.

En Perú, más de 5.000 indígenas fueron inyectados con ivermectina veterinaria por un grupo con buenas intenciones. ¿Eran conscientes de que se les estaba inyectando un producto destinado a uso animal?

En esta línea, también hay informes de inyección con productos veterinarios relacionados con la ivermectina, como la doramectina, una molécula nunca antes utilizada en humanos con un perfil de seguridad desconocido.

Si efectivamente se producen eventos adversos como resultado de este uso indiscriminado, la distribución del medicamento para indicaciones comprobadas (por ejemplo, ceguera de los ríos) podría verse afectada, ya que las personas podrían negarse a tomar la versión del medicamento para humanos como consecuencia de los efectos en quienes se inyectan productos veterinarios.

Los parásitos intestinales son muy frecuentes en los trópicos. Sabemos que los helmintos intestinales modulan la forma en que nuestro sistema inmunitario reacciona ante las infecciones. Dado que la ivermectina es un muy buen desparasitante, no sabemos cómo la desparasitación masiva puede afectar a la forma en que el cuerpo responde contra el SARS-CoV-2. Primum non nocere o quizás primum ivermectinum, secundum non nocere?

Riesgo moral. Aquellos que reciben ivermectina como tratamiento o profilaxis pueden sentirse protegidos y cumplir de manera menos eficiente con medidas comprobadas como mascarillas y distanciamiento social.

Por último, tenemos en la mayor estima a los trabajadores de la salud de primera línea y no juzgamos las decisiones y la desesperación de aquellos que enfrentan pacientes moribundos o que empeoran. Pero, ¿podría el uso de ivermectina (que es relativamente segura) ser una forma de ofrecer algo y reducir la presión política y pública sobre los responsables políticos y las autoridades sanitarias?

Entonces, ¿por qué está en las directrices nacionales de Perú, Bolivia y varios municipios de Brasil?
Muchos países, ante un aumento exponencial en el número de casos y muertes, estaban evaluando activamente el horizonte en busca de medidas preventivas y tratamientos emergentes. El estudio in vitro de Caly et al. que hizo afirmaciones enfáticas para las dosis utilizadas preparó el escenario para la ivermectina, pero fue seguido rápidamente por lo que parece ser un informe fraudulento que muestra supuestos excelentes resultados en un estudio de casos y controles. Esa preimpresión de Surgisphere, aunque posteriormente retractada, todavía se cita ampliamente como evidencia de la eficacia de la ivermectina contra la COVID-19, particularmente en América Latina.

A diferencia de muchos lugares del mundo, la ivermectina se produce localmente en forma de una formulación de gotas en muchos países de América Latina. Esta disponibilidad local puede haber influido en su popularidad.

¡Pero hay evidencia de eficacia adicional de otros estudios!
Los ensayos con resultados actualmente disponibles no se basan en el estándar de oro: los ensayos controlados aleatorizados (preferiblemente, doble ciegos). En su mayoría son estudios de casos y controles e incluso estudios que comparan dos esquemas de medicamentos completamente diferentes que difícilmente pueden probar el beneficio de la ivermectina. La pirámide de evidencia en medicina clínica (por Akobeng en BMJ) se proporciona a continuación como referencia. Se debe tener cuidado con los informes aislados de "buena experiencia clínica", dado el riesgo de sesgo mencionado anteriormente. A veces, incluso los datos ecológicos se utilizan para respaldar las afirmaciones de eficacia o falta de eficacia de la ivermectina contra la COVID-19, pero estos conjuntos de datos existentes en cada país se pueden tergiversar de manera que reflejen mejor las opiniones del analista.

Jerarquía de evidencia para preguntas sobre la efectividad de una intervención o tratamiento.
Jerarquía de evidencia para preguntas sobre la efectividad de una intervención o tratamiento. [Fuente: Akobeng AK. Understanding randomised controlled trials. Archives of Disease in Childhood 2005;90:840-844.]

¿Qué ensayos clínicos están en curso?
Según la plataforma ClinicalTrials.gov, hay 34 ensayos que usan ivermectina en pacientes con COVID-19, de los cuales 31 aún están activos (a fecha de 17 de agosto de 2020). Ninguno de los ensayos marcados como completados tiene resultados disponibles en esa plataforma.

¡Las grandes farmacéuticas no quieren que se use ivermectina porque quieren beneficiarse de medicamentos más costosos!
Existen varios modelos de negocio en la industria farmacéutica. Vender volúmenes bajos a precios altos es una opción con fines de lucro. Pero en el caso de la COVID-19, el mercado objetivo es la humanidad, es decir, unos 7.000 millones de personas que son susceptibles de tomar este medicamento ahora y en un futuro cercano. Incluso con márgenes muy, muy bajos, un volumen tan alto definitivamente compensaría a cualquier fabricante dispuesto a aumentar la producción y la distribución.

¿Qué ha hecho ISGlobal sobre este tema?
ISGlobal se ha asociado con la Clínica Universidad de Navarra para patrocinar un ensayo clínico, SAINT, que podría ayudar a probar el concepto de un efecto biológico de la ivermectina en la replicación del SARS-CoV-2. El protocolo de este ensayo está disponible abiertamente en su totalidad para cualquier persona que desee reproducirlo. El ensayo en Navarra ya ha reclutado al 20% de su objetivo y los resultados deberían estar disponibles cuatro semanas después de reclutar al último paciente. El reclutamiento ha sido más lento de lo previsto porque la aprobación regulatoria se produjo durante el cierre que llevó los casos a niveles tan bajos como 0-4 casos por día en toda Navarra durante junio y julio.

ISGlobal se ha asociado con la Clínica Universidad de Navarra para patrocinar un ensayo clínico, SAINT, que podría ayudar a probar el concepto de un efecto biológico de la ivermectina en la replicación del SARS-CoV-2

ISGlobal se ha asociado con la Universidad Cayetano Heredia en Perú para replicar el ensayo SAINT en Lima.

Varios investigadores de ISGlobal desempeñaron un papel clave en el descubrimiento del escándalo #LancetGate sobre la hidroxicloroquina y la ivermectina.

Se están realizando ensayos adicionales y sus resultados se divulgarán a su debido tiempo.

 

Actualizaciones
1 El 17 de diciembre de 2020, se actualizó la respuesta a la pregunta "¿Qué pasa con la importina alfa / beta?". Debido a la aparición de nueva evidencia, se eliminó la frase: "Esto es muy desconcertante, ya que el SARS-CoV-2 se replica en el citosol. No entra en el núcleo para su replicación. Por lo tanto, es poco probable que el mecanismo de importación ampliamente citado tenga un papel aquí". Y se reemplazó por esta otra: "Esto puede tener un efecto en la COVID-19 a través de un mecanismo directo o indirecto."

Contenidos relacionados
Ivermectina y COVID-19: ¿qué está pasando?


Ivermectina y COVID-19
Ivermectina y COVID-19
Cómo una base de datos dudosa dio forma a la respuesta de varios países latinoamericanos a la pandemia

Nota: Las personas que integran ISGlobal persiguen ideas innovadoras con total independencia. Las opiniones expresadas en este blog son, por tanto, a título personal y no necesariamente reflejan el posicionamiento institucional.

Una iniciativa de
Obra Social, Fundació "la Caixa"
Clínic Barcelona, Hospital Universitari
Parc de Salut
Universitat de Barcelona
Universitat Pompeu Fabra
Generalitat de Catalunya
Gobierno de España
Ajuntament de Barcelona
ISGlobal
Sobre ISGlobal
Investigación
Iniciativas
Análisis y Desarrollo
Formación
Actualidad
Campus Clínic
C/ Rosselló, 132, 5º 2ª 08036. Barcelona. Tel. +34 93 227 1806

Campus Mar
C/ Doctor Aiguader, 88. 08003. Barcelona. Tel. +34 93 214 7300

Excelencia Severo Ochoa
Institució CERCA
HR Excellence in Research
    
Contacto
Aviso legal
Política de privacidad
Política de Cookies
Instituto de Salud Global de Barcelona (ISGlobal), 2018.
Acceso/Acerca de la OMS/Quiénes somos/Preguntas más frecuentes
Preguntas más frecuentes
 Section navigation
¿Cómo define la OMS la salud?
«La salud es un estado de completo bienestar físico, mental y social, y no solamente la ausencia de afecciones o enfermedades». La cita procede del Preámbulo de la Constitución de la Organización Mundial de la Salud, que fue adoptada por la Conferencia Sanitaria Internacional, celebrada en Nueva York del 19 de junio al 22 de julio de 1946, firmada el 22 de julio de 1946 por los representantes de 61 Estados (Official Records of the World Health Organization, Nº 2, p. 100), y entró en vigor el 7 de abril de 1948. La definición no ha sido modificada desde 1948.

 

Constitución de la OMS: principios

Publicaciones. ¿Dónde puedo encontrar información sobre las publicaciones de la OMS?
La barra de navegación situada en la parte superior del sitio web de la OMS tiene un enlace denominado 'Publicaciones' que conduce a una página informativa sobre las publicaciones de la OMS, desde la que se puede acceder a la librería en línea, a información sobre suscripciones y a información sobre las principales publicaciones y revistas.

Publicaciones

Enlaces. ¿Puedo poner en mi sitio web un enlace al sitio web de la OMS? ¿Qué debo hacer para pedir que en el sitio de la OMS haya un enlace a mi sitio web?
En principio, todo sitio web externo puede proporcionar un hiperenlace al sitio web de la OMS sin necesidad de autorización. Sin embargo, este uso no debe infringir los derechos de propiedad intelectual de la OMS, en particular los relativos al nombre, al emblema y a los derechos de autor de la Organización. Normalmente, la OMS no proporciona enlaces a sitios web externos, a no ser que exista una clara vinculación con sus actividades. Para más información, remítase a la página Autorizaciones y licencias.

Autorizaciones y licencias

Empleo. ¿Dónde puedo encontrar información sobre las oportunidades de empleo en la OMS?
La página Empleo en la OMS proporciona una lista actualizada de los puestos vacantes y de los tipos de contratos. Cree su perfil personal en el sistema electrónico de contratación (en inglés) y preséntese a los puestos que figuren en la lista.

La página Empleo en la OMS

Pasantías. ¿Dónde puedo encontrar información sobre las pasantías?
La información al respecto se encuentra en la página Pasantías.

Pasantías

Becas y subvenciones. ¿Ofrece la OMS becas y subvenciones a la investigación?
La OMS no dispone de un programa de becas o subvenciones como tal, pero hay algunos departamentos y programas especiales de la OMS que financian investigaciones. Visite la página Grants (Subsidios) del sitio web del TDR (Investigación sobre Enfermedades Tropicales) o la página Capacity strengthening (Fortalecimiento de la capacidad) del sitio web de RHR (Investigación sobre Salud Reproductiva). Asimismo, puede consultar el sitio web de la Oficina Regional de la OMS a la que pertenezca su país. Las oficinas regionales tienen algunos programas de becas que se llevan a cabo con la cooperación de los ministerios de salud de los países.

Programa de Investigación sobre Enferemedades Tropicales - en inglés

Países

Investigación. Estoy haciendo una investigación ¿por dónde debo empezar a buscar información?
Si se trata de una investigación sobre un tema de salud en particular, empiece por buscar en la lista de Temas de salud. La página de cada tema de salud proporciona listas de otros sitios conexos, enlaces y documentos. Si busca información sobre un determinado país o región de la OMS, diríjase al Sitio web de la oficina regional de la OMS correspondiente. También puede encontrar información sobre los países a través del enlace Países situado en la barra de navegación de la izquierda. En la página Investigación encontrará una serie de recursos que podrá utilizar en su investigación, entre ellos bases de datos estadísticas y el catálogo de la biblioteca.

Temas de salud

Oficinas regionales de la OMS

Países

¿Cómo puedo solicitar información sobre la OMS?
La OMS pone información a disposición del público con arreglo a su Política de divulgación de información. La Política tiene por objeto aumentar la información disponible y se introducirá progresivamente durante un periodo de dos años. A más tardar en noviembre de 2017 se habilitará una dirección de correo electrónico para solicitar información: informationrequest@who.int.

Política de divulgación de información

Nombres de los países. ¿Quién determina los nombres oficiales de los países?
Las denominaciones oficiales de los Estados Miembros de la OMS y su posición relativa en las listas alfabéticas se basan en información recibida de los propios Estados Miembros y de las Naciones Unidas.

Países

Mapas ¿Por qué son discontinuas las líneas de las fronteras en algunos mapas?
Los límites y los nombres que figuran en los mapas, así como las denominaciones empleadas, no implican, por parte de la Organización Mundial de la Salud, juicio alguno sobre la condición jurídica de países, territorios, ciudades o zonas, o de sus autoridades, ni respecto del trazado de sus fronteras o límites. En los mapas, las líneas discontinuas representan de manera aproximada fronteras respecto de las cuales puede que no haya pleno acuerdo.

Galería de mapas - en inglés

Salud de los viajeros. Pienso viajar al extranjero en breve. ¿Dónde puedo encontrar información y consejos sobre los riesgos para la salud?
En el sitio web Salud de los viajeros (en inglés) puede encontrar información sobre las vacunas exigidas, los riesgos y las precauciones que debe adoptar dependiendo de cuál sea su país de destino.

Salud de los viajeros - en inglés

¿He recibido un email fraudulento?
Esos e-mail no proceden de la OMS ni guardan ninguna relación con proyectos o eventos de la OMS. La OMS pone sobre aviso al público acerca de esas prácticas engañosas y sugiere a quienes reciban invitaciones como las mencionadas más arriba (ya sea por correo electrónico o de alguna otra forma) que verifiquen su autenticidad antes de responder. En particular, la OMS sugiere que no se envíe dinero ni información personal a nadie que afirme otorgar fondos, subvenciones, becas, certificados o premios de lotería o de otra índole, y/o exija pagos en concepto de inscripción o de reserva de hotel, en nombre de la OMS. La OMS no tiene por norma exigir pago ninguno a quienes participan en sus reuniones.

Circulan por Internet mensajes electrónicos fraudulentos que afirman o dan a entender que proceden de la Organización Mundial de la Salud (OMS), o que están relacionados con la Organización. En algunos se pide a particulares, empresas u organizaciones sin fines de lucro que proporcionen información o dinero con la promesa de que recibirán fondos o alguna prestación a cambio. En otros mensajes se pide el abono de cuotas de inscripción en conferencias supuestamente patrocinadas por la OMS, o para reservas de hotel, también con la promesa de alguna prestación. En ocasiones, esos mensajes portan el emblema de la OMS y proceden de direcciones electrónicas semejantes a las de la OMS o las Naciones Unidas, o mencionan esas direcciones electrónicas.

Si tiene dudas sobre la autenticidad de algún mensaje electrónico, carta o llamada telefónica que supuestamente reciba de la OMS, o en nombre de la OMS, por favor escríbanos a la dirección internet@who.int. La OMS está intentado poner fin a esas prácticas engañosas, por lo que les quedaríamos muy agradecidos si señalan a nuestra atención cualquier comunicación sospechosa.

No haga caso de los mensajes falsamente relacionados con la OMS

 

Acerca de la OMS
Ayuda y servicios
Oficinas regionales de la OMS
Acceso
Política de privacidad
© 2021 OMS
unittest — Marco de prueba automatizado
Propósito:	Marco de prueba automatizado
El módulo unittest de Python se basa en el diseño del marco XUnit de Kent Beck y Erich Gamma. El mismo patrón se repite en muchos otros lenguajes, incluidos C, Perl, Java y Smalltalk. El marco implementado por unittest admite accesorios (fixtures), conjuntos de pruebas y un corredor de pruebas para permitir las pruebas automatizadas.

Estructura básica de las pruebas
Las pruebas, según lo definido por unittest, tienen dos partes: código para administrar las dependencias de prueba (llamadas fixtures) y la prueba en sí. Las pruebas individuales se crean subclasificando TestCase y anulando o agregando los métodos apropiados. En el siguiente ejemplo, el SimplisticTest tiene un solo método test(), que fallaría si a es diferente de b.

unittest_simple.py
import unittest


class SimplisticTest(unittest.TestCase):

    def test(self):
        a = 'a'
        b = 'a'
        self.assertEqual(a, b)
Ejecutar las pruebas
La forma más fácil de ejecutar pruebas de unittest es usar el descubrimiento automático disponible a través de la interfaz de línea de comandos.

$ python3 -m unittest unittest_simple.py

.
----------------------------------------------------------------
Ran 1 test in 0.000s

OK
Esta salida abreviada incluye la cantidad de tiempo que tomaron las pruebas, junto con un indicador de estado para cada prueba (el «.» En la primera línea de salida significa que pasó una prueba). Para obtener resultados de prueba más detallados, incluye la opción -v.

$ python3 -m unittest -v unittest_simple.py

test (unittest_simple.SimplisticTest) ... ok

----------------------------------------------------------------
Ran 1 test in 0.000s

OK
Resultados de las pruebas
Las pruebas tienen 3 resultados posibles, descritos en: tabla: Resultados del caso de prueba.

Resultados del caso de prueba
Resultado	Descripción
ok	La prueba pasa.
FAIL	La prueba no pasa y genera una excepción AssertionError.
ERROR	La prueba genera cualquier excepción que no es AssertionError.
No hay una forma explícita de hacer que una prueba «pase», por lo que el estado de una prueba depende de la presencia (o ausencia) de una excepción.

unittest_outcomes.py
import unittest


class OutcomesTest(unittest.TestCase):

    def testPass(self):
        return

    def testFail(self):
        self.assertFalse(True)

    def testError(self):
        raise RuntimeError('Test error!')
Cuando una prueba falla o genera un error, el rastreo se incluye en la salida.

$ python3 -m unittest unittest_outcomes.py

EF.
================================================================
ERROR: testError (unittest_outcomes.OutcomesTest)
----------------------------------------------------------------
Traceback (most recent call last):
  File ".../unittest_outcomes.py", line 18, in testError
    raise RuntimeError('Test error!')
RuntimeError: Test error!

================================================================
FAIL: testFail (unittest_outcomes.OutcomesTest)
----------------------------------------------------------------
Traceback (most recent call last):
  File ".../unittest_outcomes.py", line 15, in testFail
    self.assertFalse(True)
AssertionError: True is not false

----------------------------------------------------------------
Ran 3 tests in 0.001s

FAILED (failures=1, errors=1)
En el ejemplo anterior, testFail() falla y el rastreo muestra la línea con el código de falla. Sin embargo, depende de la persona que lee el resultado de la prueba mirar el código para descubrir el significado de la prueba fallida.

unittest_failwithmessage.py
import unittest


class FailureMessageTest(unittest.TestCase):

    def testFail(self):
        self.assertFalse(True, 'failure message goes here')
Para facilitar la comprensión de la naturaleza de una falla en la prueba, los métodos fail*() y assert*() aceptan un argumento msg, que se puede usar para generar mensaje de error detallado.

$ python3 -m unittest -v unittest_failwithmessage.py

testFail (unittest_failwithmessage.FailureMessageTest) ... FAIL

================================================================
FAIL: testFail (unittest_failwithmessage.FailureMessageTest)
----------------------------------------------------------------
Traceback (most recent call last):
  File ".../unittest_failwithmessage.py", line 12, in testFail
    self.assertFalse(True, 'failure message goes here')
AssertionError: True is not false : failure message goes here

----------------------------------------------------------------
Ran 1 test in 0.000s

FAILED (failures=1)
Probar la verdad
La mayoría de las pruebas afirman la verdad de alguna condición. Hay dos formas diferentes de escribir pruebas de verificación de la verdad, según la perspectiva del autor de la prueba y el resultado deseado del código que se está probando.

unittest_truth.py
import unittest


class TruthTest(unittest.TestCase):

    def testAssertTrue(self):
        self.assertTrue(True)

    def testAssertFalse(self):
        self.assertFalse(False)
Si el código produce un valor que se puede evaluar como verdadero, se debe usar el método assertTrue(). Si el código produce un valor falso, el método assertFalse() tiene más sentido.

$ python3 -m unittest -v unittest_truth.py

testAssertFalse (unittest_truth.TruthTest) ... ok
testAssertTrue (unittest_truth.TruthTest) ... ok

----------------------------------------------------------------
Ran 2 tests in 0.000s

OK
Probar la igualdad
Como caso especial, unittest incluye métodos para probar la igualdad de dos valores.

unittest_equality.py
import unittest


class EqualityTest(unittest.TestCase):

    def testExpectEqual(self):
        self.assertEqual(1, 3 - 2)

    def testExpectEqualFails(self):
        self.assertEqual(2, 3 - 2)

    def testExpectNotEqual(self):
        self.assertNotEqual(2, 3 - 2)

    def testExpectNotEqualFails(self):
        self.assertNotEqual(1, 3 - 2)
Cuando fallan, estos métodos de prueba especiales producen mensajes de error que incluyen los valores que se comparan.

$ python3 -m unittest -v unittest_equality.py

testExpectEqual (unittest_equality.EqualityTest) ... ok
testExpectEqualFails (unittest_equality.EqualityTest) ... FAIL
testExpectNotEqual (unittest_equality.EqualityTest) ... ok
testExpectNotEqualFails (unittest_equality.EqualityTest) ...
FAIL

================================================================
FAIL: testExpectEqualFails (unittest_equality.EqualityTest)
----------------------------------------------------------------
Traceback (most recent call last):
  File ".../unittest_equality.py", line 15, in
testExpectEqualFails
    self.assertEqual(2, 3 - 2)
AssertionError: 2 != 1

================================================================
FAIL: testExpectNotEqualFails (unittest_equality.EqualityTest)
----------------------------------------------------------------
Traceback (most recent call last):
  File ".../unittest_equality.py", line 21, in
testExpectNotEqualFails
    self.assertNotEqual(1, 3 - 2)
AssertionError: 1 == 1

----------------------------------------------------------------
Ran 4 tests in 0.001s

FAILED (failures=2)
¿Casi igual?
Además de la igualdad estricta, es posible probar la casi igualdad de los números de coma flotante usando assertAlmostEqual() y assertNotAlmostEqual().

unittest_almostequal.py
import unittest


class AlmostEqualTest(unittest.TestCase):

    def testEqual(self):
        self.assertEqual(1.1, 3.3 - 2.2)

    def testAlmostEqual(self):
        self.assertAlmostEqual(1.1, 3.3 - 2.2, places=1)

    def testNotAlmostEqual(self):
        self.assertNotAlmostEqual(1.1, 3.3 - 2.0, places=1)
Los argumentos son los valores a comparar y el número de decimales que se utilizarán para la prueba.

$ python3 -m unittest unittest_almostequal.py

.F.
================================================================
FAIL: testEqual (unittest_almostequal.AlmostEqualTest)
----------------------------------------------------------------
Traceback (most recent call last):
  File ".../unittest_almostequal.py", line 12, in testEqual
    self.assertEqual(1.1, 3.3 - 2.2)
AssertionError: 1.1 != 1.0999999999999996

----------------------------------------------------------------
Ran 3 tests in 0.001s

FAILED (failures=1)
Contenedores
Además del genérico assertEqual() y assertNotEqual(), hay métodos especiales para comparar contenedores como objetos list, dict y set.

unittest_equality_container.py
import textwrap
import unittest


class ContainerEqualityTest(unittest.TestCase):

    def testCount(self):
        self.assertCountEqual(
            [1, 2, 3, 2],
            [1, 3, 2, 3],
        )

    def testDict(self):
        self.assertDictEqual(
            {'a': 1, 'b': 2},
            {'a': 1, 'b': 3},
        )

    def testList(self):
        self.assertListEqual(
            [1, 2, 3],
            [1, 3, 2],
        )

    def testMultiLineString(self):
        self.assertMultiLineEqual(
            textwrap.dedent("""
            This string
            has more than one
            line.
            """),
            textwrap.dedent("""
            This string has
            more than two
            lines.
            """),
        )

    def testSequence(self):
        self.assertSequenceEqual(
            [1, 2, 3],
            [1, 3, 2],
        )

    def testSet(self):
        self.assertSetEqual(
            set([1, 2, 3]),
            set([1, 3, 2, 4]),
        )

    def testTuple(self):
        self.assertTupleEqual(
            (1, 'a'),
            (1, 'b'),
        )
Cada método informa la desigualdad utilizando un formato que es significativo para el tipo de entrada, lo que hace que las fallas de prueba sean más fáciles de entender y corregir.

$ python3 -m unittest unittest_equality_container.py

FFFFFFF
================================================================
FAIL: testCount
(unittest_equality_container.ContainerEqualityTest)
----------------------------------------------------------------
Traceback (most recent call last):
  File ".../unittest_equality_container.py", line 15, in
testCount
    [1, 3, 2, 3],
AssertionError: Element counts were not equal:
First has 2, Second has 1:  2
First has 1, Second has 2:  3

================================================================
FAIL: testDict
(unittest_equality_container.ContainerEqualityTest)
----------------------------------------------------------------
Traceback (most recent call last):
  File ".../unittest_equality_container.py", line 21, in
testDict
    {'a': 1, 'b': 3},
AssertionError: {'a': 1, 'b': 2} != {'a': 1, 'b': 3}
- {'a': 1, 'b': 2}
?               ^

+ {'a': 1, 'b': 3}
?               ^


================================================================
FAIL: testList
(unittest_equality_container.ContainerEqualityTest)
----------------------------------------------------------------
Traceback (most recent call last):
  File ".../unittest_equality_container.py", line 27, in
testList
    [1, 3, 2],
AssertionError: Lists differ: [1, 2, 3] != [1, 3, 2]

First differing element 1:
2
3

- [1, 2, 3]
+ [1, 3, 2]

================================================================
FAIL: testMultiLineString
(unittest_equality_container.ContainerEqualityTest)
----------------------------------------------------------------
Traceback (most recent call last):
  File ".../unittest_equality_container.py", line 41, in
testMultiLineString
    """),
AssertionError: '\nThis string\nhas more than one\nline.\n' !=
'\nThis string has\nmore than two\nlines.\n'

- This string
+ This string has
?            ++++
- has more than one
? ----           --
+ more than two
?           ++
- line.
+ lines.
?     +


================================================================
FAIL: testSequence
(unittest_equality_container.ContainerEqualityTest)
----------------------------------------------------------------
Traceback (most recent call last):
  File ".../unittest_equality_container.py", line 47, in
testSequence
    [1, 3, 2],
AssertionError: Sequences differ: [1, 2, 3] != [1, 3, 2]

First differing element 1:
2
3

- [1, 2, 3]
+ [1, 3, 2]

================================================================
FAIL: testSet
(unittest_equality_container.ContainerEqualityTest)
----------------------------------------------------------------
Traceback (most recent call last):
  File ".../unittest_equality_container.py", line 53, in testSet
    set([1, 3, 2, 4]),
AssertionError: Items in the second set but not the first:
4

================================================================
FAIL: testTuple
(unittest_equality_container.ContainerEqualityTest)
----------------------------------------------------------------
Traceback (most recent call last):
  File ".../unittest_equality_container.py", line 59, in
testTuple
    (1, 'b'),
AssertionError: Tuples differ: (1, 'a') != (1, 'b')

First differing element 1:
'a'
'b'

- (1, 'a')
?      ^

+ (1, 'b')
?      ^


----------------------------------------------------------------
Ran 7 tests in 0.005s

FAILED (failures=7)
Use assertIn() para probar la membresía del contenedor.

unittest_in.py
import unittest


class ContainerMembershipTest(unittest.TestCase):

    def testDict(self):
        self.assertIn(4, {1: 'a', 2: 'b', 3: 'c'})

    def testList(self):
        self.assertIn(4, [1, 2, 3])

    def testSet(self):
        self.assertIn(4, set([1, 2, 3]))
Cualquier objeto que admita el operador in o la interfaz de programación del contenedor se puede usar con assertIn().

$ python3 -m unittest unittest_in.py

FFF
================================================================
FAIL: testDict (unittest_in.ContainerMembershipTest)
----------------------------------------------------------------
Traceback (most recent call last):
  File ".../unittest_in.py", line 12, in testDict
    self.assertIn(4, {1: 'a', 2: 'b', 3: 'c'})
AssertionError: 4 not found in {1: 'a', 2: 'b', 3: 'c'}

================================================================
FAIL: testList (unittest_in.ContainerMembershipTest)
----------------------------------------------------------------
Traceback (most recent call last):
  File ".../unittest_in.py", line 15, in testList
    self.assertIn(4, [1, 2, 3])
AssertionError: 4 not found in [1, 2, 3]

================================================================
FAIL: testSet (unittest_in.ContainerMembershipTest)
----------------------------------------------------------------
Traceback (most recent call last):
  File ".../unittest_in.py", line 18, in testSet
    self.assertIn(4, set([1, 2, 3]))
AssertionError: 4 not found in {1, 2, 3}

----------------------------------------------------------------
Ran 3 tests in 0.001s

FAILED (failures=3)
Probar las excepciones
Como se mencionó anteriormente, si una prueba genera una excepción distinta de AssertionError, se trata como un error. Esto es muy útil para descubrir errores al modificar el código que tiene cobertura de prueba existente. Sin embargo, hay circunstancias en las que la prueba debe verificar que algún código produce una excepción. Por ejemplo, si se da un valor no válido a un atributo de un objeto. En tales casos, assertRaises() hace que el código sea más claro que atrapar la excepción en la prueba. Compara estas dos pruebas:

unittest_exception.py
import unittest


def raises_error(*args, **kwds):
    raise ValueError('Invalid value: ' + str(args) + str(kwds))


class ExceptionTest(unittest.TestCase):

    def testTrapLocally(self):
        try:
            raises_error('a', b='c')
        except ValueError:
            pass
        else:
            self.fail('Did not see ValueError')

    def testAssertRaises(self):
        self.assertRaises(
            ValueError,
            raises_error,
            'a',
            b='c',
        )
Los resultados para ambos son los mismos, pero la segunda prueba que usa assertRaises() es más sucinta.

$ python3 -m unittest -v unittest_exception.py

testAssertRaises (unittest_exception.ExceptionTest) ... ok
testTrapLocally (unittest_exception.ExceptionTest) ... ok

----------------------------------------------------------------
Ran 2 tests in 0.000s

OK
Accesorios de las pruebas
Los accesorios (fixtures) son recursos externos necesarios para una prueba. Por ejemplo, las pruebas para una clase pueden necesitar una instancia de otra clase que proporcione ajustes de configuración u otro recurso compartido. Otros accesorios de prueba incluyen conexiones de bases de datos y archivos temporales (muchas personas argumentan que el uso de recursos externos hace que tales pruebas no sean pruebas «unitarias», pero siguen siendo pruebas y aún son útiles).

unittest incluye ganchos especiales para configurar y limpiar cualquier accesorio necesario para las pruebas. Para establecer dispositivos para cada caso de prueba individual, anula setUp() en el TestCase. Para limpiarlos, anula tearDown(). Para administrar un conjunto de dispositivos para todas las instancias de una clase de prueba, anula los métodos de clase setUpClass() y tearDownClass() para el TestCase. Y para manejar operaciones de configuración especialmente costosas para todas las pruebas dentro de un módulo, usa las funciones de nivel de módulo setUpModule() y tearDownModule().

unittest_fixtures.py
import random
import unittest


def setUpModule():
    print('In setUpModule()')


def tearDownModule():
    print('In tearDownModule()')


class FixturesTest(unittest.TestCase):

    @classmethod
    def setUpClass(cls):
        print('In setUpClass()')
        cls.good_range = range(1, 10)

    @classmethod
    def tearDownClass(cls):
        print('In tearDownClass()')
        del cls.good_range

    def setUp(self):
        super().setUp()
        print('\nIn setUp()')
        # Pick a number sure to be in the range. The range is
        # defined as not including the "stop" value, so make
        # sure it is not included in the set of allowed values
        # for our choice.
        self.value = random.randint(
            self.good_range.start,
            self.good_range.stop - 1,
        )

    def tearDown(self):
        print('In tearDown()')
        del self.value
        super().tearDown()

    def test1(self):
        print('In test1()')
        self.assertIn(self.value, self.good_range)

    def test2(self):
        print('In test2()')
        self.assertIn(self.value, self.good_range)
Cuando se ejecuta esta prueba de muestra, el orden de ejecución del dispositivo y los métodos de prueba son evidentes.

$ python3 -u -m unittest -v unittest_fixtures.py

In setUpModule()
In setUpClass()
test1 (unittest_fixtures.FixturesTest) ...
In setUp()
In test1()
In tearDown()
ok
test2 (unittest_fixtures.FixturesTest) ...
In setUp()
In test2()
In tearDown()
ok
In tearDownClass()
In tearDownModule()

----------------------------------------------------------------
Ran 2 tests in 0.000s

OK
Es posible que no se invoquen todos los métodos tearDown si hay errores en el proceso de limpieza de los accesorios. Para asegurarte de que un dispositivo se libere siempre correctamente, usa addCleanup().

unittest_addcleanup.py
import random
import shutil
import tempfile
import unittest


def remove_tmpdir(dirname):
    print('In remove_tmpdir()')
    shutil.rmtree(dirname)


class FixturesTest(unittest.TestCase):

    def setUp(self):
        super().setUp()
        self.tmpdir = tempfile.mkdtemp()
        self.addCleanup(remove_tmpdir, self.tmpdir)

    def test1(self):
        print('\nIn test1()')

    def test2(self):
        print('\nIn test2()')
Esta prueba de ejemplo crea un directorio temporal y luego usa shutil para limpiarlo cuando se realiza la prueba.

$ python3 -u -m unittest -v unittest_addcleanup.py

test1 (unittest_addcleanup.FixturesTest) ...
In test1()
In remove_tmpdir()
ok
test2 (unittest_addcleanup.FixturesTest) ...
In test2()
In remove_tmpdir()
ok

----------------------------------------------------------------
Ran 2 tests in 0.002s

OK
Repetir pruebas con diferentes entradas
Con frecuencia es útil ejecutar la misma lógica de prueba con diferentes entradas. En lugar de definir un método de prueba separado para cada caso pequeño, una forma común de hacerlo es usar un método de prueba que contenga varias afirmaciones relacionadas. El problema con este enfoque es que tan pronto como una afirmación falla, el resto se omite. Una mejor solución es usar subTest() para crear un contexto para una prueba dentro de un método de prueba. Si la prueba falla, se informa la falla y las pruebas restantes continúan.

unittest_subtest.py
import unittest


class SubTest(unittest.TestCase):

    def test_combined(self):
        self.assertRegex('abc', 'a')
        self.assertRegex('abc', 'B')
        # The next assertions are not verified!
        self.assertRegex('abc', 'c')
        self.assertRegex('abc', 'd')

    def test_with_subtest(self):
        for pat in ['a', 'B', 'c', 'd']:
            with self.subTest(pattern=pat):
                self.assertRegex('abc', pat)
En este ejemplo, el método test_combined() nunca ejecuta las afirmaciones para los patrones 'c' y 'd'. El método test_with_subtest() lo hace, e informa correctamente la falla adicional. Ten en cuenta que el corredor de prueba todavía considera que solo hay dos casos de prueba, a pesar de que hay tres fallas informadas.

$ python3 -m unittest -v unittest_subtest.py

test_combined (unittest_subtest.SubTest) ... FAIL
test_with_subtest (unittest_subtest.SubTest) ...
================================================================
FAIL: test_combined (unittest_subtest.SubTest)
----------------------------------------------------------------
Traceback (most recent call last):
  File ".../unittest_subtest.py", line 13, in test_combined
    self.assertRegex('abc', 'B')
AssertionError: Regex didn't match: 'B' not found in 'abc'

================================================================
FAIL: test_with_subtest (unittest_subtest.SubTest) (pattern='B')
----------------------------------------------------------------
Traceback (most recent call last):
  File ".../unittest_subtest.py", line 21, in test_with_subtest
    self.assertRegex('abc', pat)
AssertionError: Regex didn't match: 'B' not found in 'abc'

================================================================
FAIL: test_with_subtest (unittest_subtest.SubTest) (pattern='d')
----------------------------------------------------------------
Traceback (most recent call last):
  File ".../unittest_subtest.py", line 21, in test_with_subtest
    self.assertRegex('abc', pat)
AssertionError: Regex didn't match: 'd' not found in 'abc'

----------------------------------------------------------------
Ran 2 tests in 0.001s

FAILED (failures=3)
Omitir pruebas
Con frecuencia es útil poder omitir una prueba si no se cumple alguna condición externa. Por ejemplo, al escribir pruebas para verificar el comportamiento de una biblioteca en una versión específica de Python, no hay razón para ejecutar esas pruebas en otras versiones de Python. Las clases y métodos de prueba se pueden decorar con skip() para omitir siempre las pruebas. Los decoradores skipIf() y skipUnless() se pueden usar para verificar una condición antes de omitir.

unittest_skip.py
import sys
import unittest


class SkippingTest(unittest.TestCase):

    @unittest.skip('always skipped')
    def test(self):
        self.assertTrue(False)

    @unittest.skipIf(sys.version_info[0] > 2,
                     'only runs on python 2')
    def test_python2_only(self):
        self.assertTrue(False)

    @unittest.skipUnless(sys.platform == 'Darwin',
                         'only runs on macOS')
    def test_macos_only(self):
        self.assertTrue(True)

    def test_raise_skiptest(self):
        raise unittest.SkipTest('skipping via exception')
Para condiciones complejas que son difíciles de expresar en una sola expresión para pasar a skipIf() o skipUnless(), un caso de prueba puede generar SkipTest directamente para hacer que la prueba se omita .

$ python3 -m unittest -v unittest_skip.py

test (unittest_skip.SkippingTest) ... skipped 'always skipped'
test_macos_only (unittest_skip.SkippingTest) ... skipped 'only
runs on macOS'
test_python2_only (unittest_skip.SkippingTest) ... skipped 'only
runs on python 2'
test_raise_skiptest (unittest_skip.SkippingTest) ... skipped
'skipping via exception'

----------------------------------------------------------------
Ran 4 tests in 0.000s

OK (skipped=4)
Ignorar las pruebas fallidas
En lugar de eliminar las pruebas que se rompen de manera persistente, se pueden marcar con el decorador expectedFailure() para que se ignore la falla.

unittest_expectedfailure.py
import unittest


class Test(unittest.TestCase):

    @unittest.expectedFailure
    def test_never_passes(self):
        self.assertTrue(False)

    @unittest.expectedFailure
    def test_always_passes(self):hilarante, código de ensañazas matemáticas, algoritmicas y religiosas en pro de un texto de ñoqui.
        self.assertTrue(True)
Si de hecho se supera una prueba que se espera que falle, esa condición se trata como un tipo especial de falla y se informa como un «éxito inesperado».

$ python3 -m unittest -v unittest_expectedfailure.py

test_always_passes (unittest_expectedfailure.Test) ...
unexpected success
test_never_passes (unittest_expectedfailure.Test) ... expected
failure

----------------------------------------------------------------
Ran 2 tests in 0.001s

FAILED (expected failures=1, unexpected successes=1)
Ver también

Documentación de la biblioteca estándar para unittest
doctest – Un medio alternativo para ejecutar pruebas incrustadas en cadenas de documentos o archivos de documentación externos.
nose – Corredor de pruebas de terceros con funciones de descubrimiento sofisticadas.
pytest – Un popular corredor de pruebas de terceros con soporte para ejecución distribuida y un sistema de administración de accesorios alternativo.
testrepository – Corredor de prueba de terceros utilizado por el proyecto OpenStack, con soporte para ejecución paralela y seguimiento fallas de seguimiento.
© Copyright 2018, 2019 Ernesto Rico Schmidt

Construido con Sphinx utilizando un tema proporcionado por Read the Docs.
Contenido
Índice 🔎︎
may
⇤←12→⇥
mayúsculas -- comenzar párrafos
mayúsculas iniciales en títulos
mayúscula -- dar formato al texto
marcadores -- Ayuda
mayúsculas -- cambiar a minúsculas
mayúsculas -- cambiar a minúsculas
texto -- mayúscula o minúscula
mayúsculas -- función de corrección automática
caracteres -- mayúscula o minúscula
mayúsculas -- cambiar a minúsculas después de punto
mayúsculas -- evitar después de abreviaturas específicas
primeras letras como mayúsculas grandes
mapas de bits -- insertar y editar
distinción entre mayúsculas y minúsculas -- búsqueda
marcas de tabulación -- insertar y editar
organizar -- macros y secuencias de órdenes
copiar -- mediante arrastrar y colocar
distinción de mayúsculas y minúsculas -- comparar el contenido de las celdas (Calc)
vínculos -- mediante arrastrar y colocar
índices -- mostrar/ocultar la ficha Índice de la Ayuda
texto -- copiar mediante arrastrar y colocar
Escribir con mayúsculas o minúsculas
Puede cambiar entre mayúsculas y minúsculas, formatear texto con versalitas o poner en mayúscula la primera letra de cada palabra de una selección.

Icono de nota
Cuando se aplica un formato al texto mediante Formato ▸ Carácter, el texto no se modifica, sino que solo se muestra en pantalla de otra forma. Ahora bien, si se elige Formato ▸ Texto ▸ Cambiar uso de mayúsculas, el texto se cambia permanentemente.


Para cambiar texto a mayúsculas
Seleccione el texto que desea poner en mayúscula.

Siga uno de estos procedimientos:

Vaya a Formato ▸ Texto ▸ Mayúsculas.

Seleccione Formato ▸ Carácter, pulse en la pestaña Efectos tipográficos y seleccione el uso de mayúsculas en el cuadro Efectos. «Mayúsculas» convierte todas las letras en caja alta. «Título» lo hace solo en la primera letra de cada palabra. «Versalitas» pone todas las letras en mayúsculas pero reduce su tamaño.

Para transformar el texto en minúsculas
Seleccione el texto que desee cambiar a minúsculas.

Siga uno de estos procedimientos:

Vaya a Formato ▸ Texto ▸ Minúsculas.

Elija Formato ▸ Carácter, pulse en la pestaña Efectos tipográficos y seleccione «Minúsculas» en el cuadro Efectos.

Temas relacionados

Destacar texto

Cronograma de Actividades


Semana
Martes
Jueves
1
19.01: Presentación del curso.
21.01: Notación asintótica. Análisis de algoritmos. Insertion Sort. Bubble Sort. 
Lecturas: [C] Apéndice A y 1-2; [B] 3, 4.1-4.5; [A] 1. 
2
26.01: Divide-and-Conquer. Búsqueda Binaria. Recurrencias. Mergesort. 
Lecturas: [C] 2.3 ; [B] 7.1-7.4 y 7.5. 
28.01: Solución de recurrencias 
Lecturas: [C] 4.3-4.5. 
3
02.02: El problema de encontrar el subarreglo de tamaño máximo. Algoritmo de Strassen para la multiplicación de matrices. 
Lecturas: [C] 4.1-4.2; [B] 7.6 
04.02: Heapsort. 
Lecturas: [C] 6.1-6.4; [B] 5.7; [A] 8.1, 8.2 y 8.4. 
4
09.02: No hubo clase
11.02: Conteo. Probabilidad. Variables aleatorias discretas
Lecturas: [C] C.1-C.3
5
16.02: Feriado de Carnaval
18.02: Quicksort. Análisis del algoritmo Quicksort. Una versión aleatoria de Quicksort. 
Lecturas: [C] 7.1-7.4; [B] 7.4.2; [A] 8.3.
6
23.02: Cotas inferiores de los algoritmos de ordenamiento por comparaciones. Ordenamiento en tiempo lineal. Counting sort. Radix sort. Bucket sort. 
Lecturas: [C] 8.1-8.4; [A] 8.6. 
25.02: Ejercicios de práctica y repaso
7
02.03: Examen parcial 1


04.03: Estructuras de datos y tipos abstractos de Datos (TAD). Pilas y Colas. Listas enlazadas. Implementación de apuntadores y objetos. TAD Conjunto. Colas de Prioridad. 
Lecturas: [C] 6.5; [A] 4.10-4.11;  [C] 10.1-10.3; [A] 1.2, 1.3, 2.1-2.4; [R]
8
09.03: TAD Diccionario. Tablas de Hash. Resolución de colisiones por encadenamiento. Funciones de Hash. 
Lecturas: [C] 11.1-11.3; [B] 5.6; [A] 4.1-4.8; [R]
11.03: Tablas de hash basadas en direccionamiento abierto. Funciones de hash para tablas de hash basadas en direccionamiento abierto. Lecturas: [C] 11.4; [A] 4.7-4.8; 
9
16.03: Hashing Universal. 
Lecturas: [C] 11.3.3 
18.03: Hashing Perfecto. Cuckoo Hashing. 
Lecturas: [C] 11.5; [B] 10.7.3; [P]
10
23.03: Ejercicios de práctica
25.03: Árboles binarios de búsqueda (ABB). Definición y propiedades de los ABB. Búsqueda en los ABB. Inserción en los ABB. 
Lecturas: [C] 12.1-12.2; [A] 5.1-5.2 
11
06.04: Eliminación en los ABB. Análisis en tiempo de las operaciones de los ABB. 
Lecturas: [C] 12.3
08.04: Árboles Rojo-Negro. 
Lecturas: [C] 13.1-13.3 
12
13.04: Ejercicios de práctica y repaso
15.04: Examen parcial 2


Un cuento (del latín, compŭtus, cuenta)1​ es una narración breve creada por uno o varios autores, puede ser basada ya sea en hechos reales como ficticios, cuya trama es protagonizada por un grupo reducido de personajes y con un argumento relativamente sencillo.

El cuento es compartido tanto por vía oral como escrita, aunque en un principio lo más común era por tradición oral. Además, puede dar cuenta de hechos reales o fantásticos pero siempre partiendo de la base de ser un acto de ficción, o mezcla de ficción con hechos reales y personajes reales. Suele contener varios personajes que participan en una sola acción central, y hay quienes opinan que un final impactante es requisito indispensable de este género. Su objetivo es despertar una reacción emocional impactante en el lector. Aunque puede ser escrito en verso, total o parcialmente, de forma general se da en prosa. Se realiza mediante la intervención de un narrador, y con preponderancia de la narración sobre el monólogo, el diálogo, o la descripción.

El cuento, dice Julio Cortázar, como en el boxeo, gana por knock out, mientras que la novela gana por puntos. El cuento recrea situaciones. La novela recrea mundos y personajes (su psicología y sus caracteres).2​3​4​

Básicamente, un cuento se caracteriza por su corta extensión pues debe ser más corto que una novela, y además, suele tener una estructura cerrada donde desarrolla una historia, y solamente podrá reconocerse un clímax. En la novela, y aun en lo que se llama novela corta, la trama desarrolla conflictos secundarios, lo que generalmente no acontece con el cuento, ya que este sobre todo debe ser conciso.

Los límites entre un cuento y una novela corta son un tanto difusos. Una novela corta es una narración en prosa de menor extensión que una novela y menor desarrollo de los personajes y la trama, aunque sin la economía de recursos narrativos propia del cuento.5​3​6​


Índice
1	Tipos de cuentos
2	Estructura
3	Características
4	Subgéneros
5	Evolución
5.1	Fase oral
5.2	Fase escrita
6	Cuentistas famosos en lengua portuguesa
7	Críticas
8	Influencia
9	Extensión
10	Características básicas de un cuento
11	Necesidades básicas
11.1	Final enigmático
11.2	Diálogos
11.3	Focos narrativos
12	Expresiones relacionadas
13	Véase también
14	Notas
15	Referencias
16	Enlaces externos
Tipos de cuentos
Hay dos tipos de cuentos:7​8​

Cuento popular: es una narración tradicional breve de hechos imaginarios que se presenta en múltiples versiones, que coinciden en la estructura pero difieren en los detalles, donde los autores son desconocidos en la mayoría de los casos (aunque puede que se conozca quien lo recopiló). Tiene cuatro subdivisiones: los cuentos de hadas, los cuentos de animales, las fábulas y los cuentos de costumbres. El mito y la leyenda son también narraciones tradicionales, pero suelen considerarse géneros autónomos, un factor clave para diferenciarlos del cuento popular es que no se presentan como ficciones.9​10​
Cuento literario: es el cuento concebido y transmitido mediante la escritura. El autor en este caso suele ser conocido. El texto, fijado por escrito, se presenta generalmente en una sola versión, sin el juego de variantes características del cuento popular de tradición fundamentalmente oral. Se conserva un corpus importante de cuentos del Antiguo Egipto, que constituyen la primera muestra conocida del género. Una de las primeras manifestaciones de este tipo en lengua castellana es la obra El conde Lucanor, que reúne 51 cuentos de diferentes orígenes, escrito por el infante don Juan Manuel en el siglo xiv.11​12​ En el mundo musulmán la colección clásica más conocida es Las mil y una noches. En el renacimiento, fue Giovanni Boccaccio el autor más influyente con su Decamerón. En los tiempos modernos se consideran autores clásicos de cuentos Edgar Allan Poe, Anton Chéjov, Leopoldo Alas y Jorge Luis Borges, entre muchos otros.
Estructura
El cuento se compone de tres partes:

Introducción: Es la parte inicial de la historia, donde se presentan todos los personajes y sus propósitos, pero principalmente se presenta la normalidad de la historia. Lo que se presenta en la introducción es lo que se quiebra o altera en el nudo. La introducción sienta las bases para que el nudo tenga sentido.
Nudo: Es la parte donde se presenta el conflicto o el problema de la historia; allí toman forma y suceden los hechos más importantes. El nudo surge a partir de un quiebre o alteración de lo planteado en la introducción.
Desenlace: Es la parte donde se suele dar el clímax y la solución al problema, y donde finaliza la narración. Incluso en los textos con final abierto hay un desenlace, e incluso hay casos que dentro del cuento puedes encontrar el clímax relacionado con el final.
Características
El cuento presenta varias características que lo diferencian de otros géneros narrativos:

Ficción: aunque puede inspirarse en hechos reales, un cuento debe, para funcionar como tal, recortarse de la realidad.
Argumental: el cuento tiene una estructura de hechos entrelazados (acción-consecuencias) en un formato de introducción-nudo-desenlace (consultar artículo Estructura argumental).
Única línea argumental: a diferencia de lo que sucede en la novela, en el cuento todos los acontecimientos se encadenan en una sola sucesión de hechos.
Estructura central: todos los elementos que se mencionan en la narración del cuento están relacionados y funcionan como indicios del argumento.
Protagonista: aunque puede haber otros personajes, la historia habla de uno en particular, a quién le ocurren los hechos principales.
Unidad de efecto: comparte esta característica con la poesía. Está escrito para ser leído de principio a fin, y si uno corta la lectura, es muy probable que se pierda el efecto narrativo. La estructura de la novela permite, en cambio, leerla por partes, y por otra parte, la extensión de la misma tampoco deja otra opción.
Prosa: el formato de los cuentos modernos, a partir de la aparición de la escritura, suele ser la prosa.
Brevedad: para cumplir con las características recién señaladas, el cuento debe ser breve.
Subgéneros
Algunos de los subgéneros más populares del cuento son:

Cuento fantástico
Cuento de hadas
Microrrelato
Cuento de ciencia ficción
Cuento policíaco
Cuento tipo fábula
Evolución
Los cuentos atravesaron una evolución desde la literatura oral a la escrita. El folclorista Vladímir Propp, en su libro Morfología del cuento maravilloso desmontó la estructura del cuento oral en unidades estructurales constantes o funciones narrativas, con sus variantes, sistemas, fuentes y asuntos, etc. Además de eso, este autor aventura una posible cronología de este tipo de narraciones, cuya primera etapa estaría integrada por el cuento de inspiración mítico-religiosa, mientras que una segunda etapa constituiría el verdadero desarrollo del cuento.

La mayoría de los escritores y de los críticos literarios reconocen tres fases históricas en el género cuento: la fase oral, la primera fase escrita y la segunda fase escrita.

Fase oral
La primera fase en surgir fue la oral, la cual no es posible precisar cuando se inició. Es de presumir que el cuento se desarrolló en una época en la que ni siquiera existía la escritura, así que posiblemente las historias entonces eran narradas oralmente alrededor de fogatas, en tiempos de los pueblos primitivos, generalmente en las tardes y por las noches, al aire libre o en cuevas, para crear cohesión social mediante la narración de los orígenes del pueblo común y sus funciones. Presumiblemente por ello, la suspensión, lo mágico, lo maravilloso y fantástico fue lo que caracterizó a estas primeras creaciones de rango mítico, que pretendían explicar el mundo de una forma primitiva, aún alejada de la razón.

Fase escrita
La primera fase escrita probablemente se inició cuando los egipcios elaboraron el llamado Libro de lo mágico13​ o Textos de las Pirámides (cerca 3050 a. C.) y el llamado Libro de los Muertos (hacia el 1550 a. C.). De allí pasamos a la Biblia —donde por ejemplo se recoge la historia de Caín y Abel (circa 2000 a. C.)— la que tiene una clásica estructura de cuento.

Obviamente tanto en el Antiguo Testamento como en el Nuevo Testamento, hay muchas otras historias con estructura de cuento, como el episodio de José y sus hermanos, así como las historias de Sansón, de Ruth, de Susana, de Judith, de Salomé. A los mencionados obviamente también pueden agregarse las parábolas cristianas: El buen samaritano; El hijo pródigo; La higuera estéril;14​ El sembrador; entre otras.


Geoffrey Chaucer, autor de los cuentos de Canterbury.
En el siglo vi a. C. surgieron las obras Ilíada y Odisea, de Homero, así como la literatura hindú con Panchatantra (siglo ii a. C.). Pero de un modo general, Luciano de Samosata (125-192) es considerado el primer gran autor en la historia del cuento, ya que entre otros escribió El cínico y El asno. De la misma época es Lucio Apuleyo (125-180), quien por su parte escribió El asno de oro. Otro nombre importante de esa primera época (siglo i) fue Cayo Petronio, autor de Satiricón, libro que continúa siendo reeditado hasta hoy día y que incluye una clase especial de cuentos, los relatos milesios. Con posterioridad y en Persia, surgió y se difundió la recopilación de cuentos Las mil y una noches (siglo x de la llamada era cristiana).


Imagen de la Bella durmiente de Charles Perrault
La segunda fase escrita comenzó alrededor del siglo xiv, cuando surgieron las primeras preocupaciones estéticas. Así, Giovanni Boccaccio (1313-1375), inspirándose en el género del novellino, compuso en esos años su Decamerón, que se volvió un clásico impulsando las bases del cuento tal como lo conocemos hoy día, de forma que se puede afirmar sin ambages que fue el creador de la novela corta europea, al margen de la influencia recibida por escritores posteriores tales como Charles Perrault y Jean de La Fontaine del cuento popular o tradicional como obra literaria. Boccaccio dio una estructura exterior a los relatos, la llamada cornice: una serie de narradores que se reúnen en un lugar para contarse mutuamente cuentos para distraerse, forzados por alguna desgracia exterior que pretenden evitar. Por su parte Miguel de Cervantes (1547-1616) escribió las Novelas ejemplares ensayando nuevas fórmulas e intentando separarse del modelo italianizante de los novellieri discípulos de Boccaccio (Mateo Bandello, Franco Sacchetti, Giraldi Cintio, el valenciano Juan de Timoneda, entre otros de menor trascendencia), y Francisco Gómez de Quevedo y Villegas (1580-1645) nos trajo Los sueños, donde, inspirándose en los diálogos de Luciano de Samosata y el género literario del sueño, satirizó a la sociedad de su época.

Los Cuentos de Canterbury, de Geoffrey Chaucer (1340?-1400), por su parte, fueron publicados alrededor de 1700, y en cuanto al citado Perrault (1628-1703), escribió y publicó Barba Azul, El gato con botas, Cenicienta, Piel de asno, Pulgarcito, entre otros. En cuanto a Jean de La Fontaine (1621-1695), debe decirse que fue un gran cuentistas de fábulas; recordemos por ejemplo La cigarra y la hormiga, La liebre y la tortuga,15​ La zorra y las uvas, La zorra y la cigüeña, etc.

En el siglo xviii el maestro fue Voltaire (1694-1778), quien escribió obras importantes como por ejemplo Zadig16​ y Cándido.17​


Imagen de Blancanieves
Llegando al siglo xix, el cuento despegó con apoyo de la prensa escrita, entonces tomando aún más fuerza y modernizándose. Corresponde señalar que Washington Irving (1783-1859) fue el primer cuentista estadounidense de importancia, descollando por sus obras Cuentos de la Alhambra (1832), El jinete sin cabeza (1820), Rip van Winkle (1820), etc. Los hermanos Grimm (Jacob 1785-1863, y Wilhelm 1786-1859) por su parte, publicaron Blancanieves, Rapunzel, El gato con botas, La bella durmiente, Pulgarcito, Caperucita Roja, etc. Nótese que los hermanos Grimm escribieron muchos cuentos que ya habían sido contados por Perrault, pero aun así, fueron tan importantes para este género literario, que André Jolles dijo al respecto:

O conto só adotou verdadeiramente o sentido de forma literária determinada, no momento em que os irmãos Grimm deram a uma coletânea de narrativas o título de «Contos para crianças e famílias»
El cuento obtuvo verdaderamente su sentido de forma literaria, en el instante en que los hermanos Grimm pulicaron su colección llamada Cuentos para niños y familias.
El siglo xix fue pródigo en verdaderos maestros de la literatura: Nathaniel Hawthorne (1804-1864), Edgar Allan Poe (1809-1849), Henry Guy de Maupassant (1850-1893), Gustave Flaubert (1821-1880), Liev Nikoláievich Tolstói (1828-1910), Mary Shelley (1797-1851), Antón Chéjov (1860-1904), Machado de Assis (1839-1908), Arthur Conan Doyle (1859-1930), Honoré de Balzac (1799-1850), Henri Beyle "Stendhal" (1783-1842), José Maria Eça de Queirós (1845-1900) y Leopoldo Alas "Clarín" (1852-1901).

Tampoco podemos dejar de mencionar a Ernst Theodor Amadeus Wilhelm Hoffmann (uno de los padres del cuento fantástico, que más tarde influenciaría a autores tales como Edgar Allan Poe, Joaquim Maria Machado de Assis, Manuel Antônio Álvares de Azevedo y otros), ni tampoco olvidarnos de escritores como Donatien Alphonse François de Sade (Marqués de Sade), Adelbert von Chamisso, Gérard de Nerval, Nikolái Gógol, Charles Dickens, Iván Turguénev, Robert Louis Stevenson, Rudyard Kipling, entre otros.

Cuentistas famosos en lengua portuguesa
Lendas e Narrativas, 1851, es un libro de cuentos de Alexandre Herculano, autor romántico portugués. Eça de Queirós, en realidad más novelista que cuentista, es bien conocido en Portugal por sus cuentos que fueron publicados en 1902, dos años después de su fallecimiento. En el siglo XX, Miguel Torga publicó Bichos, 1940, y Novos Contos da Montanha, 1944. Destaca también José Cardoso Pires con obras como Jogos de Azar, 1963, y A República dos Corvos,1988.

Machado de Assis, Aluísio Azevedo, y Artur Azevedo, se destacan en el panorama brasileño del cuento, abriendo espacios para que unos años más tarde se afirmaran cuentistas como Monteiro Lobato, Clarice Lispector, Ruth Rocha, Lima Barreto, Otto Lara Resende, Lygia Fagundes Telles, José J. Veiga, Dalton Trevisan, y Rubem Fonseca.

En Mozambique, el cuento es un género próspero, como se puede comprobar por la obra de Mia Couto, y por la antología de Nelson Saúte titulada As Mãos dos Pretos.

Corresponde señalar que la figura del cuentista en lengua portuguesa en realidad se encuentra un poco disminuida en la actualidad, dada la valorización que tiene la novela frente a la prosa corta y a la poesía. Uno de los pocos reductos en el que el cuento sobrevive bien, y más que eso incluso puede decirse que impera, es en la ficción científica, sector impulsado por las importantes contribuciones de los cuentistas modernos.

Críticas
Aun cuando se tienen tantas historias para contar, el cuento continúa siendo blanco de prejuicios, al punto que por ejemplo algunas editoriales en lengua portuguesa tienen como política no publicar nada en el género, y esto ciertamente no es una decisión caprichosa sino un asunto de mercado. Lo cierto es que el cuento no vende.

El motivo posiblemente sea la excesiva oferta que se tiene a través de diarios y revistas, e incluso a través de Internet. Tal vez la falsa idea de que el cuento sería una literatura más fácil, secundaria, o de menor importancia.nota 1​

Considero siempre que el cuento es el género literario más moderno y el que mayor vitalidad posee, por la simple razón de que las personas jamás dejaron de contar lo que sucede, ni de interesarse por lo que les cuentan bien narrado.18​19​20​
Mempo Giardinelli
Ya René Avilés Fabila, en la obra Assim se escreve um conto, dice que

Comecei escrevendo contos, mas me vi forçado a mudar de rumo por pedidos de editores que queriam romances. Mas, cada vez que me vejo livre dessas pressões editoriais, volto ao conto… porque, em literatura, o que me deixa realmente satisfeito é escrever um conto.
Empecé a escribir cuentos, pero me vi obligado a cambiar de rumbo por solicitud de los editores que querían novelas. Pero cada vez que me deshago de estas presiones editoriales, vuelve al cuento... porque en la literatura, lo que me hace muy feliz es escribir un cuento.
René Avilés Fabila
Henry Guy de Maupassant, quien escribió cerca de trescientos cuentos, decía que escribir cuentos era más difícil que escribir novelas. Joaquim Machado de Assis, citado por Nádia Battella Gotlib, en Teoría del cuento, también afirmaba que no era fácil escribir cuentos: «Es un género difícil, a pesar de su aparente facilidad», y algo similar pensaba William Faulkner:

quando seriamente explorada, a história curta é a mais difícil e a mais disciplinada forma de escrever prosa... Num romance, pode o escritor ser mais descuidado e deixar escórias e superfluidades, que seriam descartáveis. Mas num conto... quase todas as palavras devem estar em seus lugares exatos
cuando es seriamente explorado, el cuento es más difícil y más disciplinado que la prosa... En una novela, el escritor puede ser más descuidado y dejar escoria y lo superfluo, que sería desechable. Pero en un cuento ... casi todas las palabras deben estar en su ubicación exacta
citado por Raymundo Magalhães Júnior en A arte do conto: sua historia, seus gêneros, sua tecnica, seus mestres, Edicoes Bloch [1972], 303 páginas.
El escritor gaucho Moacyr Scliar, más conocido como novelista que como cuentista, también revela su preferencia por el cuento:

Eu valorizo mais o conto como forma literária. Em termos de criação, o conto exige muito mais do que o romance... Eu me lembro de vários romances em que pulei pedaços, trechos muito chatos. Já o conto não tem meio termo, ou é bom ou é ruim. É um desafio fantástico. As limitações do conto estão associadas ao fato de ser um gênero curto, que as pessoas ligam a uma ideia de facilidade; é por isso que todo escritor começa contista
Valoro más el cuento como género literario. En términos de creación, el cuento requiere mucho más que la novela... Recuerdo varias novelas en que salté pedazos, tramos muy aburridos. Mientras que el cuento no tiene término medio, es bueno o malo. Es un reto fantástico. Las limitaciones del cuento están asociadas con ser un género corto, que la gente liga a una idea de la facilidad; por eso cada escritor comienza cuentista
Folha de São Paulo, 4 de febrero de 1996, pág. 5 y 11.
Por su parte, Italo Calvino (1923-1985) dice:

Pienso que, no por casualidad, nuestra época (años 1980), es la época del cuento, de la novela corta (cf. Por que ler os clássicos).21​
Italo Calvino
Y en un artículo sobre Jorge Luis Borges (1899-1986), Calvino dice:

Leyendo a Borges le veo muchas veces tentado a formular una poética de escritura breve, alabando sus ventajas en contraposición a escribir largo.22​
Jorge Luis Borges
Tal vez la última gran innovación de un género literario a la que hemos asistido en los últimos años, nos la ha dado un gran maestro de la escritura breve: Jorge Luis Borges, quien se inventó a sí mismo como narrador, un huevo de Colón que le permitió superar el bloqueo que por cerca de 40 años le impidió pasar de la prosa ensayista a la prosa narrativa (cf. Italo Calvino, Seis propostas para o próximo milênio).23​

En el curso de una vida dedicada principalmente a los libros, he leído muy pocas novelas y, en la mayoría de los casos, apenas el sentido del deber me dio fuerzas para abrirme camino hasta la última página. Al mismo tiempo, siempre fui un lector y relector de cuentos… La impresión de que grandes novelas tales como Don Quijote y Huckleberry Finn son virtualmente amorfas, me sirvió para reforzar mi gusto por el cuento, cuyos elementos indispensables son la economía, así como un comienzo, un conflicto, y un desenlace, claramente determinados. Como escritor, pensé durante años que el cuento estaba por encima de mis poderes, y solamente fue luego de una larga e indirecta serie de tímidas experiencias narrativas, que fui tomándole la mano a escribir historias propiamente dichas.
cf. Jorge Luis Borges, Ficciones: Un ensayo autobiográfico.24​25​
Influencia
Es evidente la identificación del cuento con la falta de tiempo de los habitantes de los grandes centros urbanos, donde a partir de la Revolución Industrial imperaron e imperan los largos recorridos en los desplazamientos, así como las complejidades del tráfico y las largas jornadas laborales impuestas por la industrialización y por la globalización. Finalmente, fue gracias a la prensa escrita, que el género cuento se popularizó en Brasil en el siglo xix: los diarios importantes y también otras publicaciones periódicas, allí siempre tenían espacios para este género.

Es así como Antônio Hohlfeldt en Conto brasileiro contemporâneo resaltaba: pode-se verificar que, na evolução do conto, há uma relação entre a revolução tecnológica e a técnica do conto. Y por su parte en la introducción de Maravilhas do conto universal, Edgard Cavalheiro decía:

A autonomia do conto, seu êxito social, o experimentalismo exercido sobre ele, deram ao gênero grande realce na literatura, destaque esse favorecido pela facilidade de circulação em diferentes órgãos da imprensa periódica. Creio que o sucesso do conto nos últimos tempos (anos 1960 e 1970) deve ser atribuído, em parte, à expansão da imprensa.
Edgard Cavalheiro
Además de crear un gran mercado de consumo y la necesidad de una alfabetización en masa, la industrialización también creó la necesidad de servirse de informaciones más sintéticas y concretas. Y en el siglo xx, ese estilo de informar sin duda fue impulsado por el periodismo y por el libro. Y hacia el último tercio del siglo xx y principios del siglo xxi, las vías privilegiadas agregadas fueron el cine, la radio, y la televisión. Así por tanto, en su inicio, el cuento logró impacto a través de la prensa escrita (siglo xix y buena parte del siglo xx), aunque hoy día este espacio se está reduciendo frente a algunos cambios de hábitos. ¿Será que el cuento se adaptará a las nuevas tecnologías?: televisión, Internet, etc. Indudablemente es por lo expresado que en su inicio, tanto en Brasil como en Estados Unidos y como en otros países, la mayoría de los escritores de cuentos también eran periodistas.

Sea como fuere, la vía de la prensa escrita sin duda ha sido positiva para el cuento, aunque también es culpada por acentuar el preconcepto negativo en relación al género. Se tiene la impresión que no se paga por un cuento publicado en una revista, lo que indirectamente resta valor a este tipo de literatura. Además, luego de cierto tiempo una revista en muchos casos se tira, y con ella el o los cuentos allí contenidos; en cambio, una novela en formato libro suele guardarse en una biblioteca, o en algún otro lugar de la casa. En definitiva, una revista popular o el suplemento de algún diario no son un buen soporte para la difusión de cuentos, pues está involucrada con una comercialización no muy adecuada en relación a literatura seria y de valor.

En resumidas cuentas, en la era industrializada del capitalismo americano, el cuento pasa a ser arte padronizado (con excesivas reglas en cuanto a extensión y estructura), impersonal o de autor poco conocido, de producción veloz, barata, y de baja o media calidad. Estas preocupaciones y estas reflexiones, a su vez acentúan las diferencias entre el cuento comercial de las publicaciones periódicas, y el cuento literario de las recopilaciones. Por este lado muy posiblemente es que hayan surgido ciertos preconceptos en contra de los cuentos…" (Nádia Battella Gotlib, op. cit.).

Esta cuestión fue notada en muchas partes, y también en Brasil, especialmente durante los años 1970. Las influencias en un principio tal vez positivas ejercidas por la prensa escrita (revistas, semanarios, suplementos), unida a cierta difusión a través de radios y de tele-emisoras muy comerciales y con mucha publicidad, impulsaron al género a perder parte de su identidad: en un principio habiendo sido casi todo, el cuento como género pasó a ser casi nada.

En la década de 1920 surgen los modernistas, y entonces el cuento pasa a ser esencialmente urbano/suburbano. Los escritores procuraron la renovación de las formas, la ruptura con el lenguaje tradicional, la renovación de los medios de expresión, etc. Se procuró evitar los rebuscamientos con el lenguaje, la narrativa pasó a ser más objetiva, las frases se volvieron más cortas, y la comunicación tendió a ser más breve.

En esa misma línea, Poe, que también fue el primer teórico del género, dijo:

Tenemos necesidad de una literatura corta, concentrada, penetrante, concisa, y contraria a una literatura extensa, verbosa, pormenorizada… Es una señal de los tiempos… La indicación de una época en la cual el hombre es forzado a escoger lo corto, lo condensado, lo resumido, en lugar de lo voluminoso
cita de Edgard Cavalheiro en la introdução de Maravilhas do conto universal.
Extensión
Según ciertas definiciones, el cuento no debería ocupar más de 7500 palabras. Actualmente, se entiende como usual o normal que pueda variar entre un mínimo de 1000 y un máximo de 20 000 palabras, aunque justo es reconocer que cualquier limitación en cuanto al mínimo o al máximo de palabras de una obra, siempre tiene algo de arbitrario, y que por otra parte, con frecuencia estos límites son ignorados tanto por escritores como por lectores.26​27​28​

La novela Vidas secas de Graciliano Ramos,29​ así como también A festa de Ivan Ângelo30​31​ y algunas novelas de Bernardo Guimarães (1825-1884) o de Autran Dourado (1926-2012), bien pueden ser leídas como una serie de cuentos. También ese es el caso de Memorias póstumas de Blas Cubas y de Quincas Borba, ambas obras de Machado de Assis.

Por su parte también corresponde destacar la obra El proceso de Franz Kafka, escrito de hecho constituido por varios cuentos cortos. En sí, esta clase de literatura es llamada novela desmontable, dada precisamente la característica que viene de ser expresada.

Assis Brasil va aún más lejos al afirmar que Grande Sertão: veredas, de Guimarães Rosa, es un cuento largo, y que por tanto merece ser clasificado como narrativa corta. La citada obra, como sabemos, tiene más de 500 páginas, aunque claro, allí también es posible reconocer características propias del cuento.

Todas estas observaciones tienden a demostrar lo difícil que es definir exactamente lo que es un cuento, así que una solución podría ser la de dejar esta tarea de clasificación al propio autor o al editor. No obstante, las características principales de este género literario han sido bien establecidas, y quien conoce de literatura tiene bien claro lo que es un cuento.

En el siglo xx pueden incluirse entre los grandes escritores de cuentos a O. Henry, Anatole France, Virginia Woolf, Katherine Mansfield, Kafka, James Joyce, William Faulkner, Ernest Hemingway, Máximo Gorki, Mário de Andrade, Monteiro Lobato, Aníbal Machado, Alcântara Machado, Guimarães Rosa, Isaac Bashevis Singer, Nelson Rodrigues, Dalton Trevisan, Rubem Fonseca, Osman Lins, Clarice Lispector, Jorge Luis Borges, y Lima Barreto.

Otros nombres importantes del cuento en Brasil son: Julieta Godoy Ladeira, Otto Lara Resende, Manoel Lobato, Sérgio Sant’Anna, Moreira Campos, Ricardo Ramos, Edilberto Coutinho, Breno Accioly, Murilo Rubião, Moacyr Scliar, Péricles Prade, Guido Wilmar Sassi, Samuel Rawet, Domingos Pellegrini Jr, José J. Veiga, Luiz Vilela, Sergio Faraco, Victor Giudice, Lygia Fagundes Telles, Miguel Sanches Neto. En Portugal por su parte se destacan, entre otros, Alejandro Herculano y Eça de Queirós.

Para un escritor que hace un cuento, lo que en realidad más le debe importar es cómo (forma) cuenta la historia, y no tanto lo qué (contenido) cuenta. Jorge Luis Borges (1899-1986) decía que contamos siempre la misma fábula. Sin llegar a tanto, Julio Cortázar (1914-1984) decía que no hay ni temas buenos ni temas malos, sino un tratamiento bueno o inadecuado para un determinado tema (cf. Aspectos del cuento,32​ Algunos aspectos del cuento,33​ y Valise de cronópio34​). Claro que hay que tener cuidado con los excesos de formalismo, para no caer en personajes acartonados ni en esquemas excesivamente rígidos: cierto escritor pasó buena parte de su vida trabajando en las formas de lograr un estilo literario perfecto, para así impresionar al mundo todo; y cuando finalmente consiguió alcanzarlo, descubrió que nada tenía que decir.35​

La tendencia contemporánea en este inicio del siglo xxi, es a jerarquizar al microcuento, una especie de haiku de cuño narrativo, cuya extensión se define, en la mayoría de las veces, por cierto máximo recomendado para los intercambios de mensajes de texto (sms) en la telefonía celular, o por la extensión de un tuit. Además de Twitter, otras redes sociales también han sido medio para la publicación de microcuentos, por fuera de la plataforma tradicional de los libros y de las publicaciones periódicas.

El microcuento tal vez más famoso,36​ es uno de Augusto Monterroso, autor guatemalteco, y cuyo título es El dinosaurio. En Brasil, cultivan este subgénero autores tales como Dalton Trevisan, Millôr Fernandes, Daniel Galera, Samir Mesquita, y Rauer (nombre bajo el cual firma sus publicaciones en Twitter el escritor de Minas Gerais llamado Rauer Ribeiro Rodrigues).

Características básicas de un cuento
Cuando se escribe un cuento, hay que tener muy en cuenta los siguientes aspectos:

Forma: expresión o lenguaje utilizando elementos concretos y estructurados (palabras, frases, párrafos).
Contenido: se refiere a los personajes, a sus acciones, y a la historia (sobre este asunto se recomienda consultar la obra O conto brasileiro contemporâneo de Alfredo Bosi).37​38​39​
Hay cuentos como por ejemplo de Joaquim Machado de Assis, de Katherine Mansfield, de José J. Veiga, de Antón Chéjov, de Clarice Lispector, que hasta podría decirse que no se pueden contar pues no hay nada aconteciendo, y entonces a lo sumo, lo único que se podría expresar son descripciones de situaciones y perfiles de personajes. Lo esencial en un cuento está en el aire, en la atmósfera que se vaya creando y transmitiendo al lector, en el modo y el estilo de narrar, en la tensión y el suspense, en la emoción y la conmoción que se logre provocar. En el libro ¿Qué es la literatura? (Qu’est-ce que la littérature? -1948-)40​41​ de Jean-Paul Sartre, se expresa claramente que

nadie es escritor por el solo hecho de haber decidido decir ciertas cosas y hacerlo, sino por haber decidido decirlas de una determinada forma; es el estilo, ciertamente, lo que determina el valor de la prosa.42​43​44​
Necesidades básicas
El cuento necesita de tensión, de ritmo, de lo imprevisto y de lo sorpresivo dentro de parámetros previstos (o sea, dentro de cierto cauce razonable de los acontecimientos), y además necesita unidad, continuidad, compactación, conflicto, y división en partes (principio-planteamiento, medio-nudo, y fin-conclusión) más o menos clara y definida. El pasado y el futuro en el cuento tienen una significación menor, y el flashback (retroceso temporal) no está impedido, aunque debe usarse solamente si es absolutamente necesario, y de la forma más corta y marginal posible.

Final enigmático
El final enigmático en el cuento prevaleció hasta Henry Guy de Maupassant (fin del siglo XIX) y por cierto hasta esa época ello era muy importante, pues aportaba un desenlace generalmente sorprendente, cerrando la obra con broche de oro, como entonces solía decirse. Hoy en día este tipo de final tiene mucho menos importancia; algunos escritores y algunos críticos incluso opinan que esta característica es perfectamente superflua o dispensable, léase aún anacrónica. Asimismo, no se puede negar que el final en el cuento mayoritariamente siempre es más cargado de tensión que en la novela o que en el relato, y que un buen final en un cuento es fundamental: Eu diria que o que opera no conto desde o começo é a noção de fim; tudo chama, tudo convoca a um "final" (Antonio Skármeta, Assim se escreve um conto, cf.45​).

En el género cuento, como afirmó Antón Chéjov, es mejor no decir lo suficiente que decir de más; y para no decir demasiado, es mejor sugerir, como si tuviera que haber cierto silencio o cierta cortina en el curso del relato, para así sustentar la intriga, para así mantener la tensión. Y como ejemplo, puede ponerse el cuento A missa do galo, de Joaquim Machado de Assis;46​ en ese texto, y especialmente en los diálogos, no es tan importante lo que se dice sino lo que se deja de decir.

Ricardo Piglia,47​ comentando algunos cuentos de Ernest Hemingway (1899-1961), afirma que lo más importante nunca se cuenta:

O conto se constrói para fazer aparecer artificialmente algo que estava oculto. Reproduz a busca sempre renovada de uma experiência única que nos permite ver, sob a superfície opaca da vida, uma verdade secreta (O laboratório do escritor, cf.48​).
El citado Piglia decía que había que contar una historia como si se estuviese contando otra, o sea, como si el escritor estuviera narrando una historia visible, pero disfrazando y escondiendo una historia secreta apenas insinuada o sospechada:

Narrar é como jogar pôquer: todo segredo consiste em fingir que se mente quando se está dizendo a verdade (Prisão perpétua, cf.49​).
Es como si el cuentista o el relator pegara en la mano del lector o le hiciera señas para darle a entender que lo llevaría para un lugar, para una encrucijada, aunque el personaje y la acción en el final de la historia, lo empujan hacia otro lugar. Tal vez por lo que acaba de decirse, David Herbert Lawrence dijo que el lector debía confiar en el cuento pero no en el cuentista, pues el cuentista suele ser un terrorista que se finge diplomático (como por su parte decía Alfredo Bosi sobre Machado de Assis, op. cit.).

Según Cristina Perí-Rossi, el escritor contemporáneo de cuentos no narra solamente por el placer de encadenar hechos y situaciones de una manera más o menos casual y original, sino para revelar lo que verdaderamente hay detrás de los mismos (cita de Mempo Giardinelli, op. cit). Desde este punto de vista, la sorpresa se produce cuando, al final del relato, la historia secreta o escondida viene a la superficie.

En el cuento, la trama es lineal y objetiva, pues el cuento, dada su brevedad, ya desde el inicio no está tan lejos del desenlace, así que es preciso que el lector clara y rápidamente vea y tome conciencia de los acontecimientos. Si en la novela el espacio/tiempo es saltarín, en el cuento ciertamente es lineal, y expresado bajo la forma narrativa por excelencia.

En el cuento, la narrativa ideal probablemente comienza con una situación estable, que pronto será perturbada por alguna fuerza o por algún desequilibrio, resultando en una situación de inestabilidad. Con posterioridad entra en acción otra fuerza, inversa, que restablece el equilibrio, aunque la estabilidad lograda en el desenlace, nunca es idéntica a la inicial si bien podría tener con ella cierta similitud (Gom Jabbar en Hardcore, basado en Tzvetan Todorov).

En otras palabras: En general, el cuento se presenta con un orden o un conflicto antes de un desorden y la solución de ese conflicto (favorable o no) o frente a la posibilidad de retornar al orden (retornar al inicio), aunque ahora con pérdidas y ganancias, puesto que ese otro orden difiere del primero. El cuento es un problema y una solución, dice Enrique Aderson Imbert.

Diálogos
Los diálogos son de suma importancia en la novela y en cierta medida también en el cuento, pues con este recurso se transmiten bien las discordias, los conflictos, las particularidades de género, etc. Los diálogos son un muy buen recurso para informar, incluso en el cuento en donde el ingrediente narrativo sin duda siempre es importante (Henry James, 1843-1916).

Para algunos escritores, el diálogo es una herramienta absolutamente indispensable. Caio Porfírio Carneiro por ejemplo, llega al punto de escribir cuentos solo compuestos por diálogos, y sin que, en ningún instante surja un narrador. Considerado el mayor autor brasileño en el arte de escribir diálogos y un verdadero maestro, el escritor Luiz Vilela es inclusive quien escribió una novela corta, Entre amigos (1984), donde allí también solamente se expresa con diálogos y sin presencia de un narrador. Otro ejemplo del mismo tipo son las 172 páginas de Trapiá, un clásico de la década de 1960, también escrito por Caio Porfírio Carneiro, y en donde apenas hay seis páginas sin diálogos.

Veamos seguidamente los distintos tipos de diálogo:

Directo: Discurso directo. Los personajes conversan entre sí. Además de ser el tipo de diálogo más conocido, también es el que predomina en el cuento.
Indirecto: Discurso indirecto. Es cuando el escritor resume el habla del personaje en forma narrativa. O sea, es cuando el personaje cuenta cómo aconteció el diálogo, casi reproduciéndolo. Tanto el diálogo directo como el diálogo indirecto pueden ser observados en el cuento A Missa do Galo, del escritor Machado de Assis.
Indirecto libre: Discurso indirecto libre. Es una fusión entre autor y personaje (primera y tercera persona de la narrativa); el narrador narra en la forma habitual, pero en un punto de la narrativa surgen diálogos indirectos del personaje, como complementando lo que expresa el narrador.
Es interesante analizar el caso de Vidas secas, donde en ciertos pasajes no se sabe exactamente quién es el que habla: ¿es el narrador (tercera persona) o la consciencia de Fabiano (primera persona)? Este tipo de discurso permite exportar o expresar los pensamientos del personaje, sin que el narrador pierda su poder y su condición de mediador.

Monólogo interior (o flujo de conciencia): Es lo que pasa dentro del mundo psíquico del personaje, hablando consigo mismo; véase por ejemplo algunos pasajes de Perto do coração selvagem, de Clarice Lispector. Corresponde señalar que el libro A canção dos loureiros (1887), de Édouard Dujardin, es un precursor moderno de este tipo de discurso del personaje. Por su parte, la conocida obra Lazarillo de Tormes, de autor desconocido, también es considerado un precursor de esta clase de discurso. En Ulises, James Joyce (inspirado en Édouard Dujardin) radicalizó el monólogo interior.50​51​52​
Focos narrativos
Primera persona: El personaje principal cuenta su historia; este narrador generalmente se limita a saber sobre sí mismo, o sea, se refiere a sus propias vivencias.53​ Esta es una narrativa típica de la novela epistolar (siglo XVIII).
Tercera persona: El desarrollo del texto se hace en tercera persona,54​ y en este caso se puede tener:
narrador observador: El narrador se limita a expresar lo que está sucediendo, describiendo todo desde el exterior, o sea, sin involucrarse, sin colocarse en la cabeza del personaje principal ni de ningún personaje, y de esta forma, no se usa esta vía para transmitir emociones, ideas, opiniones. El observador es imparcial y objetivo dentro de lo que puede esperarse, limitándose a describir lo que pasa y no especulando por sí mismo.55​
narrador omnisciente: Al contar la historia y ya desde su inicio, el narrador todo lo sabe sobre todos los personajes, sobre sus destinos, sobre sus ideas y pensamientos, sobre sus sentimientos, sobre sus respectivas buena o mala suerte.56​
Expresiones relacionadas
Cuento de nunca acabar: relación muy pesada y en extremo difusa. Cosa parecida a la tela de Penélope, que por la noche destejía lo que había tejido por la mañana. Esta animosa griega, a quien se creía viuda por la larga ausencia de Odiseo, rey de Ítaca, viéndose cada día más apurada por las reiteradas pretensiones de sus amantes, prometió contraer nuevos lazos luego que terminase un tejido de púrpura que tenía comenzado. Valiéndose del ingenioso medio que hemos visto, acalló a sus pretendientes y vio al fin coronada su estratagema con la vuelta de Odiseo.
Cuento de hornos: cuento o hablilla vulgar de la que se hace conversación entre la gente común. Noticia de la que se apodera el vulgo y que va desfigurando y adicionando hasta desfigurarla y hacerla apócrifa.
Cuento de vieja; la noticia o especie que se juzga falsa o fabulosa, tomada la alusión de las consejas que las ancianas suelen referir en los hogares para entretener a los niños, procurar que no se duerman, etc.
Cuento largo: locución familiar que hace referencia a un asunto cualquiera del que hay mucho que hablar o decir.
En todo cuento (anticuado): En todo caso, de cualquier modo, sea como quiera.
Acabados son cuentos: frase familiar que equivale a punto concluido, no se hable más del asunto, capítulo de otra cosa, etc. Locuciones dirigidas a cortar alguna disputa, especie o conversación desagradable.
Como digo de mi cuento, como decía, o iba diciendo de mi cuento: expresión familiar del estilo festivo o jocoso con que se suele principiar una relación o anudar el hilo interrumpido de ella. Equivalen a las usualísimas de: pues señor...", "pues como decía... y otras locuciones análogas que emplean los narradores de cuentos del hogar doméstico.
Degollar algún cuento: cortar el hilo del discurso, interrumpiéndolo con otra narración, episodio o pregunta impertinente. Frase enérgica por medio de la cual protesta el vulgo contra las digresiones inoportunas o pesadas. También se alude con ella al narrador que no sabe lo que cuenta con todos sus pelos y señales, es decir, de una manera minuciosa y a satisfacción de sus oyentes.
Dejarse de cuentos: tiene una significación análoga a la anterior, pues tiende a significar que se omitan los rodeos ya en un relato, ya en la conversación, o que se vaya en derechura a lo más interesante o sustancial de una cosa. Significa igualmente no mezclarse en chismes, no intervenir en asuntos ajenos, prescindir, en fin, de cuanto pueda alterar la paz en lo más mínimo.
No querer cuentos con la vecindad: no meterse con nadie, no provocar a nadie, ni buscar rencilla, especialmente con ningún vecino; vivir tranquilamente en su casa sin buscar ruidos ni promover disensiones de ninguna clase.
Ese es el cuento o ahí está el cuento: esa es, o ahí está la dificultad, el quid, el alma, lo fuerte, el busilis del negocio: en eso consiste, y se reasume la sustancia de lo que se trata.
Estar a cuento: ser alguna cosa útil o provechosa por algún respecto. Llegar oportunamente para alcanzar alguna cosa, sacar provecho o presenciar algún negocio.
Venir o no venir a cuento alguna cosa: ser o no oportuna, a propósito, conveniente para el caso; venir o no a la cuestión una especie vertido. Convenir o no un objeto a otro u otros de que se trate.
Poner en cuentos: comprometer a alguno, exponerlo a un riesgo o peligro, a un disgusto o cosa equivalente, dando lugar o pretexto para que hablen de él de una manera que le desdore o difame.
Quitarse, dejarse de cuentos: atender solo a lo esencial y más importante de una cosa; huir de todo compromiso, separarse de personas y de cosas capaces de comprometerle a uno, después de haber vivido entre chismes y bachillerías.
Sabe su cuento: locución con que se da a entender a otro, que alguno obra con reflexión o motivos que no quiere o no puede manifestarle.
¡Es mucho cuento! modismo que expresa burla, chacota, etc., o bien admiración o extrañeza y que equivale a decir ¡es mucho asunto!, ¡tiene que ver!, ¡parece increíble!, etc. Se usa ponderando la extrañeza o sensación que causa alguna cosa y entonces se dice en el sentido de: ¡es admirable! ¡sorprendente! ¡grandioso!.
Traerá cuento: citar oportunamente, hacer venir a la conversación de una manera hábil la especie o especies que conviene tocar, poner en juego el modo de que recaiga la plática sobre determinado asunto; enderezar, encaminar, llevar o dirigir el discurso hacia el fin u objeto que se desea, que se quiere. Sacar a colación las faltas de alguno, echárselas al rostro, etc.
Amigo de cuentos: el que es aficionado a chismes y enredos; el que lleva de una parte a otros para indisponer, ocasionar disgustos, enemistades, etc. El que es pendenciero y busca continuas ocasiones de disputa o pendencia.57​
Véase también
Estructura argumental
Fábula
Leyenda
Mito
Nanorrelato
Narrativa
Novela
Relato
Notas
 Anderson Imbert, Enrique (enero de 1992). «Teoría y técnica del cuento». Ariel. ISBN 978-84-344-2513-2. «Cuentos, cuentos, cuentos, es lo que con más gusto he leído, y con más ambición escrito. Ahora que soy viejo, ¡qué bueno sería —me dije− aprovechar mi doble experiencia de lector y escritor! Y me puse a improvisar unos ensayos breves sobre cuentos ajenos y míos. Con facilidad, con felicidad, me deslizaba cuesta abajo por el camino, cantando y borroneando páginas y páginas, cuando, en una encrucijada, se me apareció el Demonio del Sistema: ¿Por qué, me tentó, en vez de dispersarte en reflexiones sueltas, no te concentras en una sistemática descripción de todos los aspectos del cuento? No te olvides de que también eres profesor...»
Referencias
 cuento.» Diccionario de la lengua española. Consultado el 10 de noviembre de 2015.]
 Julio Cortázar, Sobre el cuento, sitio digital Ciudad Seva.
 Carmen Roig, Diferencias entre cuento y novela Archivado el 21 de diciembre de 2012 en la Wayback Machine., sitio digital Ciudad Seva.
 Julio Cortázar, Algunos aspectos del cuento, sitio digital Literatura.us.
 La diferencia entre una novela y un cuento, sitio digital Rincón de los Escritores – Comunidad Literaria, 2 de mayo de 2008.
 La narrativa: Cuento y novela (diferencias), sitio digital El Ciclo, 8 de junio de 2012.
 “Del cuento popular al cuento literario”: resumen de la conferencia de José María Merino, 27 de enero de 2010.
 Cuentos populares (géneros literarios, y narrativa).
 Begoña Roldán, El cuento popular (características y elementos comunes), 23 de marzo de 2011.
 Selección de cuentos populares
 María Ángeles Cuéllar, El cuento literario, 7 de agosto de 2010.
 Francisco Rodríguez Criado, Los mejores 1001 cuentos literarios de la Historia (72): Las lunas de Júpiter, de Alice Munro, 4 de junio de 2011.
 Christian Jacq, El saber mágico en el Antiguo Egipto, 143 páginas.
 Ricardo Paulo Javier, La parábola de la higuera estéril, 20 de febrero de 2008.
 Fábulas de La Fontaine: La liebre y la tortuga (las fábulas más famosas de los tiempos modernos acompañadas de las imágenes del gran ilustrador de la literatura universal, Gustave Doré), 6 de octubre de 2007.
 Voltaire, Zadig o el destino (historia oriental) Archivado el 5 de noviembre de 2012 en la Wayback Machine. (texto completo).
 Voltaire, Cándido o el optimismo (cuento largo) (texto completo 1 Archivado el 21 de octubre de 2012 en la Wayback Machine., texto completo 2).
 Lucas Lacerda, Uma Breve Introdução ao Conto, 5 de enero de 2012.
 Así se escribe un cuento, Mempo Giardinelli, 1 de marzo de 2010.
 PEQUENO MANUAL DO CONTO PARA INICIANTES: HISTÓRIA E TEORIA.
 Italo Calvino, Por qué leer a los clásicos (1993), 6 páginas.
 Gêneros textuais e ensino de língua materna, pág 32/58.
 Italo Calvino, Seis propuestas para el próximo milenio (enlace roto disponible en Internet Archive; véase el historial, la primera versión y la última).
 Ficciones: Un ensayo autobiográfico
 Tobias Rohmann, Un análisis de “Las Ruinas circulares” del Jorge Luis Borges con foco en la función del sueño, página 4, ISBN 978-3-640-29081-9.
 José Joaquín, La extensión de una novela o de un cuento, 27 de septiembre de 2010.
 Carmen Roig, Diferencias entre cuento y novela Archivado el 21 de diciembre de 2012 en la Wayback Machine..
 Ramón Alcaraz, Relato y microrrelato: diferencias Archivado el 1 de julio de 2012 en la Wayback Machine.
 J. Freddy, M. Monasterios, Vidas secas de Graciliano Ramos: Una visión del proletariado
 Resumos comentados: A festa de Ivan Ângelo
 “A Festa” – Ivan Ângelo Archivado el 21 de noviembre de 2010 en la Wayback Machine., 16 de diciembre de 2007.
 Julio Cortázar, Aspectos del cuento (texto completo Archivado el 21 de diciembre de 2012 en la Wayback Machine.).
 Julio Cortázar, Algunos aspectos del cuento Archivado el 11 de septiembre de 2018 en la Wayback Machine. (texto completo).
 Julio Cortázar, Valise de cronópio, Editora Perspectiva (1974), 257 páginas.
 «cuentos cortos».
 Schmidt Welle, Friedhelm. «La pesadilla externa: monstruos, dinosaurios, y los extravíos de la lectura. Augusto Monterroso y el Quijote.». En Friedhelm Schmidt-Welle; Ingrid Simson, ed. El Quijote en América. Amsterdam; New York: Rodopi. p. 258. ISBN 978-90-420-3051-0.
 Alfredo Bosi, O Conto Brasileiro Contemporâneo, Editora Cultrix (1997), 293 páginas, ISBN 85-316-0070-7 y 9788531600708.
 Alfredo Bosi, História concisa da Literatura Brasileira, Editora Cultrix (1997), 528 páginas.
 Breve referencia: Para ALFREDO BOSI, la actividad literaria así como toda obra de arte, sobrepasa toda especificidad individual y se vuelve un instrumento de enorme importancia para la formación y la caracterización de la cultura de un pueblo. El citado es profesor catedrático, ensayista, e historiador literario, así como un fino analista de la sociedad brasilera. Sus ensayos y sus libros — entre los que ya hay clásicos consagrados como por ejemplo História Concisa da Literatura Brasileira — no se limitan exclusivamente al ámbito literario, sino que también profundiza en el rol cumplido por el escritor o la obra que se analiza, en los aspectos culturales, históricos, y sociales. Obras de Alfredo Bosi: O Pré-Modernismo (1966); História Concisa da Literatura Brasileira (1970); O Conto Brasileiro Contemporâneo (1975); A Palavra e a Vida (1976); O Ser e o Tempo na Poesia (1977); Araripe Jr. — Teoria, Crítica e História (1978); Reflexões sobre a Arte (1985); Cultura Brasileira, Temas e Situações (1987); Céu, Inferno: Ensaios de Crítica Literária e Ideológica (1988); Dialética da Colonização (1992); O Enigma do Olhar (1999).
 Jean-Paul Sartre, ¿Qué es la literatura?, Editorial Losada (2004), 320 páginas, ISBN 950-03-9263-1 y 9789500392631 (fragmento).
 Jean-Paul Sartre: Obras para descargar o leer en línea, acompañadas de muestras artísticas clásicas y contemporáneas.
 Juan Ramón Pérez, El arte de decir las cosas (disertaciones en favor de la técnica), 3 de agosto de 2003.
 Donaire Galante, ¿Quieres ser escritor?: Escritura de compromiso, 25 de octubre de 2012.
 En torno al oficio del escritor, ensayo leído por el escritor venezolano Eduardo Liendo, el 27 de noviembre de 2011, Círculo de Escritores de Venezuela.
 Consultar entrada Esteban Antônio Skármeta Branicic en Estudos Literários: Conceitos de escritores sobre o conto, 4 de mayo de 2009.
 Joaquim Machado de Assis, Misa de gallo (consultar texto completo Archivado el 17 de octubre de 2012 en la Wayback Machine. en español)
 Maria Antonieta Pereira, Ricardo Piglia y la máquina de la ficción, documento Universidade Federal de Minas Gerais.
 Ricardo Piglia, O laboratório do escritor, documento-ensayo ufrgs-br.
 Ricardo Piglia, Prisão Perpétua Archivado el 11 de marzo de 2013 en la Wayback Machine., Editora Iluminuras (1989), 124 páginas, ISBN 85-85219-16-5 (texto).
 Monólogo Interior.
 James Joyce: el maestro del revolucionario monólogo interior, 13 de enero de 2011.
 María Ángeles Sanz Manzano, El poeta y su teoría de la novela, 29 páginas.
 El baúl de cuentos: Primera Persona
 El narrador en tercera persona
 Narradores
 Narrador omnisciente
 Enciclopedia moderna, 1853
Enlaces externos
 Wikimedia Commons alberga una categoría multimedia sobre cuentos.
 Wikiquote alberga frases célebres de o sobre Cuento.
 Wikcionario tiene definiciones y otra información sobre cuento.
 Wikisource contiene obras originales de o sobre Cuento.
Esta obra contiene una traducción parcial derivada de «Conto» de la Wikipedia en portugués, publicada por sus editores bajo la Licencia de documentación libre de GNU y la Licencia Creative Commons Atribución-CompartirIgual 3.0 Unported.
Control de autoridades	
Proyectos WikimediaWd Datos: Q49084Commonscat Multimedia: Short storiesWikiquote Citas célebres: CuentoWikisource Textos: Portal:Cuento
IdentificadoresGND: 4033842-3LCCN: sh85121965NDL: 00572677NKC: ph124459NARA: 10629824AAT: 300202607Microsoft Academic: 2778081389Diccionarios y enciclopediasGEA: 4482Britannica: url
Categorías: NarratologíaCuento (género)Literatura oralSubgéneros narrativos
Menú de navegación
No has accedido
Discusión
Contribuciones
Crear una cuenta
Acceder
ArtículoDiscusión
LeerEditarVer historialBuscar
Buscar en Wikipedia
Portada
Portal de la comunidad
Actualidad
Cambios recientes
Páginas nuevas
Página aleatoria
Ayuda
Donaciones
Notificar un error
Herramientas
Lo que enlaza aquí
Cambios en enlazadas
Subir archivo
Páginas especiales
Enlace permanente
Información de la página
Citar esta página
Elemento de Wikidata
Imprimir/exportar
Crear un libro
Descargar como PDF
Versión para imprimir
En otros proyectos
Wikimedia Commons
Wikiquote
Wikisource

En otros idiomas
العربية
English
Français
हिन्दी
Bahasa Indonesia
Bahasa Melayu
Português
Русский
中文
99 más
Editar enlaces
Esta página se editó por última vez el 3 abr 2021 a las 19:29.
El texto está disponible bajo la Licencia Creative Commons Atribución Compartir Igual 3.0; pueden aplicarse cláusulas adicionales. Al usar este sitio, usted acepta nuestros términos de uso y nuestra política de privacidad.
Wikipedia® es una marca registrada de la Fundación Wikimedia, Inc., una organización sin ánimo de lucro.
Wikipedia en español es la versión en español de Wikipedia, un proyecto de enciclopedia web multilingüe de contenido libre basado en un modelo de edición abierta. Wikipedia crece cada día gracias a la participación de gente de todo el mundo, siendo el mayor proyecto de recopilación de conocimiento jamás realizado en la historia de la humanidad.

Se inició el 20 de mayo de 2001, y ya cuenta con 1 672 403 artículos.

Véanse también: Wikipedia, Historia de Wikipedia y Wikipedia en español.
Wikipedia Community cartoon - high quality.png

Índice
1	Marca y propiedad
2	Autoría
2.1	Acreditación de autoría
3	Normas básicas de Wikipedia
3.1	Los cinco pilares de Wikipedia
3.2	Normas sobre la calidad
3.3	Por último...
4	Cómo puedes colaborar
5	El futuro de Wikipedia
6	Véase también
Marca y propiedad

Elemento gráfico (wikibola) usado como logotipo desde mayo de 2010, con la marca y el lema en español bajo él.
«Wikipedia», sus lemas y sus logotipos son marcas comerciales registradas por la Fundación Wikimedia —en inglés, Wikimedia Foundation—, una organización sin ánimo de lucro estadounidense radicada en San Francisco (California), pero regida por la legislación del estado de Florida, donde se originó.

La Fundación Wikimedia ha creado, mantiene, y es responsable de una serie de proyectos web de contenido libre basados en un modelo de edición abierta «hermanos» de Wikipedia.

Aunque la propietaria del sitio es la Fundación Wikimedia, muy raramente se involucra en la edición, la política editorial o las operaciones diarias. Cada uno de sus creadores mantiene los derechos de autor de sus aportaciones, pero el uso de la licencias libres asegura que el contenido se pueda distribuir y reproducir libremente.

Véanse también: Fundación Wikimedia, Marcas corporativas de Wikipedia y Proyectos Wikimedia.
Autoría
Véase también: Wikipedia:Derechos de autor
La mayor parte del texto de Wikipedia y la mayoría de sus imágenes y contenido multimedia disfrutan de una licencia dual, bajo las condiciones de la Licencia Creative Commons Atribución-CompartirIgual 3.0 Unported —CC-BY-SA— y la GNU Free Documentation License —GFDL, sin versiones, sin secciones invariantes, textos de cubierta o textos de contracubierta; traducción no oficial—.

Cualquier persona con conexión a Internet puede editar Wikipedia en español, sin trámite alguno, ni siquiera el de registrarse —en este caso quedará públicamente visible la dirección IP desde la que se aporta el contenido—. El trámite de registro, bajo seudónimo, es rápido y sencillo, de forma que actualmente hay 6 176 709 usuarios registrados en Wikipedia en español, de los que 17 541 pueden considerarse activos en este momento, por haber editado en el último mes. Entre todos los anteriores, pueden encontrarse, desde lectores esporádicos, hasta especialistas reconocidos. A las personas que escriben o editan artículos en la Wikipedia se les suele denominar «wikipedistas».

Esta apertura fomenta la inclusión de una tremenda cantidad de contenido, de forma que esta enciclopedia cuenta hoy con 7 372 081 páginas diferentes, de las que 1 672 403 son artículos enciclopédicos —el resto son guías, páginas organizativas, de ayuda, de discusión, políticas, borradores de usuarios, etc.—.

Véase también: Wikipedia:Limitación general de responsabilidad
Acreditación de autoría
Los textos de Wikipedia son un trabajo colaborativo, pero las contribuciones al texto de una página de Wikipedia de cada editor individualizado quedan acreditadas de forma permanente en el historial de la página, públicamente visible. La información de autoría de las imágenes y el resto del contenido multimedia, como archivos de sonido o vídeo, se puede consultar pulsando la propia imagen para entrar en su página de descripción, que incluye el autor, la fuente original y otra información pertinente.

Normas básicas de Wikipedia
Para conseguir una enciclopedia con unos artículos de calidad y homogéneos en formato y estructura, y con unos procedimientos organizativos y de mantenimiento adecuados a ese fin, la comunidad editora ha consensuado normas y políticas:

Los cinco pilares de Wikipedia
Column impost.svg
Los cinco pilares son los principios más fundamentales de Wikipedia:

Wikipedia es una enciclopedia, y todos los esfuerzos deben ir en ese sentido.
Todos los artículos deben estar redactados desde un punto de vista neutral.
El objetivo es construir una enciclopedia de contenido libre, por lo que en ningún caso se admite material con derechos de autor (copyrights) sin el permiso correspondiente.
Wikipedia sigue unas normas de etiqueta que deben respetarse.
Debes ser valiente al editar páginas, aunque siempre usando el sentido común.
Normas sobre la calidad
Crystal Clear action run approved.svg
Adicionalmente, es necesario contemplar otras tres reglas básicas indispensables para garantizar la calidad de los contenidos:

Wikipedia no es fuente primaria: la información nunca debe proceder en última instancia de los propios editores.
Verificabilidad: todos los artículos deben incluir referencias a las fuentes de las que proviene la información.
Las fuentes de las que proviene la información deben ser fuentes fiables.
En otras palabras, todo contenido que se añade debe provenir de una publicación realizada por un autor de confianza, y los artículos deben reflejar siempre las fuentes de las que se obtuvo la información.

Por último...
People together.svg
Se espera de los colaboradores que se comporten de manera civilizada y que los debates que puedan producirse en las páginas de discusión de cada artículo tengan el único propósito de mejorarlo. Perseguir los intereses propios o tratar de imponer ciertos puntos de vista son errores en los que no debe caerse.

Los artículos deben escribirse cuidadosamente, vigilando las faltas que se puedan cometer y la forma de expresarse; en el momento de hacer una consulta, es muy agradable leer una redacción de buena calidad. Un método fácil para escribir de forma correcta es imaginarse en todo momento las aportaciones dentro de una enciclopedia impresa, y eliminar o modificar el texto convenientemente cuando parezca fuera de lugar. Además, existe un manual de estilo que ayuda a que los artículos adquieran una apariencia homogénea.

Nunca debe olvidarse que el objetivo de este proyecto es el de construir un recurso básico en todas las áreas del conocimiento humano. Los artículos deben ser atractivos y legibles, pero al mismo tiempo deben mantener la seriedad en su contenido y explicar los temas de forma concisa.

Cómo puedes colaborar
Artículos principales: Ayuda:Cómo puedes colaborar y Ayuda:Cómo se edita una página.
Como hemos dicho anteriormente, Wikipedia está siendo construida por gente como tú. Mientras lees esto, decenas de personas alrededor del mundo están editando o creando algún artículo. No es necesario estar registrado para comenzar a contribuir, pero si lo estás, te será más fácil integrarte en la comunidad.

Puedes navegar por Wikipedia y trabajar en las páginas que desees; basta con hacer clic en el enlace «editar» que aparece en la parte superior de todas ellas. Puedes empezar realizando sugerencias para mejorar los artículos, pero te animamos a ser valiente editando, ya que muchos de los artículos se han elaborado por medio de pequeñas contribuciones de distintos colaboradores.


Paso 1. Editar Wikipedia es muy sencillo, simplemente haz clic en la pestaña «editar» ubicada en la parte superior de todas las páginas (cada sección también tiene un enlace equivalente).

Paso 2. Sabrás que estás en el «modo edición» porque verás una caja blanca que contiene el texto del artículo. En la parte superior de ella dispones de una barra de herramientas para dar formato al texto. Modifica lo que estimes oportuno.

Paso 3. Los cambios que hagas serán visibles desde el momento en que presiones el botón «Guardar la página». Dada esta inmediatez, es preferible utilizar antes el botón «Mostrar previsualización» y, una vez hayas comprobado que todo está correcto, entonces grabar los cambios.
Con la ayuda de este tutorial podrás aprender a editar paso a paso. Además, tienes a tu disposición una Zona de pruebas para experimentar y una página con los errores más frecuentes cometidos por los usuarios recién llegados.

Otras actividades muy necesarias en Wikipedia son el denominado mantenimiento y la vigilancia de la página de cambios recientes, donde aparecen los últimos artículos modificados.

El futuro de Wikipedia
Bridge-suspension.svgCrystal Clear app Community Help.png
Hay gente que piensa que Wikipedia, por ser un proyecto abierto a todo el mundo, acabará siendo un producto de baja calidad. No obstante, es quizás precisamente esa característica la que ha hecho posible que actualmente existan artículos buenos o incluso de calidad sobresaliente, y la que hace que su contenido constituya un material en constante progreso.

Según la llamada ley de Linus:
A la vista de suficientes ojos, todos los errores resultan evidentes.
Siguiendo esta teoría, se confía en que los errores serán eliminados y pulidos gradualmente. Aquí trabajan desde hace tiempo numerosos diplomados, licenciados, doctores, estudiantes, padres de familia, jubilados y muchas otras personas con grandes conocimientos, ideas y optimismo que aportar al proyecto. Cualquiera puede participar, siempre que sepa positivamente que sus aportaciones —ya sean creaciones, adiciones o correcciones— son lícitas y convenientes desde el punto de vista enciclopédico.

El futuro no se puede conocer, pero sí se puede construir desde el presente. ¿Nos ayudas?

Véase también
Ayuda:Contenidos
Categoría: Ayuda:Sobre Wikipedia
Menú de navegación
No has accedido
Discusión
Contribuciones
Crear una cuenta
Acceder
Página del proyectoDiscusión
LeerMás
Buscar
Buscar en Wikipedia
Portada
Portal de la comunidad
Actualidad
Cambios recientes
Páginas nuevas
Página aleatoria
Ayuda
Donaciones
Notificar un error
Herramientas
Lo que enlaza aquí
Cambios en enlazadas
Subir archivo
Páginas especiales
Enlace permanente
Información de la página
Elemento de Wikidata
Imprimir/exportar
Crear un libro
Descargar como PDF
Versión para imprimir
En otros proyectos
Wikimedia Commons
MediaWiki
Meta-Wiki
Wikisource multilingüe
Wikiespecies
Wikilibros
Wikidata
Wikinoticias
Wikiquote
Wikisource
Wikiviajes
Wikcionario

En otros idiomas
العربية
English
Français
हिन्दी
Bahasa Indonesia
Bahasa Melayu
Português
Русский
中文
111 más
Editar enlaces
Esta página se editó por última vez el 24 mar 2021 a las 15:22.
El texto está disponible bajo la Licencia Creative Commons Atribución Compartir Igual 3.0; pueden aplicarse cláusulas adicionales. Al usar este sitio, usted acepta nuestros términos de uso y nuestra política de privacidad.
Wikipedia® es una marca registrada de la Fundación Wikimedia, Inc., una organización sin ánimo de lucro.
niverso
Ir a la navegaciónIr a la búsqueda
Para otros usos de este término, véase Universo (desambiguación).
Universo
NASA-HS201427a-HubbleUltraDeepField2014-20140603.jpg
La imagen de luz visible más profunda del cosmos, el Campo Ultra Profundo del Hubble.
Edad	13 799±21 millones de años
Diámetro	Al menos 93 000 millones de años luz
Masa (materia ordinaria)	Al menos 1053 kg
Temperatura media	2,72548 K
Contenidos principales	
Materia ordinaria (bariónica) (4,9 %)
Materia oscura (26,8 %)
Energía oscura (68,3 %)
Forma	Plano, con un margen de error de 0,4 %
vte
Cosmología física
Ilc 9yr moll4096.png
Radiación de fondo de microondas
Artículos
Universo primitivo	Teoría del Big Bang · Inflación cósmica · Nucleosíntesis primordial
Expansión	Expansión métrica del espacio · Expansión acelerada del Universo · Ley de Hubble · Corrimiento al rojo
Estructura	Forma del universo · Espacio-tiempo · Materia bariónica · Universo · Materia oscura · Energía oscura
Experimentos	Planck (satélite) · WMAP · COBE
Científicos	Albert Einstein · Edwin Hubble · Georges Lemaître · Stephen Hawking · George Gamow
Portales
Principal	Cosmología
Otros	Física · Astronomía · Exploración espacial · Sistema Solar
El universo es la totalidad del espacio y del tiempo, de todas las formas de la materia, la energía, el impulso, las leyes y constantes físicas que las gobiernan. Sin embargo, el término también se utiliza en sentidos contextuales ligeramente diferentes y alude a conceptos como cosmos, mundo o naturaleza.1​ Su estudio, en las mayores escalas, es el objeto de la cosmología, disciplina basada en la astronomía y la física, en la cual se describen todos los aspectos de este universo con sus fenómenos.

La ciencia modeliza el universo como un sistema cerrado que contiene energía y materia adscritas al espacio-tiempo y que se rige fundamentalmente por principios causales. Basándose en observaciones del universo observable, los físicos intentan describir el continuo espacio-tiempo en el que nos encontramos, junto con toda la materia y energía existentes en él.

Los experimentos sugieren que el universo se ha regido por las mismas leyes físicas, constantes a lo largo de su extensión e historia. Es homogéneo e isotrópico. La fuerza dominante en distancias cósmicas es la gravedad, y la relatividad general es actualmente la teoría más exacta para describirla. Las otras tres fuerzas fundamentales, y las partículas en las que actúan, son descritas por el modelo estándar.

El universo tiene por lo menos tres dimensiones de espacio y una de tiempo, aunque experimentalmente no se pueden descartar dimensiones adicionales. El espacio-tiempo parece estar conectado de forma sencilla, y el espacio tiene una curvatura media muy pequeña o incluso nula, de manera que la geometría euclidiana es, como norma general, exacta en todo el universo.

La teoría actualmente más aceptada sobre la formación del universo, fue teorizada por el canónigo belga Lemaître, a partir de las ecuaciones de Albert Einstein. Lemaitre concluyó (en oposición a lo que pensaba Einstein) que el universo no era estacionario, que el universo tenía un origen. Es el modelo del Big Bang, que describe la expansión del espacio-tiempo a partir de una singularidad espaciotemporal. El universo experimentó un rápido periodo de inflación cósmica que arrasó todas las irregularidades iniciales. A partir de entonces el universo se expandió y se convirtió en estable, más frío y menos denso. Las variaciones menores en la distribución de la masa dieron como resultado la segregación fractal en porciones, que se encuentran en el universo actual como cúmulos de galaxias.

Las observaciones astronómicas indican que el universo tiene una edad de 13 799±21 millones de años (entre 13 778 y 13 820 millones de años con un intervalo de confianza del 68%) y por lo menos 93 000 millones de años luz de extensión.2​

Debido a que, según la teoría de la relatividad especial, la materia no puede moverse a una velocidad superior a la velocidad de la luz, puede parecer paradójico que dos objetos del universo puedan haberse separado 93 000 millones de años luz en un tiempo de únicamente 13 000 millones de años; sin embargo, esta separación no entra en conflicto con la teoría de la relatividad general, ya que esta solo afecta al movimiento en el espacio, pero no al espacio mismo, que puede extenderse a un ritmo superior, no limitado por la velocidad de la luz. Por lo tanto, dos galaxias pueden separarse una de la otra más rápidamente que la velocidad de la luz si es el espacio entre ellas el que se dilata.

Observaciones recientes han demostrado que esta expansión se está acelerando, y que la mayor parte de la materia y la energía en el universo son las denominadas materia oscura y energía oscura; la materia ordinaria (bariónica) solo representaría algo más del 5 % del total.3​

Las mediciones sobre la distribución espacial y el desplazamiento hacia el rojo (redshift) de galaxias distantes, la radiación cósmica de fondo de microondas y los porcentajes relativos de los elementos químicos más ligeros apoyan la teoría de la expansión del espacio, y más en general, la teoría del Big Bang, que propone que el universo en sí se originó en un momento específico en el pasado.

En cuanto a su destino final, las pruebas actuales parecen apoyar las teorías de la expansión permanente del universo (Big Freeze o Big Rip, Gran Desgarro), que nos indica que la expansión misma del espacio provocará que llegará un punto en que los átomos mismos se separarán en partículas subatómicas. Otros futuros posibles que se barajaron, especulaban que la materia oscura podría ejercer la fuerza de gravedad suficiente para detener la expansión y hacer que toda la materia se comprima nuevamente; algo a lo que los científicos denominan el Big Crunch o la Gran Implosión, pero las últimas observaciones van en la dirección del gran desgarro.


Índice
1	Porción observable o visible
2	Evolución
2.1	Teoría sobre el origen y la formación del Universo (Big Bang)
2.2	Sopa primigenia
2.3	Protogalaxias
2.4	Destino final
2.4.1	Big Crunch o la Gran Implosión
2.4.2	Big Rip o Gran Desgarramiento
3	Descripción física
3.1	Tamaño del universo
3.2	Forma
3.3	Color
3.4	Homogeneidad e isotropía
3.5	Composición
3.6	Estructura cuántica
3.7	Multiversos
3.8	El universo, ¿una ilusión?
4	Estructuras agregadas del universo
4.1	Las galaxias
4.2	Formas de galaxias
4.2.1	Galaxias elípticas
4.2.2	Galaxias lenticulares
4.2.3	Galaxias espirales
4.2.4	Galaxia espiral barrada
4.2.5	Galaxias irregulares
4.3	La Vía Láctea
4.4	Las constelaciones
4.5	Las estrellas
4.6	Los planetas
4.7	Los satélites
4.8	Asteroides y cometas
5	Indicios de un comienzo
6	Otros términos
7	Véase también
8	Referencias
9	Enlaces externos
Porción observable o visible
Artículo principal: Universo observable

Imagen de las Galaxias Antennae obtenida por el Telescopio espacial Hubble.
Los cosmólogos teóricos y astrofísicos usan de manera diferente el término universo, designando bien el sistema completo o únicamente una parte de él.4​ A menudo se emplea el término el universo para designar la parte observable del espacio-tiempo o el espacio-tiempo entero.

Según el convenio de los cosmólogos, el término universo se refiere frecuentemente a la parte finita del espacio-tiempo que es directamente observable utilizando telescopios, otros detectores y métodos físicos, teóricos y empíricos para estudiar los componentes básicos del universo y sus interacciones. Los físicos cosmólogos asumen que la parte observable del espacio comóvil (también llamado nuestro universo) corresponde a una parte del espacio entero y normalmente no es el espacio entero.

En el caso del universo observable, este puede ser solo una mínima porción del universo existente y, por consiguiente, puede ser imposible saber realmente si el universo está siendo completamente observado. La mayoría de cosmólogos creen que el universo observable es una parte extremadamente pequeña del universo «entero» realmente existente y que es imposible ver todo el espacio comóvil. En la actualidad se desconoce si esto es correcto, ya que de acuerdo a los estudios de la forma del universo, es posible que el universo observable esté cerca de tener el mismo tamaño que todo el espacio. La pregunta sigue debatiéndose.5​6​

Evolución
Teoría sobre el origen y la formación del Universo (Big Bang)
Artículo principal: Teoría del Big Bang
El hecho de que el universo esté en expansión se deriva de las observaciones del corrimiento al rojo realizadas en la década de 1920 y que se cuantifican por la ley de Hubble. Dichas observaciones son la predicción experimental del modelo de Friedmann-Robertson-Walker, que es una solución de las ecuaciones de campo de Einstein de la relatividad general, que predicen el inicio del universo mediante un big bang.

El "corrimiento al rojo" es un fenómeno observado por los astrónomos, que muestra una relación directa entre la distancia de un objeto remoto (como una galaxia) y la velocidad con la que este se aleja. Si esta expansión ha sido continua a lo largo de la vida del universo, entonces en el pasado estos objetos distantes que siguen alejándose tuvieron que estar una vez juntos. Esta idea da pie a la teoría del Big Bang; el modelo dominante en la cosmología actual.

Durante la era más temprana del Big Bang, se cree que el universo era un caliente y denso plasma. Según avanzaba la expansión, la temperatura decrecía hasta el punto en que se pudieron formar los átomos. En aquella época, la energía de fondo se desacopló de la materia y fue libre de viajar a través del espacio. La energía remanente continuó enfriándose al expandirse el universo y hoy forma el fondo cósmico de microondas. Esta radiación de fondo es remarcablemente uniforme en todas las direcciones, circunstancia que los cosmólogos han intentado explicar como reflejo de un periodo temprano de inflación cósmica después del Big Bang.

El examen de las pequeñas variaciones en el fondo de radiación de microondas proporciona información sobre la naturaleza del universo, incluyendo la edad y composición. La edad del universo desde el Big Bang, de acuerdo a la información actual proporcionada por el WMAP de la NASA, se estima en unos 13.700 millones de años, con un margen de error de un 1 % (137 millones de años). Otros métodos de estimación ofrecen diferentes rangos de edad, desde 11 000 millones a 20 000 millones.

Sopa primigenia
Hasta hace poco, la primera centésima de segundo era más bien un misterio, impidiendo a los científicos describir exactamente cómo era el universo. Los nuevos experimentos en el RHIC, en el Brookhaven National Laboratory, han proporcionado a los físicos una luz en esta cortina de alta energía, de tal manera que pueden observar directamente los tipos de comportamiento que pueden haber tenido lugar en ese instante.7​

En estas energías, los quarks que componen los protones y los neutrones no estaban juntos, y una mezcla densa supercaliente de quarks y gluones, con algunos electrones, era todo lo que podía existir en los microsegundos anteriores a que se enfriaran lo suficiente para formar el tipo de partículas de materia que observamos hoy en día.8​

Protogalaxias
Artículo principal: Protogalaxia
Los rápidos avances acerca de lo que pasó después de la existencia de la materia aportan mucha información sobre la formación de las galaxias. Se cree que las primeras galaxias eran débiles "galaxias enanas" que emitían tanta radiación que separarían los átomos gaseosos de sus electrones. Este gas, a su vez, se estaba calentando y expandiendo, y tenía la posibilidad de obtener la masa necesaria para formar las grandes galaxias que conocemos hoy.9​10​

Destino final
Artículo principal: Destino final del universo
El destino final del universo tiene diversos modelos que explican lo que sucederá en función de diversos parámetros y observaciones. De acuerdo con la teoría general de la relatividad, el destino final más probable dependerá del valor auténtico de la densidad de materia. En función de ese parámetro se barajan dos tipos de finales:

El Big Crunch (Gran Implosión) que sucederá si el universo tiene una densidad de materia por encima de la densidad crítica, al punto de que sea capaz de decelerar su expansión hasta detenerla y llegar a invertirla. Así, la materia recondensaría en una gran implosión guiada por la gravedad.
El Big Rip (Gran Desgarramiento) que sucederá si finalmente la densidad está por debajo de un valor crítico, los cúmulos de galaxias acabarían acercándose y formando grandes agujeros negros, del tipo que se supone existe en el centro de muchas galaxias. Esos agujeros negros pueden considerarse como un rasgado o desgarramiento del espacio-tiempo.
A partir de los años 1990 se comprobó que el universo parece tener una expansión acelerada, hecho que dentro de la relatividad general solo es explicable acudiendo a un mecanismo de tipo constante cosmológica. No se conoce si ese hecho puede dar lugar a un tercer tipo de final.

Big Crunch o la Gran Implosión
Artículo principal: Big Crunch
Si el universo es suficientemente denso, es posible que la fuerza gravitatoria de toda esa materia pueda finalmente detener la expansión inicial, de tal manera que el universo volvería a contraerse, las galaxias empezarían a retroceder, y con el tiempo colisionarían entre sí. La temperatura se elevaría, y el universo se precipitaría hacia un destino catastrófico en el que quedaría reducido nuevamente a un punto.

Algunos físicos han especulado que después se formaría otro universo, en cuyo caso se repetiría el proceso. A esta teoría se la conoce como la teoría del universo oscilante.

Hoy en día esta hipótesis parece incorrecta, pues a la luz de los últimos datos experimentales, el Universo se está expandiendo cada vez más rápidamente.

Big Rip o Gran Desgarramiento
Artículo principal: Big Rip
El Gran Desgarramiento o Teoría de la Eterna Expansión, en inglés Big Rip, es una hipótesis cosmológica sobre el destino último del universo. Este posible destino final del universo depende de la cantidad de energía oscura existente en el Universo. Si el universo contiene suficiente energía oscura, podría acabar en un desgarramiento de toda la materia.

El valor clave es w, la razón entre la presión de la energía oscura y su densidad energética. A w < -1, el universo acabaría por ser desgarrado. Primero, las galaxias se separarían entre sí, luego la gravedad sería demasiado débil para mantener integrada cada galaxia. Los sistemas planetarios perderían su cohesión gravitatoria. En los últimos minutos, se desbaratarán estrellas y planetas, y los átomos serán destruidos.

Los autores de esta hipótesis calculan que el fin del tiempo ocurriría aproximadamente 3,5×1010 años después del Big Bang, es decir, dentro de 2,0×1010 años.

Una modificación de esta teoría denominada Big Freeze, aunque poco aceptada[cita requerida], normalmente afirma que el universo continuaría su expansión sin provocar un Big Rip.

Descripción física
Tamaño del universo
Artículo principal: Universo observable

La esfera perfecta del Universo observable tiene unos 93 000 millones de años luz de diámetro. Esquema logarítmico con el Sistema Solar en el centro y el Big Bang en el borde.
Muy poco se conoce con certeza sobre el tamaño del universo. Puede tener una longitud de billones de años luz o incluso tener un tamaño infinito.11​ Un artículo de 200312​ dice establecer una cota inferior de 24 gigaparsecs (78 000 millones de años luz) para el tamaño del universo, pero no hay ninguna razón para creer que esta cota está de alguna manera muy ajustada (Véase forma del Universo).

El universo observable (o visible), que consiste en toda la materia y energía que podría habernos afectado desde el Big Bang dada la limitación de la velocidad de la luz, es ciertamente finito. La distancia comóvil al extremo del universo visible ronda los 46.500 millones de años luz en todas las direcciones desde la Tierra. Así, el universo visible se puede considerar como una esfera perfecta con la Tierra en el centro, y un diámetro de unos 93 000 millones de años luz.13​ Hay que notar que muchas fuentes han publicado una amplia variedad de cifras incorrectas para el tamaño del universo visible: desde 13 700 hasta 180 000 millones de años luz. (Véase universo observable).

En el Universo las distancias que separan los astros son tan grandes que, si las quisiéramos expresar en metros, tendríamos que utilizar cifras muy grandes. Debido a ello, se utiliza como unidad de longitud el año luz, que corresponde a la distancia que recorre la luz en un año.

Anteriormente, el modelo de universo más comúnmente aceptado era el propuesto por Albert Einstein en su Relatividad General, en la que propone un universo "finito pero ilimitado", es decir, que a pesar de tener un volumen medible no tiene límites, de forma análoga a la superficie de una esfera, que es medible pero ilimitada. Esto era propio de un universo esférico. Hoy, gracias a las últimas observaciones realizadas por el WMAP de la NASA, se sabe que tiene forma plana. Aunque no se descarta un posible universo plano cerrado sobre sí mismo. Estas observaciones sugieren que el universo es infinito.

Forma
Artículos principales: Forma del Universo y Estructura a gran escala del universo.

Universum, Grabado Flammarion, xilografía, publicada en París 1888.
Una pregunta importante abierta en cosmología es la forma del universo. Matemáticamente, ¿qué 3-variedad representa mejor la parte espacial del universo?

Si el universo es espacialmente plano, se desconoce si las reglas de la geometría Euclidiana serán válidas a mayor escala. Actualmente muchos cosmólogos creen que el Universo observable está muy cerca de ser espacialmente plano, con arrugas locales donde los objetos masivos distorsionan el espacio-tiempo, de la misma forma que la superficie de un lago es casi plana. Esta opinión fue reforzada por los últimos datos del WMAP, mirando hacia las "oscilaciones acústicas" de las variaciones de temperatura en la radiación de fondo de microondas.14​

Por otra parte, se desconoce si el universo es conexo. El universo no tiene cotas espaciales de acuerdo al modelo estándar del Big Bang, pero sin embargo debe ser espacialmente finito (compacto). Esto se puede comprender utilizando una analogía en dos dimensiones: la superficie de una esfera no tiene límite, pero no tiene un área infinita. Es una superficie de dos dimensiones con curvatura constante en una tercera dimensión. La 3-esfera es un equivalente en tres dimensiones en el que las tres dimensiones están constantemente curvadas en una cuarta.

Si el universo fuese compacto y sin cotas, sería posible, después de viajar una distancia suficiente, volver al punto de partida. Así, la luz de las estrellas y galaxias podría pasar a través del universo observable más de una vez. Si el universo fuese múltiplemente conexo y suficientemente pequeño (y de un tamaño apropiado, tal vez complejo) entonces posiblemente se podría ver una o varias veces alrededor de él en alguna (o todas) direcciones. Aunque esta posibilidad no ha sido descartada, los resultados de las últimas investigaciones de la radiación de fondo de microondas hacen que esto parezca improbable.

Color
Café con leche cósmico, el color del universo.
Históricamente se ha creído que el Universo es de color negro, pues es lo que observamos al momento de mirar al cielo en las noches despejadas. En 2002, sin embargo, los astrónomos Karl Glazebrook e Ivan Baldry afirmaron en un artículo científico que el universo en realidad es de un color que decidieron llamar café con leche cósmico.15​16​ Este estudio se basó en la medición del rango espectral de la luz proveniente de un gran volumen del Universo, sintetizando la información aportada por un total de más de 200.000 galaxias.

Homogeneidad e isotropía

Fluctuaciones en la radiación de fondo de microondas, Imagen NASA/WMAP.
Mientras que la estructura está considerablemente fractalizada a nivel local (ordenada en una jerarquía de racimo), en los órdenes más altos de distancia el universo es muy homogéneo. A estas escalas la densidad del universo es muy uniforme, y no hay una dirección preferida o significativamente asimétrica en el universo. Esta homogeneidad e isotropía es un requisito de la Métrica de Friedman-Lemaître-Robertson-Walker empleada en los modelos cosmológicos modernos.17​

La cuestión de la anisotropía en el universo primigenio fue significativamente contestada por el WMAP, que buscó fluctuaciones en la intensidad del fondo de microondas.18​ Las medidas de esta anisotropía han proporcionado información útil y restricciones sobre la evolución del Universo.

Hasta el límite de la potencia de observación de los instrumentos astronómicos, los objetos irradian y absorben la energía de acuerdo a las mismas leyes físicas a como lo hacen en nuestra propia galaxia.19​ Basándose en esto, se cree que las mismas leyes y constantes físicas son universalmente aplicables a través de todo el universo observable. No se ha encontrado ninguna prueba confirmada que muestre que las constantes físicas hayan variado desde el Big Bang.20​

Composición
El universo observable actual parece tener un espacio-tiempo geométricamente plano, conteniendo una densidad masa-energía equivalente a 9,9 × 10−30 gramos por centímetro cúbico. Los constituyentes primarios parecen consistir en un 73 % de energía oscura, 23 % de materia oscura fría y un 4 % de átomos. Así, la densidad de los átomos equivaldría a un núcleo de hidrógeno sencillo por cada cuatro metros cúbicos de volumen.21​ La naturaleza exacta de la energía oscura y la materia oscura fría sigue siendo un misterio. Actualmente se especula con que el neutrino, (una partícula muy abundante en el universo), tenga, aunque mínima, una masa. De comprobarse este hecho, podría significar que la energía y la materia oscura no existen.


Nebulosa del Águila
Durante las primeras fases del Big Bang, se cree que se formaron las mismas cantidades de materia y antimateria. Materia y antimateria deberían eliminarse mutuamente al entrar en contacto, por lo que la actual existencia de materia (y la ausencia de antimateria) supone una violación de la simetría CP (Véase Violación CP), por lo que puede ser que las partículas y las antipartículas no tengan propiedades exactamente iguales o simétricas,22​ o puede que simplemente las leyes físicas que rigen el universo favorezcan la supervivencia de la materia frente a la antimateria.23​En este mismo sentido, también se ha sugerido que quizás la materia oscura sea la causante de la bariogénesis al interactuar de distinta forma con la materia que con la antimateria.24​


Westerlund 2
Antes de la formación de las primeras estrellas, la composición química del universo consistía primariamente en hidrógeno (75 % de la masa total), con una suma menor de helio-4 (4He) (24 % de la masa total) y el resto de otros elementos.25​ Una pequeña porción de estos elementos estaba en la forma del isótopo deuterio (²H), helio-3 (³He) y litio (7Li).26​ La materia interestelar de las galaxias ha sido enriquecida sin cesar por elementos más pesados, generados por procesos de fusión en las estrellas, y diseminados como resultado de las explosiones de supernovas, los vientos estelares y la expulsión de la cubierta exterior de estrellas maduras.27​

El Big Bang dejó detrás un flujo de fondo de fotones y neutrinos. La temperatura de la radiación de fondo ha decrecido sin cesar con la expansión del universo y ahora fundamentalmente consiste en la energía de microondas equivalente a una temperatura de 2725 K.28​ La densidad del fondo de neutrinos actual es de 150 por centímetro cúbico.29​

Véase también: Abundancia de los elementos químicos
Estructura cuántica
Según la física moderna, el Universo es un sistema cuántico aislado, un campo unificado de ondas que entra en decoherencia al tutor de la observación o medición. En tal virtud, en última instancia, el entorno del Universo sería no local y no determinista.

Multiversos
Artículos principales: Multiverso y Universos paralelos.
Los cosmólogos teóricos estudian modelos del conjunto espacio-tiempo que estén conectados, y buscan modelos que sean consistentes con los modelos físicos cosmológicos del espacio-tiempo en la escala del universo observable. Sin embargo, recientemente han tomado fuerza teorías que contemplan la posibilidad de multiversos o varios universos coexistiendo simultáneamente. Según la recientemente enunciada Teoría de Multiexplosiones se pretende dar explicación a este aspecto, poniendo en relieve una posible convivencia de universos en un mismo espacio.30​

El universo, ¿una ilusión?
Científicos del King's College de Londres lograron recrear las condiciones inmediatamente seguidas al Big Bang a través del conocimiento adquirido durante dos años de la partícula de Higgs y llegaron a la conclusión de que, posiblemente, el universo colapsó, hasta dejar de existir casi tan pronto cuando empezó,31​ lo que plantea la idea de que todo lo que vemos no existe y solo es el pasado de los astros.32​

Estructuras agregadas del universo
Las galaxias
Artículo principal: Galaxia

Imagen de la galaxia espiral M81 tomada por el Hubble.
A gran escala, el universo está formado por galaxias y agrupaciones de galaxias. Las galaxias son agrupaciones masivas de estrellas, y son las estructuras más grandes en las que se organiza la materia en el universo. A través del telescopio se manifiestan como manchas luminosas de diferentes formas. A la hora de clasificarlas, los científicos distinguen entre las galaxias del Grupo Local, compuesto por las treinta galaxias más cercanas y a las que está unida gravitacionalmente nuestra galaxia (la Vía Láctea), y todas las demás galaxias, a las que llaman "galaxias exteriores".

Las galaxias están distribuidas por todo el universo y presentan características muy diversas, tanto en lo que respecta a su configuración como a su antigüedad. Las más pequeñas abarcan alrededor de 3000 millones de estrellas, y las galaxias de mayor tamaño pueden llegar a abarcar más de un billón de astros. Estas últimas pueden tener un diámetro de 170 000 años luz, mientras que las primeras no suelen exceder de los 6000 años luz.

Además de estrellas y sus astros asociados (planetas, asteroides, etc...), las galaxias contienen también materia interestelar, constituida por polvo y gas en una proporción que varía entre el 1 y el 10 % de su masa.

Se estima que el universo puede estar constituido por unos 100 000 millones de galaxias, aunque estas cifras varían en función de los diferentes estudios.

Formas de galaxias
La creciente potencia de los telescopios, que permite observaciones cada vez más detalladas de los distintos elementos del universo, ha hecho posible una clasificación de las galaxias por su forma. Se han establecido así cuatro tipos distintos: galaxias elípticas,lenticulares, espirales, espirales barradas e irregulares.

Galaxias elípticas

Galaxia elíptica NGC 1316.
Artículo principal: Galaxia elíptica
En forma de elipse o de esferoide, se caracterizan por carecer de una estructura interna definida y por presentar muy poca materia interestelar. Se consideran las más antiguas del universo, ya que sus estrellas son viejas y se encuentran en una fase muy avanzada de su evolución.

Galaxias lenticulares
Artículo principal: Galaxia lenticular
Las galaxias de este tipo fueron en su momento galaxias espirales, pero consumieron o perdieron gran parte de materia interestelar, por lo que hoy carecen de brazos espirales y solo presenta su núcleo. Aunque a veces existe cierta cantidad de materia interestelar, sobre todo polvo, que se agrupa en forma de disco alrededor de esta. Estas galaxias constituyen alrededor del 3 % de las galaxias del universo.

Galaxias espirales
Artículo principal: Galaxia espiral
Están constituidas por un núcleo central y dos o más brazos en espiral, que parten del núcleo. Este se halla formado por multitud de estrellas y apenas tiene materia interestelar, mientras que en los brazos abunda la materia interestelar y hay gran cantidad de estrellas jóvenes, que son muy brillantes. Alrededor del 75 % de las galaxias del universo son de este tipo.

Galaxia espiral barrada
Artículo principal: Galaxia espiral barrada
Es un subtipo de galaxia espiral, caracterizados por la presencia de una barra central de la que típicamente parten dos brazos espirales. Este tipo de galaxias constituyen una fracción importante del total de galaxias espirales. La Vía Láctea es una galaxia espiral barrada.

Galaxias irregulares

Galaxia irregular NGC 1427.
Artículo principal: Galaxia irregular
Incluyen una gran diversidad de galaxias, cuyas configuraciones no responden a las tres formas anteriores, aunque tienen en común algunas características, como la de ser casi todas pequeñas y contener un gran porcentaje de materia interestelar. Se calcula que son irregulares alrededor del 5 % de las galaxias del universo.

La Vía Láctea
Artículo principal: Vía Láctea
La Vía Láctea es nuestra galaxia. Según las observaciones, posee una masa de 1012 masas solares y es de tipo espiral barrada. Con un diámetro medio de unos 100 000 años luz se calcula que contiene unos 200 000 millones de estrellas, entre las cuales se encuentra el Sol. La distancia desde el Sol al centro de la galaxia es de alrededor de 27 700 años luz (8,5 kpc) A simple vista, se observa como una estela blanquecina de forma elíptica, que se puede distinguir en las noches despejadas. Lo que no se aprecian son sus brazos espirales, en uno de los cuales, el llamado brazo de Orión, está situado nuestro sistema solar, y por tanto la Tierra.

El núcleo central de la galaxia presenta un espesor uniforme en todos sus puntos, salvo en el centro, donde existe un gran abultamiento con un grosor máximo de 16 000 años luz, siendo el grosor medio de unos 6000 años luz.


Vía Láctea
Todas las estrellas y la materia interestelar que contiene la Vía Láctea, tanto en el núcleo central como en los brazos, están situadas dentro de un disco de 100 000 años luz de diámetro, que gira sobre su eje a una velocidad lineal superior a los 216 km/s.33​

Las constelaciones
Artículo principal: Constelación

Constelación Andrómeda
Tan solo tres galaxias distintas a la nuestra son visibles a simple vista. Tenemos la Galaxia de Andrómeda, visible desde el Hemisferio Norte; la Gran Nube de Magallanes, y la Pequeña Nube de Magallanes, en el Hemisferio Sur celeste. El resto de las galaxias no son visibles al ojo desnudo sin ayuda de instrumentos. Sí que lo son, en cambio, las estrellas que forman parte de la Vía Láctea. Estas estrellas dibujan a menudo en el cielo figuras reconocibles, que han recibido diversos nombres en relación con su aspecto. Estos grupos de estrellas de perfil identificable se conocen con el nombre de constelaciones. La Unión Astronómica Internacional agrupó oficialmente las estrellas visibles en 88 constelaciones, algunas de ellas muy extensas, como Hidra o la Osa Mayor, y otras muy pequeñas como Flecha y Triángulo.

Las estrellas
Artículo principal: Estrella
Son los elementos constitutivos más destacados de las galaxias. Las estrellas son enormes esferas de gas que brillan debido a sus gigantescas reacciones nucleares. Cuando debido a la fuerza gravitatoria, la presión y a la temperatura del interior de una estrella que sea suficientemente intensa, se inicia la fusión nuclear de sus átomos, y comienzan a emitir una luz roja oscura, que después se mueve hacia el estado superior, que es en el que está nuestro Sol, para posteriormente, al modificarse las reacciones nucleares interiores, dilatarse y finalmente enfriarse.


Remanente de la supernova
Al acabarse el hidrógeno, se originan reacciones nucleares de elementos más pesados, más energéticas, que convierten la estrella en una gigante roja. Con el tiempo, esta se vuelve inestable, a la vez que lanza hacia el espacio exterior la mayor parte del material estelar. Este proceso puede durar 100 millones de años, hasta que se agota toda la energía nuclear, y la estrella se contrae por efecto de la gravedad hasta hacerse pequeña y densa, en la forma de enana blanca, azul o marrón. Si la estrella inicial es varias veces más masiva que el Sol, su ciclo puede ser diferente, y en lugar de una gigante, puede convertirse en una supergigante y acabar su vida con una explosión denominada supernova. Estas estrellas pueden acabar como estrellas de neutrones. Tamaños aún mayores de estrellas pueden consumir todo su combustible muy rápidamente, transformándose en una entidad supermasiva llamada agujero negro.


El centro galáctico visto por los telescopios 2MASS.
Los púlsares son fuentes de ondas de radio que emiten con periodos regulares. La palabra «púlsar» significa pulsating radio source (fuente de radio pulsante). Se detectan mediante radiotelescopios y se requieren relojes de extraordinaria precisión para detectar sus cambios de ritmo. Los estudios indican que un púlsar es una estrella de neutrones pequeña que gira a gran velocidad. El más conocido está en la Nebulosa del Cangrejo. Su densidad es tan grande que una muestra de cuásar del tamaño de una bola de bolígrafo tendría una masa de cerca de 100 000 toneladas. Su campo magnético, muy intenso, se concentra en un espacio reducido. Esto lo acelera y lo hace emitir gran cantidad de energía en haces de radiación que aquí recibimos como ondas de radio.

La palabra «cuásar» es un acrónimo de quasi stellar radio source (fuentes de radio casi estelares). Se identificaron en la década de 1950. Más tarde se vio que mostraban un desplazamiento al rojo más grande que cualquier otro objeto conocido. La causa era el Efecto Doppler, que mueve el espectro hacia el rojo cuando los objetos se alejan. El primer cuásar estudiado, denominado 3C 273, está a 1500 millones de años luz de la Tierra. A partir de 1980 se han identificado miles de cuásares, algunos alejándose de nosotros a velocidades del 90 % de la luz.

Se han descubierto cuásares a 12 000 millones de años luz de la Tierra; prácticamente la edad del universo. A pesar de las enormes distancias, la energía que llega en algunos casos es muy grande, equivalente a la recibida desde miles de galaxias: como ejemplo, el s50014+81 es unas 60 000 veces más brillante que toda la Vía Láctea.

Los planetas
Artículo principal: Planeta
Los planetas son cuerpos que giran en torno a una estrella y que, según la definición de la Unión Astronómica Internacional, deben cumplir además la condición de haber limpiado su órbita de otros cuerpos rocosos importantes, y de tener suficiente masa como para que su fuerza de gravedad genere un cuerpo esférico. En el caso de cuerpos que orbitan alrededor de una estrella que no cumplan estas características, se habla de planetas enanos, planetesimales, o asteroides. En nuestro Sistema Solar hay 8 planetas: Mercurio, Venus, Tierra, Marte, Júpiter, Saturno, Urano y Neptuno, considerándose desde 2006 a Plutón como un planeta enano. A finales de 2009, fuera de nuestro sistema solar se habían detectado más de 400 planetas extrasolares, pero los avances tecnológicos están permitiendo que este número crezca a buen ritmo.

Los satélites
Artículo principal: Satélite natural
Los satélites naturales son astros que giran alrededor de los planetas. El único satélite natural de la Tierra es la Luna, que es también el satélite más cercano al sol. A continuación se enumeran los principales satélites de los planetas del sistema solar (se incluye en el listado a Plutón, considerado por la UAI como un planeta enano).

Tierra: 1 satélite → Luna
Marte: 2 satélites → Fobos, Deimos
Júpiter: 63 satélites → Metis, Adrastea, Amaltea, Tebe, Ío, Europa, Ganimedes, Calisto, Leda, Himalia, Lisitea, Elara, Ananké, Carmé, Pasífae, Sinope...
Saturno: 82 satélites → Pan, Atlas, Prometeo, Pandora, Epimeteo, Jano, Mimas, Encélado, Tetis, Telesto, Calipso, Dione, Helena, Rea, Titán, Hiperión, Jápeto, Febe...
Urano: 15 satélites → Cordelia, Ofelia, Bianca, Crésida, Desdémona, Julieta, Porcia, Rosalinda, Belinda, Puck, Miranda, Ariel, Umbriel, Titania, Oberón.
Neptuno: 8 satélites → Náyade, Talasa, Despina, Galatea, Larisa, Proteo, Tritón, Nereida
Plutón: 5 satélites → Caronte, Nix, Hidra, Cerbero y Estigia
Asteroides y cometas
Artículos principales: Asteroide y Cometa.

C/2014 Q2 (Lovejoy)
En aquellas zonas de la órbita de una estrella en las que, por diversos motivos, no se ha producido la agrupación de la materia inicial en un único cuerpo dominante o planeta, aparecen los discos de asteroides: objetos rocosos de muy diversos tamaños que orbitan en grandes cantidades en torno a la estrella, chocando eventualmente entre sí. Cuando las rocas tienen diámetros inferiores a 50 m se denominan meteoroides. A consecuencia de las colisiones, algunos asteroides pueden variar sus órbitas, adoptando trayectorias muy excéntricas que periódicamente les acercan la estrella. Cuando la composición de estas rocas es rica en agua u otros elementos volátiles, el acercamiento a la estrella y su consecuente aumento de temperatura origina que parte de su masa se evapore y sea arrastrada por el viento solar, creando una larga cola de material brillante a medida que la roca se acerca a la estrella. Estos objetos se denominan cometas. En nuestro sistema solar hay dos grandes discos de asteroides: uno situado entre las órbitas de Marte y Júpiter, denominado el Cinturón de asteroides, y otro mucho más tenue y disperso en los límites del sistema solar, a aproximadamente un año luz de distancia, denominado Nube de Oort.



Mapa del universo observable con los objetos astronómicos notables conocidos en la actualidad. Los cuerpos celestes aparecen con el tamaño agrandado para poder apreciar su forma.

Indicios de un comienzo
La teoría general de la relatividad, que fue publicada por Albert Einstein en 1916, implicaba que el cosmos se hallaba en expansión o en contracción. Pero este concepto era totalmente opuesto a la noción de un universo estático, aceptada entonces hasta por el propio Einstein. De ahí que este incluyera en sus cálculos lo que denominó “constante cosmológica”, ajuste mediante el cual intentaba conciliar su teoría con la idea aceptada de un universo estático e inmutable. Sin embargo, ciertos descubrimientos que se sucedieron en los años veinte llevaron a Einstein a decir que el ajuste que había efectuado a su teoría de la relatividad era el ‘mayor error de su vida’. Dichos descubrimientos se realizaron gracias a la instalación de un enorme telescopio de 254 centímetros en el monte Wilson (California). Las observaciones formuladas en los años veinte con la ayuda de este instrumento demostraron que el universo se halla en expansión.

Hasta entonces, los mayores telescopios solo permitían identificar las estrellas de nuestra galaxia, la Vía Láctea, y aunque se veían borrones luminosos, llamados nebulosas, por lo general se tomaban por remolinos de gas existentes en nuestra galaxia. Gracias a la mayor potencia del telescopio del monte Wilson, Edwin Hubble logró distinguir estrellas en aquellas nebulosas. Finalmente se descubrió que los borrones eran lo mismo que la Vía Láctea: galaxias. Hoy se cree que hay entre 50 000 y 125 000 millones de galaxias, cada una con cientos de miles de millones de estrellas.

A finales de los años veinte, Hubble también descubrió que las galaxias se alejan de nosotros, y que lo hacen más velozmente cuanto más lejos se hallan. Los astrónomos calculan la tasa de recesión de las galaxias mediante el espectrógrafo, instrumento que mide el espectro de la luz procedente de los astros. Para ello, dirigen la luz que proviene de estrellas lejanas hacia un prisma, que la descompone en los colores que la integran.

La luz de un objeto es rojiza (fenómeno llamado corrimiento al rojo) si este se aleja del observador, y azulada (corrimiento al azul) si se le aproxima. Cabe destacar que, salvo en el caso de algunas galaxias cercanas, todas las galaxias conocidas tienen líneas espectrales desplazadas hacia el rojo. De ahí infieren los científicos que el universo se expande de forma ordenada. La tasa de dicha expansión se determina midiendo el grado de desplazamiento al rojo. ¿Qué conclusión se ha extraído de la expansión del cosmos? Pues bien, un científico invitó al público a analizar el proceso a la inversa —como una película de la expansión proyectada en retroceso— a fin de observar la historia primitiva del universo. Visto así, el cosmos parecería estar en recesión o contracción, en vez de en expansión y retornaría finalmente a un único punto de origen.

El físico Stephen Hawking concluyó lo siguiente en su libro Agujeros negros y pequeños universos (y otros ensayos), editado en 1993: «La ciencia podría afirmar que el universo tenía que haber conocido un comienzo». Pero hace años, muchos expertos rechazaban que el universo hubiese tenido principio. El científico Fred Hoyle no aceptaba que el cosmos hubiera surgido mediante lo que llamó burlonamente a big bang («una gran explosión»). Uno de los argumentos que esgrimía era que, de haber existido un comienzo tan dinámico, deberían conservarse residuos de aquel acontecimiento en algún lugar del universo: tendría que haber radiación fósil, por así decirlo; una leve luminiscencia residual.

El diario The New York Times (8 de marzo de 1998) indicó que hacia 1965 «los astrónomos Arno Penzias y Robert Wilson descubrieron la omnipresente radiación de fondo: el destello residual de la explosión primigenia». El artículo añadió: «Todo indicaba que la teoría [de la gran explosión] había triunfado».

Pero en los años posteriores al hallazgo se formuló esta objeción: Si el modelo de la gran explosión era correcto, ¿Por qué no se habían detectado leves irregularidades en la radiación? (La formación de las galaxias habría requerido un universo que contase con zonas más frías y densas que permitieran la fusión de la materia.) En efecto, los experimentos realizados por Penzias y Wilson desde la superficie terrestre no revelaban tales irregularidades.

Por esta razón, la NASA lanzó en noviembre de 1989 el satélite COBE (siglas de Explorador del Fondo Cósmico, en inglés), cuyos descubrimientos se calificaron de cruciales. “Las ondas que detectó su radiómetro diferencial de microondas correspondían a las fluctuaciones que dejaron su impronta en el cosmos y que hace miles de millones de años llevaron a la formación de las galaxias.”

Otros términos
Diferentes palabras se han utilizado a través de la historia para denotar "todo el espacio", incluyendo los equivalentes y las variantes en varios lenguajes de "cielos", "cosmos" y "mundo". El macrocosmos también se ha utilizado para este efecto, aunque está más específicamente definido como un sistema que refleja a gran escala uno, algunos, o todos estos componentes del sistema o partes. Similarmente, un microcosmos es un sistema que refleja a pequeña escala un sistema mucho mayor del que es parte.

Aunque palabras como mundo y sus equivalentes en otros lenguajes casi siempre se refieren al planeta Tierra, antiguamente se referían a cada cosa que existía (se podía ver). En ese sentido la utilizaba, por ejemplo, Copérnico. Algunos lenguajes utilizan la palabra "mundo" como parte de la palabra "espacio exterior". Un ejemplo en alemán lo constituye la palabra "Weltraum".34​

Véase también
Ver el portal sobre Astronomía Portal:Astronomía. Contenido relacionado con Astronomía.
Ver el portal sobre Cosmología Portal:Cosmología. Contenido relacionado con Cosmología.
Anexo:Localización de la Tierra en el Universo
Ambiplasma
Astrofísica
Albert Einstein
Astronomía
Big Bang
Cosmología
Cosmología física
Cosmovisión
Destino final del universo
Edad del universo
Estructura del universo a gran escala
Ley de Hubble-Lemaître
Forma del universo
Inflación cósmica
Expansión métrica del espacio
Métrica de Friedman-Lemaître-Robertson-Walker
Microcosmos
Modelo Lambda-CDM
Carl Sagan
Multiverso
Origen del universo
Panspermia
Principio antrópico
Principio holográfico
Teoría del Big Bang
Teoría del estado estacionario
Teoría de los universos fecundos
Universal (metafísica)
Universo oscilante
Universos paralelos
Referencias
 Cfr. Universal (metafísica)
 Lineweaver, Charles; Tamara M. Davis (2005). «Misconceptions about the Big Bang.» Scientific American. Consultado el 31 de marzo de 2008.
 «Primeras imágenes de la materia oscura». Consultado el 20 de diciembre de 2010.
 Munitz, Milton K. (1 de abril de 1951). «One Universe or Many?». Journal of the History of Ideas 12 (2): 231-255. doi:10.2307/2707516.
 Luminet, Jean-Pierre; Boudewijn F. Roukema (1999). «Topology of the Universe: Theory and Observations». Proceedings de la Escuela de Cosmología de Cargese (Córcega) Agosto de 1998. Consultado el 5 de enero de 2007.
 Luminet, Jean-Pierre; J. Weeks, A. Riazuelo, R. Lehoucq, J.-P. Uzan (2003). «Dodecahedral space topology as an explanation for weak wide-angle temperature correlations in the cosmic microwave background». Nature 425: 593. Consultado el 9 de enero de 2007.
 Brookhaven National Laboratory (ed.). «Heavy Ion Collisions». Archivado desde el original el 8 de abril de 2007.
 Thomas Ludlam, Larry McLerran (Octubre de 2003). Physics Today, ed. «What Have We Learned From the Relativistic Heavy Ion Collider?». Archivado desde el original el 23 de noviembre de 2006. Consultado el 28 de febrero de 2007.
 Ken Tan (15 de enero de 2007). space.com, ed. «New 'Hobbit' Galaxies Discovered Around Milky Way». Archivado desde el original el 17 de mayo de 2008. Consultado el 1 de marzo de 2007.
 The Uppsala Astronomical Observatory (ed.). «Dwarf Spheroidal Galaxies». Archivado desde el original el 2 de abril de 2012. Consultado el 1 de marzo de 2007.
 Brian Greene (2011). The Hidden Reality. Alfred A. Knopf.
 Neil J. Cornish, David N. Spergel, Glenn D. Starkman y Eiichiro Komatsu, Constraining the Topology of the Universe.astro-ph/0310233
 Lineweaver, Charles; Tamara M. Davis (2005). Scientific American, ed. «Misconceptions about the Big Bang» (en inglés). Consultado el 5 de marzo de 2007.
 «WMAP produces new results» (en inglés).
 Baldry, Ivan K.; Glazebrook, Karl (2002), «The 2dF Galaxy Redshift Survey: Constraints on Cosmic Star Formation History from the Cosmic Spectrum», The Astrophysical Journal (The American Astronomical Society, publicado el 20 de abril 2002) 569: 582–594, doi:10.1086/339477
 Associated Press (28 de agosto de 2008). «Universe: Beige, not Turquoise». Wired.com. Archivado desde el original el 24 de julio de 2008. Consultado el 1 de noviembre de 2009.
 N. Mandolesi; P. Calzolari; S. Cortiglioni; F. Delpino; G. Sironi (1986). «Large-scale homogeneity of the Universe measured by the microwave background». Letters to Nature 319: 751-753.
 Hinshaw, Gary (2006). NASA WMAP, ed. «New Three Year Results on the Oldest Light in the Universe». Consultado el 7 de marzo de 2007.
 Strobel, Nick (2001). Astronomy Notes, ed. «The Composition of Stars». Consultado el 8 de marzo de 2007.
 Astrophysics (Astronomy Frequently Asked Questions) (ed.). «Have physical constants changed with time?». Consultado el 8 de marzo de 2007.
 Gary Hinshaw (10 de febrero de 2006). NASA WMAP, ed. «What is the Universe Made Of?». Consultado el 1 de marzo de 2007.
 La Antimateria Archivado el 22 de diciembre de 2016 en la Wayback Machine.
 Difference in direct charge-parity violation between charged and neutral B meson decays,Nature 452, 332-335 (20 de marzo de 2008)
 New Theory of the Universe Marries Two of its Biggest Mysteries (31 de enero de 2007) de Laura Mgrdichian sobre el trabajo de Tom Banks, Sean Echols y Jeff L. Jones, Baryogenesis, dark matter and the pentagon. J. High Energy Phys. JHEP11 (2006) 046 (en inglés)
 Edward L. Wright (12 de septiembre de 2004). UCLA, ed. «Big Bang Nucleosynthesis». Consultado el 2 de marzo de 2007.
 M. Harwit; M. Spaans (2003). «Chemical Composition of the Early Universe». The Astrophysical Journal 589 (1): 53-57.
 C. Kobulnicky; E. D. Skillman (1997). «Chemical Composition of the Early Universe». Bulletin of the American Astronomical Society 29: 1329.
 Gary Hinshaw (15 de diciembre de 2005). NASA WMAP, ed. «Tests of the Big Bang: The CMB». Consultado el 2 de marzo de 2007.
 Belle Dumé (16 de junio de 2005). Institute of Physics Publishing, ed. «Background neutrinos join the limelight». Consultado el 2 de marzo de 2007.
 Sus modelos son especulativos pero utilizan los métodos de la física de la Royal Astronomical Society 347. 2004. pp. 921—936. Consultado el 9 de enero de 2007.
 Parnell, Brid-Aine. «Higgs Boson Seems To Prove That The Universe Doesn't Exist». Forbes (en inglés). Consultado el 8 de febrero de 2017.
 «Un estudio demuestra que el universo dejó de existir hace 14 mil millones de años». History Channel. 22 de agosto de 2014. Consultado el 8 de febrero de 2017.
 Ross Taylor, Stuart (2000) [1998]. «The place of the solar system in the universe: The extent of the universe» [Planteamiento de la cuestión: El lugar del sistema solar en el universo]. Destiny or chance: Our Solar System and its place in the Cosmos [Nuestro sistema solar y su lugar en el cosmos] (en inglés). Nueva York NY, Estados Unidos: Cambrigde University Press. p. 19. ISBN 0-521-48178-3. Consultado el 22 de noviembre de 2014.
 Albert Einstein (1952). Relativity: The Special and the General Theory (Fifteenth Edition), ISBN 0-517-88441-0.
Enlaces externos
 Wikimedia Commons alberga una categoría multimedia sobre Universo.
 Wikiquote alberga frases célebres de o sobre Universo.
 Wikcionario tiene definiciones y otra información sobre universo.
Proyecto Celestia Actividad Educativa "El Universo" dirigida a alumnos de Secundaria, Bachillerato o aficionados a la astronomía en general.
Alemañ Berenguer, Rafael Andrés (2001) Tras los Secretos del Universo ISBN 84-95495-08-2.
Vídeos sobre el Universo: Biblioteca audiovisual sobre el Cosmos.
En inglés:

El Universo de Stephen Hawking - ¿Por qué el universo es así?
Richard Powell: Un Atlas del Universo - imágenes en varias escalas, con explicaciones.
Cosmos - una "revista dimensional ilustrada desde el microcosmos al macrocosmos".
Edad del universo en Space.com.
Mi Así-Llamado Universo; argumentos a favor y en contra de universos paralelos e infinitos.
Universos paralelos, por Max Tegmark.
Seti@Home - La Búsqueda de Inteligencia Extraterrestre.
Universo - Centro de Información Espacial, por Exploreuniverse.com.
Número de galaxias en el universo.
Tamaño del universo en Space.com.
Ilustración comparando los tamaños de los planetas, el sol y otras estrellas.
Cosmología (P+F).
Control de autoridades	
Proyectos WikimediaWd Datos: Q1Commonscat Multimedia: UniverseWikiquote Citas célebres: Universo
IdentificadoresGND: 4079154-3NDL: 00574074NKC: ph116566Microsoft Academic: 84999194Diccionarios y enciclopediasBritannica: url
Categoría: Universo
Menú de navegación
No has accedido
Discusión
Contribuciones
Crear una cuenta
Acceder
ArtículoDiscusión
LeerEditarVer historialBuscar
Buscar en Wikipedia
Portada
Portal de la comunidad
Actualidad
Cambios recientes
Páginas nuevas
Página aleatoria
Ayuda
Donaciones
Notificar un error
Herramientas
Lo que enlaza aquí
Cambios en enlazadas
Subir archivo
Páginas especiales
Enlace permanente
Información de la página
Citar esta página
Elemento de Wikidata
Imprimir/exportar
Crear un libro
Descargar como PDF
Versión para imprimir
En otros proyectos
Wikimedia Commons
Wikiquote

En otros idiomas
Башҡортса
English
Suomi
Nāhuatl
Napulitano
Português
Sicilianu
සිංහල
Svenska
159 más
Editar enlaces
Esta página se editó por última vez el 3 abr 2021 a las 18:42.
El texto está disponible bajo la Licencia Creative Commons Atribución Compartir Igual 3.0; pueden aplicarse cláusulas adicionales. Al usar este sitio, usted acepta nuestros términos de uso y nuestra política de privacidad.
Wikipedia® es una marca registrada de la Fundación Wikimedia, Inc., una organización sin ánimo de lucro.
Albert Einstein
Ir a la navegaciónIr a la búsqueda
«Einstein» redirige aquí. Para otras acepciones, véase Einstein (desambiguación).
Albert Einstein
Albert Einstein Head.jpg
Albert Einstein en 1947
Información personal
Nacimiento	14 de marzo de 1879
Ulm (Reino de Wurtemberg)
Fallecimiento	18 de abril de 1955 (76 años)
Princeton (Estados Unidos)
Causa de la muerte	Aneurisma de aorta abdominal
Sepultura	National Museum of Health and Medicine
Residencia	Einsteinhaus, Alemania, Múnich y Princeton
Nacionalidad	Alemana (1879-1896, 1918-1933, 1933), sin nacionalidad (1896-1901), suiza (1901-1955), austrohúngara (1911-1912) y estadounidense (1940-1955)
Lengua materna	Alemán
Partido político	Partido Democrático Alemán (hasta 1933)
Familia
Padres	Hermann Einstein
Pauline Koch
Cónyuge	
Mileva Marić (1903-1919)
Elsa Einstein (1919-1936)
Hijos	
Hans Albert Einstein
Eduard Einstein
Lieserl Einstein
Educación
Educación	doctorado en Física y Doctor en Filosofía
Educado en	
Luitpold-Gymnasium (1888-1894)
Escuela Politécnica Federal de Zúrich (B.S. en Educación matemática; 1896-1900)
Old Kantonsschule (Albert Einstein House) (Matura (educación); 1895-1896)
Universidad de Zúrich (Doc. en Física; hasta 1905)
Supervisor doctoral	Alfred Kleiner, Heinrich Burkhardt y Heinrich Friedrich Weber
Información profesional
Ocupación	Físico teórico, filósofo de la ciencia, inventor, escritor de ciencia, pedagogo, profesor universitario (desde 1909), físico, ensayista, filósofo, escritor, profesor, científico, matemático, patent examiner (1901-1906) y catedrático
Área	Física teórica
Cargos ocupados	Catedrático
Empleador	
Universidad Carolina de Praga
Universidad de Leiden
Berna
Institute for Advanced Study
Universidad de California en Berkeley
Instituto federal suizo de propiedad intelectual (1902-1909)
Universidad de Berna (1908-1909)
Universidad de Zúrich (1909-1911)
German University in Prague (1911-1912)
Escuela Politécnica Federal de Zúrich (1912-1914)
Universidad Humboldt de Berlín (1914-1933)
Academia Prusiana de las Ciencias (1914-1933)
Deutsche Physikalische Gesellschaft (1916-1918)
Sociedad Kaiser Wilhelm (1917-1933)
Universidad de Leiden (1920-1946)
Universidad de Princeton (1933-1955)
Obras notables	
relatividad especial
relatividad general
Efecto fotoeléctrico
teoría de la relatividad
equivalencia entre masa y energía
constante de Planck
Ecuaciones del campo de Einstein
mecánica cuántica
teoría del campo unificado
Miembro de	
Royal Society
Academia Prusiana de las Ciencias
Academia Alemana de las Ciencias Naturales Leopoldina
Academia Nacional de los Linces
Sociedad Filosófica Estadounidense
Academia de Ciencias de Baviera
Academia de Ciencias de Gotinga
Academia de Ciencias de la Unión Soviética
Academia de Ciencias de Francia
Real Academia de las Ciencias de Suecia
Real Academia de Artes y Ciencias de los Países Bajos
Academia Estadounidense de las Artes y las Ciencias
Academia Nacional de Ciencias de los Estados Unidos (desde 1922)
Academia Nacional de Ciencias de los Estados Unidos (desde 1942)
Firma	Albert Einstein signature 1934.svg
Web
Sitio web	
einstein.biz
Albert Einstein (alemán: /ˈalbɛɐ̯t ˈʔaɪnʃtaɪn/; Ulm, Imperio alemán; 14 de marzo de 1879 - Princeton, Estados Unidos; 18 de abril de 1955) fue un físico alemán de origen judío, nacionalizado después suizo, austriaco y estadounidense. Se le considera el científico más importante, conocido y popular del siglo XX.1​2​

En 1905, cuando era un joven físico desconocido, empleado en la Oficina de Patentes de Berna, publicó su teoría de la relatividad especial. En ella incorporó, en un marco teórico simple fundamentado en postulados físicos sencillos, conceptos y fenómenos estudiados antes por Henri Poincaré y por Hendrik Lorentz. Como una consecuencia lógica de esta teoría, dedujo la ecuación de la física más conocida a nivel popular: la equivalencia masa-energía, E=mc². Ese año publicó otros trabajos que sentarían algunas de las bases de la física estadística y de la mecánica cuántica.

En 1915, presentó la teoría de la relatividad general, en la que reformuló por completo el concepto de la gravedad.3​ Una de las consecuencias fue el surgimiento del estudio científico del origen y la evolución del Universo por la rama de la física denominada cosmología. En 1919, cuando las observaciones británicas de un eclipse solar confirmaron sus predicciones acerca de la curvatura de la luz, fue idolatrado por la prensa.4​ Einstein se convirtió en un icono popular de la ciencia mundialmente famoso, un privilegio al alcance de muy pocos científicos.5​

Por sus explicaciones sobre el efecto fotoeléctrico y sus numerosas contribuciones a la física teórica, en 1921 obtuvo el Premio Nobel de Física y no por la Teoría de la Relatividad, pues el científico a quien se encomendó la tarea de evaluarla no la entendió, y temieron correr el riesgo de que luego se demostrase errónea.6​7​ En esa época era aún considerada un tanto controvertida.

Ante el ascenso del nazismo, Einstein abandonó Alemania hacia diciembre de 1932 con destino a Estados Unidos, donde se dedicó a la docencia en el Institute for Advanced Study. Se nacionalizó estadounidense en 1940. Durante sus últimos años trabajó por integrar en una misma teoría la fuerza gravitatoria y la electromagnética.

Aunque es considerado por algunos como el «padre de la bomba atómica», abogó por el federalismo mundial, el internacionalismo, el pacifismo, el sionismo y el socialismo democrático, con una fuerte devoción por la libertad individual y la libertad de expresión.8​9​10​11​ Fue proclamado «personaje del siglo XX» y el más preeminente científico por la revista Time.12​


Índice
1	Biografía
1.1	Infancia
1.2	Juventud
1.3	Madurez
1.4	Muerte
2	Trayectoria científica
2.1	Los artículos de 1905
2.1.1	Efecto fotoeléctrico
2.1.2	Movimiento browniano
2.1.3	Relatividad especial
2.1.4	Equivalencia masa-energía
2.2	Relatividad general
2.3	Estadísticas de Bose-Einstein
2.4	Debate Bohr-Einstein
2.5	La teoría de campo unificada
3	Actividad política
4	Ética y religión
5	Algunas publicaciones
6	Eponimia
7	En la cultura popular
8	Véase también
9	Referencias
10	Bibliografía
10.1	Bibliografía general
10.2	Einstein y la teoría de la relatividad
10.3	Material digital
11	Enlaces externos
11.1	Enlaces en otros idiomas
Biografía
Infancia
Nació en la ciudad alemana de Ulm, cien kilómetros al este de Stuttgart, en el seno de una familia judía. Sus padres fueron Hermann Einstein y Pauline Koch. Hermann y Pauline se habían casado en 1876, cuando Hermann tenía casi veintinueve años y ella dieciocho.13​ La familia de Pauline vivía cerca de Stuttgart, concretamente en la ciudad de Cannstatt; allí su padre, Julius Koch, explotaba con su hermano Heinrich un comercio muy próspero de cereales. Pauline tocaba el piano y le transmitió a su hijo su amor por la música, entre otras cualidades como su "perseverancia y paciencia".14​ De su padre, Hermann, también heredó ciertos caracteres como la generosidad y la amabilidad que caracterizaron a Albert.13​

A young boy with short hair and a round face, wearing a white collar and large bow, with vest, coat, skirt and high boots. He is leaning against an ornate chair.
Einstein a los 3 años, en 1882.
En 1880 la familia se mudó a Múnich, donde se criaría durante catorce años, y su padre y el hermano de este, Jakob, quien influyó intelectualmente sobre Albert, fundaron en octubre una empresa dedicada a la instalación de agua y gas. Como el negocio marchaba bien, con el apoyo de toda la familia decidieron abrir un taller propio de aparatos eléctricos (Elektrotechnische Fabrik J. Einstein & Cie.), que suministraban a centrales eléctricas en Múnich-Schwabing, Varese y Susa en Italia, la que fracasaría tras endeudar a toda la familia. Esto causó un trauma no solo a Albert sino también al resto de la familia. A fin de saldar las deudas y financiar el traslado, el querido jardín de la casa de Múnich fue vendido a un promotor inmobiliario.13​

Desde sus comienzos, demostró cierta dificultad para expresarse, pues no empezó a hablar hasta la edad de tres años, por lo que aparentaba poseer algún retardo que le provocaría algunos problemas. Al contrario que su hermana menor, Maya, que era más vivaracha y alegre, Albert era paciente y metódico y no le gustaba exhibirse. Solía evitar la compañía de otros infantes de su edad y a pesar de que, como niños, también tenían de vez en cuando sus diferencias, únicamente admitía a su hermana en sus soledades. Cursó sus estudios primarios en una escuela católica; desde 1888 asistió al instituto de segunda enseñanza Luitpold (que en 1965 recibiría el nombre de Gymasium Albert Einstein). Sacó buenas notas en general, no tanto en las asignaturas de idiomas, pero excelentes en las de ciencias naturales. Los libros de divulgación científica de Aaron Bernstein marcaron su interés y su futura carrera. Fue un período difícil que sobrellevaría gracias a las clases de violín (a partir de 1884) que le daría su madre (instrumento que le apasionaba y que continuó tocando el resto de sus días)15​ y a la introducción al álgebra que le descubriría su tío Jacob.16​ Su paso por el Gymnasium (instituto de bachillerato), sin embargo, no fue muy gratificante: la rigidez y la disciplina militar de los institutos de secundaria de la época de Otto von Bismarck le granjearon no pocas polémicas con los profesores: en el Luitpold Gymnasium las cosas llegaron a un punto crítico en 1894, cuando Einstein tenía quince años. Un nuevo profesor, el Dr. Joseph Degenhart, le dijo que «nunca conseguiría nada en la vida». Cuando Einstein le respondió que «no había cometido ningún delito», el profesor le respondió: «tu sola presencia aquí mina el respeto que me debe la clase».17​


Einstein en 1893, a los 14 años.
Su tío, Jacob Einstein, un ingeniero con gran inventiva e ideas, convenció al padre de Albert para que construyese una casa con un taller, en donde llevarían a cabo nuevos proyectos y experimentos tecnológicos de la época a modo de obtener unos beneficios, pero, debido a que los aparatos y artilugios que afinaban y fabricaban eran productos para el futuro, en el presente carecían de compradores y el negoció fracasó. El pequeño Albert, se crio motivado por las investigaciones que se realizaban en el taller y todos los aparatos que allí había. Además, su tío incentivó sus inquietudes científicas proporcionándole libros de ciencia. Según relata el propio Einstein en su autobiografía, de la lectura de estos libros de divulgación científica nacería un constante cuestionamiento de las afirmaciones de la religión; un librepensamiento decidido que fue asociado a otras formas de rechazo hacia el Estado y la autoridad. Un escepticismo poco común en aquella época, a decir del propio Einstein. El colegio no lo motivaba, y aunque era excelente en matemáticas y física, no se interesaba por las demás asignaturas. A los quince años, sin tutor ni guía, emprendió el estudio del cálculo infinitesimal. La idea, claramente infundada, de que era un mal estudiante proviene de los primeros biógrafos que escribieron sobre Einstein, que confundieron el sistema de calificación escolar de Suiza (un 6 en Suiza es la mejor calificación) con el alemán (un 6 es la peor nota).18​En este «Erziehungsrat» aparece con nota 6 en todas las asignaturas: Álgebra, Física, Geometría, Geometría Analítica y Trigonometría.

En 1894, la compañía Hermann sufría importantes dificultades económicas y los Einstein se mudaron de Múnich a Pavía, en Italia, cerca de Milán. Albert permaneció en Múnich para terminar sus cursos antes de reunirse con su familia en Pavía, pero la separación duró poco tiempo: antes de obtener su título de bachiller decidió abandonar el Gymnasium. Sin consultarlo con sus padres, Albert se puso en contacto con un médico (el hermano mayor de Max Talmud, un estudiante de medicina que iba todos los viernes a comer a la casa de los padres de Einstein) para que certificara que padecía de agotamiento y necesitaba un tiempo sin asistir a la escuela, y convenció a un profesor para que certificara su excelencia en el campo de las matemáticas. Las autoridades de la escuela le dejaron ir. Justo después de las Navidades de 1894, Albert abandonó Múnich y se fue a Milán para reunirse con sus padres.17​

Juventud
Así, la familia Einstein intentó matricular a Albert en la Escuela Politécnica Federal de Zúrich pero, al no tener el título de bachiller, tuvo que presentarse a una prueba de acceso que suspendió a causa de una calificación deficiente en una asignatura de letras. Esto supuso que fuera rechazado inicialmente, pero el director del centro, impresionado por sus resultados en ciencias, le aconsejó que continuara sus estudios de bachiller y que obtuviera el título que le daría acceso directo al Politécnico. Su familia lo envió a Aarau para terminar sus estudios secundarios en la escuela cantonal de Argovia, a unos 50 km al oeste de Zúrich, donde Einstein obtuvo el título de bachiller alemán en 1896, a la edad de dieciséis años. Ese mismo año renunció a su ciudadanía alemana, presuntamente para evitar el servicio militar, pasando a ser un apátrida. Inició los trámites para naturalizarse suizo. A fines de 1896, a la edad de diecisiete años, Einstein ingresó en la Escuela Politécnica Federal de Zúrich (Suiza), probablemente el centro más importante de la Europa central para estudiar ciencias fuera de Alemania, matriculándose en la Escuela de orientación matemática y científica, con la idea de estudiar física.17​

Three young men in suits with high white collars and bow ties, sitting.
Conrad Habicht, Maurice Solovine y Einstein, los fundadores de la efímera Academia Olimpia.
Durante sus años en la políticamente vibrante Zúrich, descubrió la obra de diversos filósofos: Henri Poincaré, Baruch Spinoza, David Hume, Immanuel Kant, Karl Marx[cita requerida] y Ernst Mach. También tomó contacto con el movimiento socialista a través de Friedrich Adler y con cierto pensamiento inconformista y revolucionario en el que mucho tuvo que ver su amigo de toda la vida Michele Besso. En octubre de 1896, conoció a Mileva Marić, una compañera de clase serbia, de talante feminista y radical, de la que se enamoró. En 1900, Albert y Mileva se graduaron en el Politécnico de Zürich y en 1901, a la edad de veintidós años, consiguió la ciudadanía suiza. Durante este período discutía sus ideas científicas con un grupo de amigos cercanos, incluyendo a Mileva, con la cual tuvo en secreto una hija en enero de 1902, llamada Lieserl. Al día de hoy nadie sabe qué fue de la niña, asumiéndose que fue adoptada en la Serbia natal de Mileva, después de que ambos contrajeran matrimonio, el 6 de enero de 1903, en la ciudad de Berna. No obstante, esta teoría difícilmente puede demostrarse, ya que solo se dispone de pruebas circunstanciales. Los padres de Einstein siempre se opusieron al matrimonio, hasta que en 1902 su padre cayó enfermo de muerte y consintió. Mas su madre nunca se resignó al mismo.19​20​


Casa de Albert Einstein en Suiza.
Se graduó en 1900, obteniendo el diploma de profesor de matemática y de física, pero no pudo encontrar trabajo en la Universidad, por lo que ejerció como tutor en Winterthur, Schaffhausen y Berna. Su compañero de clase Marcel Grossmann, un hombre que más adelante desempeñaría un papel fundamental en las matemáticas de la relatividad general, le ofreció un empleo fijo en la Oficina Federal de la Propiedad Intelectual de Suiza, en Berna, una oficina de patentes, donde trabajó de 1902 a 1909.21​ Su personalidad le causó también problemas con el director de la Oficina, quien le enseñó a «expresarse correctamente».

En esta época, Einstein se refería con amor a su mujer Mileva como «una persona que es mi igual y tan fuerte e independiente como yo». Abram Joffe, en su biografía de Einstein, argumenta que durante este periodo fue ayudado en sus investigaciones por Mileva. Esto se contradice con otros biógrafos como Ronald W. Clark, quien afirma que Einstein y Mileva llevaban una relación distante que le brindaba la soledad necesaria para concentrarse en su trabajo.22​

En mayo de 1904, Einstein y Mileva tuvieron un hijo, al que llamaron Hans Albert Einstein. Ese mismo año consiguió un trabajo permanente en la Oficina de Patentes. Poco después finalizó su doctorado presentando una tesis titulada Una nueva determinación de las dimensiones moleculares, consistente en un trabajo de 17 folios que surgió de una conversación mantenida con Michele Besso, mientras se tomaban una taza de té; al azucarar Einstein el suyo, le preguntó a Besso:
¿Crees que el cálculo de las dimensiones de las moléculas de azúcar podría ser una buena tesis de doctorado?

Albert Einstein en 1904 (edad: 25)
En 1905, redactó varios trabajos fundamentales sobre la física de pequeña y gran escala. En el primero de ellos explicaba el movimiento browniano, en el segundo el efecto fotoeléctrico y los dos restantes desarrollaban la relatividad especial y la equivalencia masa-energía. El primero de ellos le valió el grado de doctor por la Universidad de Zúrich en 1906, y su trabajo sobre el efecto fotoeléctrico le haría merecedor del Premio Nobel de Física en 1921, por sus trabajos sobre el movimiento browniano y su interpretación sobre el efecto fotoeléctrico. Estos artículos fueron enviados a la revista Annalen der Physik y son conocidos generalmente como los artículos del annus mirabilis («año milagroso»).23​

Madurez

Albert Einstein en 1920.
En 1908, a la edad de veintinueve años, fue contratado en la Universidad de Berna, Suiza, como profesor y conferenciante (privatdozent). Einstein y Mileva tuvieron un nuevo hijo, Eduard, nacido el 28 de julio de 1910. Poco después la familia se mudó a Praga, donde Einstein obtuvo la plaza de professor de física teórica, el equivalente a catedrático, en la Universidad Alemana de Praga, debiendo adoptar la nacionalidad austríaca para poder acceder al cargo.24​ En esta época trabajó estrechamente con Marcel Grossmann y Otto Stern. También comenzó a llamar al tiempo matemático «cuarta dimensión».25​En 1913, justo antes de la Primera Guerra Mundial, fue elegido miembro de la Academia Prusiana de Ciencias. Estableció su residencia en Berlín, donde permaneció durante diecisiete años. El emperador Guillermo le invitó a dirigir la sección de Física del Instituto Kaiser Wilhelm de Física.26​

El 14 de febrero de 1919, a la edad de treinta y nueve años, se divorció de Mileva, después de un matrimonio de dieciséis años, y algunos meses después, el 2 de junio de 1919, se casó con una prima suya, Elsa Loewenthal, cuyo apellido de soltera era Einstein; Loewenthal era el apellido de su primer marido, Max Loewenthal. Elsa era tres años mayor que él y le había estado cuidando tras sufrir un fuerte estado de agotamiento. Einstein y Elsa no tuvieron hijos.

El destino de la hija de Albert y Mileva, Lieserl, nacida antes de que sus padres se casaran o encontraran trabajo, es desconocido. De sus dos hijos, el primero, Hans Albert, se mudó a California, donde llegó a ser profesor universitario, aunque con poca interacción con su padre; el segundo, Eduard, sufría esquizofrenia y fue internado en 1932 en una institución para tratamiento de enfermedades mentales en Zúrich. Fue el primero de muchos ingresos. Einstein quería llevar a su hijo enfermo a Princeton, pero la embajada de EE. UU. no lo admitió por sus malos antecedentes. Eduard falleció en el centro psiquiátrico en 1965.27​

En Berlín en los años 1920, la fama de Einstein despertaba acaloradas discusiones. En los diarios conservadores se podían leer editoriales que atacaban su teoría. Se convocaban conferencias-espectáculo tratando de argumentar lo disparatada que resultaba la teoría especial de la relatividad. Incluso se le atacaba, en forma velada, no abiertamente, en su condición de judío. En el resto del mundo, la teoría de la relatividad era apasionadamente debatida en conferencias populares y textos.28​

En Alemania, las expresiones de odio a los judíos alcanzaron niveles muy elevados. Varios físicos de ideología nazi, algunos tan notables como los premios Nobel de Física Johannes Stark y Philipp Lenard, intentaron desacreditar sus teorías.29​ Otros físicos que enseñaban la teoría de la relatividad, como Werner Heisenberg, fueron vetados en sus intentos de acceder a puestos docentes.30​

En 1923 visitó España, entablando relación con José Ortega y Gasset. Al desembarcar en Barcelona, y dadas las ideas socialistas que profesaba,31​ aceptó una invitación para dar una conferencia en la sede de la CNT, donde entabló amistad con Ángel Pestaña. Preguntó qué significaban las siglas CNT (Confederación Nacional del Trabajo), y cuando lo comprendió, y dadas las ideas anarquistas del sindicato, propuso eliminar la palabra "Nacional", que en Alemania tenía connotaciones violentas.32​ En su visita también conoció brevemente a Santiago Ramón y Cajal y adicionalmente recibió un homenaje del rey Alfonso XIII de España, quien lo nombra miembro de la Real Academia de Ciencias.33​

Antes del ascenso del nazismo —Adolf Hitler llegó al poder como canciller el 30 de enero de 1933—, había dejado Alemania en diciembre de 1932 para zarpar inciertamente hacia Estados Unidos, país donde enseñó en el Institute for Advanced Study, agregando a su nacionalidad suiza la estadounidense en 1940, a la edad de sesenta y un años.34​

Para la camarilla nazi los judíos no son solo un medio que desvía el resentimiento que el pueblo experimenta contra sus opresores; ven también en los judíos un elemento inadaptable que no puede ser llevado a aceptar un dogma sin crítica, y que en consecuencia amenaza su autoridad –por el tiempo que tal dogma exista– con motivo de su empeño en esclarecer a las masas.
La prueba de que este problema toca el fondo de la cuestión la proporciona la solemne ceremonia de la quema de libros, ofrecida como espectáculo por el régimen nazi poco tiempo después de adueñarse del poder.
Einstein. Nueva York. 1938.35​
Antes de decidirse por el exilio estadounidense, en 1933 el gobierno de la Segunda República española ofreció a Einstein incorporarse como investigador a la Universidad Central de Madrid. Medió en estas gestiones el entonces embajador en el Reino Unido, Ramón Pérez de Ayala, a iniciativa del ministro Fernando de los Ríos. Finalmente, ante la situación de inestabilidad política en Europa y el ascenso al poder de la CEDA en España, Einstein declinó la oferta. Ante la posibilidad de que el científico alemán aceptara el puesto, sectores de la derecha española mostraron su malestar y hubo algunas reacciones antisemitas. El diario católico El Debate (vinculado a la CEDA) publicó un editorial el 12 de abril (titulado Todo es relativo) donde se refería a Einstein como "el judío"; en otro artículo del mismo periódico se negaba que fuese una víctima de la persecución hitleriana y que su destierro fuera forzado: «El ministro socialista se ha apresurado a ofrecerle protección. Judaísmo y marxismo se identifican y confunden», se añadía.36​37​

Einstein, en 1939 decide ejercer su influencia participando en cuestiones políticas que afectan al mundo. Redacta la célebre carta a Roosevelt, para promover el proyecto atómico e impedir que los «enemigos de la humanidad» lo hicieran antes:
…puesto que dada la mentalidad de los nazis, habrían consumado la destrucción y la esclavitud del resto del mundo.38​
Durante sus últimos años, Einstein trabajó por integrar en una misma teoría las cuatro interacciones fundamentales, tarea aún inconclusa.39​

Muerte
El 16 de abril de 1955, Albert Einstein experimentó una hemorragia interna causada por la ruptura de un aneurisma de la aorta abdominal, que anteriormente había sido reforzada quirúrgicamente por el doctor Rudolph Nissen en 1948. Einstein rechazó la cirugía, diciendo: «Quiero irme cuando quiero. Es de mal gusto prolongar artificialmente la vida. He hecho mi parte, es hora de irse. Yo lo haré con elegancia». Murió en el Hospital de Princeton a primera hora del 18 de abril de 1955 a la edad de setenta y seis años.40​ En la mesilla quedaba el borrador del discurso por el séptimo aniversario de la independencia de Israel, que jamás llegaría a pronunciar, y que empezaba así: «Hoy les hablo no como ciudadano estadounidense, ni tampoco como judío, sino como ser humano».

Einstein no quiso tener un funeral rutilante, con la asistencia de dignatarios de todo el mundo. De acuerdo con su deseo, su cuerpo fue incinerado en la misma tarde, antes de que la mayor parte del mundo se enterara de la noticia. En el crematorio solo hubo doce personas, entre las cuales estuvo su hijo mayor. Sus cenizas fueron esparcidas en el río Delaware a fin de que el lugar de sus restos no se convirtiera en objeto de mórbida veneración. Pero hubo una parte de su cuerpo que no se quemó.

Durante la autopsia, el patólogo del hospital Thomas Stoltz Harvey41​ extrajo el cerebro de Einstein para conservarlo, sin el permiso de su familia, con la esperanza de que la neurociencia del futuro fuera capaz de descubrir lo que hizo a Einstein ser tan inteligente. Lo conservó durante varias décadas, hasta que finalmente lo devolvió a los laboratorios de Princeton cuando tenía más de ochenta años. Pensaba que el cerebro de Einstein «le revelaría los secretos de su genialidad y que así se haría famoso». Hasta ahora, el único dato científico medianamente interesante obtenido del estudio del cerebro es que una parte de él —la parte que, entre otras cosas, está relacionada con la capacidad matemática— es más grande que en otros cerebros.

Son recientes y escasos los estudios detallados del cerebro de Einstein. En 1985, por ejemplo, la profesora Marian Diamond, de la Universidad de California en Berkeley, informó de un número de células gliales (que nutren a las neuronas) de superior calidad en áreas del hemisferio izquierdo, encargado del control de las habilidades matemáticas. En 1999, la neurocientífica Sandra Witelson informaba que el lóbulo parietal inferior de Einstein, un área relacionada con el razonamiento matemático, era un 15% más ancho de lo normal. Además, encontró que su cisura de Silvio, un surco que normalmente se extiende desde la parte delantera del cerebro hasta la parte posterior, no recorría todo el camino.

Trayectoria científica
En 1901 apareció el primer trabajo científico de Einstein: trataba de la atracción capilar. Publicó dos trabajos en 1902 y 1903, sobre los fundamentos estadísticos de la termodinámica, corroborando experimentalmente que la temperatura de un cuerpo se debe a la agitación de sus moléculas, una teoría aún discutida en esa época.42​

Los artículos de 1905
En 1905 finalizó su doctorado presentando una tesis titulada Una nueva determinación de las dimensiones moleculares. Ese mismo año escribió cuatro artículos fundamentales sobre la física de pequeña y gran escala. En ellos explicaba el movimiento browniano, el efecto fotoeléctrico y desarrollaba la relatividad especial y la equivalencia masa-energía. El trabajo de Einstein sobre el efecto fotoeléctrico le proporcionaría el Premio Nobel de física en 1921. Estos artículos fueron enviados a la revista Annalen der Physik y son conocidos generalmente como los artículos del annus mirabilis (del latín: «año milagroso»). La Unión Internacional de Física Pura y Aplicada, junto con la Unesco, conmemoraron 2005 como el Año Mundial de la Física43​ celebrando el centenario de publicación de estos trabajos.

Efecto fotoeléctrico
Artículo principal: Efecto fotoeléctrico

Un diagrama ilustrando la emisión de los electrones de una placa metálica, requiriendo de la energía que es absorbida de un fotón
El primero de sus artículos de 1905 se titulaba Un punto de vista heurístico sobre la producción y transformación de luz. En él, Einstein proponía la idea de «quanto» de luz (ahora llamados fotones) y mostraba cómo se podía utilizar este concepto para explicar el efecto fotoeléctrico.

La teoría de los cuantos de luz fue un fuerte indicio de la dualidad onda-corpúsculo y de que los sistemas físicos pueden mostrar tanto propiedades ondulatorias como corpusculares. Este artículo constituyó uno de los pilares básicos de la mecánica cuántica. Una explicación completa del efecto fotoeléctrico solamente pudo ser elaborada cuando la teoría cuántica estuvo más avanzada. Por este trabajo, y por sus contribuciones a la física teórica, Einstein recibió el Premio Nobel de Física de 1921.

Movimiento browniano
Artículo principal: Movimiento browniano
Su segundo artículo, titulado Sobre el movimiento requerido por la teoría cinética molecular del calor de pequeñas partículas suspendidas en un líquido estacionario, cubría sus estudios sobre el movimiento browniano.

El artículo sobre el movimiento browniano, el cuarto en grado de importancia, está estrechamente relacionado, con el artículo sobre teoría molecular. Se trata de una pieza de mecánica estadística muy elaborada, destacable por el hecho que Einstein no había oído hablar de las mediciones de Robert Brown de la década de 1820 hasta finales de ese mismo año (1905); así pues, escribió este artículo, titulándolo Sobre la teoría del movimiento browniano.44​

El artículo explicaba el fenómeno haciendo uso de las estadísticas del movimiento térmico de los átomos individuales que forman un fluido. El movimiento browniano había desconcertado a la comunidad científica desde su descubrimiento unas décadas atrás. La explicación de Einstein proporcionaba una evidencia experimental incontestable sobre la existencia real de los átomos. El artículo también aportaba un fuerte impulso a la mecánica estadística y a la teoría cinética de los fluidos, dos campos que en aquella época permanecían controvertidos.

Antes de este trabajo los átomos se consideraban un concepto útil en física y química, pero al contrario de lo que cuenta la leyenda, la mayoría de los físicos contemporáneos ya creían en la teoría atómica y en la mecánica estadística desarrollada por Boltzmann, Maxwell y Gibbs; además ya se habían hecho estimaciones bastante buenas de los radios del núcleo y del número de Avogadro. El artículo de Einstein sobre el movimiento atómico entregaba a los experimentalistas un método sencillo para contar átomos mirando a través de un microscopio ordinario.44​

Wilhelm Ostwald, uno de los líderes de la escuela antiatómica, comunicó a Arnold Sommerfeld que había sido transformado en un creyente en los átomos por la explicación de Einstein del movimiento browniano.

Relatividad especial
Artículo principal: Teoría de la relatividad especial

Una de las fotografías tomadas del eclipse de 1919 durante la expedición de Arthur Eddington, en el que se pudieron confirmar las predicciones de Einstein acerca de la curvatura de la luz en presencia de un campo gravitatorio
El tercer artículo de Einstein de ese año se titulaba Zur Elektrodynamik bewegter Körper («Sobre la electrodinámica de cuerpos en movimiento»). En este artículo Einstein introducía la teoría de la relatividad especial estudiando el movimiento de los cuerpos y el electromagnetismo en ausencia de la fuerza de interacción gravitatoria.45​

La relatividad especial resolvía los problemas abiertos por el experimento de Michelson y Morley en el que se había demostrado que las ondas electromagnéticas que forman la luz se movían en ausencia de un medio. La velocidad de la luz es, por lo tanto, constante y no relativa al movimiento. Ya en 1894, George Fitzgerald había estudiado esta cuestión demostrando que el experimento de Michelson y Morley podía ser explicado si los cuerpos se contraen en la dirección de su movimiento. De hecho, algunas de las ecuaciones fundamentales del artículo de Einstein habían sido introducidas anteriormente (1903) por Hendrik Lorentz,46​ físico neerlandés, dando forma matemática a la conjetura de Fitzgerald.47​

Esta famosa publicación está cuestionada como trabajo original de Einstein, debido a que en ella omitió citar toda referencia a las ideas o conceptos desarrollados por estos autores así como los trabajos de Poincaré. En realidad Einstein desarrollaba su teoría de una manera totalmente diferente a estos autores deduciendo hechos experimentales a partir de principios fundamentales y no dando una explicación fenomenológica a observaciones desconcertantes. El mérito de Einstein estaba por lo tanto en explicar lo sucedido en el experimento de Michelson y Morley como consecuencia final de una teoría completa y elegante basada en principios fundamentales y no como una explicación ad-hoc o fenomenológica de un fenómeno observado.45​

Su razonamiento se basó en dos axiomas simples: En el primero reformuló el principio de simultaneidad, introducido por Galileo Galilei siglos antes, por el que las leyes de la física deben ser invariantes para todos los observadores que se mueven a velocidades constantes entre ellos, y el segundo, que la velocidad de la luz es constante para cualquier observador. Este segundo axioma, revolucionario, va más allá de las consecuencias previstas por Lorentz o Poincaré que simplemente relataban un mecanismo para explicar el acortamiento de uno de los brazos del experimento de Michelson y Morley. Este postulado implica que si un destello de luz se lanza al cruzarse dos observadores en movimiento relativo, ambos verán alejarse la luz produciendo un círculo perfecto con cada uno de ellos en el centro. Si a ambos lados de los observadores se pusiera un detector, ninguno de los observadores se pondría de acuerdo en qué detector se activó primero (se pierden los conceptos de tiempo absoluto y simultaneidad).48​ La teoría recibió el nombre de «teoría especial de la relatividad» o «teoría restringida de la relatividad» para distinguirla de la teoría de la relatividad general, que fue introducida por Einstein en 1915 y en la que se consideran los efectos de la gravedad y la aceleración.49​

Equivalencia masa-energía
Artículo principal: Equivalencia entre masa y energía

La famosa fórmula E = mc2 es mostrada usando la iluminación en el rascacielos Taipei 101 durante el evento del Año Mundial de la Física en 2005.
El cuarto artículo de aquel año se titulaba Ist die Trägheit eines Körpers von seinem Energieinhalt abhängig y mostraba una deducción de la fórmula de la relatividad que relaciona masa y energía. En este artículo se exponía que la variación de masa de un objeto que emite una energía L, es:

{\displaystyle {\frac {L}{V^{2}}}}{\frac  {L}{V^{2}}}
donde V era la notación de la velocidad de la luz usada por Einstein en 1905.

Esta fórmula implica que la energía E de un cuerpo en reposo es igual a su masa m multiplicada por la velocidad de la luz al cuadrado:

{\displaystyle E=mc^{2}\,}E=mc^{2}\,
Muestra cómo una partícula con masa posee un tipo de energía, «energía en reposo», distinta de las clásicas energía cinética y energía potencial. La relación masa-energía se utiliza comúnmente para explicar cómo se produce la energía nuclear; midiendo la masa de núcleos atómicos y dividiendo por el número atómico se puede calcular la energía de enlace atrapada en los núcleos atómicos. Paralelamente, la cantidad de energía producida en la fisión de un núcleo atómico se calcula como la diferencia de masa entre el núcleo inicial y los productos de su desintegración, multiplicada por la velocidad de la luz al cuadrado.

Relatividad general
Artículo principal: Teoría general de la relatividad
En noviembre de 1915, Einstein presentó una serie de conferencias en la Academia Prusiana de las Ciencias en las que describió la teoría de la relatividad general. La última de estas charlas concluyó con la presentación de la ecuación que reemplaza a la ley de gravedad de Isaac Newton. En esta teoría todos los observadores son considerados equivalentes y no únicamente aquellos que se mueven con una velocidad uniforme. La gravedad no es ya una fuerza o acción a distancia, como era en la gravedad newtoniana, sino una consecuencia de la curvatura del espacio-tiempo. La teoría proporcionaba las bases para el estudio de la cosmología y permitía comprender las características esenciales del Universo, muchas de las cuales no serían descubiertas sino con posterioridad a la muerte de Einstein.50​

La relatividad general fue obtenida por Einstein a partir de razonamientos matemáticos, experimentos hipotéticos (gedanken experiment) y rigurosa deducción matemática sin contar realmente con una base experimental. El principio fundamental de la teoría era el denominado principio de equivalencia. A pesar de la abstracción matemática de la teoría, las ecuaciones permitían deducir fenómenos comprobables. El 29 de mayo de 1919, Arthur Eddington fue capaz de medir, durante un eclipse, la desviación de la luz de una estrella al pasar cerca del Sol, una de las predicciones de la relatividad general. Cuando se hizo pública esta confirmación la fama de Einstein se incrementó enormemente y se consideró un paso revolucionario en la física. Desde entonces la teoría se ha verificado en todos y cada uno de los experimentos y verificaciones realizados hasta el momento.51​

A pesar de su popularidad, o quizás precisamente por ella, la teoría contó con importantes detractores entre la comunidad científica que no podían aceptar una física sin un sistema de referencia absoluto.

Estadísticas de Bose-Einstein
Artículo principal: Estadística de Bose-Einstein
En 1924 Einstein recibió un artículo de un joven físico indio, Satyendranath Bose, denominado La ley de Plank y la hipótesis del cuanto de luz, describiendo a la luz como un gas de fotones y pidiendo la ayuda de Einstein para su publicación. Einstein se dio cuenta de que el mismo tipo de estadísticas podían aplicarse a grupos de átomos y publicó el artículo, conjuntamente con Bose, en alemán, la lengua más importante en física en la época. Las estadísticas de Bose-Einstein explican el comportamiento de los tipos básicos de partículas elementales denominadas bosones.52​

Debate Bohr-Einstein
Esta sección es un extracto de Debate Bohr-Einstein[editar]
	
Existen desacuerdos sobre la neutralidad en el punto de vista de la versión actual de este artículo o sección.
En la página de discusión puedes consultar el debate al respecto.

Niels Bohr con Albert Einstein en casa de Paul Ehrenfesten Leiden (diciembre de 1925). La foto es todo un estudio de caracteres: el empírico y el teórico.
El debate Bohr-Einstein era un nombre popular dado a una serie de amistosas discusiones públicas entre Albert Einstein y Niels Bohr acerca de la física cuántica. Sus discusiones son muy recordados debido a su importancia en la filosofía de la ciencia. El sentido y significación de estos debates son escasamente comprendidos, pero su gran importancia fue tenida en cuenta por el propio Bohr y escrita en su artículo Discusiones con Einstein sobre los Problemas Epistemológicos en la Física Atómica publicados en un volumen dedicado a Einstein.

La posición de Einstein con respecto a la mecánica cuántica es significativamente más sutil y de mente más abierta que lo que ha sido a veces presentado en los manuales técnicos y artículos científicos populares. Sus poderosas y constantes críticas a la mecánica cuántica obligaron a sus defensores a aguzar y refinar su comprensión acerca de las implicaciones filosóficas y científicas de sus propias teorías.
La teoría de campo unificada
Einstein dedicó sus últimos años a la búsqueda de una de las más importantes teorías de la física, la llamada teoría de campo unificada. Dicha búsqueda, después de su teoría general de la relatividad, consistió en una serie de intentos tendentes a generalizar su teoría de la gravitación para lograr unificar y resumir las leyes fundamentales de la física, específicamente la gravitación y el electromagnetismo. En el año 1950, expuso su teoría de campo unificada en un artículo titulado «Sobre la teoría generalizada de la gravitación» (On the Generalized Theory of Gravitation) en la revista Scientific American.

Aunque Albert Einstein fue mundialmente célebre por sus trabajos en física teórica, paulatinamente fue aislándose en su investigación, y sus intentos no tuvieron éxito. Persiguiendo la unificación de las fuerzas fundamentales, Albert ignoró algunos importantes desarrollos en la física, siendo notablemente visible en el tema de las fuerzas nuclear fuerte y nuclear débil, que no se entendieron bien sino después de quince años tras la muerte de Einstein (cerca del año 1970), mediante numerosos experimentos en física de altas energías. Los intentos propuestos por la teoría de cuerdas o la teoría M, evidencian que aún perdura su ímpetu para conseguir demostrar la gran teoría de la unificación de las leyes de la física.53​

Actividad política
Los acontecimientos de la Primera Guerra Mundial empujaron a Einstein a comprometerse políticamente, tomando partido. Sentía desprecio por la violencia, la bravuconería, la agresión y la injusticia.54​ Fue uno de los miembros más conocidos del Partido Democrático Alemán (DDP).

Albert Einstein fue un pacifista convencido. En 1914, 93 prominentes intelectuales alemanes firmaron el Manifiesto para el mundo civilizado para apoyar al káiser y desafiar a las «hordas de rusos aliados con mongoles y negros que pretenden atacar a la raza blanca», justificando la invasión alemana de Bélgica; pero Einstein se negó a firmarlo junto con otros tres intelectuales, que pretendían impulsar un contramanifiesto, exclamando posteriormente:55​

Es increíble lo que Europa ha desatado con esta locura. (…)
En estos momentos uno se da cuenta de lo absurda que es la especie animal a la que pertenece.
Albert Einstein.

Con Oppenheimer en 1947
Con el auge del nazismo en Alemania, Einstein deja su país y decide residir en Estados Unidos.

Un grupo de enemigos de sus teorías en la Alemania nazi creó una asociación en su contra, e incluso un hombre fue acusado de promover su asesinato. Además, se publicó un libro titulado Cien autores en contra de Einstein,56​ ante el cual Einstein se limitó a decir: «¿Por qué cien? Si estuviera equivocado, bastaría con uno solo».57​

En 1939 se produce su más importante participación en cuestiones mundiales. El Informe Smyth, aunque con sutiles recortes y omisiones, narra la historia de cómo los físicos trataron, sin éxito, de interesar a la Marina y al Ejército en el proyecto atómico. Pero la célebre carta de Einstein a Roosevelt escrita el 2 de agosto fue la que consiguió romper la rigidez de la mentalidad militar. Sin embargo, Einstein, que siente desprecio por la violencia y las guerras, es considerado el «padre de la bomba atómica».58​ En plena Segunda Guerra Mundial apoyó una iniciativa de Robert Oppenheimer para comenzar el programa de desarrollo de armas nucleares conocido como Proyecto Manhattan.

En su discurso pronunciado en Nueva York, en diciembre de 1945, expuso:

En la actualidad, los físicos que participaron en la construcción del arma más tremenda y peligrosa de todos los tiempos, se ven abrumados por un similar sentimiento de responsabilidad, por no hablar de culpa. (…)
Nosotros ayudamos a construir la nueva arma para impedir que los enemigos de la humanidad lo hicieran antes, puesto que dada la mentalidad de los nazis habrían consumado la destrucción y la esclavitud del resto del mundo. (…)
Hay que desear que el espíritu que impulsó a Alfred Nobel cuando creó su gran institución, el espíritu de solidaridad y confianza, de generosidad y fraternidad entre los hombres, prevalezca en la mente de quienes dependen las decisiones que determinarán nuestro destino. De otra manera, la civilización quedaría condenada.
Einstein: Hay que ganar la paz (1945).59​
La causa socialista
En mayo de 1949, Monthly Review publicó (en Nueva York) un artículo suyo titulado «¿Por qué el socialismo?»60​ en el que reflexiona sobre la historia, las conquistas y las consecuencias de la «anarquía económica de la sociedad capitalista», artículo que hoy sigue teniendo vigencia. Una parte muy citada del mismo habla del papel de los medios privados en relación con las posibilidades democráticas de los países:

La anarquía económica de la sociedad capitalista tal como existe hoy es, en mi opinión, la verdadera fuente del mal. (…)
El capital privado tiende a concentrarse en pocas manos, en parte debido a la competencia entre los capitalistas, y en parte porque el desarrollo tecnológico y el aumento de la división del trabajo animan la formación de unidades de producción más grandes a expensas de las más pequeñas. El resultado de este proceso es una oligarquía del capital privado cuyo enorme poder no se puede controlar con eficacia incluso en una sociedad organizada políticamente de forma democrática. Esto es así porque los miembros de los cuerpos legislativos son seleccionados por los partidos políticos, financiados en gran parte o influidos de otra manera por los capitalistas privados quienes, para todos los propósitos prácticos, separan al electorado de la legislatura. La consecuencia es que los representantes del pueblo de hecho no protegen suficientemente los intereses de los grupos no privilegiados de la población. (…)
Estoy convencido de que hay solamente un camino para eliminar estos graves males, el establecimiento de una economía socialista, acompañado por un sistema educativo orientado hacia metas sociales.
Albert Einstein, Why Socialism?61​

Einstein y Elsa arribando a Nueva York junto con los líderes sionistas de la World Zionist Organization en 1921
La causa sionista
Originario de una familia judía asimilada, abogó parcialmente por la causa sionista. Entre 1921 y 1932 pronunció diversos discursos, con el propósito de ayudar a recoger fondos para la colectividad judía y sostener la Universidad Hebrea de Jerusalén, fundada en 1918, y como prueba de su creciente adhesión a la causa sionista. Sin embargo, aunque estaba a favor de que Palestina fuese un "hogar" para los judíos, tal y como afirmaba la Declaración Balfour, estaba en contra de la creación de un Estado judío. Así, en enero de 1946, en una declaración ante el Comité Angloamericano de Investigación que interrogó a varias personalidades sobre la creación de un Estado judío, Einstein dijo:
La idea de un Estado (judío) no coincide con lo que siento, no puedo entender para qué es necesario. Está vinculada a un montón de dificultades y es propia de mentes cerradas. Creo que es mala.62​
Einstein abogó por un Estado binacional donde judíos y palestinos tuvieran los mismos derechos:63​ «Nosotros, esto es, judíos y árabes, debemos unirnos y llegar a una comprensión recíproca en cuanto a las necesidades de los dos pueblos, en lo que atañe a las directivas satisfactorias para una convivencia provechosa».64​

El Estado de Israel se creó en 1948. Cuando Jaim Weizmann, el primer presidente de Israel y viejo amigo de Einstein, murió en 1952, Abba Eban, embajador israelí en Estados Unidos, le ofreció la presidencia. Einstein rechazó el ofrecimiento diciendo: «Estoy profundamente conmovido por el ofrecimiento del Estado de Israel y a la vez apenado y avergonzado por no poder aceptarlo. Durante toda mi vida he tratado con cuestiones objetivas, por lo que carezco de la aptitud natural y de la experiencia para tratar como es debido con la gente y para desempeñar funciones oficiales. Soy el más afligido por estas circunstancias, porque mi relación con el pueblo judío se ha convertido en mi vínculo humano más fuerte, desde que tomé plena conciencia de nuestra precaria situación entre las naciones del mundo».

La causa pacifista
Einstein, pacifista convencido, impulsó el conocido Manifiesto Russell-Einstein, un llamamiento a los científicos para unirse en favor de la desaparición de las armas nucleares. Este documento sirvió de inspiración para la posterior fundación de las Conferencias Pugwash, que en 1995 se hicieron acreedoras del Premio Nobel de la Paz.

Ética y religión

Estatua de Albert Einstein en la Academia Israelí de Ciencias y Humanidades
Einstein se declaró agnóstico, y en ocasiones se declaró también ateo aunque algunos historiadores niegan este extremo.65​ Dijo una vez que creía en el Dios «panteísta» de Baruch Spinoza, pero no en un dios personal, una creencia que criticó.66​67​Einstein distingue tres estilos que suelen entremezclarse en la práctica de la religión. El primero está motivado por el miedo y la mala comprensión de la causalidad y, por tanto, tiende a inventar seres sobrenaturales. El segundo es social y moral, motivado por el deseo de apoyo y amor. Ambos tienen un concepto antropomórfico de Dios. El tercero –que Einstein considera el más maduro–, está motivado por un sentido de asombro ante la Naturaleza.68​

En una carta a la Asociación Central de Ciudadanos Alemanes de la Fe Judía, en 1920, les escribe:
Ni soy ciudadano alemán, ni hay nada en mí que pueda definirse como «fe judía». Pero soy judío y estoy orgulloso de pertenecer a la comunidad judía, aunque no los considero en absoluto los elegidos de Dios.69​

Estatua de Einstein en el Parque de las Ciencias de Granada, obra de Miguel Barranco
En cierta ocasión, en una reunión, se le preguntó a Einstein si creía o no en un dios a lo que respondió: «Creo en el dios de Spinoza, que es idéntico al orden matemático del Universo».

Una cita más larga de Einstein aparece en Science, Philosophy, and Religion, A Symposium (Simposio de ciencia, filosofía y religión), publicado por la Conferencia de Ciencia, Filosofía y Religión en su Relación con la Forma de Vida Democrática:

Cuanto más imbuido esté un hombre en la ordenada regularidad de los eventos, más firme será su convicción de que no hay lugar —del lado de esta ordenada regularidad— para una causa de naturaleza distinta. Para ese hombre, ni las reglas humanas ni las «reglas divinas» existirán como causas independientes de los eventos naturales. De seguro, la ciencia nunca podrá refutar la doctrina de un dios que interfiere en eventos naturales, porque esa doctrina puede siempre refugiarse en que el conocimiento científico no puede posar el pie en ese tema. Pero estoy convencido de que tal comportamiento de parte de las personas religiosas no solamente es inadecuado sino también fatal. Una doctrina que se mantiene no en la luz clara sino en la oscuridad, que ya ha causado un daño incalculable al progreso humano, necesariamente perderá su efecto en la humanidad. En su lucha por el bien ético, las personas religiosas deberían renunciar a la doctrina de la existencia de Dios, esto es, renunciar a la fuente del miedo y la esperanza, que en el pasado puso un gran poder en manos de los sacerdotes. En su labor, deben apoyarse en aquellas fuerzas que son capaces de cultivar el bien, la verdad y la belleza en la misma humanidad. Esto es de seguro, una tarea más difícil pero incomparablemente más meritoria y admirable.
En una carta fechada en marzo de 1954, que fue incluida en el libro Albert Einstein: su lado humano (en inglés), editado por su fiel secretaria Helen Dukas y su colaborador Banesh Hoffman y publicada por Princeton University Press, Einstein dice:

Por supuesto era una mentira lo que se ha leído acerca de mis convicciones religiosas; una mentira que es repetida sistemáticamente. No creo en un dios personal y no lo he negado nunca sino que lo he expresado claramente. Si hay algo en mí que pueda ser llamado religioso es la ilimitada admiración por la estructura del mundo, hasta donde nuestra ciencia puede revelarla.
La carta al filósofo Eric Gutkind, del 3 de enero de ese mismo año, subastada en mayo de 2008,70​ deja al parecer las cosas más claras. Dice Einstein:
La palabra dios para mí no es más que la expresión y producto de las debilidades humanas, la Biblia, una colección de honorables pero aún primitivas leyendas que sin embargo son bastante infantiles. Ninguna interpretación, sin importar cuán sutil sea, puede (para mí) cambiar esto…
También hay una carta poco conocida de Einstein, enviada a Guy H. Raner Jr, el 2 de julio de 1945, en respuesta a un rumor de que un sacerdote jesuita lo había convertido al cristianismo, en la cual Einstein se declara directamente ateo (citado por Michael R. Gilmore en Skeptic Magazine, v. 5, No.2)71​

He recibido su carta del 10 de junio. Nunca he hablado con un sacerdote jesuita en mi vida y estoy asombrado por la audacia de tales mentiras sobre mí. Desde el punto de vista de un sacerdote jesuita, soy, por supuesto, y he sido siempre un ateo.
William Hermanns, veterano superviviente de Verdún, profesor de literatura alemana, entrevistó varias veces a Einstein, la primera en Berlín en 1930. En esa ocasión planteó la idea de una religión cósmica, una idea a la que había hecho referencia en la conversación sobre la realidad que había tenido con Rabindranath Tagore y que después desarrolló y tituló «Religión y Ciencia», publicado en el New York Times en 1930. Einstein siguió desarrollando esta idea y Herrmanns, que la consideraba compatible con las creencias tradicionales se propuso fundar un movimiento que integrara las tradiciones judías, cristiana, vedista, budista e islámica. Estaba dispuesto a obtener declaraciones concisas y precisas sobre Dios. Einstein no pudo serlo más:
Con respecto a Dios, no puedo aceptar ningún concepto basado en la autoridad de la Iglesia. Desde que tengo uso de razón me ha molestado el adoctrinamiento de las masas. No creo en el miedo a la vida, en el miedo a la muerte, en la fe ciega. No puedo demostrar que no haya un dios personal, pero si hablara de él, mentiría. No creo en el dios de la teología, en el dios que premia el bien y castiga el mal. Mi dios creó las leyes que se encargan de eso. Su universo no está gobernado por quimeras, sino por leyes inmutables.72​
Para Einstein, su religión cósmica y su condición judía no guardaban relación entre sí. Cuando se le preguntó si existía un punto de vista judío replicó:
En el sentido filosófico no hay, en mi opinión, un punto de vista específicamente judío. Para mí, el judaísmo tiene que ver casi exclusivamente con la actitud moral en la vida y hacia la vida […] El judaísmo no es, pues, una religión trascendental; tiene que ver como vivimos la vida y, hasta cierto punto, con cómo la entendemos […], y nada más. Tengo dudas si se le puede llamar religión en el sentido aceptado de la palabra, o bien considerarla no como una "fe", sino como la santificación de la vida en el sentido suprapersonal que se les exige a los judíos.73​
Einstein decía que la moral no era dictada por Dios, sino por la humanidad:74​

No creo en la inmoralidad del individuo, y considero la ética una preocupación exclusivamente humana sobre la que no hay ninguna autoridad sobrehumana.
Algunas publicaciones
Artículo principal: Anexo:Publicaciones científicas de Albert Einstein
Einstein, Albert (1901) [manuscrito recibido 16 de diciembrw 1900], «Folgerungen aus den Capillaritätserscheinungen» [Conclusions Drawn from the Phenomena of Capillarity] (PDF), escrito en Zúrich, Suiza, Annalen der Physik (Berlín) (en alemán) (Hoboken, NJ, publicado el 14 de marzo de 2006) 309 (3): 513-523, Bibcode:1901AnP...309..513E, doi:10.1002/andp.19013090306 – via Wiley Online Library
Einstein, Albert (1905a) [manuscrito recibido 18 de marzo 1905], «Über einen die Erzeugung und Verwandlung des Lichtes betreffenden heuristischen Gesichtspunkt» [On a Heuristic Viewpoint Concerning the Production and Transformation of Light] (PDF), escrito en Berna, Suiza, Annalen der Physik (Berlín) (en alemán) (Hoboken, NJ, publicado el 10 de marzo de 2006) 322 (6): 132-148, Bibcode:1905AnP...322..132E, doi:10.1002/andp.19053220607 – via Wiley Online Library
Einstein, Albert (1905b) [manuscrito completado 30 abril y recibido 20 de julio 1905]. «Eine neue Bestimmung der Moleküldimensionen» [A new determination of molecular dimensions] (PDF). Escrito en Berne, Switzerland, published by Wyss Buchdruckerei. Dissertationen Universität Zürich (PhD Thesis) (en alemán) (Zurich, Switzerland: ETH Zúrich, publicado el 2008). doi:10.3929/ethz-a-000565688 – via ETH Bibliothek.
Einstein, Albert (1905c) [manuscrito recibido 11 de mayo 1905], «Über die von der molekularkinetischen Theorie der Wärme geforderte Bewegung von in ruhenden Flüssigkeiten suspendierten Teilchen» [On the Motion – Required by the Molecular Kinetic Theory of Heat – of Small Particles Suspended in a Stationary Liquid] (PDF), escrito en Berna, Suiza, Annalen der Physik (Berlín) (en alemán) (Hoboken, NJ, publicado el 10 de marzo de 2006) 322 (8): 549-560, Bibcode:1905AnP...322..549E, doi:10.1002/andp.19053220806 – via Wiley Online Library
Einstein, Albert (1905d) [manuscrito recibido 30 de junio 1905], «Zur Elektrodynamik bewegter Körper» [On the Electrodynamics of Moving Bodies] (PDF), escrito en Berna, Suiza, Annalen der Physik (Berlín) (en alemán) (Hoboken, NJ, publicado el 10 de marzo de 2006) 322 (10): 891-921, Bibcode:1905AnP...322..891E, doi:10.1002/andp.19053221004 – via Wiley Online Library
Einstein, Albert (1905e) [manuscrito recibido 27 de septiembre 1905], «Ist die Trägheit eines Körpers von seinem Energieinhalt abhängig?» [Does the Inertia of a Body Depend Upon Its Energy Content?] (PDF), escrito en Berna, Suiza, Annalen der Physik (Berlín) (en alemán) (Hoboken, NJ, publicado el 10 de marzo de 2006) 323 (13): 639-641, Bibcode:1905AnP...323..639E, doi:10.1002/andp.19053231314 – via Wiley Online Library
Einstein, Albert (1915) [manuscrito publicado 25 de noviembre 1915], «Die Feldgleichungen der Gravitation» [The Field Equations of Gravitation] (Online page images), Königlich Preussische Akademie der Wissenschaften (en alemán) (Berlín): 844-847 – via ECHO, Cultural Heritage Online, Max Planck Institute for the History of Science
Einstein, Albert (1917a), «Kosmologische Betrachtungen zur allgemeinen Relativitätstheorie» [Cosmological Considerations in the General Theory of Relativity], Königlich Preussische Akademie der Wissenschaften, Berlín (en alemán)
Einstein, Albert (1917b), «Zur Quantentheorie der Strahlung» [On the Quantum Mechanics of Radiation], Physikalische Zeitschrift (en alemán) 18: 121-128, Bibcode:1917PhyZ...18..121E
Einstein, Albert (1923) [manuscrito publicado 1923, en inglés 1967]. Grundgedanken und Probleme der Relativitätstheorie [Fundamental Ideas and Problems of the Theory of Relativity] (PDF) (en alemán (1923) inglés (1967)). Nobel Lectures, Physics 1901-1921. Estocolmo: Nobelprice.org (publicado el 3 de febrero de 2015) – via Nobel Media AB 2014.
Einstein, Albert (1924) [manuscrito publicado 10 de julio 1924], «Quantentheorie des einatomigen idealen Gases» [Quantum theory of monatomic ideal gases] (Online page images), Sitzungsberichte der Preussischen Akademie der Wissenschaften, Physikalisch-Mathematische Klasse (en alemán) (Múnic: Königlich Preussische Akademie der Wissenschaften, Berlín): 261-267 – via ECHO, Cultural Heritage Online, Max Planck Institute for the History of Science. First of a series of papers on this topic.
Einstein, Albert (12 de marzo de 1926) [manuscrito publicado 1 de marzo 1926], «Die Ursache der Mäanderbildung der Flußläufe und des sogenannten Baerschen Gesetzes» [On Baer's law and meanders in the courses of rivers], escrito en Berlín, Die Naturwissenschaften (en alemán) (Heidelberg, Germany: Springer-Verlag) 14 (11): 223-224, Bibcode:1926NW.....14..223E, ISSN 1432-1904, doi:10.1007/BF01510300 – via SpringerLink
Einstein, Albert (1926b), escrito en Berne, Switzerland, R. Fürth, ed., Investigations on the Theory of the Brownian Movement (PDF), Translated by A. D. Cowper, USA: Dover Publications (publicado el 1956), ISBN 978-1-60796-285-4, consultado el 4 de enero de 2015
Einstein, Albert; Podolsky, Boris; Rosen, Nathan (15 de mayo de 1935) [manuscrito recibido 25 de marzo 1935], «Can Quantum-Mechanical Description of Physical Reality Be Considered Complete?» (PDF), Physical Review (American Physical Society) 47 (10): 777-780, Bibcode:1935PhRv...47..777E, doi:10.1103/PhysRev.47.777 – via APS Journals
Einstein, Albert (9 de noviembre de 1940), «On Science and Religion», Nature (Edimburgo: Macmillan Publishers Group) 146 (3706): 605-607, Bibcode:1940Natur.146..605E, ISBN 0-7073-0453-9, doi:10.1038/146605a0
Einstein, Albert (4 de diciembre de 1948), «To the editors of the New York Times», New York Times (Melville, New York: AIP, American Inst. of Physics), ISBN 0-7354-0359-7, archivado desde el original el 17 de diciembre de 2007, consultado el 31 de marzo de 2016
Einstein, Albert (mayo de 1949), «Why Socialism? (Reprise)», Monthly Review (New York: Monthly Review Foundation, publicado el mayo de 2009) 61 (01 (mayo)), archivado desde el original el 11 de enero de 2006, consultado el 16 de enero de 2006 – via MonthlyReview.org
Einstein, Albert (1950), «On the Generalized Theory of Gravitation», Scientific American CLXXXII (4): 13-17, doi:10.1038/scientificamerican0450-13
Einstein, Albert (1954), Ideas and Opinions, New York: Random House, ISBN 0-517-00393-7
Einstein, Albert (1969), Albert Einstein, Hedwig und Max Born: Briefwechsel 1916-1955 (en alemán), Múnich: Nymphenburger Verlagshandlung, ISBN 3-88682-005-X
Einstein, Albert (1979), Autobiographical Notes, Paul Arthur Schilpp (Centennial edición), Chicago: Open Court, ISBN 0-87548-352-6. The chasing a light beam thought experiment is described on pages 48–51.
Collected Papers: Stachel, John; Martin J. Klein; A. J. Kox; Michel Janssen; R. Schulmann; Diana Komos Buchwald, eds. (21 de julio de 2008) [manuscrito publicado entre 1987-2006], «The Collected Papers of Albert Einstein», Einstein's Writings (Princeton University Press), 1-10, archivado desde el original el 17 de febrero de 2013, consultado el 31 de marzo de 2016. Más información sobre los volúmenes publicados se halla en la web Einstein Papers Project y en Princeton University Press Einstein Page
Eponimia

Sello rumano de 2005 con la imagen de Albert Einstein

Torre Einstein (Potsdam)
Además de numerosas calles, plazas y ciudades de varios países del mundo, así como distintas instituciones académicas, una amplia relación de elementos relacionados con la ciencia llevan el nombre de Einstein en su memoria:

Química

El elemento Einstenio fue nombrado en su honor.
Matemáticas
Convenio de suma de Einstein, notación abreviada usada en álgebra tensorial.
Física
Anillo de Einstein, efecto gravitatorio sobre la luz estelar.
Ecuaciones del campo de Einstein, 10 ecuaciones de la teoría de la relatividad general.
Estadística de Bose-Einstein, tipo de mecánica estadística aplicable a bosones en equilibrio térmico.
Condensado de Bose-Einstein, estado de agregación de la materia que se da en ciertos materiales a temperaturas cercanas al cero absoluto.
Einstein (unidad de medida), unidad de medida de cantidad de radiación.
Relación de Einstein (teoría cinética), relacionada con el movimiento browniano.
Modelo de Einstein, utilizado en la física de sólidos de la mecánica cuántica.
Espacio
Observatorio Einstein, un detector espacial de rayos X astronómicos.
Albert Einstein (nave), nave espacial de carga europea lanzada en 2013.
Astronomía
Torre Einstein, observatorio erigido en 1924 situado en Potsdam.
Einstein, cráter de impacto lunar.
Einstein, asteroide número 2001 del catálogo del Minor Planet Center.
Cruz de Einstein, quasar afectado por el fenómenos gravitatorios que afectan a su luz.
Museo
Einsteinhaus, casa-museo localizada en Berna, dedicada al físico alemán.
Premio
Medalla Albert Einstein, reconociendo anualmente desde 1979 trabajos relacionados con la obra de Einstein.
Medalla Albert Einstein de la UNESCO.
En la cultura popular
{{{Alt
Famosa fotografía de Einstein sacando la lengua
Albert Einstein ha sido objeto e inspiración para muchas obras de la cultura popular.

En el cumpleaños 72 de Einstein, el 14 de marzo de 1951, el fotógrafo de United Press, Arthur Sasse, intentaba persuadirlo para que no sonriera ante la cámara, pero ese día, al haber sonreído a los fotógrafos muchas veces, Einstein le sacó la lengua. Esta fotografía se convirtió en una de las más populares jamás tomadas. Einstein disfrutó de esta foto y le pidió a UPI que le diera nueve copias para uso personal, una de las cuales firmó para un reportero. El 19 de junio de 2009, la fotografía firmada original se vendió en una subasta por $ 74,324, un récord para una foto de Einstein.75​76​

Einstein es un modelo favorito para las representaciones de genios o científicos locos; Su rostro expresivo y sus peinados distintivos han sido ampliamente copiados y exagerados. Frederic Golden, de la revista Time , escribió que Einstein era "el sueño de un dibujante hecho realidad".77​

El nombre de "Einstein" se ha convertido en sinónimo de una persona extremadamente inteligente. También se puede usar sarcásticamente cuando alguien dice lo obvio o demuestra falta de sabiduría o inteligencia.

Einstein también ha sido objeto de muchas citas que se han hecho especialmente populares en Internet y se le han atribuido falsamente, incluida "la definición de locura".78​

Véase también
Equivalencia entre masa y energía (E=mc²)
Efecto fotoeléctrico
Física teórica
Historia de la electricidad
Mecánica cuántica
Movimiento browniano
Onda gravitatoria
Relatividad general
Teoría de la relatividad
Teoría de la relatividad especial
Premio Nobel de Física
"Albert Einstein: creador y rebelde"
Referencias
 Alfonseca, Manuel (1996). Diccionario Espasa 1.000 grandes científicos. Madrid: Espasa Calpe. p. 740. ISBN 84-239-9236-5.. Pág. 171
 Lazare Iglesis (1984). Miniserie de televisión Hello, Einstein! (cuatro capítulos). Francia. Video: [1]
 Einstein, Albert (25 de noviembre de 1915). «Die Feldgleichungun der Gravitation». Sitzungsberichte der Preussischen Akademie der Wissenschaften zu Berlin (en alemán): 844-847. Consultado el 12 de septiembre de 2006.
 El London Times publicó el 7 de noviembre de 1919 los siguientes titulares: Revolución en la ciencia. Nueva teoría del universo. Las ideas de Newton derrocadas.
 Alfonseca, 1998.
 Kaku, 2005, p. 98.
 Anders Bárány (2001). «El Premio Nobel y el fantasma de Einstein». Project Syndicate.
 Einstein, Albert (2000). Mis ideas y opiniones. Barcelona: Antoni Bosch editor. pp. 167/169. ISBN 8493051632. «Nuestra deuda con el sionismo (discurso pronunciado en Nueva York, 1938) […] En nuestra situación, una cosa debe destacarse en especial: el pueblo judío ha contraído una deuda de gratitud con el sionismo. El movimiento sionista ha revivido entre los judíos el sentimiento comunitario, y ha llevado a cabo un esfuerzo que supera todas las expectativas».
 Rodrigo, Agustín Andreu (2004). El libro de las estatuas. Valencia: Editorial Universitaria Politécnica Valencia. p. 287. ISBN 8497055586. «Como consecuencia de su identificación con el pueblo judío durante su estancia en Berlín, Einstein se hizo ferviente sionista a partir de 1919, tras algunas dudas iniciales.»
 Hawking, Stephen W.; Mlodinow, Leonard (2005). Brevísima historia del tiempo. Barcelona: Critica. pp. 183/184. ISBN 8484326373. «La segunda gran causa de Einstein fue el sionismo […]. Su apoyo explícito a la causa sionista, sin embargo, fue reconocido en 1952, cuando se le ofreció la presidencia de Israel».
 Laca Arocena, Francisco A. (enero-abril). «Albert Einstein: Un sionismo pacifista». Acta Universitaria (Universidad de Guanajuato) 19 (1): 12-20. ISSN 0188-6266.
 Frank Pellegrini. «Albert Einstein» (en inglés). Time. Archivado desde el original el 25 de noviembre de 2005.
 Smith, Peter D. (2003). Jesús Domingo, ed. Einstein. v.o. inglés (2ª edición). Life&Times. p. 191. ISBN 84-7902-557-3.
 Ibid., p XVIII.
 Calderón Hoffmann, Leonora (1994). Mi abuela Lola Hoffmann. Santiago: Cuatro Vientos. p. 29. ISBN 9562420140. «Recuerdo haber asistido, con mi hermano Konstantin (que era profesor de física en Berlín), a un concierto de violín ejecutado por Albert Einstein en beneficio de estudiantes judíos pobres. Estábamos ubicados en la segunda fila y desde allí podíamos apreciar claramente sus emociones. Al subir al escenario se notaba muy nervioso, pero al comenzar su interpretación de una pieza de Mendelssohn, los ojos del genio de la física se cerraron y su rostro se relajó completamente, me dio la impresión de alguien soñando maravillas. Fue muy impactante.»
 Einstein como estudiante
 Robinson, 2010, p. 36.
 Certificado de graduación de Einstein en 1896 emitido por la Escuela Cantonal de Argovia, Robinson (2000, p. 26)
 Robinson, 2010, pp. 40 y 141.
 Renn, Jurgen; Schulmann, Robert (1992). Albert Einstein/Mileva Maric: “The love letters”. Princeton: Princeton University Press.
 Robinson, 2010, p. 40.
 "Las cartas de amor de Einstein", de Robert Schulmann, R.SChulmann, 2005.
 Robinson, 2010, «1905, el año milagroso», pp. 52 a 65.
 von Hirschhausen, Ulrike (2007). «Von imperialer Inklusion zur nationalen Exklusion:Staatsbürgerschaft in Österreich- Ungarn 1867-1923» (WZB Discussion Paper). ZKD – Veröffentlichungsreihe der Forschungsgruppe, „Zivilgesellschaft, Citizenship und politische Mobilisierung in Europa“ Schwerpunkt Zivilgesellschaft, Konflikte und Demokratie, Wissenschaftszentrum Berlin für Sozialforschung (No. SP IV 2007-403). Berlin, Germany: WZB Social Science Research Center Berlin. p. 8. ISSN 1860-4315. Consultado el 4 de agosto de 2015. «Eine weitere Diskontinuität bestand viertens darin, dass die Bestimmungen der österreichischen Staatsbürgerschaft, die in den ersten Dritteln des Jahrhunderts auch auf Ungarn angewandt worden waren, seit 1867 nur noch für die cisleithanische Reichshälfte galten. Ungarn entwickelte hingegen jetzt eine eige-ne Staatsbürgerschaft.»
 Hawking, 2002, pp. 13 y 33.
 Einstein, 1996, p. 70.
 Robinson, 2010, pp. 145-147.
 Einstein, 1996, pp. 9-11.
 Philipp Lenard: Ideelle Kontinentalsperre, München 1940.
 Robinson, 2010, cap. 10, esp. o. 163.
 Einstein, Albert. Artículo subido a su web por la Universidad Complutense de Madrid. (Monthly Review, Nueva York, mayo de 1949.). «¿Por qué socialismo?».
 Mario Mucnick. Libro editado por Aleph Editores. (2011). «Oficio editor.».
 Montes-Santiago, J. (16 de julio de 2017). «[The meeting of Einstein with Cajal (Madrid, 1923): a lost tide of fortune]». Revista De Neurologia 43 (2): 113-117. ISSN 0210-0010. PMID 16838259.
 Robinson, 2010, pp. 168-169.
 Einstein, 1996, p. 46.
 Álvarez Chillida, Gonzalo (2002). El antisemitismo en España: La imagen del judío (1812-2002). Marcial Pons. p. 335.
 Sánchez Ron, José Manuel; Glick, Thomas F. (1983). La España posible de la Segunda República. La oferta a Einstein de una cátedra extraordinaria en la Universidad Central (Madrid, 1933). Editorial de la Universidad Complutense.
 Robinson, 2010, pp. 198-199.
 Robinson, 2010, pp. 102-120.
 The Day Albert Einstein Died: A Photographer’s Story. Revista Life. Consultado el 3 de diciembre de 2020.
 NPR: The Long, Strange Journey of Einstein's Brain
 Whitrow, Einstein: El hombre y su obra, p. 27.
 Lanzamiento del Año Mundial de la Física
 Robinson, 2010, p. 125.
 Robinson, 2010, p. 124.
 Arnau, Juan (11 de diciembre de 2020). Einstein: la luz que mide las cosas. Babelia, El País. Consultado el 12 de diciembre de 2020. (requiere suscripción).
 Hawking, 2002, pp. 6-7.
 Robinson, 2010, pp. 42-51.
 Robinson, 2010, pp. 66-77.
 Robinson, 2010, p. 74-77.
 Robinson, 2010, p. 76.
 Robinson, 2010, pp. 90-91.
 Robinson, 2010, pp. 108 y 128.
 Einstein, 1996, p. 12.
 Kaku, 2005, p. 85.
 Agencia EFE. Elespectador.com (10 de diciembre de 2009). «Un libro analiza las motivaciones de los enemigos de Einstein». Archivado desde el original el 24 de septiembre de 2015.
 Stephen Hawking (2001). «El Universo en una Cáscara de Nuez. Capítulo 1.».
 Einstein, 1996, p. 13.
 Einstein, 1996, p. 63.
 «Einstein: «Por qué el socialismo»». Marxists Internet Archive.
 Albert Einstein: Why socialism?, en Monthly Review, mayo de 1949.
 Edward Corrigan, "Albert Einstein on Palestine"
 "Einstein and Complex Analyses of Zionism" Jewish Daily Forward, 24 de julio de 2009
 Einstein, 1996, p. 21.
 Isaacson, Walter (2008). Einstein: His Life and Universe. Nueva York: Simon y Schuster, pp. 390.
 Einstein, Albert "Gelegentliches", Soncino Gesellschaft, Berlin, 1929, p. 9 "This firm belief, a belief bound up with a deep feeling, in a superior mind that reveals itself in the world of experience, represents my conception of God. In common parlance this may be described as "pantheistic" (Spinoza)."
 Hoffmann, Banesh (1972). Albert Einstein Creator and Rebel. New York: New American Library, p. 95. "It seems to me that the idea of a personal God is an anthropological concept which I cannot take seriously. I feel also not able to imagine some will or goal outside the human sphere. My views are near those of Spinoza: admiration for the beauty of and belief in the logical simplicity of the order which we can grasp humbly and only imperfectly."
 Albert Einstein (5 de abril de 2009). «Religion and Science». New York Times (en inglés).
 Robinson, 2010, p. 176.
 «Einstein, lejos de Dios». El País. 13 de mayo de 2008. Consultado el 21 de noviembre de 2012.
 Austin, Cline. «Einstein Quotes on Atheism & Freethought: Was Einstein an Atheist, Freethinker?». About.com Religion & Spirituality (en inglés). Consultado el 2 de febrero de 2017.
 Robinson, 2010, pp. 176-177.
 Robinson, 2010, p. 178.
 Kaku, 2005, p. 101.
 «Various things about Albert Einstein». www.einstein-website.de. Consultado el 27 de abril de 2019.
 «Photo Of Einstein Nets $74K At Auction - Boston News Story - WCVB Boston». web.archive.org. 22 de junio de 2009. Consultado el 27 de abril de 2019.
 Golden, Frederic (31 de diciembre de 1999). «Albert Einstein». Time (en inglés estadounidense). ISSN 0040-781X. Consultado el 27 de abril de 2019.
 «9 Albert Einstein Quotes That Are Completely Fake». Gizmodo Australia (en inglés). 16 de mayo de 2015. Consultado el 27 de abril de 2019.
Bibliografía
Bibliografía general
Alfonseca, Manuel (1998). Diccionario Espasa. 1.000 grandes científicos. Madrid: Espasa Calpe. ISBN 84-239-9236-5.
Albert Einstein. (2004). "Colección Grandes Biografías, 59". Editorial Planeta-De Agostini. Barcelona, España. ISBN 84-395-4730-7.
Amis, Martin. (2005). Los monstruos de Einstein. Ediciones Minotauro. Barcelona, España. ISBN 84-450-7089-4.
Clark, Ronald W., Einstein: The Life and Times, 1971, ISBN 0-380-44123-3.
Conferencia de Ciencia, Filosofía y Religión en su Relación con la Forma de Vida Democrática, Science, Philosophy, and Religion, A Symposium (Simposio de ciencia, filosofía y religión), Nueva York, 1941.
Dukas, Helen, y Banesh Hoffman, Albert Einstein: The Human Side (Albert Einstein, el lado humano), Princeton University Press.
Einstein, Albert (marzo de 1996). Este es mi pueblo. Título original Dies ist mein volk. Buenos Aires: Leviatan. ISBN 978-9505163076.
Hart, Michael H., The 100, Carol Publishing Group, 1992, ISBN 0-8065-1350-0.
Isaacson, Walter (2008). Einstein. Su vida y su universo. Debate. ISBN 978-84-8306-788-8.
Kaku, Michio (2005). El Universo de Einstein: Cómo la visión de Albert Einstein transformó nuestra comprensión del espacio y el tiempo. Antoni Bosch editor. ISBN 84-95348-17-9.
Otero Carvajal, Luis Enrique: "Einstein y la revolución científica del siglo XX", Cuadernos de Historia Contemporanéa n.º 27 (2005), ISSN 0214-400X.
Pais, Abraham, Subtle is the Lord. The Science and the Life of Albert Einstein, 1982, ISBN 0-19-520438-7.
Parker, Barry, Einstein's Brainchild, 280 págs., 2000, Prometheus Books, ISBN 1-57392-857-7 (en inglés)
Einstein y la teoría de la relatividad
Einstein, Albert (1941). «Demostración de la no existencia de campos gravitacionales sin singularidades de masa total no nula». Revista de Matemáticas (Argentina: Universidad Nacional de Tucumán): 280. OL OL18968949M.
Einstein, Albert, El significado de la relatividad, Espasa Calpe, 1971.
Greene, Brian, El universo elegante, Planeta, 2001.
Hawking, Stephen, Breve historia del tiempo, Planeta, 1992, ISBN 968-406-356-3.
— (2002). El Universo en una cáscara de nuez (6ª edición). ISBN 84 8432 293 9.
Robinson, Andrew (2010). Einstein; Cien años de relatividad. Blume. ISBN 978 84 8076 882 5.
Russell, Bertrand, El ABC de la relatividad, 1925.
Schwinger, Julian (1986): Einstein's Legacy: The Unity of Space and Time. Scientific American Library. 250 págs. Nueva York ISBN 0-7167-5011-2 (El Legado de Einstein. La unidad del espacio y el tiempo. Prensa Científica, S.A., Biblioteca Scientific American. 250 págs. Barcelona, 1995, ISBN 84-7593-054-9)
Material digital
Byron Preiss Multimedia. (2001). Einstein y su teoría de la relatividad. "Colección Ciencia Activa". Anaya Multimedia-Anaya Interactiva. Madrid, España. ISBN 84-415-0247-1. (dos CD y un manual).
Enlaces externos
En el Marxists Internet Archive está disponible una sección con obras de Albert Einstein.
Albert Einstein, El mundo como yo lo veo (ensayo), 'Internet Archive WayBack Machine', 28 de septiembre de 2007.
Varios libros de Albert Einstein en castellano, sitio digital 'Libroteca'.
Escritos de Albert Einstein (1879-1955), sitio digital 'Marxists Internet Archive (sección en español)'.
Biografía de Albert Einstein, 'Internet Archive WayBack Machine', 2 de diciembre de 2008.
¿Fue Albert Einstein un extraterrestre?, 'Internet Archive WayBack Machine', 23 de septiembre de 2005.
Alberto Einstein, Sobre la Teoría de la Relatividad, ediciones 'Altaya', 2016, ISBN 8822832299 y 9788822832290 (texto parcial en línea).
La muerte de Einstein (ruptura de aneurisma de la aorta).
José Manuel Sánchez Ron, "Einstein, Israel y Palestina", diario 'El País', 2 de mayo de 2002.
Albert Einstein, Mis ideas y opiniones, editor 'Antoni Bosch', 2000, ISBN 8493051632 y 9788493051631, 342 páginas (texto parcial en línea).
Enlaces en otros idiomas
Archivos Oficiales de Einstein Online (en inglés)
Archivos Albert Einstein (en inglés)
Revista TIME 100: Albert Einstein (en inglés)
Albert Einstein (en inglés)
Tesis doctoral (en alemán)
Werke von und über Albert Einstein im Katalog Helveticat der Schweizerischen Nationalbibliothek
Nachlass von Albert Einstein in der Archivdatenbank HelveticArchives der Schweizerischen Nationalbibliothek
Einstein-Website von Hans-Josef Küpper (Ausführliche Biografie)
Zeittafel
Mensch Einstein, Website zu Leben und Werk Einsteins des RBB
Einsteingalerie (Fotosammlung)
Albert-Einstein Oberschule Berlin-Neukölln Deutschlands einzige Institution, die er offiziell nach sich hat benennen lassen
Das verschmähte Genie ETH Campus Life
Friedrich Dürrenmatt zum 100. Geburtstag von Albert Einstein: Ein Vortrag (1979) gehalten an der ETH Zürich, wo Einstein studierte und unterrichtete
Die Akte Einstein in: freitag del 22 de abril de 2005
Control de autoridades	
Proyectos WikimediaWd Datos: Q937Commonscat Multimedia: Albert EinsteinWikiquote Citas célebres: Albert EinsteinWikisource Textos: Autor:Albert Einstein
IdentificadoresWorldCatVIAF: 75121530ISNI: 0000 0001 2281 955XBNE: XX834035BNF: 119016075 (data)BNC: 000034649CANTIC: a10077078GND: 118529579LCCN: n79022889NCL: 369710NDL: 00438728NKC: jn19990002019NLA: 36582360CiNii: DA00708434NARA: 10582637SNAC: w63c6p77SUDOC: 026849186ULAN: 500240971Scopus: 22988279600BIBSYS: 90053072UB: a1279550MGP: 53269ICCU: IT\ICCU\CFIV\035853KNAW: PE00000116Leopoldina: 3232Open Library: OL3175986AGoogle Académico: qc6CJjYAAAAJDiccionarios y enciclopediasGEA: 4887HDS: 028814Britannica: urlRepositorios digitalesDialnet: 278727Proyecto Gutenberg: 1630
Categorías: HombresNacidos en 1879Fallecidos en 1955Albert EinsteinLaureados con el Premio Nobel 1921Nacidos en UlmAgnósticos de AlemaniaAgnósticos de SuizaAgnósticos de Estados UnidosJudíos de SuizaJudíos de AlemaniaDoctores honoris causa por la Universidad de Ginebra
Menú de navegación
No has accedido
Discusión
Contribuciones
Crear una cuenta
Acceder
ArtículoDiscusión
LeerVer códigoVer historialBuscar
Buscar en Wikipedia
Portada
Portal de la comunidad
Actualidad
Cambios recientes
Páginas nuevas
Página aleatoria
Ayuda
Donaciones
Notificar un error
Herramientas
Lo que enlaza aquí
Cambios en enlazadas
Subir archivo
Páginas especiales
Enlace permanente
Información de la página
Citar esta página
Elemento de Wikidata
Imprimir/exportar
Crear un libro
Descargar como PDF
Versión para imprimir
En otros proyectos
Wikimedia Commons
Wikiquote
Wikisource

En otros idiomas
Aragonés
العربية
Беларуская
Deutsch
English
Euskara
فارسی
עברית
Hrvatski
201 más
Editar enlaces
Esta página se editó por última vez el 14 mar 2021 a las 19:58.
El texto está disponible bajo la Licencia Creative Commons Atribución Compartir Igual 3.0; pueden aplicarse cláusulas adicionales. Al usar este sitio, usted acepta nuestros términos de uso y nuestra política de privacidad.
Wikipedia® es una marca registrada de la Fundación Wikimedia, Inc., una organización sin ánimo de lucro.
