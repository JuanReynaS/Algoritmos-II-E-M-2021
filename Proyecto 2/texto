CÃ³mo leer y escribir archivos en Python
abril 26, 2019 Por Daniel RodrÃ­guez Dejar un comentario
Tiempo de lectura: 4 minutos


Comparte
Tuitea
Pinear
Comparte
La importaciÃ³n y exportaciÃ³n de datos desde archivos son tareas que se realizan con bastante asiduidad. Por ello, en Python, librerÃ­as como pandas incorporan herramientas para el manejo de archivos CSV o en formato Microsoft Excel. Pero, si los datos no se encuentran en un formato estÃ¡ndar, estas herramientas pueden no ser las adecuadas. Por lo tanto, puede ser necesario manejar los archivos directamente. En esta entrada se va a mostrar cÃ³mo leer y escribir archivos en Python.

Abrir un archivo para leer o escribir en Python
Antes de leer o escribir archivos con Python es necesario es necesario abrir una conexiÃ³n. Lo que se puede hacer con el comando open(), al que se le ha de indicar el nombre del archivo. Por defecto la conexiÃ³n se abre en modo lectura, con lo que no es posible escribir en el archivo. Para poder escribir es necesario utilizar la opciÃ³n "w" con la que se eliminarÃ¡ cualquier archivo existente y crearÃ¡ uno nuevo. Otra opciÃ³n que se puede utilizar es "a", con la que se aÃ±adirÃ¡ nuevo contenido al archivo existente. Las opciones se pueden ver en el siguiente cÃ³digo.

# Abre el archivo para escribir y elimina los archivos anteriores si existen
fic = open("text.txt", "w")
# Abre el archivo para agregar contenido
fic = open("text.txt", "a")
# Abre el archivo en modo lectura
fic = open("text.txt", "r")
En todos los casos, una vez finalizado las operaciones de lectura y escritura con los archivos, una buena prÃ¡ctica es cerrar el acceso. Para lo que se debe utilizar el mÃ©todo close().

Escribir archivos de texto en Python
Antes guardar un archivo es necesario disponer de un vector con las cadenas de texto que se desean guardar. Para ello se puede crear un vector al que se le puede llamar data.

data = ["LÃ­nea 1", "LÃ­nea 2", "LÃ­nea 3", "LÃ­nea 4", "LÃ­nea 5"]
Para escribir el contenido de este vector en un archivo se puede hacer de dos maneras: lÃ­nea a lÃ­nea o de una sola vez.

Escribir el archivo lÃ­nea a lÃ­nea
El mÃ©todo mÃ¡s fÃ¡cil directo para volcar el vector en un archivo es escribir el contenido lÃ­nea a lÃ­nea. Para ello se puede iterar sobre el archivo y utilizar el mÃ©todo write de archivo. Este proceso es lo que se muestra en el siguiente ejemplo.

fic = open("text_1.txt", "w")
for line in data:
    fic.write(line)
    fic.write("\n")
    
fic.close()
NÃ³tese que los elementos de vector no finalizan con el carÃ¡cter salto de lÃ­nea. Por lo tanto, es necesario aÃ±adir este despuÃ©s de escribir cada lÃ­nea. Ya que, de lo contrario, todos los elementos se escribirÃ­an en una Ãºnica lÃ­nea en el archivo de salida.

Una forma de escribir el archivo lÃ­nea a lÃ­nea sin que sea necesario incluir el salto de lÃ­nea es con la funciÃ³n print. Para lo es necesario incluir la opciÃ³n file con la conexiÃ³n al archivo. Esta opciÃ³n se puede ver en el siguiente ejemplo.

fic = open("text_2.txt", "w")
for line in data:
    print(line, file=fic)
    
fic.close()
Escribir el archivo de una vez
Finalmente, en el caso de que los dato se encuentren en un objeto iterable se puede utilizar el mÃ©todo writelines para volcar este de golpe. Aunque es necesario tener en cuenta que este mÃ©todo no agrega el salto de lÃ­nea, por lo que puede ser necesario agregarlo con antelaciÃ³n.

fic = open("text_3.txt", "w")
fic.writelines("%s\n" % s for s in data)
fic.close()
En el ejemplo se puede apreciar que se ha iterado sobre el vector para agregar el salto de lÃ­nea para cada elemento.

Leer archivos de texto en Python
La lectura de los archivos, al igual que la escritura, se puede hacer de dos maneras: lÃ­nea a lÃ­nea o de una sola vez.

Leer el archivo de una vez
El procedimiento para leer los archivos de texto mÃ¡s sencillo es hacerlo de una vez con el mÃ©todo readlines. Una vez abierto el archivo solamente se ha de llamar a este mÃ©todo para obtener el contenido. Por ejemplo, se puede usar el siguiente cÃ³digo.

fic = open('text_1.txt', "r")
lines = fic.readlines()
fic.close()
En esta ocasiÃ³n lines es un vector en el que cada elemento es una lÃ­nea del archivo. Alternativamente, en lugar del mÃ©todo readlines se puede usar la funciÃ³n list para leer los datos.

fic = open('text_1.txt', "r")
lines = list(fic)
fic.close()
Leer el archivo lÃ­nea a lÃ­nea
En otras ocasiones puede ser necesario leer el archivo lÃ­nea a lÃ­nea. Esto se puede hacer simplemente iterando sobre el fichero una vez abierto. En casa iteraciÃ³n se podrÃ¡ hacer con cada lÃ­nea cualquier operaciÃ³n que sea necesaria. En el siguiente ejemplo cada una de las lÃ­neas se agrega a un vector.

fic = open('text_1.txt', "r")
lines = []
for line in fic:
    lines.append(line)
fic.close()
Eliminar los saltos de lÃ­nea en el archivo importado
Los tres mÃ©todos que se han visto para leer los archivos importan el salto de lÃ­nea. Por lo que puede ser necesario eliminarlo antes de trabajar con los datos. Esto se puede conseguir de forma sencilla con el mÃ©todo rstrip de las cadenas de texto de Python. Lo que se puede hacer iterando sobre el vector.

[s.rstrip('\n') for s in lines]
Conclusiones
Hoy se ha visto cÃ³mo leer y escribir archivos en Python utilizando solamente las funciones estÃ¡ndar del lenguaje. Explicando tres mÃ©todos tanto para escribir los archivos como para leerlos. Aunque normalmente para la lectura de archivos CSV en Python lo mÃ¡s fÃ¡cil es utilizar pandas, puede ser que sea necesario procesar los datos de una forma no estÃ¡ndar. En estas situaciones es cuando los visto en esta entrada es bastante Ãºtil.
En este tutorial aprenderemos a utilizar diccionarios de datos en Python y algunos de sus mÃ©todos mÃ¡s importantes.

Python es un lenguaje de programaciÃ³n interpretado de alto nivel y orientado a objetos, con el cual podemos crear todo tipo de aplicaciones. Entre sus diversos tipos de estructuras de datos, se encuentra "Diccionarios de Datos". En este tutorial aprenderemos a utilizar esta estructura revisando sus mÃ©etodos mÃ¡s utilizados.

Â¿QuÃ© es un Diccionario de datos?
Un Diccionario es una estructura de datos y un tipo de dato en Python con caracterÃ­sticas especiales que nos permite almacenar cualquier tipo de valor como enteros, cadenas, listas e incluso otras funciones. Estos diccionarios nos permiten ademÃ¡s identificar cada elemento por una clave (Key).

Para definir un diccionario, se encierra el listado de valores entre llaves. Las parejas de clave y valor se separan con comas, y la clave y el valor se separan con dos puntos.

diccionario = {'nombre' : 'Carlos', 'edad' : 22, 'cursos': ['Python','Django','JavaScript'] }
Podemos acceder al elemento de un Diccionario mediante la clave de este elemento, como veremos a continuaciÃ³n:

print diccionario['nombre'] #Carlos
print diccionario['edad']#22
print diccionario['cursos'] #['Python','Django','JavaScript']
TambiÃ©n es posible insertar una lista dentro de un diccionario. Para acceder a cada uno de los cursos usamos los Ã­ndices:

print diccionario['cursos'][0]#Python
print diccionario['cursos'][1]#Django
print diccionario['cursos'][2]#JavaScript
Para recorrer todo el Diccionario, podemos hacer uso de la estructura for:

for key in diccionario:
  print key, ":", diccionario[key]
MÃ©todos de los Diccionarios
dict ()

Recibe como parÃ¡metro una representaciÃ³n de un diccionario y si es factible, devuelve un diccionario de datos.

dic =  dict(nombre='nestor', apellido='Plasencia', edad=22)

dic â†’ {â€˜nombreâ€™ : 'nestor', â€˜apellidoâ€™ : 'Plasencia', â€˜edadâ€™ : 22}
zip()

Recibe como parÃ¡metro dos elementos iterables, ya sea una cadena, una lista o una tupla. Ambos parÃ¡metros deben tener el mismo nÃºmero de elementos. Se devolverÃ¡ un diccionario relacionando el elemento i-esimo de cada uno de los iterables.

dic = dict(zip('abcd',[1,2,3,4]))

dic â†’   {â€˜aâ€™ : 1, â€™bâ€™ : 2, â€˜câ€™ : 3 , â€˜dâ€™ : 4}
items()

Devuelve una lista de tuplas, cada tupla se compone de dos elementos: el primero serÃ¡ la clave y el segundo, su valor.

dic =   {â€˜aâ€™ : 1, â€™bâ€™ : 2, â€˜câ€™ : 3 , â€˜dâ€™ : 4}
items = dic.items()

items â†’ [(â€˜aâ€™,1),(â€˜bâ€™,2),(â€˜câ€™,3),(â€˜dâ€™,4)]
keys()

Retorna una lista de elementos, los cuales serÃ¡n las claves de nuestro diccionario.

dic =  {â€˜aâ€™ : 1, â€™bâ€™ : 2, â€˜câ€™ : 3 , â€˜dâ€™ : 4}
keys= dic.keys()

keysâ†’ [â€˜aâ€™,â€™bâ€™,â€™câ€™,â€™dâ€™] 
values()

Retorna una lista de elementos, que serÃ¡n los valores de nuestro diccionario.

dic =  {â€˜aâ€™ : 1, â€™bâ€™ : 2, â€˜câ€™ : 3 , â€˜dâ€™ : 4}
values= dic.values()

valuesâ†’ [1,2,3,4] 
clear()

Elimina todos los Ã­tems del diccionario dejÃ¡ndolo vacÃ­o.

dic 1 =  {â€˜aâ€™ : 1, â€™bâ€™ : 2, â€˜câ€™ : 3 , â€˜dâ€™ : 4}
dic1.clean()

dic1 â†’ { }
copy()

Retorna una copia del diccionario original.

dic = {â€˜aâ€™ : 1, â€™bâ€™ : 2, â€˜câ€™ : 3 , â€˜dâ€™ : 4}
dic1 = dic.copy()

dic1 â†’ {â€˜aâ€™ : 1, â€™bâ€™ : 2, â€˜câ€™ : 3 , â€˜dâ€™ : 4}
fromkeys()

Recibe como parÃ¡metros un iterable y un valor, devolviendo un diccionario que contiene como claves los elementos del iterable con el mismo valor ingresado. Si el valor no es ingresado, devolverÃ¡ none para todas las claves.

dic = dict.fromkeys(['a','b','c','d'],1)

dic â†’  {â€˜aâ€™ : 1, â€™bâ€™ : 1, â€˜câ€™ : 1 , â€˜dâ€™ : 1}
get()

Recibe como parÃ¡metro una clave, devuelve el valor de la clave. Si no lo encuentra, devuelve un objeto none.

dic = {â€˜aâ€™ : 1, â€™bâ€™ : 2, â€˜câ€™ : 3 , â€˜dâ€™ : 4}
valor = dic.get(â€˜bâ€™) 

valor â†’ 2
pop()

Recibe como parÃ¡metro una clave, elimina esta y devuelve su valor. Si no lo encuentra, devuelve error.

dic = {â€˜aâ€™ : 1, â€™bâ€™ : 2, â€˜câ€™ : 3 , â€˜dâ€™ : 4}
valor = dic.pop(â€˜bâ€™) 

valor â†’ 2
dic â†’ {â€˜aâ€™ : 1, â€˜câ€™ : 3 , â€˜dâ€™ : 4}
setdefault()

Funciona de dos formas. En la primera como get

dic = {â€˜aâ€™ : 1, â€™bâ€™ : 2, â€˜câ€™ : 3 , â€˜dâ€™ : 4}
valor = dic.setdefault(â€˜aâ€™)

valor â†’ 1
Y en la segunda forma, nos sirve para agregar un nuevo elemento a nuestro diccionario.

dic = {â€˜aâ€™ : 1, â€™bâ€™ : 2, â€˜câ€™ : 3 , â€˜dâ€™ : 4}
valor = dic.setdefault(â€˜eâ€™,5)

dic â†’ {â€˜aâ€™ : 1, â€™bâ€™ : 2, â€˜câ€™ : 3 , â€˜dâ€™ : 4 , â€˜eâ€™ : 5}
update()

Recibe como parÃ¡metro otro diccionario. Si se tienen claves iguales, actualiza el valor de la clave repetida; si no hay claves iguales, este par clave-valor es agregado al diccionario.

dic 1 = {â€˜aâ€™ : 1, â€™bâ€™ : 2, â€˜câ€™ : 3 , â€˜dâ€™ : 4}
dic 2 = {â€˜câ€™ : 6, â€™bâ€™ : 5, â€˜eâ€™ : 9 , â€˜fâ€™ : 10}
dic1.update(dic 2)

dic 1 â†’ {â€˜aâ€™ : 1, â€™bâ€™ : 5, â€˜câ€™ : 6 , â€˜dâ€™ : 4 , â€˜eâ€™ : 9 , â€˜fâ€™ : 10}
Estos son algunos de los mÃ©todos mÃ¡s Ãºtiles y mÃ¡s utilizados en los Diccionarios. Python es un gran lenguaje de programaciÃ³n que nos permite programar de una manera realmente sencilla. Si deseas conocer mucho mÃ¡s y aprender a profundidad esta tecnologÃ­a, ingresa al Curso de Python que tenemos en Devcode. Â¡Te esperamos!

Â¿Te gusto el tutorial?
AyÃºdanos a llegar a mÃ¡s personas

 
 
user
Carlos Eduardo Plasencia Prado
Backend Developer | Python / Django junior - Javascript / Node.js

@plasenciacar
ython: Agregar y eliminar elementos de un diccionario
Objetivo: mostrar cÃ³mo agregar y eliminar elementos de un diccionario en Python.

En la anterior entrega explicamos el concepto de diccionario en Python y vimos el modo de crearlos. Hoy presentaremos dos tÃ©cnicas bÃ¡sicas: la agregaciÃ³n y eliminaciÃ³n de elementos.

Comencemos con la creaciÃ³n de un par de diccionarios de ejemplo sobre los que practicaremos desde el intÃ©rprete interactivo:

>>> gazpacho = {}
>>> menda = {'Nombre':'Javier', 'Apellido':'Montero'}
El primero, gazpacho, es un simple diccionario vacÃ­o que utilizaremos para almacenar la receta del gazpacho; el otro, menda, recoge algunos datos sobre mi persona. MÃ¡s adelante veremos que podemos crear estructuras de datos mÃ¡s complejas, basadas en los diccionarios, que podremos emplear para mantener una agenda de nuestros contactos personales y sin necesidad de recurrir a un gestor de bases de datos.

Para agregar un par clave-valor a un diccionario, recurrimos a la siguiente sintaxis:

diccionario[clave] = valor

ProbÃ©moslo con gazpacho:

>>> gazpacho['Aceite'] = '300 ml'

Verificamos que gazpacho, que antes estaba vacÃ­o, ahora contiene ese par:

>>> gazpacho
{'Aceite': '300 ml'}
Continuemos con la receta secreta:

>>> gazpacho['Vinagre'] = '100 ml'
>>> gazpacho['Pepino'] = 1
>>> gazpacho['Pimiento'] = 1
Veamos cÃ³mo evoluciona nuestra creaciÃ³n:

>>> gazpacho
{'Pimiento': 1, 'Aceite': '300 ml', 'Vinagre': '100 ml', 'Pepino': 1}
Observa la salida de esta Ãºltima instrucciÃ³n y recuerda lo que dijimos sobre el orden de los diccionarios: no son una estructura ordenada, aunque veremos formas de hacerlos mÃ¡s presentables si lo deseamos.

AÃ±adamos nuevos elemendos al menda:

>>> menda['URL'] = 'http://www.elclubdelautodidacta.es'
>>> menda['Twitter'] = '@pradery'
tras lo cual,

>>> menda
{'URL': 'http://www.elclubdelautodidacta.es', 'Nombre': 'Javier', 'Twitter': '@pradery', 'Apellido': 'Montero'}
Las claves han de ser Ãºnicas. Si tratamos de agregar otra ya existente, simplemente el valor nuevo sustituirÃ¡ al antiguo:

>>> gazpacho['Pimiento'] = 2
>>> menda['URL'] = 'http://elclubdelautodidacta.es/wp/'
Observa cÃ³mo los valores correspondientes son actualizados:

>>> gazpacho
{'Pimiento': 2, 'Aceite': '300 ml', 'Vinagre': '100 ml', 'Pepino': 1}
>>> menda
{'URL': 'http://elclubdelautodidacta.es/wp/', 'Nombre': 'Javier', 'Twitter': '@pradery', 'Apellido': 'Montero'}
Para borrar un par clave-valor de un diccionario disponemos de la sentencia del, que emplearemos del siguiente modo:

del diccionario[clave]

Por ejemplo:

>>> del gazpacho['Aceite']
>>> gazpacho
{'Pimiento': 2, 'Vinagre': '100 ml', 'Pepino': 1}
>>> del menda['URL']
>>> menda
{'Nombre': 'Javier', 'Twitter': '@pradery', 'Apellido': 'Montero'}
Con del podrÃ­amos cargarnos incluso el objeto completo:

>>> del gazpacho

A partir de este momento el gazpacho ha dejado de existir y ha pasado a mejor vida:

>>> gazpacho
Traceback (most recent call last):
  File "<pyshell#28>", line 1, in <module>
    gazpacho
NameError: name 'gazpacho' is not defined
Y con menda mejor no lo hago, no sea que traiga mala suerteâ€¦

Javier Montero GabarrÃ³

Python: Agregar y eliminar elementos de un diccionario

El texto de este artÃ­culo se encuentra sometido a una licencia Creative Commons del tipo CC-BY-NC-ND (reconocimiento, no comercial, sin obra derivada, 3.0 unported)

El Club del Autodidacta

Consulta el Ã­ndice completo de artÃ­culos relacionados con Python.

Tal vez te interese...
Python â€“ Buscando a Wally.txt
Python â€“ Troceando desde el lado izquierdo
Comparando objetos en Python
Python â€“ Sumando objetos
Python â€“ Una tortuga de brocha fina
Â¿Conoces a alguien a quien le pueda interesar este artÃ­culo?
Haz clic para compartir en Facebook (Se abre en una ventana nueva)Haz clic para compartir en Twitter (Se abre en una ventana nueva)
Autor Javier MonteroPublicado el07/08/2012CategorÃ­aspythonEtiquetasdel, diccionarios, python
16 opiniones en â€œPython: Agregar y eliminar elementos de un diccionarioâ€
Isaac Lacoba (@IsaacLacoba)dice:
31/12/2012 a las 13:10
Gracias por tu post. Me ha ahorrado bastante tiempo. ğŸ˜‰

Responder
Javier Monterodice:
02/01/2013 a las 19:39
Me alegro de que te haya sido Ãºtil. Saludos. ğŸ™‚


About
Prensa
Legal
PolÃ­tica de privacidad
TÃ©rminos de Servicio
Cookie Policy
STACK EXCHANGE
NETWORK
Technology
Life / Arts
Culture / Recreation
Science
Other
Blog
Facebook
Twitter
LinkedIn
Instagram
diseÃ±o del sitio / logo Â© 2021 Stack Exchange Inc; contribuciones de los usuarios bajo licencia cc by-sa. rev 2021.3.30.38947
Mujercitas (pelÃ­cula de 2019)
Ir a la navegaciÃ³nIr a la bÃºsqueda
Little Women
Mujercitas 2019.png
TÃ­tulo	Mujercitas
Ficha tÃ©cnica
DirecciÃ³n	
Greta Gerwig
ProducciÃ³n	Denise Di Novi
Amy Pascal
Robin Swicord
Arnon Milchan
Guion	Greta Gerwig
Basada en	Mujercitas, de Louisa May Alcott
MÃºsica	Alexandre Desplat
FotografÃ­a	Yorick Le Saux
Protagonistas	Saoirse Ronan
TimothÃ©e Chalamet
Florence Pugh
Eliza Scanlen
James Norton
Laura Dern
Emma Watson
Louis Garrel
Meryl Streep
Bob Odenkirk
Chris Cooper
Ver todos los crÃ©ditos (IMDb)
Datos y cifras
PaÃ­s	Estados Unidos
AÃ±o	2019
Estreno	
25 de diciembre de 2019 (Estados Unidos)
GÃ©nero	Drama
Romance
DuraciÃ³n	134 minutos
Idioma(s)	InglÃ©s
CompaÃ±Ã­as
Productora	Columbia Pictures
Di Novi Pictures
Pascal Pictures
New Regency Pictures
Sony Pictures Entertainment
DistribuciÃ³n	Sony Pictures Releasing
InterCom
Presupuesto	US$40.000.000
RecaudaciÃ³n	US$205.914.9761â€‹
Ficha en IMDb
Ficha en FilmAffinity
[editar datos en Wikidata]
Mujercitas2â€‹3â€‹ (en inglÃ©s Little Women) es una pelÃ­cula dramÃ¡tica estadounidense de 2019 escrita y dirigida por Greta Gerwig. Es la sÃ©ptima adaptaciÃ³n cinematogrÃ¡fica de la novela de 1868 del mismo nombre de Louisa May Alcott. Narra las vidas de las hermanas March, Jo, Meg, Amy y Beth, en Concord, Massachusetts, durante el siglo XIX. EstÃ¡ protagonizada por un reparto coral compuesto por Saoirse Ronan, Emma Watson, Florence Pugh, Eliza Scanlen, Laura Dern, TimothÃ©e Chalamet, Meryl Streep, Tracy Letts, Bob Odenkirk, James Norton, Louis Garrel y Chris Cooper. Gerwig altera cronolÃ³gicamente la trama de la obra y alterna pasajes de los primeros aÃ±os de las niÃ±as con su vida profesional, con la intenciÃ³n de hacer una nueva visiÃ³n de la novela tras 150 aÃ±os, haciendo hincapiÃ© en el subtexto feminista de la obra4â€‹. En la nueva versiÃ³n, ademÃ¡s, se fusiona el texto original con la vida de Louisa May Alcott, de quien Jo era una especie de alter ego5â€‹.

Sony Pictures iniciÃ³ el desarrollo de la pelÃ­cula en 2013, Amy Pascal se incorporÃ³ para producir en 2015 y Gerwig fue contratada para escribir su guion al aÃ±o siguiente. Usando otros escritos de Alcott como inspiraciÃ³n, Gerwig escribiÃ³ el guion en 2018. Fue nombrada directora ese mismo aÃ±o, tras su triunfo con Lady Bird, y es la segunda pelÃ­cula dirigida por la directora, nominada a los Premios Ã“scar en 2018.6â€‹ La filmaciÃ³n se llevÃ³ a cabo de octubre a diciembre de 2018 en el estado de Massachusetts, y la ediciÃ³n comenzÃ³ el dÃ­a despuÃ©s del final de la filmaciÃ³n.

Mujercitas se estrenÃ³ en el Museo de Arte Moderno de la ciudad de Nueva York el 7 de diciembre de 2019 y se estrenÃ³ en cines en los Estados Unidos el 25 de diciembre de 2019 por Sony Pictures Releasing. La pelÃ­cula recibiÃ³ elogios de la crÃ­tica,7â€‹ con elogios especiales por el guion y la direcciÃ³n de Gerwig, asÃ­ como por las actuaciones del elenco, y recaudÃ³ 216 millones de dÃ³lares en todo el mundo. Entre sus numerosos elogios, la pelÃ­cula recibiÃ³ seis nominaciones a los Premios de la Academia, incluyendo Mejor PelÃ­cula, Mejor Actriz (Ronan), Mejor Actriz de Reparto (Pugh) y Mejor Guion Adaptado, y ganÃ³ el Premio al Mejor DiseÃ±o de Vestuario. TambiÃ©n recibiÃ³ nominaciones para los Premios BAFTA y los Globos de Oro.


Ãndice
1	Argumento
2	Reparto
3	ProducciÃ³n
3.1	Desarrollo y casting
3.2	Escritura
3.3	DiseÃ±o de vestuario
3.4	FilmaciÃ³n y ediciÃ³n
4	Estreno
5	RecepciÃ³n
5.1	Taquilla
5.2	CrÃ­tica
5.3	Premios y nominaciones
6	Referencias
7	Enlaces externos
Argumento
En 1868, Jo March, profesora de la ciudad de Nueva York, acude al Sr. Dashwood, un editor que acepta publicar una historia que ha escrito. Su hermana menor, Amy, que estÃ¡ en ParÃ­s con la tÃ­a March, asiste a una fiesta con su amigo de la escuela y vecino, Laurie. Amy se enoja por el comportamiento de Laurie, que se encuentra ebrio y se burla de ella por su intenciÃ³n de casarse con el rico empresario Fred Vaughn. En Nueva York, Jo se siente herida cuando Friedrich Bhaer, un profesor enamorado de ella, critica constructivamente sus escritos, lo que hace que dÃ© por terminada su amistad. DespuÃ©s de enterarse por una carta de que la enfermedad de su hermana menor Beth ha empeorado, Jo regresa a su casa en Concord, Massachusetts.

Siete aÃ±os antes, en una fiesta con su hermana mayor, Meg, Jo se hace amiga de Laurie. La maÃ±ana de Navidad, la madre de las niÃ±as, "Marmee", las convence para que den su desayuno a su pobre vecina, la seÃ±ora Hummel, y a sus hijos hambrientos. Cuando regresan a casa, encuentran una mesa llena de comida que les trajo su vecino y abuelo de Laurie, el Sr. Laurence. Marmee les a las hermanas una carta de su padre, que lucha en la Guerra Civil Estadounidense. Jo visita regularmente a la tÃ­a March para leerle, con la esperanza de que la tÃ­a March la invite a viajar por Europa.

Cuando Meg, Jo, Laurie y John Brooke -el tutor de Laurie y futuro esposo de Meg-, van al teatro, Amy celosa, quema los manuscritos de Jo. A la maÃ±ana siguiente, Amy, quiere reconciliarse con Jo, la persigue mientras Jo va con Laurie hasta un lago donde patinan. Laurie y Jo salvan a Amy tras hundirse en el agua frÃ­a de invierno. El Sr. Laurence invita a la pequeÃ±a Beth a tocar el piano de su difunta hija en su casa.

De vuelta al presente, Meg habla con su marido, John, tras comprar una tela tan cara que no podÃ­an pagar y le expresa su decepciÃ³n por ser pobre. En Europa, Laurie visita a Amy -quien ha decidido abandonar su carrera como pintora- para disculparse por su comportamiento de la noche anterior y le ruega que no se case con Fred, y que acepte su propia peticiÃ³n de matrimonio. Aunque estÃ¡ enamorada de Laurie, Amy se niega, molesta por ser siempre segundo plato, tras la negativa que Jo le dio a Lauire ante la misma peticiÃ³n. Aun asÃ­, Amy rechaza la propuesta de Fred y acepta la de Laurie.

Jo recuerda cuando tuvo que cortarse el pelo para que su madre pudiese viajar a Nueva York a cuidar de su padre, que habÃ­a sido herido en guerra. Por entonces el Sr. Laurence regalÃ³ su piano a Beth y descubrieron quela niÃ±a habÃ­a contraÃ­do la escarlatina, posiblemente en casa de los Hummel. Para evitar contraer la enfermedad, Amy es enviada a quedarse con la tÃ­a March, quien le aconseja que mantenga a su familia formalizando un buen matrimonio. Beth se recupera a tiempo para la Navidad del pasado, durante la cual su padre tambiÃ©n regresa a casa Meg y John Brooke se comprometen. Jo intenta convencerla de que huya, pero Meg expresa su alegrÃ­a por casarse con John. La tÃ­a March anuncia su viaje por Europa, pero se lleva a Amy en lugar de Jo. Tras la boda, Laurie le propone matrimonio a Jo, quien lo rechaza y le explica que no quiere casarse.

En el presente, John pide a Meg que convierta la cara tela en un vestido si le hace feliz. Meg revela que la habÃ­a vendido y le asegura que es feliz tal y como viven. Tras el empeoramiento, Beth fallece. Marmee informa a la familia de que Amy regresa de Europa con una tÃ­a March enferma. Jo se pregunta a sÃ­ misma si se apresurÃ³ a rechazar la proposiciÃ³n de matrimonio de Laurie y le escribe una carta. Amy se prepara para abandonar Europa y le dice a Laurie que ha rechazado la propuesta de Fred; se besan y luego se casan en el viaje de retorno a casa. Jo y Laurie acuerdan ser solo amigos, y se deshace de la carta que le escribiÃ³ y no llegÃ³ a enviar. Jo comienza a escribir una novela basada en la vida de ella y sus hermanas y envÃ­a los primeros capÃ­tulos a un poco impresionado Sr. Dashwood. Bhaer sorprende a Jo al presentarse en la casa March camino a California.

En Nueva York, el Sr. Dashwood acepta publicar la novela de Jo pues sus propias hijas exigen saber cÃ³mo termina la historia, pero se niega a aceptar que la protagonista permanezca soltera al final. Para apaciguarlo, Jo termina su novela con la protagonista, ella misma, impidiendo que Bhaer se vaya a California. Negocia con Ã©xito derechos de autor y regalÃ­as con el Sr. Dashwood. Tras el fallecimiento de la tÃ­a March, Jo hereda su casa y la abre como una escuela para niÃ±as y niÃ±os, donde enseÃ±an Meg, Amy, John y Bhaer. Jo observa la impresiÃ³n de su novela, titulada Mujercitas.

Reparto
Saoirse Ronan como Josephine "Jo" March.
Emma Watson como Margaret "Meg" March.
Florence Pugh como Amy March.
Eliza Scanlen como Elizabeth "Beth" March.
Laura Dern como Marmee March.
TimothÃ©e Chalamet como Theodore "Laurie" Laurence.
Meryl Streep como TÃ­a March.
Tracy Letts como el SeÃ±or Dashwood.
Bob Odenkirk como el padre de la familia March.
James Norton como John Brooke.
Louis Garrel como Friedrich Bhaer.
Chris Cooper como el SeÃ±or Laurence.
Jayne Houdyshell como Hannah.
Rafael Silva como el amigo de Friedrich.
Dash Barber como Fred Vaughn.
Hadley Robinson como Sallie Gardiner Moffat.
Abby Quinn como Annie Moffat.
Maryann Plunkett como la SeÃ±ora Kirke.
Edward Fletcher como el sirviente del SeÃ±or Laurence.
Sasha Frolova como la SeÃ±ora Hummel.
ProducciÃ³n
Desarrollo y casting
En octubre de 2013, se anunciÃ³ que una nueva adaptaciÃ³n cinematogrÃ¡fica de la novela Mujercitas de Louisa May Alcott estaba en desarrollo en Sony Pictures, con Olivia Milch escribiendo el guion y Robin Swicord y Denise Di Novi como productoras.8â€‹ En marzo de 2015, Amy Pascal se uniÃ³ como productora de la nueva adaptaciÃ³n, y Sarah Polley fue contratada para escribir el guion y potencialmente dirigir.9â€‹ En Ãºltima instancia, la participaciÃ³n de Polley nunca fue mÃ¡s allÃ¡ de las discusiones iniciales.10â€‹ En agosto de 2016, Greta Gerwig fue contratada para escribir el guion.11â€‹ En junio de 2018, Gerwig fue anunciado como el director de la pelÃ­cula ademÃ¡s de ser su guionista.12â€‹ Se habÃ­a enterado de los planes de Sony para adaptar el libro en 2015 e instÃ³ a su agente a que la pusiera en contacto con el estudio, admitiendo que si bien ella "no estaba en la lista de nadie para dirigir esta pelÃ­cula", era algo a lo que aspiraba hacer, citando cÃ³mo el libro la inspirÃ³ a convertirse en escritora y directora.13â€‹ AdemÃ¡s de ser la primera pelÃ­cula de estudio de Gerwig que dirigiÃ³, Mujercitas fue su segundo esfuerzo como directora en solitario.14â€‹15â€‹

TambiÃ©n se anunciÃ³ en junio de 2018 que Meryl Streep, Emma Watson, Saoirse Ronan, TimothÃ©e Chalamet y Florence Pugh habÃ­an sido elegidos para papeles no revelados.16â€‹ Gerwig habÃ­a trabajado con Ronan y Chalamet en su primera pelÃ­cula como directora en solitario, Lady Bird,17â€‹ mientras buscaba elegir a Pugh despuÃ©s de ver su actuaciÃ³n en la pelÃ­cula Lady Macbeth.18â€‹ Eliza Scanlen, a quien Gerwig vio protagonizar la miniserie Sharp Objects,19â€‹ se uniÃ³ al elenco al mes siguiente.20â€‹ James Norton y Laura Dern fueron elegidos en agosto.21â€‹22â€‹ Ese mismo mes, Emma Watson se uniÃ³ al elenco, reemplazando a Stone, quien tuvo que abandonar debido a conflictos de programaciÃ³n con la promociÃ³n de La Favorita.23â€‹ En septiembre, Louis Garrel, Bob Odenkirk y Chris Cooper se unieron al elenco en papeles secundarios.24â€‹25â€‹26â€‹ New Regency Pictures fue anunciado como un financiador adicional de la pelÃ­cula en octubre.27â€‹

Escritura
Gerwig comenzÃ³ a escribir el guion durante un viaje a Big Sur, California, luego de la ceremonia de los Premios de la Academia 2018, utilizando como inspiraciÃ³n las cartas y diarios de Alcott, asÃ­ como "pinturas de mujeres jÃ³venes del siglo XIX".19â€‹ TambiÃ©n se inspirÃ³ en las otras historias de Alcott para los diÃ¡logos.28â€‹ Gerwig escribiÃ³ muchas lÃ­neas de diÃ¡logo superpuestas que se "leerÃ­an una encima de la otra".29â€‹ AdemÃ¡s, afirmÃ³ que un monÃ³logo de la pelÃ­cula se inspirÃ³ en una conversaciÃ³n que tuvo con Streep sobre "los desafÃ­os que enfrentaban las mujeres en el DÃ©cada de 1860".30â€‹ Para "enfocar la pelÃ­cula en [sus personajes] como adultos", Gerwig incorporÃ³ una lÃ­nea de tiempo no lineal.31â€‹ El final difiere del de la novela al describir "los placeres de un romance dentro de una historia sobre Alcott realizando sus ambiciones artÃ­sticas", que Gerwig creÃ­a que honra la verdadera visiÃ³n de Alcott dado que Alcott tenÃ­a que "satisfacer las expectativas narrativas de la Ã©poca".32â€‹33â€‹

DiseÃ±o de vestuario
La pelÃ­cula requiriÃ³ "aproximadamente 75 trajes de Ã©poca principales", cada uno de los cuales tomÃ³ "aproximadamente 40 horas" para crear.19â€‹ La diseÃ±adora de vestuario, Jacqueline Durran, combinÃ³ "un espÃ­ritu de vestuario libre" y "la rigidez victoriana tradicional" al vestir a los personajes.34â€‹ Queriendo hacer que "la ropa vintage pareciera codiciada para el espectador moderno", combinÃ³ "etiquetas de lana" con "faldas a cuadros de muy buen gusto", "capas largas de color carmesÃ­" y "gorras de vendedor de periÃ³dicos".35â€‹ DistinguiÃ³ los guardarropas de la infancia y la edad adulta de los personajes teniendo en cuenta "la lÃ³gica interna de cada uno" y manteniendo "la conexiÃ³n entre los dos", y a cada personaje se le asignÃ³ un "color central", incluido el rojo para el personaje de Ronan, el verde y el lavanda para Watson, marrÃ³n y rosa para Scanlen y azul claro para Pugh.36â€‹ TambiÃ©n hizo que los personajes compartieran y reutilizaran las mismas piezas de vestuario para reforzar sus relaciones entre ellos.37â€‹ AdemÃ¡s de diseÃ±ar el personaje de Ronan con "vestidos holgados de algodÃ³n" y "faldas de lana lisas",35â€‹ incorporÃ³ "referencias modernas" y utilizÃ³ "un joven Bob Dylan", la subcultura Teddy Boy y la pintura del artista francÃ©s James Tissot. El cÃ­rculo de la Rue Royale como inspiraciÃ³n para estilizar el de Chalamet.38â€‹

FilmaciÃ³n y ediciÃ³n

La filmaciÃ³n tuvo lugar principalmente en Harvard, Massachusetts.
El elenco, con la excepciÃ³n de Pugh debido a sus compromisos de filmaciÃ³n con la pelÃ­cula Midsommar, comenzÃ³ los ensayos de la pelÃ­cula dos semanas antes de la filmaciÃ³n.29â€‹ La fotografÃ­a principal comenzÃ³ en Boston en octubre de 2018, 39â€‹ con Harvard, Massachusetts, como ubicaciÃ³n principal.40â€‹ Las ubicaciones adicionales incluyeron Lancaster, la Universidad de Harvard en Cambridge, Crane Beach en Ipswich y Concord, todas en el estado de Massachusetts.41â€‹42â€‹ La casa de la familia March se construyÃ³ desde cero en un terreno en Concord,19â€‹ mientras que el Arnold Arboretum de Harvard se utilizÃ³ para rodar una escena ambientada en un parque de ParÃ­s del siglo XIX con Pugh, Chalamet y Streep.43â€‹ Castle Hill en Ipswich tambiÃ©n se utilizÃ³ para duplicar las escenas europeas.44â€‹ El director de fotografÃ­a Yorick Le Saux filmÃ³ la pelÃ­cula en formato de 35 mm.45â€‹

Gerwig descubriÃ³ que estaba embarazada durante la producciÃ³n y lo mantuvo en privado durante todo el proceso.14â€‹ TambiÃ©n impuso la prohibiciÃ³n de los telÃ©fonos mÃ³viles en el set durante el rodaje.46â€‹ DespuÃ©s de terminar la fotografÃ­a principal el 16 de diciembre de 2018, Gerwig comenzÃ³ a editar la pelÃ­cula junto con el editor Nick Houy al dÃ­a siguiente y luego la proyectÃ³ para los ejecutivos de Sony Pictures en la ciudad de Nueva York el 10 de marzo de 2019, tres dÃ­as antes de dar a luz a un hijo.33â€‹

Estreno
El 19 de junio de 2019, Vanity Fair lanzÃ³ las primeras imÃ¡genes fijas,47â€‹ y el avance oficial se lanzÃ³ el 13 de agosto.48â€‹ Mujercitas tuvo su estreno mundial en el Museo de Arte Moderno de la ciudad de Nueva York el 7 de diciembre de 2019,49â€‹ y tambiÃ©n se proyectÃ³ para inaugurar el Festival Internacional de Cine de RÃ­o de Janeiro el 9 de diciembre.50â€‹ Fue estrenada en cines en los Estados Unidos el 25 de diciembre de 2019 por Sony Pictures Releasing.51â€‹52â€‹ Deadline Hollywood informÃ³ que Sony gastÃ³ aproximadamente $70 millones en la promociÃ³n de la pelÃ­cula.53â€‹

Mujercitas estaba programada originalmente para un estreno en cines en China el 14 de febrero de 2020, pero esto se descartÃ³ debido a la pandemia de COVID-19.54â€‹ La pelÃ­cula fue lanzada digitalmente el 10 de marzo de 2020 y en DVD y Blu-ray el 7 de abril.55â€‹56â€‹ En mayo, Variety informÃ³ que una vez mÃ¡s estaba destinado a ser lanzado en China en una fecha no especificada despuÃ©s de la pandemia.57â€‹ La pelÃ­cula se estrenÃ³ en Dinamarca y JapÃ³n en junio despuÃ©s de que ambos paÃ­ses reabrieran sus salas de cine luego de cierres pandÃ©micos.58â€‹ Finalmente fue lanzada en China el 25 de agosto.59â€‹

RecepciÃ³n
Taquilla
Mujercitas recaudÃ³ $108.1 millones en los Estados Unidos y CanadÃ¡, y $108.5 millones en otros paÃ­ses, para un total mundial de $216.6 millones, contra un presupuesto de producciÃ³n de $40 millones.60â€‹61â€‹ En abril de 2020, Deadline Hollywood calculÃ³ su beneficio neto en 56 millones de dÃ³lares.53â€‹

En los Estados Unidos y CanadÃ¡, la pelÃ­cula se estrenÃ³ junto con Spies in Disguise y la expansiÃ³n de Diamantes en Bruto, y se proyectaba que recaudarÃ­a entre 18 y 22 millones de dÃ³lares en 3308 salas durante su fin de semana de estreno de cinco dÃ­as. GanÃ³ $6.4 millones el dÃ­a de Navidad y $6 millones en su segundo dÃ­a,62â€‹ y pasÃ³ a debutar con $16.8 millones (un total de $29.2 millones durante el perÃ­odo de cinco dÃ­as de Navidad), terminando en cuarto lugar.63â€‹64â€‹ En su segundo fin de semana, la pelÃ­cula recaudÃ³ $13.6 millones, terminando tercero.65â€‹ Luego ganÃ³ $7.8 millones y $6.4 millones, respectivamente, los siguientes fines de semana.66â€‹67â€‹

En junio de 2020, la pelÃ­cula recaudÃ³ 495000 dÃ³lares y 255000 dÃ³lares durante su primer fin de semana en JapÃ³n y su segundo fin de semana en Dinamarca, respectivamente.68â€‹ Ese mismo mes, superÃ³ los $100 millones en la taquilla internacional luego de lanzamientos en otros 12 mercados.69â€‹ La pelÃ­cula ganÃ³ $4.7 millones durante los primeros seis dÃ­as de su lanzamiento en agosto de 2020 en China.70â€‹

CrÃ­tica


Saoirse Ronan (arriba) y Florence Pugh fueron nominadas al Premio de la Academia a Mejor Actriz y Mejor Actriz de Reparto, respectivamente.
En el sitio web del agregador de reseÃ±as Rotten Tomatoes, la pelÃ­cula tiene una calificaciÃ³n de aprobaciÃ³n del 95% basada en 403 reseÃ±as, con una calificaciÃ³n promedio de 8.54/10. El consenso de los crÃ­ticos del sitio web dice: "Con un elenco estelar y un recuento inteligente y sensible de su material original clÃ¡sico, Mujercitas de Greta Gerwig demuestra que algunas historias son verdaderamente atemporales".71â€‹ En Metacritic, tiene una puntuaciÃ³n media ponderada de 91 sobre de 100, sobre la base de 57 crÃ­ticas, lo que indica "aclamaciÃ³n universal".72â€‹ Las audiencias encuestadas por CinemaScore le dieron a la pelÃ­cula una calificaciÃ³n promedio de "Aâ€“" en una escala de A+ a F, y los espectadores encuestados por PostTrak le dieron un promedio de cinco de cinco.63â€‹

Escribiendo para IndieWire, Kate Erbland destacÃ³ la "ambiciosa narraciÃ³n elÃ­ptica" de Gerwig y elogiÃ³ su direcciÃ³n por no ser ni "torpe" ni "sermoneadora".73â€‹ Anthony Lane de The New Yorker dijo que "puede que sea la mejor pelÃ­cula hecha hasta ahora por una mujer estadounidense".74â€‹ Lindsey Bahr, de Associated Press, tambiÃ©n elogiÃ³ la direcciÃ³n de Gerwig, considerÃ¡ndola un "logro asombroso" y una "declaraciÃ³n de artista".75â€‹ Al premiar la pelÃ­cula con tres y medio de cuatro, Brian Truitt de USA Today elogiÃ³ la escritura de Gerwig como "magnÃ­fica" y dijo que "hace que el tiempo y el lenguaje de Alcott se sientan efervescentemente modernos y autÃ©nticamente nostÃ¡lgicos".76â€‹ Mick LaSalle, que escribe para el San Francisco Chronicle, le dio a la pelÃ­cula una crÃ­tica mixta, en la que elogiÃ³ la direcciÃ³n de Gerwig pero criticÃ³ la lÃ­nea de tiempo no lineal y los personajes "presumidos".77â€‹

Los crÃ­ticos elogiaron las actuaciones del elenco, con David Rooney de The Hollywood Reporter destacando su "encantador trabajo de conjunto", y Alonso Duralde de TheWrap diciendo que no hubo "un solo momento artificial" de ninguno de los actores.78â€‹79â€‹ Caryn James de BBC Online calificÃ³ la actuaciÃ³n de Ronan de "luminosa",80â€‹ y Leah Greenblatt de Entertainment Weekly sugiriÃ³ que "lleva casi todas las escenas en las que se encuentra".81â€‹ David Sims de The Atlantic destacÃ³ la actuaciÃ³n de Pugh, escribiendo que convirtiÃ³ a su personaje en "una heroÃ­na tan rica y convincente como [Ronan]",82â€‹ mientras que Clarisse Loughrey de The Independent declarÃ³ que Pugh "se las arregla para robar el show".83â€‹ En su reseÃ±a para NPR, Justin Chang elogiÃ³ las actuaciones de Ronan y Pugh como "increÃ­blemente buenas".84â€‹ Chalamet tambiÃ©n fue elogiado por Peter Travers de Rolling Stone y Ann Hornaday de The Washington Post por el "encanto innato y la vulnerabilidad conmovedora", asÃ­ como la "fisicalidad lÃºdica" en su actuaciÃ³n.85â€‹86â€‹

Si bien la pelÃ­cula en general recibiÃ³ seis nominaciones al Premio de la Academia, Gerwig no fue nominada a Mejor Director, lo que se considerÃ³ un desaire.87â€‹88â€‹ Allison Pearson de The Daily Telegraph calificÃ³ esto como un "estÃ¡ndar completamente nuevo de idiotez", y opinÃ³ que "menosprecia la experiencia de las mujeres",89â€‹ mientras que Dana Stevens de Slate teorizÃ³ que los miembros de la Academia creen que "las mujeres solo pueden tener un pequeÃ±o reconocimiento, como un regalo "y que Gerwig" ahora puede ser ignorada con seguridad "ya que anteriormente habÃ­a sido nominada para Lady Bird.90â€‹ Escribiendo para Los Angeles Times, los psicÃ³logos sociales Devon Proudfoot y Aaron Kay concluyeron que el desaire se debiÃ³ a una "tendencia psicolÃ³gica general a ver sin saberlo el trabajo de las mujeres como menos creativo que el de los hombres".91â€‹

Premios y nominaciones
En la 92Âª ediciÃ³n de los Premios de la Academia, Mujercitas recibiÃ³ seis nominaciones, incluyendo Mejor PelÃ­cula, Mejor Actriz (Ronan), Mejor Actriz de Reparto (Pugh) y Mejor Guion Adaptado,92â€‹93â€‹ y ganÃ³ el Premio al Mejor DiseÃ±o de Vestuario.7â€‹94â€‹ La pelÃ­cula tambiÃ©n recibiÃ³ nueve nominaciones en los Premios de la CrÃ­tica CinematogrÃ¡fica 2019, ganando como Mejor Guion Adaptado,95â€‹96â€‹ cinco nominaciones en los Premios BAFTA 2019,97â€‹ y dos en los Premios Globo de Oro 2019.98â€‹ Fue elegida por el American Film Institute como una de las diez mejores pelÃ­culas del aÃ±o.99â€‹

Referencias
 Â«'Little Women' (2019)Â». Box Office Mojo. Consultado el 18 de marzo de 2020.
 https://www.ecartelera.com/pelÃ­culas/mujercitas-2019/
 https://www.elle.com/es/living/ocio-cultura/a19594863/nuevo-mujercitas-reparto/
 Â«Entrevista a Greta GerwigÂ».
 Â«La nueva â€œMujercitasâ€: una mirada real sobre el deseo y las ambiciones del universo femeninoÂ».
 Â«Â«Pensaba que solo los hombres podÃ­an dirigirÂ»: Greta Gerwig, la realizadora de â€˜Mujercitasâ€™ explica su pasiÃ³n por las mujeres que escribenÂ».
 Carras, Christi (9 de febrero de 2020). Â«The only Oscar 'Little Women' won was for costume designÂ». Los Angeles Times. Archivado desde el original el 10 de febrero de 2020. Consultado el 15 de junio de 2020. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 Kroll, Justin (18 de octubre de 2013). Â«Sony Sets Up 'Little Women' Adaptation with Olivia Milch Writing (EXCLUSIVE)Â». Variety. Archivado desde el original el 12 de enero de 2020. Consultado el 8 de enero de 2020. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 Sneider, Jeff (18 de marzo de 2015). Â«Amy Pascal, Sarah Polley Team on 'Little Women' Remake at SonyÂ». TheWrap. Archivado desde el original el 5 de octubre de 2018. Consultado el 29 de junio de 2018. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 Whipp, Glenn (5 de julio de 2018). Â«Why it's a perfect time for Greta Gerwig's version of 'Little Women'Â». Los Angeles Times. Archivado desde el original el 9 de noviembre de 2019. Consultado el 10 de noviembre de 2019. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 Calvario, Liz (6 de agosto de 2016). Â«'Little Women': Greta Gerwig Will Rewrite Sony's Remake of Louisa May Alcott NovelÂ». IndieWire. Archivado desde el original el 11 de abril de 2019. Consultado el 15 de junio de 2020. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 Kroll, Justin (29 de junio de 2018). Â«Greta Gerwig Eyes 'Little Women' With Meryl Streep, Emma Watson, Saoirse Ronan, Timothee Chalamet CirclingÂ». Variety. Archivado desde el original el 29 de junio de 2018. Consultado el 29 de junio de 2018. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 Salisbury, Mark (17 de enero de 2020). Â«Greta Gerwig on fighting to make 'Little Women': "I was not on anybody's list to direct this film"Â». Screendaily.com. Screen International. Archivado desde el original el 29 de mayo de 2020. Consultado el 16 de junio de 2020. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 Kaufman, Amy (24 de diciembre de 2019). Â«'Little Women' director Greta Gerwig didn't just make a 'women's movie'Â». Los Angeles Times. Archivado desde el original el 24 de diciembre de 2019. Consultado el 31 de julio de 2020. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 Miller, Jenni (25 de diciembre de 2019). Â«Greta Gerwig's 'Little Women' is the adaptation every Jo March always neededÂ». NBC News. Archivado desde el original el 27 de diciembre de 2019. Consultado el 1 de agosto de 2020. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 N'Duka, Amanda (29 de junio de 2018). Â«Greta Gerwig To Helm 'Little Women' At Sony; Meryl Streep, Emma Stone, TimothÃ©e Chalamet, Saoirse Ronan In TalksÂ». Deadline Hollywood. Archivado desde el original el 10 de julio de 2018. Consultado el 29 de junio de 2018. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 Canfield, David (17 de octubre de 2019). Â«Little Women: Timothee Chalamet and Saoirse Ronan talk reunionÂ». Entertainment Weekly. Archivado desde el original el 18 de octubre de 2019. Consultado el 15 de junio de 2020. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 Keegan, Rebecca (5 de enero de 2020). Â«The Season of Florence PughÂ». The Hollywood Reporter. Archivado desde el original el 5 de enero de 2020. Consultado el 15 de junio de 2020. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 Sandberg, Bryn Elise (29 de noviembre de 2019). Â«Making of 'Little Women': Greta Gerwig Gives Modern Take on 1868 Novel for Big ScreenÂ». The Hollywood Reporter. Archivado desde el original el 30 de noviembre de 2019. Consultado el 15 de junio de 2020. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 N'Duka, Amanda (24 de julio de 2018). Â«'Little Women': 'Sharp Objects' Actress In Talks For The Role Of Beth MarchÂ». Deadline Hollywood. Archivado desde el original el 27 de julio de 2018. Consultado el 24 de julio de 2018. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 N'Duka, Amanda (2 de agosto de 2018). Â«Sony's 'Little Women' Adaptation Adds 'Flatliners' Actor James NortonÂ». Deadline Hollywood. Archivado desde el original el 17 de junio de 2019. Consultado el 15 de junio de 2020. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 Galuppo, Mia (14 de agosto de 2018). Â«Laura Dern in Talks to Join Meryl Streep in 'Little Women'Â». The Hollywood Reporter. Archivado desde el original el 15 de agosto de 2018. Consultado el 14 de agosto de 2018. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 Kroll, Justin (24 de agosto de 2018). Â«Emma Watson Joins Greta Gerwig's Adaptation of 'Little Women'Â». Variety. Archivado desde el original el 24 de agosto de 2018. Consultado el 24 de agosto de 2018. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 N'Duka, Amanda (5 de septiembre de 2018). Â«Louis Garrel Cast In 'Little Women' Movie At SonyÂ». Deadline Hollywood. Archivado desde el original el 6 de septiembre de 2018. Consultado el 5 de septiembre de 2018. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 N'Duka, Amanda (24 de septiembre de 2018). Â«'Better Call Saul's Bob Odenkirk Joins Greta Gerwig's 'Little Women' RemakeÂ». Deadline Hollywood. Archivado desde el original el 25 de septiembre de 2018. Consultado el 24 de septiembre de 2018. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 N'Duka, Amanda (28 de septiembre de 2018). Â«Oscar Winner Chris Cooper Boards Greta Gerwig's 'Little Women' AdaptationÂ». Deadline Hollywood. Archivado desde el original el 29 de septiembre de 2018. Consultado el 28 de septiembre de 2018. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 Fleming Jr, Mike (2 de octubre de 2018). Â«New Regency Co-Finances Two Sony Films: 'Little Women' & 'Girl In The Spider's Web'Â». Deadline Hollywood. Archivado desde el original el 3 de octubre de 2018. Consultado el 2 de octubre de 2018. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 White, Abbey (23 de diciembre de 2019). Â«Greta Gerwig on How Her 'Little Women' Adaptation Became "A Movie About Making Movies"Â». The Hollywood Reporter. Archivado desde el original el 23 de diciembre de 2019. Consultado el 15 de junio de 2020. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 Kaufman, Amy (31 de octubre de 2019). Â«How Saoirse Ronan and Florence Pugh updated 'Little Women' for modern feministsÂ». Los Angeles Times. Archivado desde el original el 9 de diciembre de 2019. Consultado el 9 de diciembre de 2019. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 Weinberg, Lindsay (2 de noviembre de 2019). Â«Greta Gerwig Says Meryl Streep Inspired a Powerful Scene in 'Little Women'Â». The Hollywood Reporter. Archivado desde el original el 4 de noviembre de 2019. Consultado el 15 de junio de 2020. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 Topel, Fred (30 de diciembre de 2019). Â«[WATCH] Greta Gerwig And Cast Discuss Focusing On 'Little Women' As AdultsÂ». Deadline Hollywood. Archivado desde el original el 15 de junio de 2020. Consultado el 15 de junio de 2020. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 Nicolaou, Elena (27 de diciembre de 2019). Â«Why Greta Gerwig's Little Women Movie Radically Changed the Book's EndingÂ». O, The Oprah Magazine. Archivado desde el original el 15 de junio de 2020. Consultado el 15 de junio de 2020. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 Whipp, Glenn (16 de diciembre de 2019). Â«Why Greta Gerwig kept her perfect 'Little Women' ending a secretÂ». Los Angeles Times. Archivado desde el original el 20 de diciembre de 2019. Consultado el 15 de junio de 2020. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 Kinosian, Janet (25 de diciembre de 2019). Â«Designing 'Women' lets Jacqueline Durran get a little freer with Victorian costumesÂ». Los Angeles Times. Archivado desde el original el 25 de diciembre de 2019. Consultado el 15 de junio de 2020. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 Syme, Rachel (13 de enero de 2020). Â«How Jacqueline Durran, the "Little Women" Costume Designer, Remixes Styles and ErasÂ». The New Yorker. Archivado desde el original el 13 de enero de 2020. Consultado el 4 de agosto de 2020. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 Grobar, Matt (2 de enero de 2020). Â«Costume Designer Jacqueline Durran Talks 'Little Women' Timelines, '1917' Military Attire & Entering Domain Of Superheroes With 'The Batman'Â». Deadline Hollywood. Archivado desde el original el 2 de enero de 2020. Consultado el 15 de junio de 2020. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 Gonzales, Erica (9 de febrero de 2020). Â«Jo and Laurie Shared Clothes on Purpose in Little WomenÂ». Harper's Bazaar. Archivado desde el original el 29 de febrero de 2020. Consultado el 15 de junio de 2020. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 Ivie, Devon (18 de diciembre de 2019). Â«TimothÃ©e Chalamet Little Women Outfits and Fashion InterviewÂ». Vulture. Archivado desde el original el 18 de diciembre de 2019. Consultado el 4 de agosto de 2020. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 Nechamkin, Sarah (9 de octubre de 2018). Â«Everything We Know About Greta Gerwig's Little Women AdaptationÂ». The Cut. Archivado desde el original el 14 de agosto de 2019. Consultado el 15 de junio de 2020. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 Schaffstall, Katherine (8 de febrero de 2020). Â«Oscars: 10 Things to Know About Best Picture Nominee 'Little Women'Â». The Hollywood Reporter. Archivado desde el original el 15 de junio de 2020. Consultado el 15 de junio de 2020. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 Slane, Kevin (24 de diciembre de 2019). Â«'Little Women' was filmed entirely in Massachusetts. Here are the historic, picturesque locations from the movie.Â». Boston.com. Archivado desde el original el 26 de diciembre de 2019. Consultado el 15 de junio de 2020. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 Goldstein, Meredith (21 de diciembre de 2019). Â«A big stage for Concord in 'Little Women'Â». The Boston Globe. Archivado desde el original el 22 de diciembre de 2019. Consultado el 15 de junio de 2020. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 Blackwell, Deborah (1 de noviembre de 2018). Â«Harvard's Arnold Arboretum attracts 'Little Women' with Meryl StreepÂ». The Harvard Gazette. Archivado desde el original el 18 de noviembre de 2018. Consultado el 17 de noviembre de 2018. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 Gale, Natalie (19 de diciembre de 2019). Â«Inside the Filming of Greta Gerwig's Little WomenÂ». Northshore Magazine. Archivado desde el original el 17 de junio de 2020. Consultado el 16 de junio de 2020. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 Thompson, Anne (23 de diciembre de 2019). Â«Little Women: 10 Decisions That Turned It Into a Modern Movie ClassicÂ». IndieWire. Archivado desde el original el 23 de diciembre de 2019. Consultado el 4 de agosto de 2020. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 Ford, Rebecca (3 de enero de 2020). Â«Why Quentin Tarantino and More Directors Are Banning Cellphones on SetÂ». The Hollywood Reporter. Archivado desde el original el 4 de enero de 2020. Consultado el 15 de junio de 2020. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 Saraiya, Sonia (19 de junio de 2019). Â«Exclusive First Look: Greta Gerwig and Saoirse Ronan's 'Little Women'Â». Vanity Fair. Archivado desde el original el 19 de junio de 2019. Consultado el 19 de junio de 2019. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 Beresford, Trilby (13 de agosto de 2019). Â«Greta Gerwig's 'Little Women' Releases First TrailerÂ». The Hollywood Reporter. Consultado el 13 de agosto de 2019.
 Bell, Keaton (9 de diciembre de 2019). Â«All the Photos From Inside the New York Premiere of Little WomenÂ». Vogue. Archivado desde el original el 10 de diciembre de 2019. Consultado el 15 de junio de 2020. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 Cajueiro, Marcelo (7 de diciembre de 2019). Â«Rio Fest's Compact Edition Opens Amidst Sectorial CrisisÂ». Variety. Archivado desde el original el 7 de diciembre de 2019. Consultado el 15 de junio de 2020. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 McClintock, Pamela (18 de julio de 2018). Â«Quentin Tarantino's Manson Movie Shifts Off Sharon Tate Murder Anniversary DateÂ». The Hollywood Reporter. Archivado desde el original el 19 de julio de 2018. Consultado el 18 de julio de 2018. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 Eldredge, Kristy (27 de diciembre de 2019). Â«Opinion â€“ Men Are Dismissing 'Little Women.' What a Surprise.Â». The New York Times. Archivado desde el original el 27 de diciembre de 2019. Consultado el 27 de diciembre de 2019. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 D'Alessandro, Anthony (7 de abril de 2019). Â«'Little Women,' Big Profits: Remake Lands At No. 24 In Deadline's 2019 Most Valuable Blockbuster TournamentÂ». Deadline Hollywood. Archivado desde el original el 7 de abril de 2020. Consultado el 7 de abril de 2019. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 Brzeski, Patrick (3 de febrero de 2020). Â«China Releases for 'Dolittle,' '1917,' 'Jojo Rabbit' Canceled Amid Coronavirus CrisisÂ». The Hollywood Reporter. Archivado desde el original el 5 de febrero de 2020. Consultado el 15 de junio de 2020. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 Russian, Ale (9 de marzo de 2020). Â«See Saoirse Ronan, TimothÃ©e Chalamet and Emma Watson Get Silly Behind-the-Scenes on Little WomenÂ». People. Archivado desde el original el 10 de marzo de 2020. Consultado el 15 de junio de 2020. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 West, Amy (2 de abril de 2020). Â«Little Women made a Game of Thrones mistake that was just spotted by fansÂ». Digital Spy. Archivado desde el original el 5 de abril de 2020. Consultado el 15 de junio de 2020. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 Davis, Rebecca (13 de mayo de 2020). Â«'Little Women,' '1917' Likely Among First Films to Hit Reopened Chinese TheatersÂ». Variety. Archivado desde el original el 14 de mayo de 2020. Consultado el 15 de junio de 2020. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 Roxborough, Scott (15 de junio de 2020). Â«Little Women: Denmark, Japan Give Cinemas Hope for Post-Virus RecoveryÂ». The Hollywood Reporter. Archivado desde el original el 16 de junio de 2020. Consultado el 16 de junio de 2020. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 Grater, Tom (6 de agosto de 2020). Â«'Little Women' Locks China Release For August 25Â». Deadline Hollywood. Archivado desde el original el 7 de agosto de 2020. Consultado el 7 de agosto de 2020. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 Â«Little Women (2019)Â». Box Office Mojo. Amazon. Archivado desde el original el 13 de abril de 2020. Consultado el 31 de agosto de 2020. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 Â«Little Women (2019)Â». The Numbers. Archivado desde el original el 3 de enero de 2020. Consultado el 31 de agosto de 2020. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 McClintock, Pamela (25 de diciembre de 2019). Â«Box Office: 'Star Wars: Rise of Skywalker' Unwraps Huge $32M on Christmas DayÂ». The Hollywood Reporter. Archivado desde el original el 26 de diciembre de 2019. Consultado el 26 de diciembre de 2019. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 D'Alessandro, Anthony (28 de diciembre de 2019). Â«'Star Wars: Rise Of Skywalker' Chasing 'Last Jedi' With $76M 2nd Weekend; 'Little Women' Not So Tiny With $29M 5-DayÂ». Deadline Hollywood. Archivado desde el original el 29 de diciembre de 2019. Consultado el 29 de diciembre de 2019. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 Â«Domestic 2019 Weekend 52Â». Box Office Mojo. Amazon. Archivado desde el original el 30 de diciembre de 2019. Consultado el 3 de enero de 2020. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 D'Alessandro, Anthony (5 de enero de 2020). Â«'Star Wars: Rise Of Skywalker' Dips To $34M+ Third Weekend; 'Grudge' Doesn't Scream With $11M+ & 'F' CinemaScoreÂ». Deadline Hollywood. Archivado desde el original el 5 de enero de 2020. Consultado el 5 de enero de 2020. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 D'Alessandro, Anthony (12 de enero de 2020). Â«'1917' Strong With $36M+, But 'Like A Boss' & 'Just Mercy' Fighting Over 4th With $10M; Why Kristen Stewart's 'Underwater' Went Kerplunk With $6M+Â». Deadline Hollywood. Archivado desde el original el 12 de enero de 2020. Consultado el 12 de enero de 2020. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 D'Alessandro, Anthony (19 de enero de 2020). Â«'Bad Boys For Life' So Great With $100M+ Worldwide; 'Dolittle' Still A Dud With $57M+ Global â€“ Box Office UpdateÂ». Deadline Hollywood. Archivado desde el original el 20 de enero de 2020. Consultado el 19 de enero de 2020. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 Tartaglione, Nancy (14 de junio de 2020). Â«Little Women Marches Towards $100M Overseas â€“ International Box OfficeÂ». Deadline Hollywood. Archivado desde el original el 14 de junio de 2020. Consultado el 15 de junio de 2020. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 Moreau, Jordan (21 de junio de 2020). Â«'Little Women' Crosses $100 Million at the International Box OfficeÂ». Variety. Archivado desde el original el 22 de junio de 2020. Consultado el 22 de junio de 2020. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 Tartaglione, Nancy (30 de agosto de 2020). Â«'Tenet' Triumphs With $53M Worldwide Launch From 40 Offshore Markets & Canada â€“ International Box OfficeÂ». Deadline Hollywood. Archivado desde el original el 27 de agosto de 2020. Consultado el 30 de agosto de 2020. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 Â«Little Women (2019)Â». Rotten Tomatoes. Fandango Media. Archivado desde el original el 18 de diciembre de 2019. Consultado el 19 de agosto de 2020. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 Â«'Little Women' (2019) ReviewsÂ». Metacritic. CBS Interactive. Archivado desde el original el 18 de diciembre de 2019. Consultado el 25 de diciembre de 2019. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 Erbland, Kate (25 de noviembre de 2019). Â«'Little Women' Review: Greta Gerwig Marries Tradition With Meta Modernity in Stunning AdaptationÂ». IndieWire. Archivado desde el original el 29 de febrero de 2020. Consultado el 29 de febrero de 2020. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 Lane, Anthony (6 de enero de 2020). Â«Greta Gerwig's "Little Women," ReviewedÂ». The New Yorker. Archivado desde el original el 25 de diciembre de 2019. Consultado el 15 de junio de 2020. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 Bahr, Lindsey (16 de diciembre de 2019). Â«Review: Greta Gerwig's 'Little Women' is a new classicÂ». Associated Press. Archivado desde el original el 15 de junio de 2020. Consultado el 15 de junio de 2020. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 Truitt, Brian (25 de noviembre de 2019). Â«Review: Greta Gerwig's all-star 'Little Women' adapts a classic with modern wit, resonanceÂ». USA Today. Archivado desde el original el 11 de febrero de 2020. Consultado el 29 de febrero de 2020. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 LaSalle, Mick (19 de diciembre de 2019). Â«Gerwig's 'Little Women' are snootier than we rememberÂ». San Francisco Chronicle. Archivado desde el original el 20 de diciembre de 2019. Consultado el 6 de julio de 2020. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 Rooney, David (25 de noviembre de 2019). Â«'Little Women': Film ReviewÂ». The Hollywood Reporter. Archivado desde el original el 26 de noviembre de 2019. Consultado el 15 de junio de 2020. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 Duralde, Alonso (25 de noviembre de 2019). Â«'Little Women' Film Review: Greta Gerwig's New Spin on a Beloved TaleÂ». TheWrap. Archivado desde el original el 28 de noviembre de 2019. Consultado el 15 de junio de 2020. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 James, Caryn (16 de diciembre de 2019). Â«Why Little Women is a triumphÂ». BBC Online. Archivado desde el original el 15 de junio de 2020. Consultado el 15 de junio de 2020. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 Greenblatt, Leah (25 de noviembre de 2019). Â«Little Women review: Greta Gerwig's remake is a warm blanket in a cold worldÂ». Entertainment Weekly. Archivado desde el original el 26 de enero de 2020. Consultado el 15 de junio de 2020. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 Sims, David (25 de diciembre de 2019). Â«Greta Gerwig Captures the Poignancy of 'Little Women'Â». The Atlantic. Archivado desde el original el 26 de diciembre de 2019. Consultado el 15 de junio de 2020. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 Loughrey, Clarisse (27 de diciembre de 2019). Â«Little Women review: Greta Gerwig's loving adaptation waltzes with a literary ghostÂ». The Independent. Archivado desde el original el 25 de diciembre de 2019. Consultado el 15 de junio de 2020. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 Chang, Justin (20 de diciembre de 2019). Â«'Little Women' Again? Greta Gerwig's Adaptation Is Both Faithful And RadicalÂ». NPR. Archivado desde el original el 20 de diciembre de 2019. Consultado el 15 de junio de 2020. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 Travers, Peter (23 de diciembre de 2019). Â«Greta Gerwig Delivers a 'Little Women' for a New GenerationÂ». Rolling Stone. Archivado desde el original el 8 de abril de 2020. Consultado el 14 de junio de 2020. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 Hornaday, Ann (17 de diciembre de 2019). Â«Part Alcott, part Gerwig, 'Little Women' is a very nearly perfect filmÂ». The Washington Post. Archivado desde el original el 18 de diciembre de 2019. Consultado el 14 de junio de 2020. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 Aurthur, Kate (4 de febrero de 2020). Â«Greta Gerwig on 'Little Women's' Oscar Nominations â€” and That One Big SnubÂ». Variety. Archivado desde el original el 9 de febrero de 2020. Consultado el 24 de marzo de 2020. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 Butler, Bethonie (8 de febrero de 2020). Â«The biggest female director Oscar snubs of the past decadeÂ». The Washington Post. Archivado desde el original el 8 de febrero de 2020. Consultado el 24 de marzo de 2020. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 Pearson, Allison (14 de enero de 2020). Â«Greta Gerwig's Oscars snub proves Hollywood is still pale, male and staleÂ». The Daily Telegraph. Archivado desde el original el 15 de enero de 2020. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 Stevens, Dana (13 de enero de 2020). Â«2020 Oscar nominations snub Greta Gerwig for Best Director: Does the Academy think Little Women directed itself?Â». Slate. Archivado desde el original el 14 de enero de 2020. Consultado el 16 de junio de 2020. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 Proudfoot, Devon; Kay, Aaron (8 de febrero de 2020). Â«Op-Ed: A scientific reason for Greta Gerwig's Oscar snub: The creativity of women is judged more harshlyÂ». Los Angeles Times. Archivado desde el original el 8 de febrero de 2020. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 Nordyke, Kimberly; Konerman, Jennifer; Strause, Jackie; Howard, Annie (13 de enero de 2020). Â«Oscar Nominations 2020: The Complete List of NomineesÂ». The Hollywood Reporter. Archivado desde el original el 13 de enero de 2020. Consultado el 15 de enero de 2020. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 Wilson, Jordan (13 de enero de 2020). Â«Oscars: Greta Gerwig's Adaptation Brings 'Little Women' Noms Tally to 14Â». The Hollywood Reporter. Archivado desde el original el 13 de enero de 2020. Consultado el 13 de enero de 2020. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 N'Duka, Amanda (9 de febrero de 2020). Â«Jacqueline Durran Nabs Second Career Oscar Award For Costume Design For 'Little Women'Â». Deadline Hollywood. Archivado desde el original el 10 de febrero de 2020. Consultado el 10 de febrero de 2020. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 Malkin, Marc (8 de diciembre de 2019). Â«Critics' Choice: 'The Irishman,' 'Once Upon a Time in Hollywood' Lead Movie NominationsÂ». Variety. Archivado desde el original el 9 de diciembre de 2019. Consultado el 8 de diciembre de 2019. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 Ramos, Dino-Ray (12 de enero de 2020). Â«Critics' Choice Awards: 'Once Upon A Time In Hollywood' Wins Best Picture, Netflix And HBO Among Top Honorees â€“ Full Winners ListÂ». Deadline Hollywood. Archivado desde el original el 14 de enero de 2020. Consultado el 15 de enero de 2020. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 Tartaglione, Nancy (7 de enero de 2020). Â«BAFTA Film Awards Nominations: 'Joker', 'The Irishman', 'Once Upon A Time In Hollywood' Lead â€“ Full ListÂ». Deadline Hollywood. Archivado desde el original el 8 de enero de 2020. Consultado el 7 de enero de 2020. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 Nordyke, Kimberly; Konerman, Jennifer; Howard, Annie (9 de diciembre de 2019). Â«Golden Globes: Full List of NominationsÂ». The Hollywood Reporter. Archivado desde el original el 10 de diciembre de 2019. Consultado el 9 de diciembre de 2019. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 Hipes, Patrick (4 de diciembre de 2019). Â«AFI Awards Film: 'The Irishman', '1917', 'Little Women' Among Top 10Â». Deadline Hollywood. Archivado desde el original el 4 de diciembre de 2019. Consultado el 4 de diciembre de 2019. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
Enlaces externos
PÃ¡gina web oficial
Mujercitas en Internet Movie Database (en inglÃ©s).
Little Women en Metacritic (en inglÃ©s).
Little Women en Rotten Tomatoes (en inglÃ©s).
Little Women en Box Office Mojo (en inglÃ©s)
Control de autoridades	
Proyectos WikimediaWd Datos: Q56881140IdentificadoresWorldCatVIAF: 69155707017022410579LCCN: no2019055758
CategorÃ­as: PelÃ­culas en inglÃ©sPelÃ­culas de Estados UnidosPelÃ­culas de 2019PelÃ­culas de Columbia PicturesPelÃ­culas dramÃ¡ticasPelÃ­culas romÃ¡nticasPelÃ­culas de Regency EnterprisesPelÃ­culas sobre hermanasPelÃ­culas sobre la guerra de SecesiÃ³nPelÃ­culas dramÃ¡ticas de Estados UnidosPelÃ­culas dramÃ¡ticas de los aÃ±os 2010PelÃ­culas candidatas al premio Ã“scar a la mejor pelÃ­culaAdaptaciones cinematogrÃ¡ficas de Mujercitas
MenÃº de navegaciÃ³n
No has accedido
DiscusiÃ³n
Contribuciones
Crear una cuenta
Acceder
ArtÃ­culoDiscusiÃ³n
LeerEditarVer historialBuscar
Buscar en Wikipedia
Portada
Portal de la comunidad
Actualidad
Cambios recientes
PÃ¡ginas nuevas
PÃ¡gina aleatoria
Ayuda
Donaciones
Notificar un error
Herramientas
Lo que enlaza aquÃ­
Cambios en enlazadas
Subir archivo
PÃ¡ginas especiales
Enlace permanente
InformaciÃ³n de la pÃ¡gina
Citar esta pÃ¡gina
Elemento de Wikidata
Imprimir/exportar
Crear un libro
Descargar como PDF
VersiÃ³n para imprimir

En otros idiomas
Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©
Deutsch
English
FranÃ§ais
Bahasa Indonesia
PortuguÃªs
Ğ ÑƒÑÑĞºĞ¸Ğ¹
Tiáº¿ng Viá»‡t
ä¸­æ–‡	!"#$%&%()(=()/=)=/)?/=?/=Ã‘Ã‘))=Ã‘/)99
21 mÃ¡s
Editar enlaces
Esta pÃ¡gina se editÃ³ por Ãºltima vez el Â°22 mar 2021 a las 18:14.
El texto estÃ¡ disponible bajo la Lic	|encia Creative Commons AtribuciÃ³n Compartir Igual 3.0; pueden aplicarse clÃ¡usulas adicionales. Al usar este sitio, usted acepta nuestros tÃ©rminos de uso y nuestra polÃ­tica de privacidad.
WikipediaÂ® es una marca registrada de la FundaciÃ³n Wikimedia, Inc., una organizaciÃ³n sin Ã¡nimo de lucro.
	"#import string as st
import pmli as pm 
import sys
import operator as op

class crearAyudante():
    def __init__(self):
        self.MAX = 27
        self.lista1 = [i for i in st.ascii_lowercase]
        self.lista1.insert(14,"Ã±")
        
        self.dicc = [pm.crearPMLI(i) for i in self.lista1]

    def esPalabraValida(self, s):
        assert(type(s) == str)
        for letra in s:
            if not (letra in self.lista1):
                return False
        return True
        
    def distance(self, s1, s2):
        d=dict()
        for i in range(len(s1)+1):
           d[i]=dict()
           d[i][0]=i
        for i in range(len(s2)+1):
           d[0][i] = i
        for i in range(1, len(s1)+1):
            for j in range(1, len(s2)+1):
                d[i][j] = min(d[i][j-1]+1, d[i-1][j]+1, d[i-1][j-1]+(not s1[i-1] == s2[j-1]))
        return d[len(s1)][len(s2)]    

    def cargarDiccionario(self, fname):
        with open(fname) as archivo:
            for linea in archivo:
                linea = linea[:-1].lower()
                # print(linea)
                if self.esPalabraValida(linea) is True:
                    for i in range(self.MAX):
                        if self.dicc[i].l == linea[0] and  not linea.isspace():
                            #print(i, self.dicc[i].l, len(self.dicc))
                            #resolver las lineas en blamco
                            self.dicc[i].agregarPalabra(linea)
                            break              
                else:
                    return

    def borrarPalabra(self, p):
        assert(self.esPalabraValida(p) is True)
        for i in range(self.MAX):
            if self.dicc[i].l == p[0] and self.dicc[i].eliminarPalabra(p) is True:
                return           

    def buscarPalabra(self, p):
        assert(self.esPalabraValida(p) is True)
        for i in range(self.MAX):
            if self.dicc[i].l == p[0] and self.dicc[i].buscarPalabra(p) is True:
                return True

    def mostrar(self):
        for i in range(self.MAX):
            print(self.dicc[i].mostrar(), "\n")

    def corregirTexto(self, finput):
        with open(finput) as archivo:
            for linea in archivo:
                lista = [i for i in linea.split()
                         if (self.esPalabraValida(i) is True) and (self.buscarPalabra(i) is not True)]
                
                for palabra in lista:
                    # print(palabra)
                    dlev = {}
                    for i in range(self.MAX):
                        aux = self.dicc[i].pal.tabla
                        dlev1 = {}
                        for j in range(len(aux)):
                            dlev1 = {}
                            if aux[j] is not None:
                                if len(dlev) < 4:
                                    dlev[aux[j]] = self.distance(aux[j], palabra)

                                else:    
                                    # print(dlev, palabra, dlev1,"\n")
                                    pal_dicc = (aux[j], self.distance(aux[j], palabra))

                                    dlev = dict(sorted([i for i in dlev.items()], key=lambda x: x[1]))
                                    for i in dlev.items():

                                        if i[1] <= pal_dicc[1] and len(dlev1) < 4:
                                            dlev1[i[0]] = i[1]  

                                        elif i[1] > pal_dicc[1] and len(dlev1) < 4:       
                                            dlev1[pal_dicc[0]] = pal_dicc[1]    
                                            pal_dicc = i

                                    dlev = dlev1
                    
                    with open("foutput.txt", "a") as archivo:
                        salida = dlev
                        salida = [clave for clave in salida.keys()]
                        salida.insert(0, palabra)
                        salida = ",".join(salida)
                        archivo.write(salida)
                        archivo.write("\n")



                                
                        
                        

                
                # for i in d.items():
                # print("*", lista,"\n")


helpo = crearAyudante()
helpo.cargarDiccionario(sys.argv[1])

# print(helpo.mostrar())

# elpo.borrarPalabra(sys.argv[2])
# helpo.buscarPalabra(sys.argv[3])
helpo.corregirTexto(sys.argv[4])
helpo.mostrar()




# with open(sys.argv[4]) as archivo:
#    for linea in archivo:
#        lista = linea.split()
#        print(lista)
Series Editors
Gerhard Goos
Universitat Karlsruhe
Postfach 69 80
Vincenz-Priessnitz-StraBe 1
D-76131 Karlsruhe, Germany
Juris Hartmanis
Cornell University
Department of Computer Science
4130 Upson Hall
Ithaca, NY 14853, USA
Author
Gerhard Reinelt
Institut fur Angewandte Mathematik, Universitat Heidelberg
Im Neuenheimer Feld 294, D-69120 Heidelberg, Germany
CR Subject Classification (1991): G.2,1.3.5, G.4,1.2.8, J.l-2
ISBN 3-540-58334-3 Springer-Verlag Berlin Heidelberg New York
ISBN 0-387-58334-3 Springer-Verlag New York Berlin Heidelberg
CIP data applied for
This work is subject to copyright. All rights are reserved, whether the whole or part
of the material is concerned, specifically the rights of translation, reprinting, re-use
of illustrations, recitation, broadcasting, reproduction on microfilms or in any other
way, and storage in data banks. Duplication of this publication or parts thereof is
permitted only under the provisions of the German Copyright Law of September 9,
1965, in its current version, and permission for use must always be obtained from
Springer-Verlag. Violations are liable for prosecution under the German Copyright
Law.
Â© Springer-Verlag Berlin Heidelberg 1994
Printed in Germany
Typesetting: Camera-ready by author
SPIN: 10475419
45/3140-543210 - Printed on acid-free paperPreface to the Online Edition
Still today I am receiving requests for reprints of the book, but unfortunately it
is out of print. Therefore, since the book still seems to receive some attention, I pro-
posed to Springer Verlag to provide a free online edition. I am very happy that Springer
agreed. Except for the correction of some typographical errors, the online edition is just
a copy of the printed version, no updates have been made. In particular, Table 13.1
gives the status of TSPLIB at the time of publishing the book. For accessing TSPLIB the
link http://www.iwr.uni-heidelberg.de/iwr/comopt/software/TSPLIB95/ should be
used instead of following the procedure described in Chapter 13.
Heidelberg, January 2001
Gerhard ReineltPreface
More than fifteen years ago, I was faced with the following problem in an assignment
for a class in computer science. A brewery had to deliver beer to five stores, and the task
was to write a computer program for determining the shortest route for the truck driver to
visit all stores and return to the brewery. All my attemps to find a reasonable algorithm
failed, I could not help enumerating all possible routes and then select the best one.
Frustrated at that point, I learnt later that there was no fast algorithm for solving this
problem. Moreover, I found that this problem was well known as the traveling salesman
problem and that there existed a host of published work on finding solutions. Though
no efficient algorithm was developed, there was a tremendous progress in designing fast
approximate solutions and even in solving ever larger problem instances to optimality. I
started some work on the traveling salesman problem several years ago, first just writing
demos for student classes, but then trying to find good and better solutions more effec-
tively. I experienced the fascination of problem solving that, I think, everyone studying
the traveling salesman problem will experience. In addition, I found that the problem has
relevance in practice and that there is need for fast algorithms.
The present monograph documents my experiments with algorithms for finding good
approximate solutions to practical traveling salesman problems. The work presented here
profited from discussions and meetings with several people, among them Thomas Christof,
Meinrad Funke, Martin GrÃ¶tschel, Michael JÃ¼nger, Manfred Padberg, Giovanni Rinaldi,
and Stefan Thienel, not naming dozens of further international researchers.
It is the aim of this text to serve as a guide for practitioners, but also to show that
the work on the traveling salesman problem is not at all finished. The TSP will stimulate
further efforts and continue to serve as the classical benchmark problem for algorithmic
ideas.
Heidelberg, June 1994
Gerhard ReineltContents
1 Introduction
2 Basic Concepts
2.1
2.2
2.3
2.4
2.5
. . . . . . . . . . . . . . . . . . . . . . . . . . 1
. . . . . . . . . . . . . . . . . . . . . . . . . 4
Graph Theory . . . . . . . .
Complexity Theory . . . . . .
Linear and Integer Programming
Data Structures
. . . . . . .
Some Fundamental Algorithms .
.
.
.
.
. 4
7
12
14
25
. . . . . . . . . . . . . . . 31
Some Related Problems . . . . . . . . . . . . . . . . . . . .
Practical Applications of the TSP . . . . . . . . . . . . . . . .
The Test Problem Instances . . . . . . . . . . . . . . . . . . 31
35
40
3 Related Problems and Applications
3.1
3.2
3.3
4 Geometric Concepts
4.1
4.2
4.3
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
42
Voronoi Diagrams . . . . . . . . . . . . . . . . . . . . . . .
Delaunay Triangulations . . . . . . . . . . . . . . . . . . . .
Convex Hulls . . . . . . . . . . . . . . . . . . . . . . . . . 42
48
54
. . . . . . . . . . . . . . . . . . . . . . . . . 64
Nearest Neighbors
. . . . . . . . . . . . . . . . . . . . . .
Candidates Based on the Delaunay Graph . . . . . . . . . . . .
Other Candidate Sets . . . . . . . . . . . . . . . . . . . . . 64
67
70
6 Construction Heuristics
6.1
6.2
6.3
6.4
6.5
.
.
.
.
.
. . . . . . . . . . . . . . . . . . . . . .
5 Candidate Sets
5.1
5.2
5.3
.
.
.
.
.
. . . . . . . . . . . . . . . . . . . . .
Nearest Neighbor Heuristics . . . . .
Insertion Heuristics . . . . . . . . .
Heuristics Using Spanning Trees . . .
Savings Methods and Greedy Algorithm
Comparison of Construction Heuristics
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
73
73
82
89
94
97VIII
7 Improving Solutions
7.1
7.2
7.3
7.4
7.5
7.6
. . . . . . . . . . . . . . . . . . . . . . . 100
Node and Edge Insertion . . . . . .
2-Opt Exchange
. . . . . . . . .
Crossing Elimination . . . . . . .
The 3-Opt Heuristic and Variants . .
Lin-Kernighan Type Heuristics . . .
Comparison of Improvement Heuristics
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
8 Fast Heuristics for Large Geometric Problems
8.1
8.2
8.3
8.4
Space Filling Curves
. .
Strip Heuristics . . . . .
Partial Representation . .
Decomposition Approaches
9 Further Heuristic Approaches
9.1
9.2
9.3
9.4
Simulated Annealing
Evolutionary Strategies
Tabu Search . . . .
Neural Networks . .
10 Lower Bounds
10.1
10.2
10.3
10.4
. .
and
. .
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
100
105
115
117
123
130
. . . . . . . . . 133
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
133
137
139
145
. . . . . . . . . . . . . . . . . . 153
. . . .
Genetic
. . . .
. . . .
. . . . . . . . .
Algorithms
. . .
. . . . . . . . .
. . . . . . . . .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
153
157
158
159
. . . . . . . . . . . . . . . . . . . . . . . . . . 161
Bounds from Linear Programming
. .
Simple Lower Bounds . . . . . . . .
Lagrangean Relaxation . . . . . . .
Comparison of Lower Bounds . . . . .
11 A Case Study: TSPs in PCB Production
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
161
163
172
184
. . . . . . . . . . . . 187
11.1 Drilling of Printed Circuit Boards . . . . . . . . . . . . . . . . 187
11.2 Plotting of PCB Production Masks . . . . . . . . . . . . . . . 193
12 Practical TSP Solving
. . . . . . . . . . . . . . . . . . . . . . 200
12.1 Determining Optimal Solutions . . . . . . . . . . . . . . . . . 200
12.2 An Implementation Concept . . . . . . . . . . . . . . . . . . 204
12.3 Interdependence of Algorithms . . . . . . . . . . . . . . . . . 207
Appendix: TSPLIB
References
Index
. . . . . . . . . . . . . . . . . . . . . . . 211
. . . . . . . . . . . . . . . . . . . . . . . . . . . 214
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 222Chapter 1
Introduction
The most prominent member of the rich set of combinatorial optimization problems is
undoubtly the traveling salesman problem (TSP), the task of finding a route through
a given set of cities with shortest possible length. It is one of the few mathematical
problems that frequently appear in the popular scientific press (Cipra (1993)) or even
in newspapers (Kolata (1991)). It has a long history, dating back to the 19th century
(Hoffman & Wolfe (1985)).
The study of this problem has attracted many researchers from different fields, e.g.,
Mathematics, Operations Research, Physics, Biology, or Artificial Intelligence, and there
is a vast amount of literature on it. This is due to the fact that, although it is easily
formulated, it exhibits all aspects of combinatorial optimization and has served and
continues to serve as the benchmark problem for new algorithmic ideas like simulated
annealing, tabu search, neural networks, simulated tunneling or evolutionary methods
(to name only a few of them).
On the other hand, the TSP is interesting not only from a theoretical point of view.
Many practical applications can be modeled as a traveling salesman problem or as
variants of it. Therefore, there is a tremendous need for algorithms. The number of
cities in practical applications ranges from some dozens up to even millions (in VLSI
design). Due to this manifold area of applications there also has to be a broad collection
of algorithms to treat the various special cases.
In the last two decades an enormous progress has been made with respect to solving
traveling salesman problems to optimality which, of course, is the ultimate goal of every
researcher. Landmarks in the search for optimal solutions are the solution of a 48-city
problem (Dantzig, Fulkerson & Johnson (1954)), a 120-city problem (GrÃ¶tschel
(1980)), a 318-city problem (Crowder & Padberg (1980)), a 532-city problem (Pad-
berg & Rinaldi (1987)), a 666-city problem (GrÃ¶tschel & Holland (1991)), a
2392-city problem (Padberg & Rinaldi (1991)), a 3038-city problem (Applegate,
Bixby, ChvÃ tal & Cook (1991)), and of a 4461-city problem (Applegate, Bixby,
ChvÃ tal & Cook (1993)). This progress is only partly due to the increasing hardware
power of computers. Above all, it was made possible by the development of mathemat-
ical theory (in particular polyhedral combinatorics) and of efficient algorithms. But,
despite of these achievements, the traveling salesman problem is far from being solved.
Many aspects of the problem still need to be considered and questions are still left to
be answered satisfactorily.
First, the algorithms that are able to solve the largest (with respect to the number of
cities) problems to optimality are not stable in the following sense: solution times vary
strongly for different problems with the same number of cities and there is no function
G. Reinelt: The Traveling Salesman, LNCS 840, pp. 1-3, 1994.
ï›™ Springer-Verlag Berlin Heidelberg 19942
Chapter 1. Introduction
depending on the number of cities that only gives a slight idea of the time necessary
to solve a particular problem. Already problems with some hundred nodes can be very
hard for these algorithms and require hours of CPU time on supercomputers. And,
there is a lot of theoretical knowledge that has not yet gone into implementations.
Second, problems arising in practice may have a number of cities that is far beyond
the capabilities of any exact algorithm available today. There are very good heuristics
yielding solutions which are only a few percent above optimality. However, they still
can be improved with respect to running time or quality of the computed solutions.
Third, requirements in the production environment may make many algorithms or
heuristics unsuitable. Possible reasons are that not enough real time or CPU time is
available to apply certain algorithms, that the problem instances are simply too large,
or that not enough real time or man power is at hand to code a method one would like
to apply.
These arguments visualize the potential that is still inherent in the traveling salesman
problem.
The present monograph is meant to be a contribution to practical traveling salesman
problem solving. Main emphasis will be laid on the question of how to find good or
acceptable tours for large problem instances in short time. We will discuss variants
and extensions of known approaches and discuss some new ideas that have proved to
be useful. Furthermore we will indicate some directions of future research. Literature
will be reviewed to some extent, but a complete coverage of the knowledge about the
TSP is beyond the purpose and available space of this tract. For an introduction we
recommend the book Lawler, Lenstra, Rinnooy Kan & Shmoys (1985) and the
survey article JÃ¼nger, Reinelt & Rinaldi (1994).
Nevertheless, even without consulting further references the present text is meant to
be a guide for readers who are concerned with applications of the TSP and aims at
providing sufficient information for their successful treatment.
We give a short survey of the topics that will be addressed. Chapter 2 covers basic con-
cepts that we need throughout this monograph. This chapter contains an introduction
to complexity theory and describes some fundamental data structures and algorithms.
The many possible applications of the TSP are indicated in Chapter 3. Of particular
importance are Euclidean instances. To exploit the underlying geometric structure we
use Voronoi diagrams and convex hulls which are discussed in Chapter 4. A basic ingre-
dient of fast heuristics will be a limitation of the scope of search for good tours. This is
accomplished by candidate sets which restrict algorithms to certain subsets of promising
connections. The construction of reasonable candidate sets is the topic of Chapter 5.
Construction heuristics to find starting tours are given in Chapter 6. Emphasis is laid
on improving standard approaches and making them applicable for larger problems.
Many of these heuristics are also useful in later chapters. Chapter 7 is concerned with
the improvement of given tours. It is shown how data structures can be successfully
employed to come up with very efficient implementations. An important issue is cov-
ered in Chapter 8: the treatment of very large problem instances in short time. Several
types of approaches are presented. A short survey of recent heuristic methods is con-
tained in Chapter 9. Lower bounds are the topic of Chapter 10. Besides variants of
known approaches we comment on heuristics for computing Lagrange multipliers. The
algorithms described in this text have been successfully applied in an industry project.Chapter 1. Introduction
3
We discuss this project in depth in Chapter 11. Chapter 12 addresses the question of
computing optimal solutions as well as solutions with quality guarantee and discusses
some lines of future research. In particular, a proposal for a hardware and software
setup for the effective treatment of traveling salesman problems in practice is presented.
The appendix gives information of how getting access to TSPLIB, a publicly available
collection of TSP instances, and lists the current status of these problem instances.
In this monograph we will not describe the approaches down to the implementation
level. But, we will give enough information to facilitate implementations and point
out possible problems. Algorithms are presented on a level that is sufficient for their
understanding and for guiding practical realizations.
Extensive room is spent for computational experiments. Implementations were done
carefully, however, due to limited time for coding the software, not always the absolutely
fastest algorithm could be used. The main point is the discussion of various algorithmic
ideas and their comparison using reasonable implementations. We have not restricted
ourselves to only tell â€œsuccess storiesâ€, but we rather point out that sometimes even
elaborate approaches fail in practice.
Summarizing, it is the aim of this monograph to give a comprehensive survey on heuristic
approaches to traveling salesman problem solving and to motivate the development and
implementation of further and possibly better algorithms.Chapter 2
Basic Concepts
The purpose of this chapter is to survey some basic knowledge from computer science
and mathematics that we need in this monograph. It is intended to provide the reader
with some fundamental concepts and results. For a more detailed representation of the
various subjects we shall refer to appropriate textbooks.
2.1 Graph Theory
Many combinatorial optimization problems can be formulated as problems in graphs.
We will therefore review some basic definitions from graph theory.
An undirected graph (or graph) G = (V, E) consists of a finite set of nodes V and a
finite set of edges E. Each edge e has two endnodes u, v and is denoted by e = uv or
e = {u, v}. We call such a graph undirected because we do not distinguish between the
edges uv and vu. However, we will sometimes speak about head and tail of an edge.
If e = uv then e is incident to v and to u. The set of edges incident to a node v is
denoted by Î´(v). The number |Î´(v)| is the degree of v.
A graph G  = (V  , E  ) is called a subgraph of G = (V, E) if V  âŠ† V and E  âŠ† E. For an
edge set E âŠ† E we define V (E) := {u, v âˆˆ V |uv âˆˆ E}. Conversely, for a node set V âŠ† V
we define E(V ) := {uv âˆˆ E | u âˆˆ V and v âˆˆ V }. We call the subgraph G  = (V (E), E)
edge induced (by E) and the subgraph G  = (V , E(V )) node induced (by V ).
A graph G = (V, E) is said to be complete if for all u, v âˆˆ V it contains edge uv. We
denote the complete graph on n nodes by K n = (V n , E n ) and assume unless otherwise
stated that V n = {1, 2, . . . , n}.
Two graphs G  = (V  , E  ) and G  = (V  , E  ) are isomorphic if there exists a bijective
mapping f : V  â†’ V  such that uv âˆˆ E  if and only if f (u)f (v) âˆˆ E  , e.g., the complete
graph K n is unique up to isomorphism.
A graph G = (V, E) is called bipartite if its node set V can be partitioned into two
nonempty disjoint sets V 1 , V 2 with V 1 âˆªV 2 = V such that no two nodes in V 1 and no two
nodes in V 2 are connected by an edge. If |V 1 | = m, |V 2 | = n and E = {ij | i âˆˆ V 1 , j âˆˆ V 2 }
then we call G the complete bipartite graph K m,n .
An edge set P = {v 1 v 2 , v 2 v 3 , . . . , v kâˆ’1 v k } is called a walk or more precisely a [v 1 , v k ]â€“
walk. If v i  = v j for all i  = j then P is called path or [v 1 , v k ]â€“path. The length of a
walk or path is the number of its edges and is denoted by |P |. If in a walk v 1 = v k we
speak of a closed walk.
A set of edges C = {v 1 v 2 , v 2 v 3 , . . . , v kâˆ’1 v k , v k v 1 } with v i  = v j for i  = j is called a cycle
(or k-cycle). An edge v i v j , 1 â‰¤ i  = j â‰¤ k, not in C is called chord of C. The length of
G. Reinelt: The Traveling Salesman, LNCS 840, pp. 4-30, 1994.
ï›™ Springer-Verlag Berlin Heidelberg 19942.1. Graph Theory
5
a cycle C is denoted by |C|. For convenience we shall sometimes abbreviate the cycle
{v 1 v 2 , v 2 v 3 , . . . , v k v 1 } by (v 1 , v 2 , . . . v k ) and also say that a graph G is a cycle if its edge
set forms a cycle. A graph or edge set is called acyclic if it contains no cycle. An
acyclic graph is also called forest.
A graph G = (V, E) is said to be connected if it contains for every pair of nodes a
path connecting them; otherwise G is called disconnected. A spanning tree is a
connected forest containing all nodes of the graph.
A nonempty edge set F âŠ† E is said to be a cut of the graph G = (V, E) if V can be
partitioned into two nonempty disjoint subsets V 1 , V 2 with V 1 âˆª V 2 = V such that the
following holds: F = {uv âˆˆ E | u âˆˆ V 1 , v âˆˆ V 2 }. Equivalently, F is a cut if there exists
a node set W âŠ† V such that F = Î´(W ).
Sometimes it is useful to associate a direction with the edges of a graph. A directed
graph (or digraph) D = (V, A) consists of a finite set of nodes V and a set of arcs
A âŠ† V Ã— V \ {(v, v) | v âˆˆ V } (we do not consider loops or multiple arcs). If e = (u, v) is
an arc of D with endnodes u and v then we call u its tail and v its head. The arc e is
said to be directed from u to v, incident from u and incident to v. The number
of arcs incident to a node v is called the indegree of v and the number of arcs incident
from v is called the outdegree of v. The degree of v is the sum of its indegree and
outdegree. For a node v the sets of arcs incident from v, incident to v, and incident from
or to v are denoted by Î´ + (v), Î´ âˆ’ (v), and Î´(v), respectively. Two nodes are adjacent if
there is an arc connecting them.
Most of the definitions for undirected graphs carry over in a straightforward way to
directed graphs. For example, diwalks, dipaths and dicycles are defined analogously
to walks, paths, and cycles with the additional requirement that the arcs are directed
in the same direction.
A digraph D = (V, A) is said to be complete if for all u, v âˆˆ V it contains both arcs
(u, v) and (v, u). We denote the complete digraph on n nodes by D n = (V n , A n ).
For each digraph D = (V, A) we can construct its underlying graph G = (V, E) by
setting E = {uv | u and v are adjacent in D}.
A digraph D = (V, A) is called connected (disconnected) if its underlying graph is
connected (disconnected). D is called diconnected if for each pair u, v of its nodes
there are a [u, v]â€“ and a [v, u]â€“dipath in D. A node v âˆˆ V is called articulation node
or cutnode of a digraph (graph) if the removal of v and all arcs (edges) having v as an
endnode disconnects the digraph (graph). A connected digraph (graph) is said to be
2-connected if it contains no articulation node.
To avoid degenerate situations we assume that, unless otherwise noted, all graphs and
digraphs contain at least one edge, respectively arc.
A walk (diwalk) that traverses every edge (arc) of a graph (digraph) exactly once is
called Eulerian trail (Eulerian ditrail). If such a walk (diwalk) is closed we speak of
a Eulerian tour. A graph (digraph) is Eulerian if its edge (arc) set can be traversed
by a Eulerian tour.
A cycle (dicycle) of length n in a graph (digraph) on n nodes is called Hamiltonian
cycle (Hamiltonian dicycle) or Hamiltonian tour. A path (dipath) of length n is
called Hamiltonian path (Hamiltonian dipath). A graph (digraph) containing a
Hamiltonian tour is called Hamiltonian.6
Chapter 2. Basic Concepts
Often we have to deal with graphs where a rational number (edge weight) is associated
with each edge. We call a function c : E â†’ Q (where Q denotes the set of rational
numbers) a weight function defining a weight c(e) (or c e , or c uv ) for every edge e =
uv âˆˆ E. (In the context of practical computations it makes no sense to admit arbitrary
real-valued functions since only rational numbers are representable on a computer.) The
weight of a set of edges F âŠ† E is defined as

c uv .
c(F ) :=
uvâˆˆF
The weight of a tour is usually called its length, a tour of smallest weight is called
shortest tour. The problem in the focus of this monograph is the so-called (symmetric)
traveling salesman problem.
(Symmetric) Traveling Salesman Problem
Given the complete graph K n with edge weights c uv find a shortest Hamiltonian tour
in K n .
A symmetric TSP is said to satisfy the triangle inequality, if c uv â‰¤ c uw + c wv for
all distinct nodes u, v, w âˆˆ V . Of particular interest are metric traveling salesman
problems. These are problems where the nodes correspond to points in some space and
where the edge weights are given by evaluating some metric distance between corre-
sponding points. For example, a Euclidean TSP is defined by a set of points in the
plane. The corresponding graph contains a node for every point and edge weights are
given by the Euclidean distance of the points associated with the end nodes.
We list some problems on graphs related to the traveling salesman problem which will
be referred to at some places.
Asymmetric Traveling Salesman Problem
Given the complete digraph D n with arc weights c uv find a shortest Hamiltonian tour
in D n .
Chinese Postman Problem
Given a graph G = (V, E) with edge weights c uv for uv âˆˆ E find a shortest closed walk
in G containing all edges at least once.
Hamiltonian Cycle Problem
Given a graph G = (V, E) decide if G contains a Hamiltonian cycle.
Eulerian Tour Problem
Given a graph G = (V, E) decide if G is Eulerian.
Though quite similar, these problems are very different with respect to their hardness.
It is a topic of the next section to give a short introduction into complexity theory and
its impact on the traveling salesman problem.2.2. Complexity Theory
7
2.2 Complexity Theory
When dealing with combinatorial problems or algorithms one is often interested in
comparing problems with respect to their hardness and algorithms with respect to their
efficiency. Often it is intuitively clear that some problem is more difficult to solve than
another problem and that one algorithm takes longer than another algorithm. The work
of Cook (Cook (1971)) laid the foundation for putting these questions into an exact
mathematical framework. Based on the notion of deterministic and nondeterministic
Turing machines it makes the classification of problems as â€œhardâ€ or â€œeasyâ€ possible and
allows the measurement of efficiency of algorithms. Although being only a theoretical
model this concept had a great impact on the design and the analysis of algorithms.
For our purposes it is sufficient to introduce the concepts of complexity theory in a more
informal manner. If we omit certain subtleties we can take a real-world computer as our
computational model and think of an algorithm as a procedure written in some high-
level programming language. For a thorough study of complexity issues we recommend
Garey & Johnson (1979).
For reasons of exactness we distinguish in this section between â€œproblemsâ€ and â€œin-
stances of problemsâ€. A problem or problem class is a question defined on several
formal parameters, e.g., determine whether a graph contains a Hamiltonian cycle or
compute a shortest Hamiltonian tour in a weighted graph. If we associate concrete
values to these formal parameters we create a particular instance of the problem. A
particular graph defines an instance of the Hamiltonian cycle problem and a particular
weight function c : E n â†’ Q gives an instance of the traveling salesman problem for the
complete graph K n = (V n , E n ).
These two examples show in addition that we have to distinguish between two types
of problems. One type is the so-called decision problem which requires a â€œyesâ€ or
â€œnoâ€ answer and the other type is the optimization problem demanding to exhibit a
solution which optimizes some objective function. We first give the basic terminology
which has been defined for decision problems and then show how optimization problems
can also be handled within this concept.
The performance of an algorithm has to be measured in some way depending on the
â€œsizesâ€ of the problem instances to be solved. Therefore we associate with each instance
I of a certain problem class S a size or encoding length l(I) which is defined as
the number of bits required to represent the actual parameters in the usual binary
encoding scheme. If A is an algorithm for the solution of problem S then we define
its running time (for instance I) as the number of elementary operations (addition,
multiplication, etc.) which have to be executed on a computer to solve instance I. The
time complexity or running time of an algorithm A for a problem class S is then
defined as a function t A : N â†’ N giving for each natural number n the number t A (n)
of elementary operations that the algorithm has to execute at most to solve an instance
of size n.
Note, that we assume that arithmetic operations are executed in constant time, i.e.,
independent of the size of the numbers involved. This is not correct in general, but is
feasible in our context.
Of course, in the usual case we will not be able to derive an explicit formula for evaluating
t A (n). On the other hand, we are not interested in concrete values of t A but rather in8
Chapter 2. Basic Concepts
the rate of growth of t A with increasing n. If it is not possible to give the exact rate
of growth we are interested in lower and upper bounds for this rate. In the case of a
problem we are also interested in bounds for the running time of algorithms that are
able to solve the problem. We introduce some notations to express knowledge about
the rate of growth or bounds on this rate.
Definition 2.1 Let f : N â†’ N and g : N â†’ N be given.
(i) We say that f is O(g) if there exist positive constants c and n 0 such that 0 â‰¤
f (n) â‰¤ c Â· g(n) for all n â‰¥ n 0 .
(ii) We say that f is Î©(g) if there exist positive constants c and n 0 such that 0 â‰¤
c Â· g(n) â‰¤ f (n) for all n â‰¥ n 0 .
(iii) We say that f is Î˜(g) if there exist positive constants c 1 , c 2 , and n 0 such that
0 â‰¤ c 1 Â· g(n) â‰¤ f (n) â‰¤ c 2 Â· g(n) for all n â‰¥ n 0 .
The three notations define asymptotic upper, lower, and tight bounds, respectively, on
the rate of growth of f . An alternate definition of an asymptotic lower bound is obtained
by replacing â€œfor all n â‰¥ n 0 â€ in 2.1 (iii) by â€œfor infinitely many nâ€. Asymptotic upper
bounds are of practical interest since they give a worst case running time of an
algorithm. It is usually harder to derive nontrivial asymptotic lower bounds, but we
will occasionally be able to give such bounds.
We also use the Î©- and Î˜-notation for problems. With the first notation we indicate
lower bounds on the running time of any algorithm that solves the problem, with the
second notation we indicate that an algorithm with best possible time complexity exists
to solve the problem.
An algorithm A is said to have polynomial time complexity if there exists a polyno-
mial p such that t A (n) = O(p(n)). All other algorithms are said to be of exponential
time complexity (although there are superpolynomial functions not being exponen-
tial). Edmonds (1965) was the first to emphasize the difference between polynomial and
nonpolynomial algorithms. It is now commonly accepted that only algorithms having a
polynomial worst case time complexity should be termed efficient algorithms.
We denote by P the class of decision problems which can be solved by polynomial time
algorithms.
The Eulerian tour problem can easily be solved in polynomial time. Using a result from
graph theory the following algorithm tests whether a connected graph G = (V, E) is
Eulerian.
procedure eulerian(G)
(1) For every v âˆˆ V compute its degree |Î´(v)|.
(2) If all node degrees are even then the graph is Eulerian. If the degree of at least
one node is odd then the graph is not Eulerian.
end of eulerian2.2. Complexity Theory
9
This algorithm runs in time Î˜(n + m). Surprisingly, also the Chinese postman problem
can be solved in polynomial time (Edmonds & Johnson (1973)).
However, many problems (in fact, most of the interesting problems in combinatorial
optimization) can up to date not be solved (and probably are not solvable) by polynomial
time algorithms. From a theoretical viewpoint they could be solved in the following way.
If the answer to an instance I (of a decision problem) is â€œyesâ€, then in a first step some
string s whose length is polynomial in the input size is guessed nondeterministically.
In a second step it is verified that s proves that the problem has a â€œyesâ€ answer. The
verification step is performed (deterministically) in time polynomial both in the length
of s and in the size of I. If the answer to I is â€œnoâ€ then there exists no such string
and the algorithm is assumed to run forever. E.g., in the Hamiltonian cycle problem
the string s could be the encoding of a Hamiltonian cycle (if the graph contains such
a cycle); the length of s is polynomial in the input length, and it can be easily verified
whether s is indeed the encoding of a Hamiltonian cycle.
Obviously this procedure cannot be realized in practice. The formal model enabling
such computations is the so-called nondeterministic Turing machine. For our pur-
poses we can think of the instruction set of an ordinary computer enhanced by the
instruction â€œInvestigate the following two branches in parallelâ€. The time complexity of
a nondeterministic algorithm is the maximal number of elementary steps that is required
to solve a decision problem if it has a â€œyesâ€ answer.
The class of decision problems that can be solved in polynomial time using such non-
deterministic algorithms is called NP.
Note the important point that there is an asymmetry between â€œyesâ€ and â€œnoâ€ answers
here. The question of how to show that a decision problem has a â€œnoâ€ answer is not
considered in this concept.
An important subclass of NP consists of the NP-complete problems. These are the
hardest problems in NP in the sense that if one of them is shown to be in P then
P=NP. Let A be an algorithm for the solution of problem B. We say that a problem
C is polynomially reducible to problem B if it can be solved in polynomial time by
an algorithm that uses A as a subroutine provided that each subroutine call of A only
counts as one step. A problem is then called NP-complete if every problem in NP is
polynomially reducible to it.
The Hamiltonian cycle problem is one member of the broad collection of NP-complete
problems (for a derivation of this result see Johnson & Papadimitriou (1985)).
The question P=NP? is one of the most famous unsolved questions in complexity theory.
Since this question has now been attacked for two decades and since NP-complete
problems proved to be substantially hard in practice it is commonly accepted that
P = NP should be the probable answer to this question (if it can be decided at all).
We want to emphasize again that our representation is kept on an informal level, and it
is intended to give just an idea of the concepts of complexity theory. Especially we have
not considered space complexity which measures the amount of storage an algorithm
requires.
We now discuss how optimization problems like the traveling salesman problem can be
dealt with. With the TSP we associate the following decision problem which can be
analyzed using the above concepts.10
Chapter 2. Basic Concepts
Traveling Salesman Decision Problem
Given the complete graph K n with edge weights c uv and a number b decide if there
exists a Hamiltonian tour in K n with length at most b.
This decision problem is NP-complete (Johnson & Papadimitriou (1985)).
If the traveling salesman problem is in P then obviously also the corresponding decision
problem is in P. An optimization problem having the property that the existence of
a polynomial time algorithm for the solution of an associated decision problem implies
the polynomial solvability of an NP-complete problem, is said to be NP-hard.
On the other hand, assume there exists a polynomial time algorithm for the solution
of the TSP decision problem. If all edge weights are integral and the largest weight in
absolute value of an edge is c then clearly the optimal solution of the traveling salesman
problem is not smaller than âˆ’c Â· n and not larger than c Â· n. Using the algorithm to solve
the decision problem we can find the shortest tour length using the following approach.
procedure tsplength(G)
(1) Set L = âˆ’c Â· n and U = c Â· n.
(2) As long as L < U perform the following steps.
(2.1) Set b =  L+U
2 .
(2.2) If there exists a Hamiltonian tour of length at most b then set U = b, otherwise
set L = b + 1.
end of tsplength
Applying this binary search technique we can find the length of the shortest tour by
at most log(c Â· n) + 1 calls of the solution algorithm for the TSP decision problem.
(Throughout this text we will use log to denote the logarithm with base 2).
To completely solve the optimization problem we have to exhibit an optimal solution.
This is now easily done once the shortest length is known.
procedure tsptour(G)
(1) Let U be the optimal tour length found by algorithm tsplength.
(2) For all u = 1, 2, . . . , n and all v = 1, 2, . . . , n perform the following steps.
(2.1) Set s uv = c uv and c uv = c Â· n + 1.
(2.2) If there does not exist a Hamiltonian tour of length U in the modified graph
then restore c uv = s uv .
end of tsptour2.2. Complexity Theory
11
After execution of this procedure the edges whose weights have not been altered give
the edges of an optimal tour.
The procedures tsplength and tsptour call a polynomial number (in n and log c) of times
the algorithm for the solution of the traveling salesman decision problem. Optimization
problems with the property that they can be polynomially reduced to a decision problem
in NP are called NP-easy. Problems which are both NP-easy and NP-hard (like the
traveling salesman problem) are called NP-equivalent. If P  = NP then no NP-hard
problem can be solved in polynomial time, if P=NP then every NP-easy problem is in
P.
So far we have considered the general traveling salesman problem. One might hope
that there are special cases where the problem can be solved in polynomial time. Un-
fortunately, such cases rarely have practical importance (Burkard (1990), van Dal
(1992), van der Veen (1992), Warren (1993)). For most practical situations, namely
for symmetric distances with triangle inequality, for Euclidean instances, for bipartite
planar graphs, or even for grid graphs, the traveling salesman problem remains NP-hard.
A different important issue is the question of whether algorithms can be designed which
deliver solutions with requested or at least guaranteed quality in polynomial time (poly-
nomial in the problem size and in the desired accuracy). Whereas for other NP-hard
problems such possibilities do exist, there are only discouraging results for the general
TSP. For a particular problem instance let c opt denote the length of a shortest tour and
c H denote the length of a tour computed by heuristic H. There are two basic results
relating these two values.
Theorem 2.2 Unless P=NP there does not exist for any constant r â‰¥ 1 a polynomial
time heuristic H such that c H â‰¤ r Â· c opt for all problem instances.
A proof is given in Sahni & Gonzales (1976).
A fully polynomial approximation scheme for a minimization problem is a heuristic
H which computes for a given problem instance and any Îµ > 0 a feasible solution
satisfying c H â‰¤ (1 + Îµ) Â· c opt in time polynomial in the size of the instance and in Îµ âˆ’1 .
It is an easy exercise to prove that to require polynomiality also in the encoding length
of Îµ is equivalent to require a polynomial algorithm for the exact solution. It is very
unlikely that fully polynomial approximation schemes exist for the traveling salesman
problem since the following result holds.
Theorem 2.3 Unless P=NP there does not exist a fully polynomial approximation
scheme for the Euclidean traveling salesman problem.
A proof can be found in Johnson & Papadimitriou (1985). The result holds in
general for TSPs with triangle inequality.
Despite these theoretical results we can nevertheless design heuristics that determine
good or very good tours in practice. The theorems tell us that for every heuristic there
are however problem instances where it fails badly. There are a few approximation
results for problems with triangle inequality which will be addressed in Chapter 6.
It should be pointed out that the complexity of an algorithm derived by theoretical
analysis might be insufficient to predict its behaviour when applied to real-world in-
stances of a problem. This is mainly due to the fact that only worst case analysis is12
Chapter 2. Basic Concepts
performed which may be different from average behaviour, and, in addition, polynomial
algorithms can cause a large amount of CPU time if the polynomial is not of low degree.
In fact, for practical applications only algorithms having running time at most O(n 3 )
would be rated efficient, but even algorithms with running times as low as O(n 2 ) may
not be applicable in certain situations.
Another point is, that the proof of NP-hardness of a problem does not imply the nonex-
istence of a reasonable algorithm for the solution of problem instances arising in practice.
It is the aim of this study to show that in the case of the traveling salesman problem
algorithms can be designed which are capable of finding good approximate solutions to
even large sized real-world instances within moderate time limits.
2.3 Linear and Integer Programming
Linear and integer programming is not a central topic of this tract. However, at some
points we will make references to concepts and results of linear and integer programming.
We give a short survey on these. Highly recommendable references in this area are the
prize-winning books Schrijver (1986) and Nemhauser & Wolsey (1988).
Let A be an m Ã— n-matrix (constraint matrix), b be an m-vector (right hand side) and c
be an n-vector (objective function), where all entries of A, b, and c are rational numbers.
Given these data the linear programming problem is defined as follows.
Linear Programming Problem
Find a vector x âˆ— maximizing the objective function c T x over the set {x âˆˆ Q | Ax â‰¤ b}.
A linear program may be given in various forms which can all be transformed to the
above. For example we may have equality constraints, nonnegativity conditions for
some variables, or the objective function is to be minimized.
In its general form a linear programming problem is given as
max c T x + d T y
Ax + By â‰¤ a
Cx + Dy = b
x â‰¥ 0
with appropriately dimensioned matrices and vectors.
A fundamental concept of linear programming is duality. The dual linear program
to the program given above (which is then called the primal linear program) is
defined as
min u T a + v T b
u T A + v T C â‰¥ c T
u T B + v T D = d T
u â‰¥ 0.
It is easily verified that the dual of the dual problem is again the primal problem.
One important aspect of the duality concept is stated in the following theorem.2.3. Linear and Integer Programming
13
Theorem 2.4 Let P and D be a pair of dual linear programs as defined above.
Suppose there exist vectors (x âˆ— , y âˆ— ) and (u âˆ— , v âˆ— ) satisfying the constraints of P , resp.,
D. Then we have
(i) The objective function value of (x âˆ— , y âˆ— ) (in problem P ) is less than or equal to
the objective function value of (u âˆ— , v âˆ— ) (in problem D).
(ii) Both problems have optimal solutions and their objective function values are
equal.
Duality exhibits further relations between primal and dual problem. But since they
are not important in the sequel we omit them here. Note in particular, that the dual
problem can be used to give bounds for the optimal value of the primal problem (and
vice versa).
The first algorithm for solving linear programming problems was the Simplex method
invented by Dantzig (1963). Since then implementations of this method have been
considerably improved. Today, even very large sized linear programming problems with
several ten thousands of variables and constraints can be solved routinely in moderate
CPU time (Bixby (1994)). The running time of the Simplex method cannot be bounded
by a polynomial, in fact there are examples where exponential running time is necessary
to solve a problem.
However, the linear programming problem, i.e., the problem of maximizing a linear ob-
jective function subject to linear constraints is in P. This was proved in the famous
papers Khachian (1979) (using the Ellipsoid method) and Karmarkar (1984) (us-
ing an Interior-point method). Though both of these algorithms are polynomial, only
interior point methods are competitive with the Simplex method (Lustig, Marsten
& Shanno (1994)).
These facts illustrate again that complexity analysis is in the first place only a theoretical
tool to assess hardness of problems and running time of algorithms.
A step beyond polynomial solvability is taken if we require feasible solutions to have
integral entries. The integer linear programming problem is defined as follows.
Integer Linear Programming Problem
Let A, b, and c be appropriately dimensioned with rational entries. Find a vector x âˆ—
maximizing the objective function c T x over the set {x âˆˆ Q | Ax â‰¤ b, x integer}.
This problem is NP-complete and no duality results are available. We show that the
traveling salesman problem can be formulated as an integer linear program.
To be able to apply methods of linear algebra to graph theory we associate vectors to
edge sets in the following way. Let G = (V, E) be a graph. If |E| = m then we denote
by Q E the m-dimensional rational vector space where the components of the vectors
x âˆˆ Q E are indexed by the edges uv âˆˆ E. We denote a component by x uv or x e if
e = uv.
The incidence vector x F âˆˆ Q E of an edge set F âŠ† E is defined by setting x F
uv = 1 if
F
uv âˆˆ F and by setting x uv = 0 otherwise. Similarly, if we associate a variable x uv to
each edge uv we denote by x(F ) the formal sum of the variables belonging to the edges
of F .14
Chapter 2. Basic Concepts
Now consider the TSP for the complete graph K n with edge weights c uv . With the
interpretation that x uv = 1 if edge uv is contained in a tour and x uv = 0 otherwise, the
following is a formulation of the TSP as an integer linear program.

min
c uv x uv
uvâˆˆE
x(Î´(v)) = 2,
x(C) â‰¤ |C| âˆ’ 1,
x uv âˆˆ {0, 1},
for all u âˆˆ V,
for all cycles C âŠ† E n , |C| < n,
for all u, v âˆˆ V.
The TSP can be successfully attacked within the framework of linear and integer pro-
gramming for surprisingly large problem sizes. We will comment on this issue in Chap-
ter 12.
2.4 Data Structures
In this section we discuss some data structures that are useful for implementing traveling
salesman problem algorithms. They are all used in the software package TSPX (Reinelt
(1991b)) with which all experiments in this monograph were conducted. The exposition
is based on the books Tarjan (1983) and Cormen, Leiserson & Rivest (1989) on
algorithms and data structures. A further reference on the foundations of algorithms is
Knuth (1973).
2.4.1 Binary Search Trees
A rooted tree is a connected acyclic graph with one distinguished node, the so-called
root or root node of the tree. Therefore, if the graph has n nodes a tree consists of
n âˆ’ 1 edges. The depth of the tree is the length (number of edges) of the longest path
from the root to any other node. Every node v is connected to the root by a unique
path. The length of this path is said to be the depth or the level of v. We say that
a tree is binary if at most two edges are incident to the root node and at most three
edges are incident to every other node.
If node u is the first node encountered when traversing the unique path from v to the
root then u is called father of v, denoted by f [v] in the following. By definition f [r] = 0
if r is the root node. If v is a node in a binary tree then there are at most two nodes
with v as their father. These nodes are called sons of v, and we define one of them to
be the right son and the other one to be the left son of v. Either of them may be
missing. A node without sons is called leaf of the tree. To represent a binary tree we
store for every node v its father f [v], its right son r[v] and its left son l[v]. If one of
them is missing we assign 0 to the respective entry.
Figure 2.1 shows a binary tree with root 5 and leaves 2, 7, 3, and 9.15
2.4. Data Structures
5
9
8
10
4
2
1
6
7
3
Figure 2.1 A binary tree on 10 nodes
By assigning to each node v a number k[v], the key of the node, we can store information
in such a tree.
Let a 1 , a 2 , . . . , a n be a sequence of keys assigned to a set of n nodes.
Definition 2.5 A binary tree on these nodes is called search tree if it satisfies the
following conditions.
(i) If u = r[v] then k[u] â‰¥ k[v].
(ii) If w = l[v] then k[w] â‰¤ k[v].
If the following procedure is called with the root of the search tree as parameter then
it prints the stored numbers in increasing order.
procedure inorder(v)
(1) If v = 0 then return.
(2) Call inorder(l[v]).
(3) Print k[v].
(4) Call inorder(r[v]).
end of inorder
Algorithms to find the smallest key stored in a binary search tree or to check if some
key is present are obvious.
The depth of an arbitrary binary search tree can be as large as n âˆ’ 1 in which case the
tree is a path. Hence checking if a key is present in a binary search tree can take time
O(n) in the worst case. This is also the worst case time for inserting a new key into the
tree.
On the other hand, if we build the tree in a clever way, we could realize it with depth
log n. In such a tree searching can be performed much faster.
This observation leads us to the concept of balanced binary search trees which allow
searching in worst case time O(log n). There are many possibilities for implementing
balanced trees. We describe the so-called red-black trees.16
Chapter 2. Basic Concepts
For the proper definition we have to change our notion of binary search trees. Now we
distinguish between internal nodes (having a key associated with them) and external
nodes (leafs of the tree). Every internal node is required to have two sons, no internal
node is a leaf of the tree.
Definition 2.6 A binary tree is called red-black tree if it satisfies the following
conditions.
(i) Every node is either red or black.
(ii) Every external node (leaf) is black.
(iii) If a node is red then its sons are black.
(iv) Every path from a node down to a leaf contains the same number of black nodes.
It can be shown that a red-black tree with n non-leaf nodes has depth at most 2 log(n+1).
Therefore searching in a red-black tree takes time O(log n).
Insertion of a new key can also be performed in time O(log n) because of the small
depth of the tree. But we have to ensure that the red-black property still holds after
having inserted a key. To do this we have to perform some additional fixing operations.
Basically, we color the newly inserted node red and then reinstall the red-black condition
on the path from the new node to the root. At every node on this path we spend constant
time to check correctness and to fix node colors and keys if necessary. So the overall time
spent for inserting a new node and reestablishing the red-black condition is O(log n).
2.4.2 Disjoint Sets Representation
Very frequently we need to manage a partition of some ground set V = {1, 2, . . . , n} into
disjoint subsets S 1 , S 2 , . . . , S k , i.e., sets satisfying âˆª ki=1 S i = V and S i âˆ© S j = âˆ… for all
i  = j. Operations to be performed are merging two subsets and identifying the subset
containing a given element.
The data structure to store such a partition consists of a collection of trees each repre-
senting one subset. The nodes of each tree correspond to those elements of the ground
set which belong to the respective subset, and the root node of each tree is said to be
the representative of the respective set. For simplification of algorithms we define
here the father of the root to be the root itself.
Suppose we have an initialized data structure representing some partition. Identification
of the subset, i.e., the representative of the subset, to which an element v âˆˆ V belongs
is achieved by the following function.
function find(v)
(1) While f [v]  = v set v = f [v].
(2) Return v.
end of find
Joining two subsets S and T where we are given elements x âˆˆ S and y âˆˆ T is accom-
plished by the following procedure.2.4. Data Structures
17
procedure union(x, y)
(1) Set u = find(x).
(2) Set v = find(y).
(3) Set f [u] = v.
end of union
The representative of the new set is v.
We will exclusively use the disjoint set representation in the following context. The
ground set V is the node set of some graph with edge set E. Initially, the ground set
is partitioned into n sets containing one element each. We then scan the edges of E in
some order depending on the application. If the endnodes of the current edge satisfy
some condition then the sets containing these endnodes are merged. Usually n âˆ’ 1
merge operations are performed so that the final partition consists just of the set V .
The number of find operations is bounded by 2|E|. An application of this principle is
used for implementing Kruskalâ€™s spanning tree algorithm to be discussed in section 2.5.
Without further modifications the above implementation can result in trees which are
paths. This is the worst case for performing find operations. Ideally, we would like to
have trees of depth 1 for set representation. But to achieve this we have to traverse one
of the trees participating in a union operation. On the other hand, this can result in a
running time of O(m 2 ) for m union operations if the wrong trees are chosen.
Fortunately, there are two improvements to overcome these problems. We implement
the find operations with additional path compression and union operations as union
by rank. In addition we now store for each node r the number of nodes n[r] in the tree
rooted at r. The modified procedures are now.
function find and compress(v)
(1) If v  = f [v] then set f [v] = find and compress(f [v]).
(2) Return f [v].
end of find and compress
After execution of this procedure for a node v all nodes on the path from v to the root
(including v) in the previous tree now have the root node as their father.
procedure union by rank(x, y)
(1) Set u = find and compress(x).
(2) Set v = find and compress(y).
(3) If n[u] < n[v] then set f [u] = v and n[v] = n[v] + n[u]. Otherwise set f [v] = u
and n[u] = n[u] + n[v].
end of union by rank18
Chapter 2. Basic Concepts
This procedure makes the root of the larger tree the father of the root of the smaller
tree after the union operation. Looking more closely at this principle one realizes that
it is not necessary to know the exact number of nodes in the trees. It suffices to store
a rank at the root node which is incremented by one if trees of equal rank are merged.
This way one can avoid additions of integers.
Initialization of a single element set is simply done by the following code.
procedure make set(v)
(1) Set f [v] = v and n[v] = 1.
end of make set
The modified implementation of union/find turns out to perform very efficiently for our
purposes.
Theorem 2.7 If m operations are performed using disjoint sets representation by
trees where n of them are make set operations and the other ones are union operations
(by rank) for disjoint sets and find operations (with path compression) then this can be
performed in time O(m log âˆ— n).
A proof of this instructive theorem can be found in Tarjan (1983) or Cormen, Leis-
erson & Rivest (1989). The number log âˆ— n is defined via log (i) n as follows.

n
if i = 0,
(i)
(iâˆ’1)
n) if i > 0 and log (iâˆ’1) n > 0,
log n = log(log
undefined
otherwise,
and then
log âˆ— n = min{i â‰¥ 0 | log (i) n â‰¤ 1}.
In fact, in the above theorem a slightly better bound of O(mÎ±(m, n)) where Î± denotes
the inverse of the Ackermann function can be proved. But this is of no importance for
practical computations since already log âˆ— n â‰¤ 5 for n â‰¤ 2 65536 . So we can speak of
linear running time of the fast union-find algorithm in practice (for our applications).
2.4.3 Heaps and Priority Queues
A heap is a data structure to store special binary trees satisfying an additional heap
condition. These binary trees have the property that except for the deepest level all
levels contain the maximal possible number of nodes. The deepest level is filled from
â€œleft to rightâ€ if we imagine the tree drawn in the plane. To every tree node there is an
associated key, a real number. These keys are stored in a linear array A according to a
special numbering that is assumed for the tree nodes. The root receives number 1 and if
a node has number i then its left son has number 2i and its right son has number 2i + 1.
If a node has number i then its key is stored in A[i]. Therefore, if such a binary tree
has k nodes then the corresponding keys are stored in A[1] through A[k]. The special
property that array A has to have is the following.2.4. Data Structures
19
Definition 2.8 An array A of length n satisfies the heap property if for all 1 < i â‰¤ n
we have A[ 2 i ] â‰¤ A[i].
Stated in terms of binary trees this condition means that the key of the father of a node
is not larger than the key of the node. The heap property implies that the root has the
smallest key among all tree nodes, or equivalently, A[1] is the smallest element of the
array.
Alternatively, we can define the heap property as â€œA[ 2 i ] â‰¥ A[i]â€. This does not make
an essential difference for the following, since only the relative order of the keys is
reversed.
As a first basic operation we have to be able to turn an array filled with arbitrary keys
into a heap. To do this we have to use the subroutine heapify with argument i which
fixes the heap property for the subtree rooted at node i (where it is assumed that the
two subtrees rooted at the left, resp. right, son of i are already heaps).
procedure heapify(i)
(1) Let n be the number of elements in heap A (nodes in the binary tree). If 2i > n
or 2i + 1 > n the array entries A[2i], resp. A[2i + 1] are assumed to be +âˆ.
(2) Let k be the index of i, 2i, and 2i + 1 whose array entry is the smallest.
(3) If k  = i then exchange A[i] and A[k] and perform heapify(k).
end of heapify
It is easy to see that heapify(i) takes time O(h) if h is the length of the longest path
from node i down to a leaf in the search tree. Therefore heapify(1) takes time O(log n).
Suppose we are given an array A of length n to be turned into a heap. Since all leaves
represent 1-element heaps the following procedure does the job.
procedure build heap(A)
(1) For i =  n 2  downto 1 perform heapify(i).
end of build heap
A careful analysis of this procedure shows that an arbitrary array of length n can be
turned into a heap in time O(n). Note that the binary tree represented by the heap is
not necessarily a search tree.
Except for sorting (see section 2.5) we use the heap data structure for implementing
priority queues. As the name suggests such a structure is a queue of elements such
that elements can be accessed one after the other according to their priority. The
first element of such a queue will always be the element with highest priority. For the
following we assume that an element has higher priority than another element if its key
is smaller.
The top element of a heap is therefore the element of highest priority. The most fre-
quently applied operation on a priority queue is to extract the top-priority element and
assure that afterwards the element of the remaining ones with highest priority is in the
first position. This operation is implemented using the heap data structure. We assume
that the current size of the heap is n.20
Chapter 2. Basic Concepts
function extract top(A)
(1) Set t = A[1].
(2) Set A[1] = A[n] and decrease the heap size by 1.
(3) Call heapify(1).
(4) Return t.
end of extract top
Because of the call of heapify(1), extracting the top element and fixing the heap property
needs time O(log n). Inserting a new element is accomplished as follows.
procedure insert key(k)
(1) Increase the heap size by 1 and set i to the new size.
(2) While i > 1 and A[ 2 i ] > k
(2.1) Set A[i] = A[ 2 i ] and i =  2 i .
(3) Set A[i] = k.
end of insert key
Since in the worst case we have to scan the path from the new leaf to the root, a call of
insert key takes time O(log n).
2.4.4 Graph Data Structures
Very frequently we have to store undirected graphs G = (V, E) with |V | = n nodes and
|E| = m edges where n is large, say in the range of 1,000 to 100,000. The number of
edges to be stored depends on the application.
Matrix type data structures are the (node-edge) incidence matrix and the (node-node)
adjacency matrix. The incidence matrix A is an n Ã— m-matrix whose entries a ie are
defined by

1 if i is an endnode of edge e,
a ie =
0 otherwise.
The adjacency matrix is an n Ã— n-matrix B whose entries b ij are defined by

1 if ij is an edge of G,
b ij =
0 otherwise.
Since we need O(nm) or O(n 2 ) storage for these matrices they cannot be used for large
graphs. This also limits the use of distance matrices for the definition of edge weights
in large problem instances. Fortunately, for large TSPs, distances between nodes are
usually given by a distance function.
If we just want to store a graph we can use an edge list consisting of two arrays tail
and head such that tail[e] and head[e] give the two endnodes of the edge e. This is21
2.4. Data Structures
appropriate if some graph is generated edge by edge and no specific operation has to be
performed on it.
Another possibility is to use a system of adjacency lists. Here we store for each node
a list of its adjacent nodes. This is done by an array adj of length 2m containing the
adjacent nodes and an array ap of length n. These arrays are initialized such that the
neighbors of node i are given in adj[ap[i]], adj[ap[i] + 1], through adj[ap[i + 1] âˆ’ 1]. This
data structure is suitable if we have to scan neighbors of a node and if the graph remains
unchanged. Adding edges is time consuming since large parts of the array may have to
be moved.
If we have to add edges and want to avoid moving parts of arrays, then we have to
use linked lists. Since this is our most frequent operation on graphs we have used the
following data structure to store an undirected graph. The two arrays tail and head
contain the endnodes of the edges. The arrays nxtt and nxth are initialized such that
nxtt[e] gives the number of a further edge in this structure having tail[e] as one endnode
and nxth[e] gives the number of a further edge having head[e] as an endnode. An entry 0
terminates a linked list of edges for a node. For each node v the array entry first[v]
gives the number of the first edge having v as an endnode.
To get used to this form of storing a graph we give an example here. Suppose we
have a subgraph of the complete graph on six nodes consisting of the edges {1, 2},
{1, 5}, {2, 5}, {2, 3}, {3, 5}, {4, 5} and {4, 6}. This graph could be stored e.g., by the
assignment shown in Table 2.2.
Index first Index head tail nxth nxtt
1
2
3
4
5
6 6
5
5
7
7
4 1
2
3
4
5
6
7 1
3
2
4
2
1
4 2
5
5
6
3
5
5 0
0
1
0
3
1
4 0
0
2
0
2
3
6
Table 2.2 Example for subgraph data structure
Suppose the current graph has m edges and is stored using this data structure. Adding
a new edge can then be performed in constant time using the following piece of code.
procedure add edge(i, j)
(1) Set tail[m + 1]=i, nxtt[m + 1]=first[i], and first[i] = m + 1.
(2) Set head[m + 1]=j, nxth[m + 1]=first[j], and first[j] = m + 1.
(3) Set m = m + 1.
end of add edge
Of course, if we add more edges than dynamic memory space has been allocated for we
have to allocate additional space.22
Chapter 2. Basic Concepts
2.4.5 Representing Tours
An easy way to store a tour is to use an array t of length n and let t[k] be the k-th node
visited in the tour. However, this is not sufficient as we shall see below. We also have
to impose a direction on the tour. We therefore store with each node i its predecessor
pred[i] and its successor succ[i] in the tour with respect to the chosen orientation.
When using heuristics to find short tours one has to perform a sequence of local modi-
fications of the current tour to improve its length. We explain our method to perform
modifications in an efficient way using the example of 2-opt moves. A 2-opt move
consists of removing two edges from the current tour and reconnecting the resulting
paths in the best possible way. This operation is depicted in Figure 2.3 where broken
arcs correspond to directed paths.
j l j l
k i k i
Figure 2.3 A 2-opt move
Note that we have to have an imposed direction of the tour to be able to decide which
pair of new edges can be added to form a new tour. Adding edges jk and il would result
in an invalid subgraph consisting of two subtours. Furthermore, the direction on one of
the paths has to be reversed which takes time O(n).
Since we have to make a sequence of such 2-opt moves we do not update the tour
structure as in Figure 2.3 but rather store the current tour as a sequence of unchanged
intervals of the starting tour. This is of particular importance for some heuristics where
2-opt moves are only tentative and might not be realized to form the next tour. For
each interval we store the direction in which it is traversed in the current tour. The
result of the 2-opt move in our example would be a doubly linked sequence of intervals
as in Figure 2.4 where we also indicate that the path represented by the interval [k, j]
has been reversed.
[ l , i ]
[ k , j ]
Figure 2.4 Result of the 2-opt move
To make the approach clearer we use concrete node numbers l = 4, i = 7, k = 3, j = 10
and perform another 2-opt move involving nodes 11 and 5 on the path from 4 to 7 and
nodes 8 and 16 on the path from 3 to 10 as in Figure 2.5.23
2.4. Data Structures
10
4
10
4
16 11 16 11
8 5 8 5
3
3
7
7
Figure 2.5 A second 2-opt move
After execution of this move we have the interval sequence shown in Figure 2.6. Note
that the segment between nodes 3 and 8 was reversed before the 2-opt move, so it is
not reversed any more after the move.
[ 4 , 11 ]
[ 8 , 3 ]
[ 7 , 5 ]
[ 16 , 10 ]
Figure 2.6 Result of the second 2-opt move
The new interval sequence was obtained by splitting two intervals and reconnecting the
intervals in an appropriate way. Note that also in the interval representation of a tour
we have to reorient paths of the sequence. But, if we limit the number of moves in such
a sequence by k this can be performed in O(k).
One difficulty has been omitted so far. If we want to perform a 2-opt move involving
four nodes we have to know the intervals in which they are contained to be able to
choose the correct links. We cannot do this efficiently without additional information.
We store with each node its rank in the starting tour. This rank is defined as follows:
An arbitrary node gets rank 1, its successor gets rank 2, etc., until the predecessor of the
rank 1 node receives rank n. Since we know for each interval the ranks of its endnodes
and whether it has been reversed with respect to the starting tour or not, we can check
which interval contains a given node if we store the intervals in a balanced binary search
tree.
Applying this technique the interval containing a given node can be identified in time
O(log k) if we have k intervals.
This way of maintaining tour modifications is extensively used in the implementation
of a Lin-Kernighan type improvement method (see Chapter 7). Experience with this
data structure was also reported in Applegate, ChvÃ¡tal & Cook (1990). They used
splay trees (Tarjan (1983)) instead of red-black trees in their implementation.
Of course, the number of intervals should not become too large because the savings in
execution time decreases with the number of intervals.
Finally, or if we have too many intervals, we have to clear the interval structure and
generate correct successor and predecessor pointers to represent the current tour. This24
Chapter 2. Basic Concepts
is done in the obvious way. Note that the longest path represented as an interval can
remain unchanged, i.e., for its interior nodes successors, predecessors, and ranks do not
have to be altered.
[11] [12] [1] [2]
1 3 5 8
[10] 6 10 [3]
[9] 9 11 [4]
1 (12)
[ 5 , 3 ]
2 7 12 4
[8] [7] [6] [5]
1 3 5 8
6 10
9 11
2 7 12 4
1 3 5 8
9 11
12
4
[ 3 , 12 ]
3 (3)
10
7
6 (7)
[ 5 , 4 ]
6
2
1 (5)
1 (2)
6 (2)
8 (5)
[ 5 , 8 ]
[ 2 , 3 ]
[ 4 , 10 ]
[ 7 , 12 ]
Figure 2.7 An example for representing a tour
To visualize this approach we give a sequence of 2-opt moves starting from a basic tour
together with the resulting interval sequences and search trees in Figure 2.7. Ranks of
nodes are listed in brackets. Each node of the search tree represents an interval. We give
for each node the rank of the endnode with lower rank and in parentheses the number
of nodes in the interval.2.5. Some Fundamental Algorithms
25
2.5 Some Fundamental Algorithms
In this chapter we review some basic algorithms which are not traveling salesman prob-
lem specific but are used as building blocks in many heuristics. For an extensive dis-
cussion we refer again to Cormen, Leiserson & Rivest (1989).
2.5.1 Sorting
Given a set A = {a 1 , a 2 , . . . , a n } of n integer or rational numbers, the sorting problem
consists of finding the sequence of these numbers in increasing (or decreasing) order. In
many cases the numbers will correspond to the weights of the edges of a subgraph.
There is a variety of algorithms we cannot discuss here. One example of a very sim-
ple sorting algorithm shall be given first. This algorithm recursively subdivides a set
into two halves, sorts the subsets and then merges the sorted sequences. Suppose the
numbers are stored in B[1], B[2], through B[n].
procedure mergesort(B, l, u)
(1) If l â‰¥ u âˆ’ 1 sort B[l] through B[u] by comparisons and return.
).
(2) Perform mergesort(B, l,  l+u
2
 + 1, u).
(3) Perform mergesort(B,  l+u
2
(4) Rearrange B to represent the sorted sequence.
end of mergesort
The call mergesort(B, 1, n) sorts the array in time O(n log n). This will follow from
considerations in section 2.5.3 since Step (1) is executed in constant time and Step (4)
can be performed in time O(u âˆ’ l).
Another sorting algorithm makes use of the heap data structure presented in the pre-
vious chapter. Since in a heap the top element is always the smallest one we can
successively generate the sorted sequence of the elements of A using the heap.
procedure heapsort(B)
(1) Call build heap(B).
(2) For i = n downto 1 perform the following steps.
(2.1) Exchange B[1] and B[i].
(2.2) Decrement the heap size by 1.
(2.3) Call heapify(1).
end of heapsort26
Chapter 2. Basic Concepts
After execution of heapsort(A) we have the elements of A sorted in increasing order in
A[n], A[n âˆ’ 1], through A[1].
The running time is easily derived from the discussion in the section on heaps. Step (1)
takes time O(n),  and, since Step (2.3) takes time O(log i), we obtain the overall running
n
time as O(n) + i=1 O(log i) = O(n log n).
Therefore both merge sort and heap sort seem to be more or less equivalent. However,
heap sort has an important advantage. It is able to generate the sorted sequence as
long as needed. If for some reason the remaining sequence is not of interest any more
at some point we can exit from heapsort prematurely.
A final remark is in order. It can be shown that sorting based on the comparison of two
elements cannot be performed faster than in O(n log n) time. So the above discussion
shows that the sorting problem has time complexity Î˜(n log n) (in this computational
model).
Faster sorting algorithms can only be achieved if some assumptions on the input can
be made, e.g., that all numbers are integers between 1 and n. Expected linear running
time of some sorting algorithms can be shown for specific input distributions.
2.5.2 Median Finding
We could also have implemented a sorting algorithm by recursively doing the following
for a set A = {a 1 , a 2 , . . . , a n }. First identify a median of A, i.e., a value a such that half
of the elements of A are below a and half of the elements are above a. More precisely,
we identify a number a such that we can partition A into two sets A 1 and A 2 satisfying
A 1 âŠ† {a i | a i â‰¤ a}, A 2 âŠ† {a i | a i â‰¥ a}, |A 1 | =  n 2 , and |A 2 | =  n 2 . We then sort A 1
and A 2 separately. The concatenation of the respective sorted sequences give a sorting
of A.
In particular for geometric problems defined on points in the plane we often need to
compute horizontal or vertical lines separating the point set into two (approximately
equally sized) parts. For this we need medians with respect to the x- or y-coordinates
of the points.
A natural way to find a median is to sort the n points. The element at position  n 2 
gives a median. However, one can do better as is shown in the following sophisticated
algorithm which is also very instructive. The algorithm requires a subroutine that for a
given input b rearranges an array such that in the first part all elements are at most as
large as b while in the second part all elements are at least as large as b. Assume again
that array B contains n numbers in B[1] through B[n].
function partition(B, b)
(1) Set i = 1 and j = n.
(2) Repeat the following steps until i â‰¥ j.
(2.1) Decrement j by 1 until B[j] â‰¤ b.
(2.2) Increment i by 1 until B[i] â‰¥ b.
(2.3) If i < j exchange B[i] and B[j].27
2.5. Some Fundamental Algorithms
(3) Return j.
end of partition
After execution of this algorithm we have B[l] â‰¤ b for all l = 1, 2, . . . , i and B[l] â‰¥ b for
all l = j, j + 1, . . . , n.
The procedure for finding a median can now be given. In fact, the procedure does a
little bit more. It finds the i-th smallest element of a set.
procedure find ith element(B, i)
(1) Partition B into  n 5  groups of 5 elements each and one last group containing the
remaining elements.
(2) Sort each set to find its â€œmiddleâ€ element. If the last set has even cardinality l
we take the element at position 2 l + 1.
(3) Apply find ith element to find the median b of the set of medians found in Step (2).
(4) Let k = partition(B, b).
(5) If i â‰¤ k then find the i-th smallest element of the lower side of the partition,
otherwise find the (i âˆ’ k)-th smallest element of the higher side of the partition.
end of find ith element
The call find ith element(B,  n+1
2 ) now determines a median of B.
A running time analysis shows that this algorithm runs in linear time which is best
possible since every element of B has to be considered in a median finding procedure.
Hence medians can be found in time Î˜(n).
2.5.3 Divide and Conquer
We have already applied the divide and conquer principle without having named it
explicitly. The basic idea is to divide a problem into two (or more) subproblems, solve
the subproblems, and then construct a solution of the original problem by combining
the solutions of the subproblems in an appropriate way.
Since this is a very important and powerful principle, we want to cite the main result for
deriving the running time of a divide and conquer algorithm of Cormen, Leiserson
& Rivest (1989). Such an algorithm has the following structure where we assume that
it is applied to some set S, |S| = n, to solve some (unspecified) problem.
procedure divide and conquer(S)
(1) Partition S into subproblems S 1 , S 2 , . . . , S a of size less than
n
.
b
(2) For each subproblem S i perform divide and conquer(S i ).
(3) Combine the subproblem solutions to solve the problem for S.
end of divide and conquer28
Chapter 2. Basic Concepts
In the following we assume that f is the running time function for performing Steps (1)
and (3).
Theorem 2.9 gives a formula to compute the running time of divide and conquer algo-
rithms. (It does not matter that nb is not necessarily integral, we can both substitute
 nb  or  nb ).
Theorem 2.9 Let a â‰¥ 1 and b > 1 be constants and let T (n) be defined by the
recurrence T (n) = aT ( nb )+f (n). Then T (n) can be bounded asymptotically (depending
on f ) as follows.
(i) If f (n) = Î˜(n log b aâˆ’Îµ ) for some constant Îµ > 0, then T (n) = Î˜(n log b a ).
(ii) If f (n) = O(n log b a ), then T (n) = Î˜(n log b a log n).
(iii) If f (n) = Î©(n log b a+Îµ ) for some constant Îµ > 0 and if a Â· f ( nb ) â‰¤ c Â· f (n) for some
constant c < 1 and all sufficiently large n, then T (n) = Î˜(f (n)).
This theorem provides a very useful tool. Take as an example the merge sort algorithm.
Step (1) is performed in constant time and Step (4) is performed in time O(u âˆ’ l), hence
f (n) = O(n). For the theorem we have a = 2 and b = 2. Therefore case (ii) applies and
we obtain a running time of Î˜(n log n) for merge sort.
2.5.4 Minimum Spanning Trees
Let G = (V, E), |V | = n, |E| = m, be a connected graph with edge weights c uv for all
uv âˆˆ E. A minimum spanning tree of G is an acyclic connected subset T âŠ† E such
that c(T ) is minimal among these edge sets. Clearly a spanning tree has n âˆ’ 1 edges.
The following algorithm computes a minimum length spanning tree of G (Prim (1957)).
procedure prim(G)
(1) Set T = âˆ… and Q = {1}.
(2) For all i = 2, 3, . . . , n set d[i] = c 1i and p[i] = 1 if {1, i} âˆˆ E, resp., d[i] = âˆ and
p[i] = 0 if {1, i} âˆˆ
/ E.
(3) As long as |T | < n âˆ’ 1 perform the following.
(3.1) Let d[j] = min{d[l] | l âˆˆ V \ Q}.
(3.2) Add edge {j, p[j]} to T and set Q = Q âˆª {j}.
(3.3) For all l âˆˆ V \ Q check if c jl < d[l]. If this is the case set d[l] = c jl and
p[l] = j.
(4) T is a minimum spanning tree of G.
end of prim2.5. Some Fundamental Algorithms
29
The running time of this algorithm is Î˜(n 2 ). It computes in each iteration a mini-
mum spanning tree for the subgraph G Q = (Q, E(Q)), and so upon termination of the
algorithm we have a minimum spanning tree of G.
The implementation of Primâ€™s algorithm given here is best possible if we have complete
or almost complete graphs G. If we want to compute minimum spanning trees for graphs
with fewer edges then other implementations are superior. The idea is to maintain the
nodes not yet connected to the tree in a binary heap (where the keys are the shortest
distances to the tree). Since keys are only decreased, every update in Step (3.3) can
be performed in time O(log n). Because we have to scan the adjacency list of j to see
which distances might be updated, Step (3.3) takes altogether time O(m log n). Finding
the minimum of a heap and updating it has to be performed n âˆ’ 1 times requiring
time O(log n) each time. Therefore we obtain running time O(m log n) for the heap
implementation of Primâ€™s algorithm.
Using more advanced techniques (as e.g., binomial heaps or Fibonacci heaps) one can
even achieve time O(m + n log n).
A different approach to finding a minimum spanning tree was given in Kruskal (1956).
It is particularly suited for sparse graphs.
procedure kruskal(G)
(1) Build a heap of the edges of G (with respect to smaller weights).
(2) Set T = âˆ….
(3) As long as |T | < n âˆ’ 1 perform the following.
(3.1) Get the top edge {u, v} from the heap and update the heap.
(3.2) If u and v belong to different connected components of the graph (V, T ) then
add edge {u, v} to T .
(4) T is a minimum spanning tree of G.
end of kruskal
The idea of this algorithm is to generate minimum weight acyclic edge sets with in-
creasing cardinality until such an edge set of cardinality n âˆ’ 1 is found (which is then a
minimum spanning tree of G).
This algorithm is implemented using fast union-find techniques. Combining results for
maintaining heaps and for applying fast union-find operations we obtain a running time
of O(m log m) for this algorithm.
Instead of using a heap we could sort in a first step all edges with respect to increasing
length and then use only union-find to identify different components. Using a heap we
can stop as soon as |T | = n âˆ’ 1 and do not necessarily have to sort all edges.
If G is not connected a slight modification of the above computes the minimum weight
acyclic subgraph of cardinality n âˆ’ k where k is the number of connected components
of G.30
Chapter 2. Basic Concepts
2.5.5 Greedy Algorithms
The greedy algorithm is a general algorithmic principle for finding feasible solutions of
combinatorial optimization problems. It is characterized by a myopic view, performing
the construction of a feasible solution step by step based only on local knowledge of the
problem.
To state it in general, we need a suitable definition of a combinatorial optimization
problem. Let E = {e 1 , e 2 , . . . , e m } be a finite set where each element has an associated
the set of so-called feasible solutions. For a set
weight c i , 1 â‰¤ i â‰¤ m, and I âŠ† 2 E be 
I âŠ† E, its weight is given as c(I) = e i âˆˆI c i . The optimization problem consists of
finding a feasible solution I âˆ— âˆˆ I such that c(I âˆ— ) = min{c(I) | I âˆˆ I}.
The greedy algorithm works as follows.
procedure greedy(I, c)
(1) Sort E such that c 1 â‰¤ c 2 â‰¤ . . . , c m .
(2) Set I = âˆ….
(3) For i = 1, 2, . . . , m:
(3.1) If I âˆª {e i } âˆˆ I, then set I = I âˆª {e i }.
end of greedy
Due to the sorting in Step (1), the algorithm needs time Î©(m log m). It does not need
more time, if the test â€œI âˆª {e i } âˆˆ I?â€ can be performed in time O(log m).
Inspite of its simplicity, the greedy principle has some important aspects, in particular
in the context of matroids and independence systems, but we do not elaborate on this.
In general, however, it delivers feasible solutions of only moderate quality. Note, that
Kruskalâ€™s algorithm for computing minimum spanning trees is nothing but the greedy
algorithm. Because the spanning tree problem is essentially an optimization problem
over a matroid, the greedy algorithm is guaranteed to find an optimal solution in this
case.Chapter 3
Related Problems and Applications
For several practical problems it is immediately seen that the TSP provides the suit-
able optimization model. In many cases, however, this is either not straightforward
or the pure TSP has to be augmented by further constraints. In this chapter we first
discuss some optimization problems that are related to the TSP. Some of them can
be transformed to a pure TSP in a reasonable way, others are at least related in the
sense that algorithms developed for the TSP can be adapted to their solution. Then
we survey some application areas where the TSP or its relatives can be used to treat
practical problems. Further aspects are found in Garfinkel (1985). Finally, we intro-
duce the collection of sample problem instances that we will use in the sequel for testing
algorithms.
3.1 Some Related Problems
Note that we can assume without loss of generality that a symmetric TSP is always a
minimization problem and that all distances c ij are positive. First, if we are looking
for the longest Hamiltonian cycle we can multiply all edge weights by âˆ’1 and solve
a minimization problem. Second, we can add a constant to all edge weights without
affecting the ranking of tours with respect to their lengths. Hence all edge weights can
be made positive. In the sequel we will not explicitly mention this fact. Without loss
of generality we can also assume that all edge lengths are integer numbers.
Observe, however, that approximation results may be no longer valid after having mod-
ified edge weights. For example, if a very large constant is added to each edge weight,
then every tour is near optimal.
3.1.1 Traveling Salesman Problems in General Graphs
There may be situations where we want to find shortest Hamiltonian tours in arbitrary
graphs G = (V, E), in particular in graphs which are not complete. If it is required that
each node is visited exactly once and that only edges of the given graph must be used
then we can do the following.
Add all missing edges giving them a sufficiently large

c
weight M (e.g., M =
ijâˆˆE ij ) and apply an algorithm for the symmetric TSP in
complete graphs. If this algorithm terminates with an optimal tour containing none of
the edges with weight M , then this tour is also optimal for the original problem. If an
edge with weight M is contained in the optimal tour then the original graph does not
G. Reinelt: The Traveling Salesman, LNCS 840, pp. 31-41, 1994.
ï›™ Springer-Verlag Berlin Heidelberg 199432
Chapter 3. Related Problems and Applications
contain a Hamiltonian cycle. Note however, that heuristics cannot guarantee to find a
tour in G even if one exists.
The second way to treat such problems is to allow that nodes may be visited more than
once and edges be traversed more than once. If the given graph is connected we can
always find a feasible roundtrip under this relaxation. This leads us to the so-called
graphical traveling salesman problem.
3.1.2 The Graphical Traveling Salesman Problem
For an arbitrary connected graph G with edge weights, the graphical traveling sales-
man problem (GTSP) consists of finding a closed walk in G for the salesman to visit
every city requiring the least possible total distance. The salesman may only use edges
of G, but is allowed to visit a city or to traverse an edge more than once. This is
sometimes a more practical definition of the TSP because we may have cases where the
underlying graph of connections does not even contain a Hamiltonian cycle and where
some direct transitions from a city i to a city j are not possible. In the formulation as
a GTSP we explicitly stick to the given graph.
To avoid degenerate situations we have to have nonnegative edge weights. Otherwise
we could use an edge as often as we like in both directions to achieve an arbitrarily
small length of the solution. We transform a GTSP to a symmetric TSP as follows.
Let K n = (V n , E n ) be the complete graph on n nodes. For every pair i, j of nodes we
compute the shortest path from i to j in the graph G. The length d ij of this path is
taken as the weight of edge ij in K n . Now the shortest Hamiltonian tour in K n can be
transformed to a shortest closed walk in G visiting all nodes.
Naddef & Rinaldi (1993) discuss relation between the TSP and the GTSP in detail.
3.1.3 The Shortest Hamiltonian Path Problem
We are given a graph G = (V, E) with edge weights c ij . Two special nodes, say v s and
v t , of V are also given. The task is to find a path from v s to v t visiting each node of V
exactly once with minimal length, i.e., to find the shortest Hamiltonian path in G from
v s to v t .
This problem can be solved as a standard TSP in two ways.
a) Choose M sufficiently large and assign weight âˆ’M to the edge from v s to v t .
Then compute the optimal traveling salesman tour in this graph. This tour has
to contain edge v s v t and thus solves the Hamiltonian path problem.
b) Add a new node 0 to V and edges from 0 to v s and to v t with weight 0. Each
Hamiltonian tour in this new graph corresponds to a Hamiltonian path from v s
to v t in the original graph with the same length.
If the terminating point of the Hamiltonian path is not fixed, then we can solve the
problem by introducing a new node 0 and adding edges from all nodes v âˆˆ V \ {v s } to 0
with zero length. Now we can solve the Hamiltonian path problem with starting point
v s and terminating point v t = 0 which solves the original problem.
If neither starting point nor terminating point are specified, then we just add node 0
and connect all other nodes to 0 with edges of length zero. In this new graph we solve
the standard TSP.3.1. Some Related Problems
33
3.1.4 Hamiltonian Path and Cycle Problems
Sometimes it has to be checked if a given graph G = (V, E) contains a Hamiltonian
cycle or path at all. This question can be answered by solving a symmetric TSP in the
complete graph K n (with n = |V |) where all edges of the original graph obtain weight
1 and all other edges obtain weight 2. Then G contains a Hamiltonian cycle if and only
if the shortest Hamiltonian cycle in K n has length n. If this shortest cycle has length
n + 1, then G is not Hamiltonian, but contains a Hamiltonian path.
3.1.5 The Asymmetric Traveling Salesman Problem
If the cost of traveling from city i to city j is not necessarily the same as of traveling
from city j to city i, then an asymmetric traveling salesman problem has to be solved.
Let D = (W, A), W = {1, 2, . . . , n}, A âŠ† W Ã— W , and d ij be the arc weight of (i, j) âˆˆ A.
We define a graph G = (V, E) by
V = W âˆª {n + 1, n + 2, . . . , 2n},
E = {(i, n + i) | i = 1, 2, . . . , n}
âˆª {(n + i, j) | (i, j) âˆˆ A}.
Edge weights are assigned as follows
c i,n+i = âˆ’ M for i = 1, 2, . . . , n,
c n+i,j = d ij for (i, j) âˆˆ A,

where M is a sufficiently large number, e.g., M = (i,j)âˆˆA d ij . It is easy to see that
for each directed Hamiltonian cycle in D with length d D there is a Hamiltonian cycle
in G with length c G = d D âˆ’ nM . In addition, since an optimal tour in G contains all
edges with weight âˆ’M , it induces a directed Hamiltonian cycle in D. Hence we can
solve asymmetric TSPs as symmetric TSPs.
3.1.6 The Multisalesmen Problem
We give the asymmetric version of this problem. Instead of just one salesman now there
are m salesmen available who are all located in a city n + 1 and have to visit cities
1, 2, . . . , n. The task is to select some (or all) of these salesmen and assign tours to
them such that in the collection of all these tours together each city is visited exactly
once. The activation of salesman j incurs a fixed cost w j . The cost of the tour of
salesman j is the sum of the intercity distances of his tour (starting at and returning
to city n + 1). The m-salesmen problem (m-TSP) now consists of selecting a subset
of the salesmen and assigning a tour to each of them such that each city is visited by
exactly one salesman and such that the total cost of visiting all cities this way is as
small as possible.
In Bellmore & Hong (1974) it was observed that this problem can be transformed
to an asymmetric TSP involving only one salesman. We give their construction.34
Chapter 3. Related Problems and Applications
Let D = (V, A) be the digraph where V = {1, 2, . . . , n, n + 1} and A âŠ† V Ã— V gives the
possible transitions between cities. Let d ij be the distance from city i to city j.
We construct a new digraph D = (V  , A  ) as follows.
V  = V âˆª {n + 2, n + 3, . . . , n + m},
A  = A
âˆª {(n + i, j) | 2 â‰¤ i â‰¤ m, (n + 1, j) âˆˆ A}
âˆª {(j, n + i) | 2 â‰¤ i â‰¤ m, (j, n + 1) âˆˆ A}
âˆª {(n + i, n + i âˆ’ 1) | 2 â‰¤ i â‰¤ m}.
The weights d  ij for the arcs (i, j) in A  are defined via
d  ij =
d  n+i,j
d  j,n+i
= d n+1,j +
= d j,n+1 +
d ij , for 1 â‰¤ i â‰¤ n, 1 â‰¤ j â‰¤ n, (i, j) âˆˆ A,
1
w ,
2 i
1
2 w i ,
for 1 â‰¤ i â‰¤ m, 1 â‰¤ j â‰¤ n, (n + 1, j) âˆˆ A,
for 1 â‰¤ i â‰¤ m, 1 â‰¤ j â‰¤ n, (j, n + 1) âˆˆ A,
d  n+i,n+iâˆ’1 = 12 w iâˆ’1 âˆ’ 12 w i , for 2 â‰¤ i â‰¤ m.
It is not difficult to verify that the shortest tour in D  relates to an optimal solution of
the corresponding m-salesmen problem for D.
Observe in addition, that with an easy modification we can require that every salesman
is to be activated. We simply eliminate all edges (n + i, n + i âˆ’ 1) for 2 â‰¤ i â‰¤ m. Of
course, the fixed costs w j can now be ignored.
A different transformation is given in Jonker & Volgenant (1988). Solution algo-
rithms are discussed in Gavish & Srikanth (1986).
3.1.7 The Rural Postman Problem
We are given a graph G = (V, E) with edge weights c ij and a subset F âŠ† E. The Rural
Postman Problem consists of finding a shortest closed walk in G containing all edges
in F . If F = E we have the special case of a Chinese postman problem which can be
solved in polynomial time using matching techniques (Edmonds & Johnson (1973)).
The standard symmetric TSP can easily be transformed to a rural postman problem.
Therefore, in general the rural postman problem is NP-hard. In Chapter 11 we will en-
counter a special version of the rural postman problem and find approximative solutions
for it using TSP methods.
3.1.8 The Bottleneck Traveling Salesman Problem
Instead of tours with minimal total length one searches in this problem for tours whose
longest edge is as short as possible. This bottleneck traveling salesman problem
can be solved by a sequence of TSPs. To see this observe that the absolute values of the
distances are not of interest under this objective function. We may reduce distances as
long as they compare exactly as before. Hence we may assume that we have at most
1
1
2 n(n âˆ’ 1) different distances and that the largest of them is not greater than 2 n(n âˆ’ 1).
We now solve problems of the following kind for some parameter b.3.2. Practical Applications of the TSP
35
â€œIs there a Hamiltonian cycle of the original graph consisting only of edges with length
at most b?â€
This problem can be transformed to a standard TSP. By performing a binary search
on the parameter b (starting with b = 14 n(n âˆ’ 1)) we can identify the smallest such b
leading to a â€œyesâ€ answer by solving at most O(log n) TSPs.
3.1.9 The Prize Collecting Traveling Salesman Problem
We are given a graph G = (V, E) with edge weights c ij , node weights u i (representing
benefits received when visiting the respective city), and a special base node v 0 (with
c v 0 = 0). The Prize Collecting Traveling Salesman Problem consists of finding
a cycle in G containing the node v 0 such that the sum of the edge weights of the cycle
minus the sum of the benefits of the nodes of the cycle is minimized. We can get rid of
the node weights if we substitute the edge weights c ij by c ij âˆ’ 12 v i âˆ’ 12 v j . Now the prize
collection TSP amounts to finding a shortest cycle in G containing v 0 . More details are
given in Balas (1989) and Ramesh, Yoon & Karwan (1992).
We have seen that a variety of problems can be transformed to symmetric TSPs or are
at least related to it. However, each such transformation has to be considered with
some care before actually trying to use it for practical problem solving. E. g., the
shortest path computations necessary to treat a GTSP as a TSP take time O(n 3 ) which
might not be acceptable in practice. Most transformations require the introduction of
a large number M . This can lead to numerical problems or may even prevent finding
feasible solutions at all using only heuristics. In particular, for LP-based approaches,
the usage of the â€œbig M â€ cannot be recommended in general. But, in any case, these
transformations provide a basic means for using a TSP code to treat related problems.
3.2 Practical Applications of the TSP
Since we are aiming at the development of algorithms and heuristics for practical travel-
ing salesman problem solving we give a survey on some of the possible applications. The
list is not complete but covers the most important cases. In addition we have included
problems which cannot be transformed to pure TSPs, but which can be attacked using
variants of the methods to be described later.
3.2.1 Drilling of Printed Circuit Boards
The drilling problem for printed circuit boards (PCBs) is a standard application of the
symmetric traveling salesman problem. To connect a conductor on one layer with a
conductor on another layer or to position (in a later stage of the PCB production) the
pins of integrated circuits, holes have to be drilled through the board. The holes may
be of different diameters. To drill two holes of different diameters consecutively, the
head of the machine has to move to a tool box and change the drilling equipment. This36
Chapter 3. Related Problems and Applications
is quite time consuming. Thus it is clear at the outset that one has to choose some
diameter, drill all holes of the same diameter, change the drill, drill the holes of the
next diameter etc.
Thus, this drilling problem can be viewed as a sequence of symmetric traveling salesman
problems, one for each diameter resp. drill, where the â€œcitiesâ€ are the initial position
and the set of all holes that can be drilled with one and the same drill. The â€œdistanceâ€
between two cities is the time it takes to move the head from one position to the other.
The goal is to minimize the travel time for the head of the machine.
We will discuss an application of the drilling problem in depth in Chapter 11.
3.2.2 X-Ray Crystallography
A further direct application of the TSP occurs in the analysis of the structure of crystals
(Bland & Shallcross (1989), Dreissig & Uebach (1990)). Here an X-ray diffrac-
tometer is used to obtain information about the structure of crystalline material. To
this end a detector measures the intensity of X-ray reflections of the crystal from various
positions. Whereas the measurement itself can be accomplished quite fast there is a con-
siderable overhead in positioning time since up to 30,000 positions have to be realized
for some experiments. In the two examples that we refer to, the positioning involves
moving four motors. The time needed to move from one position to the other can be
computed very accurately. For the experiment the sequence in which the measurements
at the various positions are taken is irrelevant. Therefore, in order to minimize the total
positioning time the best sequence for the measurements has to be determined. This
problem can be modeled as a symmetric TSP.
3.2.3 Overhauling Gas Turbine Engines
This application was reported by Plante, Lowe & Chandrasekaran (1987) and
occurs when gas turbine engines of aircrafts have to be overhauled. To guarantee a
uniform gas flow through the turbines there are so-called nozzle-guide vane assemblies
located at each turbine stage. Such an assembly basically consists of a number of
nozzle guide vanes affixed about its circumference. All these vanes have individual
characteristics and the correct placement of the vanes can result in substantial benefits
(reducing vibration, increasing uniformity of flow, reducing fuel consumption). The
problem of placing the vanes in the best possible way can be modeled as a symmetric
TSP.
3.2.4 The Order-Picking Problem in Warehouses
This problem is associated with material handling in a warehouse (Ratliff & Rosen-
thal (1981)). Assume that at a warehouse an order arrives for a certain subset of the
items stored in the warehouse. Some vehicle has to collect all items of this order to
ship them to the customer. The relation to the TSP is immediately seen. The storage
locations of the items correspond to the nodes of the graph. The distance between two
nodes is given by the time needed to move the vehicle from one location to the other.3.2. Practical Applications of the TSP
37
The problem of finding a shortest route for the vehicle with minimal pickup time can
now be solved as a TSP. In special cases this problem can be solved easily (see van Dal
(1992) for an extensive discussion).
3.2.5 Computer Wiring
A special case of connecting components on a computer board is reported in Lenstra
& Rinnooy Kan (1974). Modules are located on a computer board and a given subset
of pins has to be connected. In contrast to the usual case where a Steiner tree connection
is desired, here the requirement is that no more than two wires are attached to each
pin. Hence we have the problem of finding shortest Hamiltonian paths with unspecified
starting and terminating points.
A similar situation occurs for the so-called testbus wiring. To test the manufactured
board one has to realize a connection which enters the board at some specified point,
runs through all the modules, and terminates at some specified point. For each module
we also have a specified entering and leaving point for this test wiring. This problem also
amounts to solving a Hamiltonian path problem with the difference that the distances
are not symmetric and that starting and terminating point are specified.
3.2.6 Clustering of a Data Array
This application is also reported in Lenstra & Rinnooy Kan (1974). An (r, s)-
matrix A = (a ij ) is given representing relationships between two finite sets of elements
R = {R 1 , R 2 , . . . , R r } and S = {S 1 , S 2 , . . . , S s }. The entry a ij gives the strength of
the relationship between R i âˆˆ R and S j âˆˆ S. The task is to identify clusters of highly
related elements.
To this end, a permutation of the rows and columns of A has to be found which max-
imizes the sum of all products of horizontally or vertically adjacent pairs of entries of
A. We transform this problem as follows.
If Ï and Ïƒ are permutations of R and S, respectively, then the corresponding measure
of effectiveness is
ME(Ï, Ïƒ) =
sâˆ’1 
r

a i,Ïƒ(j) a i,Ïƒ(j+1) +
j=1 i=1
râˆ’1 
s

a Ï(i),j a Ï(i+1),j .
i=1 j=1
The two terms can be evaluated separately. We only consider the first one. Let V =
{1, 2, . . . , s} and E = V Ã— V . We define weights for the edges of G = (V, E) by
c ij = âˆ’
r

a ki a kj .
k=1
Then the problem of maximizing the first term amounts to finding a shortest Hamilto-
nian path in G with arbitrary starting and terminating node. The clustering problem
is then solved as two separate such problems, one for the rows and one for the columns.
The special cases where A is symmetric or where A is a square matrix and where we
only allow simultaneous permutations of rows and columns lead to a single Hamiltonian
path problem.38
Chapter 3. Related Problems and Applications
3.2.7 Seriation in Archeology
Suppose archeologists have discovered a graveyard and would like to determine the
chronological sequence of the various gravesites. To this end each gravesite is classi-
fied according to the types of items contained in it. A distance measure between two
gravesites is introduced reflecting the diversity between their respective contents. A very
likely chronological sequence can be found by computing the shortest Hamiltonian path
in the graph whose nodes correspond to the gravesites and where distances are given
due to the criterion above. In fact, this was one of the earliest applications mentioned
for the TSP.
3.2.8 Vehicle Routing
Suppose that in a city n mail boxes have to be emptied every day within a certain
period of time, say 1 hour. The problem is to find the minimal number of trucks to do
this and the shortest time to do the collections using this number of trucks. As another
example, suppose that customers require certain amounts of some commodities and a
supplier has to satisfy all demands with a fleet of trucks. Here we have the additional
problem to assign customers to trucks and to find a delivery schedule for each truck so
that its capacity is not exceeded and the total travel cost is minimized.
The vehicle routing problem is solvable as a TSP if there is no time constraint or if
the number of trucks is fixed (say m). In this case we obtain an m-salesmen problem.
Nevertheless, one can apply methods for the TSP to find good feasible solutions for this
problem (see Lenstra & Rinnooy Kan (1974)).
3.2.9 Scheduling
We are given n jobs that have to be performed on some machine. The time to process
job j is t ij if i is the job performed immediately before j (if j is the first job then its
processing time is t 0j ). The task is to find an execution sequence for the jobs such that
the total processing time is as short as possible.
We define the directed graph D = (V, A) with node set V = {0, 1, 2, . . . , n} and arc
set A = {1, 2, . . . , n} Ã— {1, 2, . . . , n} âˆª {(0, i) | i = 1, 2, . . . , n}. Arc weights are t ij for
(i, j) âˆˆ A. The scheduling problem can now be solved by finding a shortest (directed)
Hamiltonian path starting at node 0 with arbitrary terminating node.
Sometimes it is requested that the machine returns to its initial state after having
performed the last job. In this case we add arcs (i, 0) for every i = 1, 2 . . . , n where t i0
is the time needed to return to the initial state if job i was performed last. Now the
scheduling problem amounts to solving the asymmetric TSP in the new digraph.
Suppose the machine in question is an assembly line and that the jobs correspond to
operations which have to be performed on some product at the workstations of the line.
In such a case the primary interest would lie in balancing the line. Therefore instead of
the shortest possible time to perform all operations on a product the longest individual
processing time needed on a workstation is important. To model this requirement a
bottleneck TSP is more appropriate.3.2. Practical Applications of the TSP
39
In Lenstra & Rinnooy Kan (1974) it is shown that the following job-shop scheduling
problem can be transformed to an asymmetric TSP. We are given n jobs that have to
be processed on m machines. Each job consists of a sequence of operations (possibly
more than m) where each operation has to be performed on one of the machines. The
operations have to be performed one after the other in a sequence which is given in ad-
vance. As a restriction we have that no passing is allowed (we have the same processing
order of jobs on every machine) and that each job visits each machine at least once.
The problem is to find a schedule for the jobs that minimizes the total processing time.
3.2.10 Mask Plotting in PCB Production
For the production of each layer of a printed circuit board, as well as for layers of
integrated semiconductor devices, a photographic mask has to be produced. In our case
for printed circuit boards this is done by a mechanical plotting device. The plotter
moves a lens over a photosensitive coated glass plate. The shutter may be opened or
closed to expose specific parts of the plate. There are different apertures available to
be able to generate different structures on the board. Two types of structures have
to be considered. A line is exposed on the plate by moving the closed shutter to one
endpoint of the line, then opening the shutter and moving it to the other endpoint of
the line. Then the shutter is closed. A point type structure is generated by moving the
appropriate aperture to the position of that point then opening the shutter just to make
a short flash, and then closing it again. Exact modeling of the plotter control problem
leads to a problem more complicated than the TSP and also more complicated than the
rural postman problem.
We will discuss an application of the plotting problem in Chapter 11.
3.2.11 Control of Robots
In order to manufacture some workpiece a robot has to perform a sequence of operations
on it (drilling of holes of different diameters, cutting of slots, planishing, etc.). The
task is to determine a sequence to perform the necessary operations that leads to the
shortest overall processing time. A difficulty in this application arises because there
are precedence constraints that have to be observed. So here we have the problem of
finding the shortest Hamiltonian path (where distances correspond to times needed for
positioning and possible tool changes) that satisfies certain precedence relations between
the operations. This problem cannot be formulated as a TSP in a straightforward way,
but can be treated by applying methods similar to those presented in the forthcoming
chapters.40
Chapter 3. Related Problems and Applications
3.3 The Test Problem Instances
Throughout this tract we will use a set of sample problems compiled from various sources
to compare the different approaches and to examine the behaviour of algorithms with
respect to problem sizes.
To choose a suitable test set one has to decide between contradicting goals.
â€“ Too many computational results might bore the reader.
â€“ Too few results may not exhibit too much insight.
â€“ Problems from different sources might not be comparable in a fair way because
distance computations may be more complicated and time consuming in one case.
â€“ Not all problems are suitable for every approach.
To overcome these difficulties we have chosen to proceed as follows.
â€“ We have selected a set of sample problem instances which can always be treated
by our methods and which are of the same type. This set consists of twenty-four
Euclidean problems in the plane of sizes from 198 to 5934 nodes and is given
in Table 3.1. This table also gives the currently best known upper and lower
bounds for the respective problems. A number in boldface indicates that an
optimal solution is known (and proved!).
Problem
d198
lin318
fl417
pcb442
u574
p654
rat783
pr1002
u1060
pcb1173
d1291
rl1323
fl1400
u1432
fl1577
d1655
vm1748
rl1889
u2152
pr2392
pcb3038
fl3795
fnl4461
rl5934
Size
198
318
417
442
574
654
783
1002
1060
1173
1291
1323
1400
1432
1577
1655
1748
1889
2152
2392
3038
3795
4461
5934
Bounds
15780
42029
11861
50778
36905
34643
8806
259045
224094
56892
50801
270199
[19849,20127]
152970
[22137,22249]
62128
336556
316536
[64163,64294]
378032
137694
[28594,28772]
182566
[554070,556146]
Table 3.1 Bounds for sample problems
â€“ In addition we sometimes report about results on other problems. Most of these
problem instances are contained in the library TSPLIB of traveling salesman3.3. The Test Problem Instances
41
problem instances (see Reinelt (1991a)) and are therefore at the disposal of the
reader to conduct own experiments.
In our experiments, we have almost completely dispensed of random problem instances.
Only at some points we have included a few random problems for extrapolating CPU
times. For these problem instances, the points are located in a square and are drawn
independently from a uniform distribution. To our opinion, this should be the primary
reason for considering random problems. With respect to the assessment of algorithms
one should prefer the treatment of real instances (if available) because these are the
problems whose solution is of interest. Moreover, real problems have properties that
cannot be modeled by random distributions in an appropriate way.
Concerning the CPU times that are either explicitly given or presented in a graphical
display the following remarks apply.
â€“ All CPU times are given in seconds on the SUN SPARCstation 10/20. All software
(unless otherwise noted) has been written in C and was compiled using cc with
option -O4 under the operating system SUNOS 4.1.2. The function get rusage
was used for measuring running times.
â€“ Distance matrices are not used. Except for precomputed distances for candidate
sets, distances are always computed by evaluating the Euclidean distance function
and not by a matrix-lookup.
â€“ If a figure displays CPU times for problems up to size 6,000 then these are times
for the 24 standard problem instances listed in Table 3.1.
â€“ If a figure displays CPU times for problems up to size 20,000 then this includes
in addition the times for further problem instances, in particular for the real
problems rl11849, brd14051, and d18512, and for random problems of size 8,000
to 20,000.
Usually, we will not give the explicit length of tours produced by the various heuristics.
Rather we give their quality with respect to the lower bounds of Table 3.1. More
precisely, if c H is the length of a tour computed by heuristic H and if c L is the lower
bound of Table 3.1 for the respective problem instance, we say that the heuristic tour
has quality 100 Â· (c H /c L âˆ’ 1) percent.Chapter 4
Geometric Concepts
Many traveling salesman problem instances arising in practice have the property that
they are defined by point sets in the 2-dimensional plane (e.g., drilling and plotting
problems to be discussed in Chapter 11). Though true distances can usually be not
given exactly or only by very complicated formulae, they can very well be approxi-
mated by metric distances. To speed up computations we can therefore make use of
geometric properties of the point set. In this chapter we introduce concepts for deriv-
ing informations about the structure of point sets in the plane and review some basic
algorithms. A textbook on computational geometry is Edelsbrunner (1987).
4.1 Voronoi Diagrams
Although known since quite some time (Voronoi (1908)) the Voronoi diagram has
only recently received the attention of many researchers in the field of computational
geometry. It has several attractive features and puts particular emphasis on proximity
relations between points.
Let S = {P 1 , P 2 , . . . , P n } be a finite subset of R m and let d : R m Ã— R m âˆ’â†’ R be a
metric. We define the Voronoi region VR(P i ) of a point P i via
VR(P i ) = {P âˆˆ R m | d(P, P i ) â‰¤ d(P, P j ) for all j = 1, 2, . . . , n, j = i},
i.e., VR(P i ) is the set of all points that are at least as close to P i as to any other point
of S. The set of all n Voronoi regions is called the Voronoi diagram VD(S) of S.
Other names are Dirichlet tessellation or Thiessen tessellation. In the following
we call the elements of S generators.
We consider only the 2-dimensional case. With each generator P i we associate its
Cartesian coordinates (x i , y i ). For the first part we assume that d is the Euclidean
metric (L 2 ), i.e.,

d((x 1 , y 1 ), (x 2 , y 2 )) = (x 1 âˆ’ x 2 ) 2 + (y 1 âˆ’ y 2 ) 2 .
For two generators P i and P j we define the perpendicular bisector B(P i , P j ) = {P âˆˆ
R 2 | d(P, P i ) = d(P, P j )}. If we define the half space B ij = {x âˆˆ R 2 | d(P i , x) â‰¤
d(P j , x)} then we see that
n

VR(P i ) =
B ij .
j=1
j = i
G. Reinelt: The Traveling Salesman, LNCS 840, pp. 42-63, 1994.
ï›™ Springer-Verlag Berlin Heidelberg 19944.1. Voronoi Diagrams
43
This implies that in the case of the Euclidean metric the Voronoi regions are convex
polygons with at most n âˆ’ 1 vertices. Figure 4.1 shows the Voronoi diagram for the
points defining the traveling salesman problem instance rd100, a random problem on
100 points.
Figure 4.1 Voronoi diagram for rd100 (L 2 -metric)
We call nonempty intersections of two or more Voronoi regions Voronoi points if they
have cardinality 1 and Voronoi edges otherwise. Note that every Voronoi point is
the center of a circle through (at least) three generators and every Voronoi edge is a
segment of a perpendicular bisector of two generators.
A generator configuration is called degenerate if there are four generators lying on a
common circle. We say that two Voronoi regions are adjacent if they intersect. Note
that only in degenerate cases two regions can intersect in just one point. Degeneracy
does not occur in Figure 4.1 as is usually the case for randomly generated points.
Some observations are important.
Proposition 4.1 The Voronoi diagram of a generator P i is unbounded if and only if
P i lies on the boundary of the convex hull of S.
Proof. If VR(P i ) is bounded then P i is contained in the interior of the convex hull of
those generators whose regions are adjacent to VR(P i ). On the other hand, if VR(P i )
is unbounded it cannot be situated in the interior of conv(S).44
Chapter 4. Geometric Concepts
An unbounded region does not necessarily imply that a generator is a vertex of the
convex hull. Consider for example the case where all generators are located on a line.
Then all regions are unbounded, but only two generators define the convex hull of the
set.
Proposition 4.2 If d(P i , P j ) â‰¤ d(P i , P k ) for all k = i, j then VR(P i ) and VR(P j )
intersect in a Voronoi edge.
Proof. If P j is a nearest neighbor of P i then VR(P j ) and VR(P i ) intersect by definition.
If they intersect in only one point then this Voronoi point is the center of a circle through
P i , P j , and at least two more generators. At least two of these generators must be nearer
to P i than P j .
Proposition 4.3 The Voronoi diagram of n generators has at most 2n âˆ’ 4 Voronoi
points and at most 3n âˆ’ 6 Voronoi edges.
Proof. This property follows easily from Eulerâ€™s formula stating the relation between
the number of vertices, edges and facets of a convex polygon in the 3-dimensional
Euclidean space. Namely, if n v , n e , and n f denote the respective numbers we have
n f + n v = n e + 2.
We can apply this formula to a Voronoi diagram if we connect all Voronoi edges ex-
tending to infinity to a common imaginary Voronoi point (edges extending to infinity
in both directions are ignored here). We have n f = n Voronoi regions. Every Voronoi
point is the endnode of at least three Voronoi edges. Hence we obtain n v â‰¤ 23 n e and
therefore n e â‰¤ 3n âˆ’ 6. This implies n v â‰¤ 2n âˆ’ 4.
The Voronoi diagram gives a concise description of proximity relations between the
generators and also exhibits the information about which generators lie on the boundary
of the convex hull of the set S.
Before talking about time complexity of Voronoi diagram computations we have to
specify what the result of such a computation has to be. As the result of a Voronoi
diagram algorithm we require a data structure that allows for an easy access to the
Voronoi edges forming a specific region as well as to all Voronoi edges containing a given
Voronoi point. This can, e.g., be achieved by a data structure proposed in Ottmann
& Widmayer (1990). We store a doubly linked list of the Voronoi edges together with
the following information for every edge:
â€“ tail and head nodes, u and v, of the edge (the number âˆ’1 indicates that the edge
extends to infinity),
â€“ the Voronoi regions V l and V r to the left and to the right of the edge (where left
and right are with respect to some arbitrarily imposed direction of the edge),
â€“ pointers to the next Voronoi edge of region V l (resp. V r ) having u (resp. v) as one
endnode.
A straightforward algorithm for computing the Voronoi diagram takes time O(n 2 ). We
just compute each VR(P i ) as the intersection of the halfspaces B ij for j = i.
A lower bound on the running time of every Voronoi diagram algorithm is established
as follows. Consider the situation that all generators are located on the x-axis, i.e.,
their y-coordinates are 0. If we have the Voronoi diagram of this set we easily obtain
the sorted sequence of the generators with respect to their x-coordinates. Since sorting4.1. Voronoi Diagrams
45
cannot be performed in less than O(n log n) time we have a lower bound on the running
time for Voronoi diagram computations. Note that sorting and computing the Voronoi
diagram are essentially equivalent in this 1-dimensional case. The following section will
describe an algorithm that achieves this best possible running time.
In the present and in the following sections which also deals with geometric problems
we will not discuss the algorithms in full detail. When solving geometric problems
very often certain â€œdegenerateâ€ situations can occur that may have the consequence
that some object or some operation is not uniquely specified. Explaining the idea of a
geometric algorithm is often easy but when it comes to implementing the algorithm one
will usually face severe difficulties. It is a commonly observed fact that, when coding
algorithms for geometric problems, about 10â€“20% of the code account for the â€œrealâ€
algorithm while the remaining part is only necessary to deal with degenerate situations.
Almost the same is true when describing such algorithms in detail. We will therefore not
discuss degenerate situations explicitly, but will assume that points defining a geometric
problem are in general position, i.e., no degeneracy occurs. The reader should have in
mind, however, that an implementation must take care of these cases.
The O(n log n) algorithm to be described now was given in Shamos and Hoey (1975).
It is a divide and conquer approach that divides the set of generators recursively into
two halves, computes the respective Voronoi diagram for the two parts, and merges
them to obtain the Voronoi diagram of the whole set S.
procedure divide and conquer
(1) If the current set has at most three elements compute its Voronoi diagram and
return.
(2) Partition S into two sets S 1 = {P i 1 , P i 2 , . . . P i l } and S 2 = {P i l+1 , P i l+2 , . . . , P i n }
where l = n 2 such that there is a vertical line separating S 1 and S 2 .
(3) Perform the divide and conquer algorithm recursively to compute VD(S 1 ) and
VD(S 2 ).
(4) Obtain VD(S) from VD(S 1 ) and VD(S 2 ).
end of divide and conquer
The partition in Step (2) can be found in linear time if we have sorted the elements of
S in a preprocessing step with respect to their x-coordinates. There is also a more com-
plicated algorithm achieving linear running time without preprocessing (see Chapter 2
for median finding).
The critical step is Step (4). If this step can be performed in linear time (linear in
|S 1 | + |S 2 |) then the basic recurrence relation for divide and conquer algorithms gives a
time bound of O(n log n) for the overall Voronoi computation.
Merging two Voronoi diagrams (where one set of generators is to the left of the other
set of generators) basically consists of identifying the thick â€œmerge lineâ€ in the center
of Figure 4.2.
We cannot go into detail here and only discuss the principle of the construction of
this merge line. The line is constructed from the bottom to the top. One first has to
identify the respective generators in VD(S 1 ) and VD(S 2 ) with minimal y-coordinates46
Chapter 4. Geometric Concepts
(and therefore unbounded Voronoi regions). The perpendicular bisector between these
two generators then gives the lower part of the merge line. The process of merging the
two diagrams can now be visualized as extending the merge line upwards until an edge
of either of the two Voronoi diagrams is hit. At such a point the merge line may change
direction because it will now be part of the perpendicular bisector between two other
generators. This process is continued until the final part of the merge line is part of the
bisector of the two generators in the respective diagrams with maximal y-coordinates.
Figure 4.2 The merge line for merging two Voronoi diagrams
Implementing these steps carefully results in a running time of O(|S 1 | + |S 2 |) for the
merge process. This gives the desired optimal running time of the divide and conquer
algorithm.
The principle of the next algorithm (see Green & Sibson (1978)) is to start out with
the Voronoi diagram for three generators and then to successively take into account the
remaining generators and update the current Voronoi diagram accordingly.
procedure incremental algorithm
(1) Compute the Voronoi diagram VD({P 1 , P 2 , P 3 }).
(2) For t = 4, 5, . . . , n compute the diagram VD({P 1 , P 2 , . . . , P t }) as follows.
(2.1) Find P s , 1 â‰¤ s â‰¤ t âˆ’ 1, such that P t âˆˆ VR(P s ), i.e., find the nearest neighbor
of P t among the generators already considered.47
4.1. Voronoi Diagrams
(2.2) Start with the perpendicular bisector of P t and P s and find its intersection
with a boundary edge of VR(P s ). Suppose this edge is also the boundary
edge of VR(P i ). Find the other intersection of the bisector between P t and
P i with the boundary of VR(P i ). Proceed this way by entering neighboring
regions and computing intersections between the current bisector B(P t , P j )
and the boundary of VR(P j ) until the starting region VR(P s ) is reached
again. Eliminate everything within the resulting closed walk. (In the case of
an unbounded region VR(P t ) some further details have to be observed.)
end of incremental algorithm
Figure 4.3 shows a typical step of the incremental algorithm. The broken lines indicate
the bisectors that are followed in Step (2.2). The new diagram is obtained by deleting
everything inside the convex polygon determined by the broken lines.
P
v
P
r
P u
P t
P w
P s
Figure 4.3 A step of the incremental algorithm
The running time of this algorithm can only be bounded above by O(n 2 ). In Ohya,
Iri & Murota (1984) the algorithm is examined for application in practice. It turns
out that an enormous speed up can be obtained if the generators are considered in a
clever sequence. Ohya et al. use a bucketing approach which allows the nearest neighbor
in Step (2.1) to be guessed with high probability in constant time. Step (2.2) is not
that critical because usually only very local updates have to be performed to obtain the
new Voronoi diagram. In practical experiments linear running time on several classes of
randomly generated point sets was observed. In particular, the incremental algorithm
outperformed the divide and conquer method.
It is interesting to note, that selecting the next generator at random results in observed
3
O(n 2 ) running time.48
Chapter 4. Geometric Concepts
Expected linear running time is not proven in Ohya, Iri & Murota (1984), but some
evidence is given for it. For generators that are independently drawn from a uniform
distribution on the unit square, the expected number of generators to be examined
in Step (2.1) to find the nearest neighbor is bounded by a constant. In addition, the
expected number of Voronoi edges of a Voronoi region in any intermediate diagram is
bounded by a constant. These are two basic results that suggest that also a rigorous
mathematical proof of linear expected running time for this algorithm can be obtained.
A further algorithm for Voronoi diagram construction has been given in Fortune
(1987). This algorithm uses a sweep-line principle to compute the diagram in time
O(n log n).
Cronin (1990) and RujÃ¡n, Evertsz and Lyklema (1988) employ the Voronoi dia-
gram for generating traveling salesman tours.
4.2 Delaunay Triangulations
For most applications it is not the Voronoi diagram itself that is of interest. More
important are the proximity relations that it exhibits and not the concrete specification
of the Voronoi points and edges. To represent the topology of the diagram it suffices to
consider the â€œdualâ€ of the diagram.
Given the Voronoi diagram of S, its dual D(S) is the undirected graph G(S) = (S, D)
where D = {{P 1 , P 2 } | VR(P 1 ) âˆ© VR(P 2 ) = âˆ…}. It is easy to observe that G(S) is a
triangulated graph, i.e., every cycle of length at least four contains a chord. This graph
is called Delaunay triangulation (Delaunay (1934)).
An alternative definition excludes those edges {P 1 , P 2 } for which |VR(P 1 )âˆ©VR(P 2 )| = 1.
In this case the name is misleading, because we do not necessarily have a triangulation
anymore, but the resulting graph is planar (implying |D| = O(|S|)). We will use this
definition in the sequel and speak about the Delaunay graph or the straight line
dual of the Voronoi diagram.
Note that in the case of nondegeneracy (no four generators lie on a common circle) both
definitions coincide and D(S) is a planar graph.
Besides planarity D(S) (according to the modified definition) has additional important
properties.
Proposition 4.4
(i) If P i and P j are generators such that d(P i , P j ) â‰¤ d(P i , P k ) for all k = i, j then
{P i , P j } is an edge of D(S).
(ii) D(S) has at most 3n âˆ’ 6 edges.
(iii) D(S) contains a minimum spanning tree of the complete graph on n nodes where
the nodes correspond to the generators and the edge weights are respective Eu-
clidean distances.
Proof. Part (i) is clear because of Proposition 4.2 and part (ii) follows from Proposi-
tion 4.3.
For part (iii) consider Primâ€™s algorithm to compute a minimum spanning tree. In each
step we have a set V of nodes that are already connected by a spanning tree and the4.2. Delaunay Triangulations
49
set S \ V (consisting of isolated nodes). The next edge to be added to the tree is the
shortest edge connecting a node in V to a node in S \ V . This edge must be contained
in D(S) since it connects two generators whose Voronoi regions intersect in a Voronoi
edge.
Figure 4.4 shows the Delaunay triangulation corresponding to the diagram of Figure 4.1.
Figure 4.4 The Delaunay triangulation for rd100
Note that Proposition 4.4 (ii) does not hold for the general Delaunay triangulation but
only for the Delaunay graph. For example, if all generators are located on a circle then
all Voronoi regions intersect in a common Voronoi point and the Delaunay triangulation
is the complete graph on n nodes. The Delaunay graph is only a cycle of length n.
Straightforward implementations of algorithms for computing the Voronoi diagram (or
the Delaunay triangulation), in which all numerical computations are carried out in
floating point arithmetic, run into numerical problems.
Voronoi points are given as intersection points of bisectors. Due to insufficient accuracy
it may not be possible to safely decide whether two lines are parallel or almost parallel.
Moreover, the intersection points may be located â€œfar awayâ€ from the generators leading
to imprecise computations of Voronoi points.
The consequence is that due to incorrect decisions the algorithm may not work because
computed data is contradictory.
Consider the following example (Figure 4.5). We have three generators that are located
at three corners of a square. Depending on whether the fourth generator is at the fourth50
Chapter 4. Geometric Concepts
corner, or inside, or outside the square different Voronoi diagrams arise. If it cannot
be exactly differentiated between these three cases, then the correct computation of the
Voronoi diagram and hence of the Delaunay triangulation fails.
Figure 4.5 Inconsistent decisions due to round-off errors
The question of how to obtain correct results and avoid numerical difficulties is consid-
ered in Sugihara & Iri (1988), Sugihara (1988), and JÃ¼nger, Reinelt & Zepf
(1991). The principle idea is to not compute an explicit representation of the Voronoi
diagram, but to base the computation of the Delaunay graph on different logical tests.
Details are given in JÃ¼nger, Reinelt & Zepf (1991), we review the main results.
If all generators have integral coordinates between 0 and M , then one can compute
the Delaunay graph using integer numbers of value at most 6M 4 . On a computer
representing integers with binary 1-complement numbers having b bits, integers in the
interval [âˆ’2 bâˆ’1 , 2 bâˆ’1 âˆ’ 1] are available. The inequality 6M 4 â‰¤ 2 bâˆ’1 âˆ’ 1 implies
 

bâˆ’1
âˆ’ 1
4 2
M â‰¤
.
6
For the usual word length of real-world computers that means that we can allow

8 if b = 16
M â‰¤
137 if b = 32
35, 211 if b = 64 .
So, only 32-bit integer arithmetic is not enough for computing correct Delaunay trian-
gulations in practical applications. Only by using at least 64-bit arithmetic we can treat
reasonable inputs.
Of special interest are also the two metrics
â€“ Manhattan metric (L 1 ): d((x 1 , y 1 ), (x 2 , y 2 )) = |x 1 âˆ’ x 2 | + |y 1 âˆ’ y 2 |, and
â€“ Maximum metric (L âˆ ): d((x 1 , y 1 ), (x 2 , y 2 )) = max{|x 1 âˆ’ x 2 |, |y 1 âˆ’ y 2 |}.
This is due to the fact that very often distances correspond to the time a mechanical
device needs to travel from one point to the other. If this movement is performed
first in the horizontal direction and then in the vertical direction the L 1 -metric should
be chosen to approximate travel times. If the movement is performed by two motors
working simultaneously in horizontal and vertical directions then the L âˆ -metric is the
appropriate choice for modeling the movement times.4.2. Delaunay Triangulations
51
For these metrics, bisectors are no longer necessarily straight lines. They may consist of
three line segments and we can also have degenerate situations as shown in Figure 4.6.
Figure 4.6 Bisectors for L 1 - and L âˆ -metric
Here the bisectors include the shaded regions. In the L 1 -metric (left) this situation
occurs when the coordinate differences in both coordinates are the same. In the L âˆ -
metric (right picture) we have this situation if both points coincide in one of the two
coordinates. It is convenient to restrict our definition of bisectors to the bold line
segments in Figure 4.6 (the definition of Voronoi regions is changed accordingly). The
L 1 -metric Voronoi diagram for problem rd100 is shown in Figure 4.7.
Figure 4.7 Voronoi diagram for rd100 (L 1 -metric)52
Chapter 4. Geometric Concepts
The numerical analysis of the L 1 -case shows that we can compute an explicit representa-
tion of the Voronoi diagram itself using only one additional bit of accuracy than needed
to input the data. It follows that we can carry out all computations with numbers of
size at most 2M , and depending on the word length b we have the following constraints
for M

16, 383 if b = 16
1, 073, 741, 823 if b = 32
M â‰¤
4, 611, 686, 018, 427, 387, 903 if b = 64 .
Observe that also in diagrams for the Manhattan as well as for the maximum metric
the vertices of the convex hull of the generators have unbounded Voronoi regions. But
there may be further points with unbounded regions lying in the interior of the convex
hull. This is also reflected by the shape of the Delaunay graph in Figure 4.8 which
corresponds to the the Voronoi diagram of Figure 4.7.
Figure 4.8 Delaunay graph for rd100 (L 1 -metric)
Finally, we want to give an indication that Delaunay graphs can indeed be computed
very fast. We have used an implementation of the incremental Voronoi diagram algo-
rithm described in Ohya, Iri & Murota (1984) by M. JÃ¼nger and D. Zepf for the
L 2 -metric. This implementation uses floating point arithmetic, but was able to solve all
sample problem instances. Details for implementing an algorithm to compute Voronoi
diagrams for further metrics are discussed in Kaibel (1993).53
4.2. Delaunay Triangulations
Figure 4.9 shows the running time of this implementation on our set of sample problems.
CPU times are given in seconds on a SUN SPARCstation 10/20.
1.5
1.0
0.5
0.0
0
1000
2000
3000
4000
5000
6000
Figure 4.9 CPU times for computing Delaunay graphs
The figure seems to suggest indeed a linear increase of the running times with the
problem size. But, as we shall see throughout this monograph, we have to accept that
real-world problems do not behave well in the sense that smooth running time functions
can be obtained. Just the number of nodes of a problem is not sufficient to characterize
it. Real problems have certain structural properties that cannot be modeled by random
problem instances and can lead to quite different running times of the same algorithm
for different problem instances of the same size.
The number of edges of the respective Delaunay graphs is shown in Figure 4.10.
For random problems the expected number of edges forming a Voronoi region is six,
which is hence also the expected degree of a node in the Delaunay graph. Therefore we
would expect about 3n edges in a Delaunay graph which is quite closely achieved for
our sample problems.
We can conclude that Delaunay graphs can be computed very efficiently. For practical
purposes it is also important that running times are quite stable even though we have
problem instances from various sources and with different structural properties.54
Chapter 4. Geometric Concepts
20000
15000
10000
5000
0
0
1000
2000
3000
4000
5000
6000
Figure 4.10 Number of edges of the Delaunay graphs
CPU times are very well predictable (in our case as approximately n/5000 seconds on the
SPARCstation). Computing the Delaunay graph for problem d18512 took 19.4 seconds,
the number of edges of this graph was 61,126.
4.3 Convex Hulls
The convex hull of a set of points is the smallest convex set containing these points. It is
a convenient means for representing point sets. If the point set is dense then the convex
hull may very well reflect its shape. Large instances of traveling salesman problems
in the plane usually exhibit several clusters. Building the convex hull of these clusters
can result in a concise representation of the whole point set still exhibiting many of its
geometric properties.
A short description of complicated objects is also important in other areas, for example
in computer graphics or control of robots. Here movements of objects in space have
to be traced in order to avoid collisions. In such cases convex hulls can be applied to
represent the objects approximately.
We define the problem to compute the convex hull as follows. We are given a finite set
A = {a 1 , a 2 , . . . , a n } of n points in the plane where a i = (x i , y i ). The task is to identify4.3. Convex Hulls
55
those points that constitute the vertices of the convex hull conv(A) of the n points,
i.e., which are not representable as a convex combination of other points. Moreover,
we require that the computation also delivers the sequence of these vertices in correct
order. This means that if the computation outputs the vertices v 1 , v 2 , . . . , v t then the
convex hull is the (convex) polygon that is obtained by drawing edges from v 1 to v 2 ,
from v 2 to v 3 , etc., and finally by drawing the edges from v tâˆ’1 to v t and from v t to v 1 .
We will review some algorithms for computing the convex hull that also visualize some
fundamental principles in the design of geometric algorithms.
Before starting to describe algorithms we want to make some considerations concerning
the time that is at least necessary to find convex hulls. Of course, since all n input
points have to be taken into account we must spend at least time of linear order in n.
It is possible to establish a better lower bound. Let B = {b 1 , b 2 , . . . , b n } be a set of
distinct positive real numbers. Consider the set A âŠ† R 2 defined by A = {(b i , b 2 i ) | 1 â‰¤
i â‰¤ n}. Since the function f : R â†’ R with f (x) = x 2 is strictly convex none of the
points of A is a convex combination of other points. Hence computing the convex hull
of A also sorts the numbers b i . It is known that in many computational models sorting
of n numbers needs worst case time Î©(n log n).
A second way to derive a lower bound is based on the notion of maxima of vectors. Let
A = {a 1 , a 2 , . . . , a n } be a finite subset of R 2 . We define a partial ordering â€œâ€ on A by
a i = (x i , y i )  a j = (x j , y j ) if and only if x i â‰¥ x j and y i â‰¥ y j .
The maxima with respect to this ordering are called maximal vectors. In Kung,
Luccio & Preparata (1975) the worst case time lower bound of Î©(n log n) for identi-
fying the maximal vectors of A is proved. This observation is exploited in Preparata
& Hong (1977) for the convex hull problem. Suppose A is such that every a i is a vertex
of the convex hull. Identify the (w.l.o.g.) four points with maximal, resp. minimal x- or
y-coordinate. Let a j be the vertex with maximal y-coordinate and a k be the vertex with
maximal x-coordinate. The number of vertices between a j and a k may be of order n.
They are all maximal elements. Since convex hull computations can identify maximal
vectors it cannot be faster in the worst case than O(n log n). Note that this lower bound
is also valid if we do not require that the vertices of the convex hull are output in their
correct sequence.
Though the lower bound derived here may seem to be weak there are many algorithms
that compute the convex hull in worst case time O(n log n).
According to Toussaint (1985) the first efficient convex hull algorithm has been out-
lined in Bass & Schubert (1967). Their algorithm was designed to be the first step
for computing the smallest circle containing a given set of points in the plane. Though
the algorithm is not completely correct it already exhibits some of the powerful ideas
used in convex hull algorithms. It consists of an elimination step as in the throw-away
algorithm of section 4.3.3 and afterwards basically performs a scan similar to Grahamâ€™s
scan (to be described next). When corrected appropriately a worst case running time
of O(n log n) can be shown. Therefore, this algorithm can be considered as the first
O(n log n) convex hull algorithm.56
Chapter 4. Geometric Concepts
4.3.1 Grahamâ€™s Scan
This algorithm was given by Graham (1972). Its main step consists of computing a
suitable ordering of the points. Then the convex hull is built by successively scanning
the points in this order. The algorithm works as follows.
procedure graham scan
(1) Identify an interior point of conv(A), say P 0 . This can be done by finding three
points of A that are not collinear and by taking their center of gravity as P 0 .
(2) Compute polar coordinates for the points of A with respect to the center P 0 and
some arbitrary direction representing the angle zero.
(3) Sort the points with respect to their angles.
(4) If there are points with the same angle then eliminate all of them but the one
with largest radius. Let a i 1 , a i 2 , . . . , a i t be the sorted sequence of the remaining
points.
(5) Start with three consecutive points P r , P m , and P l , i.e., P r = a i k , P m = a i k+1 ,
P l = a i k+2 for some index k where indices are taken modulo t.
(6) Perform the following step for the current three points until the same triple of
points occurs for the second time.
a) If P m lies on the same side of the segment [P l , P r ] as P 0 or lies on the segment
then delete P m and set P m = P r and P r to its predecessor in the current sorted
list.
b) If P m lies on the side of the segment [P l , P r ] opposite to P 0 then set P r = P m ,
P m = P l , and P l to its successor in the current sorted list.
end of graham scan
P l
P r
P m
P 0
Figure 4.11 Grahamâ€™s scan4.3. Convex Hulls
57
Correctness of this algorithm is easily verified. Step (6) has to be performed at most
2t times for scanning the necessary triples of nodes. In Step (6a) a point is discarded
so it cannot be performed more than t âˆ’ 3 times. If Step (6b) is executed t times we
have scanned the points â€œonce around the clockâ€ and no further changes are possible.
Therefore the worst case running time is dominated by the sorting in Step (3) and we
obtain the worst case running time O(n log n).
This way we have established the worst case time complexity Î˜(n log n) for computing
convex hulls in the plane.
4.3.2 Divide and Conquer
The divide and conquer principle also applies in the case of convex hull computations
(Bentley & Shamos (1978)). In this case, the basic step consists of partitioning a
point set according to some rule into two sets of about equal size, computing their
respective convex hulls and merging them to obtain the convex hull of the whole set.
procedure divide and conquer
(1) If the current set has at most three elements compute its convex hull and return.
(2) Partition A into two sets A 1 = {a i 1 , a i 2 , . . . , a i l } and A 2 = {a i l+1 , a i l+2 , . . . , a i n }
where l = n 2 such that there is a vertical line separating A 1 and A 2 .
(3) Perform the divide and conquer algorithm recursively to compute conv(A 1 ) and
conv(A 2 ).
(4) Merge the two convex hulls to obtain the convex hull of A.
end of divide and conquer
Figure 4.12 The divide and conquer algorithm58
Chapter 4. Geometric Concepts
The partition required in Step (2) can be easily computed if the points are presorted
(which takes time O(n log n)). So, the only critical step to be examined is Step (4).
When merging two hulls we have to add two edges (the so-called upper and lower
bridges) and eliminate all edges of conv(A 1 ) and conv(A 2 ) that are not edges of
conv(A). To find these bridges we can exploit the fact that due to the sorting step
we know the leftmost point of A 1 and the rightmost point of A 2 (w.l.o.g., A 1 is left of
A 2 ). Starting at these points it is fairly simple to see that the edges to be eliminated
can be readily identified and that no other edges are considered for finding the bridges.
Therefore, the overall time needed to merge convex hulls during the algorithm is linear
in n. Due to Theorem 2.9, this establishes the O(n log n) worst case time bound for the
divide and conquer approach.
Kirkpatrick & Seidel (1986) describe a refinement of the divide and conquer ap-
proach to derive a convex hull algorithm which has worst case running time O(n log v)
where v is the number of vertices of the convex hull.
4.3.3 Throw-Away Principles
It is intuitively clear that when computing the convex hull of a set A not all points
are equally important. With high probability, points in the â€œinteriorâ€ of A will not
contribute to the convex hull whereas points near the â€œboundaryâ€ of A are very likely
vertices of the convex hull. Several approaches make use of this observation in that they
eliminate points before starting the true convex hull computation.
If we consider a convex polygon whose vertices are contained in the set A then all points
inside this polygon can be discarded since they cannot contribute to the convex hull. In
Akl & Toussaint (1978) an algorithm is given that makes use of this fact.
procedure throw away
(1) Compute the points a xmax , a xmin , a ymax , and a ymin with maximal (minimal) x-,
resp. y-coordinate.
(2) Discard all points inside the convex polygon given by these four points and identify
four regions of points to be considered. The regions are associated with the four
edges of the polygon.
(3) For each subregion, determine the convex hull of points contained in it.
(4) Construct conv(A) from these four convex hulls.
end of throw away
Any of the convex hull algorithms could be used in Step (3). Akl and Toussaint basically
use Grahamâ€™s scan modified in a way that no angles have to be computed.
In a refined version Devroye & Toussaint (1981) compute four additional points,
namely those with maximal (minimal) coordinate sum x i +y i , resp. coordinate difference
x i âˆ’ y i . Elimination is now performed using the convex polygon given by these eight
points.59
4.3. Convex Hulls
a ymax
a xmin
a xmax
a ymin
Figure 4.13 A throw-away principle
4.3.4 Convex Hulls from Maximal Vectors
Every point in R 2 can be viewed as the origin of a coordinate system with axes parallel
to the x- and y-directions. This coordinate system induces four quadrants. A point is
called maximal with respect to the set A if at least one of these quadrants (including
the axes) does not contain any other point of A. It is easily seen that every vertex of
conv(A) is maximal. Namely, let x be a vertex of conv(A) and assume that each of the
corresponding quadrants contains a point of A. Then x is contained in the convex hull of
these points and therefore cannot be a vertex of the convex hull of A. Kung, Luccio
& Preparata (1975) give an algorithm to compute the maximal vectors of a point
set in the plane in worst case time O(n log n). This leads to the following O(n log n)
algorithm for computing the convex hull.
procedure maximal vector hull
(1) Compute the set S of maximal vectors with respect to A.
(2) Let A  = S.
(3) Compute the convex hull of A  using any of the O(n log n) worst case time algo-
rithms.
(4) conv(A) = conv(A  ).
end of maximal vector hull
The expectation is that very many points can be discarded in Step (1).60
Chapter 4. Geometric Concepts
Figure 4.14 Maximal vectors
4.3.5 A New Elimination Type Algorithm
We are now going to discuss a further elimination type algorithm that uses a particularly
simple discarding mechanism. This algorithm is best suited for large dense point sets
distributed uniformly in a rectangle. It is discussed in full detail in Borgwardt,
Gaffke, JÃ¼nger & Reinelt (1991).
Assume that the points of A are contained in the unit sqaure, i.e., their coordinates are
between 0 and 1. The function h : [0, 1] Ã— [0, 1] â†’ R is defined by h(x) = h(x 1 , x 2 ) =
min{x 1 , 1 âˆ’ x 1 } Â· min{x 2 , 1 âˆ’ x 2 }.
The basic idea is to compute the convex hull of a small subset S of A such that conv(S) =
conv(A) with high probability. The value h(x) will express whether x is likely to be an
interior point of conv(A). With increasing h(x) the probability that x can be eliminated
as a candidate for a vertex of the convex hull increases.
We will discard points based on this function in a first phase and compute the convex
hull for the remaining points. It will turn out that we cannot guarantee that we have
obtained conv(A) this way. Therefore, in a second phase we have to check correctness
and possibly correct the results.
The following is a sketch of our elimination type algorithm where CH is some algorithm
to compute the convex hull of a set of points in the plane.
procedure elim hull
Phase1
(1) Choose a suitable parameter Î±.
(2) Let S Î± = {a i âˆˆ A | h(a i ) â‰¤ Î±}.
(3) Apply CH to compute the convex hull of S Î± .4.3. Convex Hulls
61
(4) Compute the minimum Î³ such that conv(S Î± ) âŠ‡ {x âˆˆ [0, 1] Ã— [0, 1] | h(x) > Î³}.
(5) If Î± â‰¥ Î³ then STOP (conv(S Î± ) = conv(A)), otherwise perform Phase 2.
Phase2
(6) Compute conv(S Î³ ) where S Î³ = {a i âˆˆ A | h(a i ) â‰¤ Î³}. Now conv(S Î³ ) = conv(A).
end of elim hull
Figure 4.15 gives an illustration of the algorithm. The set S Î± is given by the solid
points (â€˜â€¢â€™) and its convex hull by solid lines. The broken curve defines the set S Î³ and
the additional points to be considered in Phase 2 are shown as small circles (â€˜â—¦â€™). The
extreme points of the correct convex hull resulting from Phase 2 are obtained by adding
one point in the north-east corner.
A detailed analysis shows that Step (4) can be performed in linear time. Therefore the
worst case running time of this algorithm is given by the worst case running time of CH
(independent of Î±).
Figure 4.15 Illustration of the algorithm
The analysis of the average time complexity of this algorithm exhibits some interesting
consequences. If Î± is chosen carefully, then in certain random models only very few
points are contained in S Î± and with very high probability Phase 2 is not needed. In
particular, one can obtain a speed-up theorem for convex hull algorithms in the following
sense.62
Chapter 4. Geometric Concepts
Theorem 4.5 Let A be a set of n random points generated independently from the
uniform distribution on the unit square [0, 1]Ã—[0, 1]. For any algorithm CH with polyno-
mial worst-case running time the two-phase method has linear expected running time.
For a detailed analysis of the algorithm and the proper choice of Î± as well as for
a discussion of the computation of convex hulls for random points in the unit disk
D(0, 1) = {x = (x 1 , x 2 ) âˆˆ R 2 | x 21 + x 22 â‰¤ 1} we refer to Borgwardt, Gaffke,
JÃ¼nger & Reinelt (1991). Our approach can be generalized to higher dimensions. A
complete coverage of the 3-dimensional case can be found in Thienel (1991).
Assuming a uniform distribution of the n independent points over the unit square, linear
expected time algorithms have been given by Bentley & Shamos (1978), Akl &
Toussaint (1978), Devroye (1980), Devroye & Toussaint (1981), Kirkpatrick
& Seidel (1986), and Golin & Sedgewick (1988). For a survey on these and related
subjects see Lee & Preparata (1984).
We have compared the practical behaviour of five linear expected time algorithms,
namely
[1] the divide and conquer algorithm (4.3.2),
[2] the maximal vectors approach (4.3.4),
[3] the throw-away principle based on eight points (4.3.3),
[4] the throw-away principle based on four points (4.3.3),
[5] the new algorithm.
For algorithms [2], [3], [4], and [5] we apply Grahamâ€™s scan to the selected points.
All five algorithms have been implemented as Pascal programs on a SUN SPARCsta-
tion SLC which is about four times slower than the SPARCstation 10/20. We have tried
to put the same efforts into all five programs to make the comparison as fair as possible.
For instance, in all throw-away type algorithms we found that the following trick helped
to reduce the computation time. As soon as the elimination area has been determined
(a closed polygon in case [3] and [4] and the curve defined by the function h in [5]) we
inscribe the biggest possible rectangle with vertical and horizontal sides into this area.
Assume that this rectangle (always a square in case [5]) has vertices (x 1 , y 1 ), (x 2 , y 1 ),
(x 2 , y 2 ) and (x 1 , y 2 ). The elimination criterion is satisfied for a point with coordinates
(x, y) if x 1 â‰¤ x â‰¤ x 2 and y 1 â‰¤ y â‰¤ y 2 which takes only four comparisons to check. Only
if this criterion fails, we have to check the actual elimination criterion which, e.g., in
case [5] amounts to checking which quadrant (x, y) lies in, and depending on this, up
to two additions, one multiplication, and one comparison.
Figure 4.16 shows the computation times of the algorithms for computing the convex
hull of point sets in the unit square drawn independently form a uniform distribution.
The curves are based on 10 sample problems for each problem size n = 1 000, 2 000, . . .,
10 000, 20 000, . . . , 100 000, 200 000, . . . , 1 000 000. In our opinion, these curves should
be interpreted as follows. When doing practical computations, the throw-away principle
is superior compared to the divide and conquer algorithms. The four point method is
slightly better than the eight point method.
For our experiments with practical traveling salesman problems in the plane we have
coded the algorithm as follows. The points are mapped to the unit square by horizontal63
4.3. Convex Hulls
20
15
10
5
0
0
200000
400000
[1]
[2]
600000
[3]
[4]
800000
1000000
[5]
Figure 4.16 Comparison of convex hull algorithms
and vertical scaling and appropriate offsets. This is done in such a way that each of
the four sides of the square contains at least one problem point. Then the elimination
algorithm is performed using Î± = 4 log n/n. However, in the range of problem sizes
we had available, the new algorithm does not pay off since the convex hull of some
ten thousands of points can be computed in so little time that it is negligibly small
compared to the other algorithms employed for problem solving. Therefore we do not
report computing times for TSPLIB problems here.Chapter 5
Candidate Sets
In many practical applications it is required to find reasonably good tours for a traveling
salesman problem in short time. When designing fast heuristics, one is faced with the
problem that, in principle, very many connections need to be considered. For example,
in the traveling salesman problem fnl4461, tours have to be constructed by selecting
4461 out of 9,948,030 possible connections. Standard implementations of heuristics
consider all these connections which leads to substantial running times.
On the other hand, it is intuitively clear, that most of the possible connections will not
occur in short tours because they are too long. It is therefore a reasonable idea, which
we will exploit extensively in the sequel, to restrict attention to â€œpromisingâ€ edges and
to avoid considering long edges too frequently. To this end we employ several types of
candidate sets from which edges are taken with priority in the computations.
In geometric problem instances one has immediate access to long edges because their
length is related to the location of the points. In general, for problems given by a
distance matrix, already time Î©(n 2 ) has to be spent to scan all edge lengths. We will
discuss three types of candidate sets in this chapter. The first one is applicable in
general, but can be computed very fast for geometric instances. The other two sets can
only be computed for geometric instances.
5.1 Nearest Neighbors
It can be observed that most of the edges in good or optimal tours connect nodes to near
neighbors. For a TSP on n nodes and k â‰¥ 1, we define the corresponding k nearest
neighbor subgraph G k = (V, E) by setting
V ={1, 2 . . . , n},
E ={uv | v is among the k nearest neighbors of u}.
For example, an optimal solution for the problem pr2392 can be found within the
8 nearest neighbor subgraph and for pcb442 even within the subgraph of the 6 nearest
neighbors.
Figure 5.1 shows the 10 nearest neighbor subgraph for the problem u159. This subgraph
contains an optimal tour.
G. Reinelt: The Traveling Salesman, LNCS 840, pp. 64-72, 1994.
ï›™ Springer-Verlag Berlin Heidelberg 19945.1. Nearest Neighbors
65
Figure 5.1 The 10 nearest neighbor subgraph for u159
A straightforward computation of the k nearest neighbors by enumeration takes time
Î©(n 2 ) for fixed k. The following proposition shows that for Euclidean problem instances
we can exploit the Delaunay graph for nearest neighbor computations. The discussion
applies to other metrics as well.
Proposition 5.1 Let P i and P j be two generators.
(i) If the straight line connecting P i and P j intersects the interior of the Voronoi
region of a generator P l different from P i and P j then d(P i , P l ) < d(P i , P j ).
(ii) If the smallest number of edges on a path from P i to P j in the Delaunay graph
is k then there exist at least k âˆ’ 1 generators P l different from P i and P j with
d(P i , P l ) < d(P i , P j ).
Proof. For part (i) suppose that the line intersects the boundary of VR(P l ) in the
points T 1 and T 2 where w. l. o. g. d(P i , T 1 ) < d(P i , T 2 ). By definition we have d(P l , T 1 ) â‰¤
d(P i , T 1 ) and d(P l , T 2 ) â‰¤ d(P j , T 2 ). Since P i , P l , and T 1 are distinct we obtain
d(P i , P l ) â‰¤ d(P i , T 1 ) + d(P l , T 1 )
< d(P i , T 1 ) + d(T 1 , P j )
= d(P i , P j ).
Part (ii) is an immediate corollary.
Therefore, to compute the k nearest neighbors of some generator P i we only have to
examine generators which are connected to P i in the Delaunay graph by a path of length66
Chapter 5. Candidate Sets
at most k âˆ’ 1. Since, for random instances, the expected degree of a node in this graph
is six, we can expect linear running time of this procedure for fixed k.
The fast algorithm for Euclidean problem instances is the following.
procedure nearest neighbors
(1) Compute the Delaunay graph and initialize the empty candidate list.
(2) For i = 1, 2, . . . , n compute the k nearest neighbors of node i by breadth-first
search in the Delaunay graph starting at node i. Add the corresponding edges to
the candidate set.
end of nearest neighbors
5.0
4.0
3.0
2.0
1.0
0.0
0
1000
2000
3000
4000
5000
6000
Figure 5.2 CPU times for computing 10 nearest neighbor graph
Figures 5.2 and 5.3 show the running times for the 10 nearest neighbor computations
for our set of sample problems as well as the number of edges in the resulting candidate
sets. A good approximation for the cardinality of the 10 nearest neighbor candidate set
is 6n.
It is interesting to note that computation of the 10 nearest neighbor set for problem
pr2392 takes 22.3 seconds if the trivial algorithm is used. If the complete distance
matrix is stored this time reduces to 18.8 seconds which is still substantially more than
the 0.5 seconds needed with the Delaunay graph. It should be kept in mind that we
do not use complete distance matrices in this tract, but that much CPU time can be
gained if they are available.67
5.2. Candidates Based on the Delaunay Graph
70000
60000
50000
40000
30000
20000
10000
0
0
1000
2000
3000
4000
5000
6000
Figure 5.3 Number of edges of the 10 nearest neighbor graphs
There are further approaches for an efficient computation of nearest neighbors for geo-
metric problems instances, e.g., probabilistic algorithms and algorithms based on k-d-
trees (Bentley (1990)).
5.2 Candidates Based on the Delaunay Graph
In particular if point sets exhibit several clusters, the k nearest neighbor subgraph is not
connected and many edges to form good tours are missing. Here the Delaunay graph
should help since it contains important connections between the clusters.
Though it may seem to be true at a first glance, the Delaunay graph itself does not
necessarily contain a Hamiltonian tour. For example, in the case where all points
are on a line the Delaunay graph is just a path. There are also examples where the
Delaunay graph is a triangulation, but does not contain a tour or even a Hamiltonian
path (Dillencourt (1987a,1987b)).
First experiments have indicated that the Delaunay graph provides a candidate set too
small. We therefore decided to augment it using transitive relations in the following
way. If the edges {i, j} and {j, k} are contained in the Delaunay graph then we also
add edge {i, k} to the candidate set. We call this set Delaunay candidate set. The
cardinality of this set can be quite large. For example, if n âˆ’ 1 generators are located
on a circle and one generator is at the center of this circle then the Delaunay candidate
set is the complete graph on n nodes. Figure 5.4 shows the Delaunay candidate set for
problem u159.68
Chapter 5. Candidate Sets
Figure 5.4 The Delaunay candidate set for u159
The following procedure computes the Delaunay candidate set.
procedure Delaunay candidates
(1) Compute the Delaunay graph and initialize the candidate set with the edges of
the Delaunay graph.
(2) For every node i = 1, 2, . . . , n do
(2.1) For every two nodes j and k adjacent to i in the Delaunay graph add edge
{j, k} to the candidate set if it was not a candidate edge before.
end of Delaunay candidates
For random points in the plane, empirical observations show that we can expect about
9n to 10n edges in this candidate set.
Figure 5.4 illustrates that the candidate set is rather dense. Therefore we have to expect
more than 9n edges in this candidate set for practical problem instances. Furthermore,
due to long edges in the Delaunay graph, the candidate set may contain many long
edges. To avoid long edges, we usually first run a fast heuristic to compute an initial
tour (as the space filling curves heuristic described in Chapter 8) and then eliminate
all edges from the candidate set that are longer than the longest edge in this tour. For
dense point sets most long edges will be eliminated, for clustered point sets the edges
connecting the clusters will be kept. In general, however, elimination of long edges is
not critical for the performance of our heuristics.69
5.2. Candidates Based on the Delaunay Graph
5.0
4.0
3.0
2.0
1.0
0.0
0
1000
2000
3000
4000
5000
6000
Figure 5.5 CPU times for computing Delaunay candidate sets
70000
60000
50000
40000
30000
20000
10000
0
0
1000
2000
3000
4000
5000
6000
Figure 5.6 Number of edges of Delaunay candidate sets
Figures 5.5 and 5.6 give the CPU times necessary for computing the Delaunay candidate
set and the cardinality of this set. CPU time and size of the candidate set depend highly
on the point configuration and not only on the problem size. For a random problem on70
Chapter 5. Candidate Sets
15,000 nodes we obtained 44,971 (i.e., 2.99 Â· n) edges in the Delaunay graph and 147,845
(i.e., 9.86 Â· n) edges in the Delaunay candidate set.
5.3 Other Candidate Sets
We discuss a further candidate set that is easily computed and does not require such
sophisticated computations as the Delaunay graph.
The idea is to obtain near neighbors without too much effort just based on coordinates.
We outline the procedure for horizontal coordinates. We sort the points with respect to
their x-coordinates. For every point i we consider the points that appear right before or
after i in this sorted sequence. To limit the amount of work we only take those points
into account which appear at most w positions before or at most w positions after i.
Among these points we compute the k points nearest to i and choose the corresponding
edges as candidate edges.
The complete heuristic also takes the vertical coordinates into account. This is accom-
plished as follows. The parameter w specifies a search width as sketched above, the
parameter k gives the number of candidate edges that are selected from each node.
procedure candidate heuristic
(1) Initialize two sorted lists of the points by sorting them with respect to their x-
coordinates and with respect to their y-coordinates. For every point i let i x and
i y be its respective positions in the lists.
(2) Initialize the empty candidate list.
(3) For every node i = 1, 2, . . . , n do
(3.1) Let Q 1 = {j|j x âˆˆ {i x + 1, . . . , i x + w}, j y âˆˆ {i y + 1, . . . , i y + w}}, Q 2 = {j|j x âˆˆ
{i x + 1, . . . , i x + w}, j y âˆˆ {i y âˆ’ 1, . . . , i y âˆ’ w}}, Q 3 = {j|j x âˆˆ {i x âˆ’ 1, . . . , i x âˆ’
w}, j y âˆˆ {i y âˆ’ 1, . . . , i y âˆ’ w}}, and Q 4 = {j|j x âˆˆ {i x âˆ’ 1, . . . , i x âˆ’ w}, j y âˆˆ
{i y + 1, . . . , i y + w}}.
(3.2) Add edges from node i to its two nearest neighbors in every set Q j , j =
1, 2, 3, 4 (or less, if Q j contains fewer than two elements) to the candidate set
and remove the corresponding nodes from the sets. (We incorporate this step
to have candidate edges connecting i to each of the sets Q j .)
(3.3) Compute the k âˆ’ l nearest neighbors of i in the reduced set Q 1 âˆª Q 2 âˆª Q 3 âˆª Q 4
(where l is the number of edges selected in (3.2)) and add the corresponding
edges to the candidate set.
end of candidate heuristic
Figure 5.7 shows the candidate set obtained with this heuristic for problem instance u159
(parameters were k = 10 and w = 20). It contains 94% of the edges of the 10 nearest
neighbor graph. Because of pathological conditions at the border of point sets we may
also incur a number of long edges in this heuristic. These could be eliminated as above,
but keeping them has no significant effects as our experiments showed.71
5.3. Other Candidate Sets
Figure 5.7 Result of candidate heuristic for u159
5.0
4.0
3.0
2.0
1.0
0.0
0
1000
2000
3000
4000
5000
Figure 5.8 CPU times for candidate heuristic
600072
Chapter 5. Candidate Sets
70000
60000
50000
40000
30000
20000
10000
0
0
1000
2000
3000
4000
5000
6000
Figure 5.9 Number of edges of heuristic candidate sets
Figures 5.8 and 5.9 display the CPU times for computing this candidate set and the
number of its edges, respectively. Both, sizes of the candidate sets and the times to
compute them, are very well predictable depending on the size of the problem instance.
An interesting further candidate set can be obtained by taking the union of the Delaunay
graph and an appropriate nearest neighbor graph. Here we have the short connections
as well as the important connections between distant clusters.
We will investigate the usefulness of our candidate sets in the subsequent chapters.Chapter 6
Construction Heuristics
Starting with this chapter we will now consider computational aspects of the traveling
salesman problem. For the beginning we shall consider pure construction procedures,
i.e., heuristics that determine a tour according to some construction rule, but do not
try to improve upon this tour. In other words, a tour is successively built and parts
already built remain in a certain sense unchanged throughout the algorithm.
Many of the construction heuristics presented here are known and computational results
are available (Golden & Stewart (1985), Arthur & Frendeway (1985), Johnson
(1990), Bentley (1992)) We include them for the sake of completeness of this tract
and for having a reference to be compared with other algorithms on our sample problem
instances. Moreover, most evaluations of heuristics performed in the literature lack
from the fact that either fairly small problem instances or only random instances were
examined.
The following types of algorithms will be discussed:
â€“ nearest neighbor heuristics,
â€“ insertion heuristics,
â€“ heuristics based on spanning trees, and
â€“ savings heuristics.
This does not cover by far all the approaches that have been proposed. But we think
that the ideas presented here provide the reader with the basic principles that can also
be adapted to other combinatorial optimization problems.
For the following, we will always assume that we are given the complete undirected
graph K n with edge weights c uv for every pair u and v of nodes. For ease of notation
we will denote the node set by V and assume that V = {1, 2, . . . , n}. The question to
be addressed is to find good Hamiltonian tours in this graph.
6.1 Nearest Neighbor Heuristics
This heuristic for constructing a traveling salesman tour is near at hand. The salesman
starts at some city and then visits the city nearest to the starting city. From there he
visits the nearest city that was not visited so far, etc., until all cities are visited, and
the salesman returns to the start,
6.1.1 The Standard Version
Formulated as an algorithm we obtain the following procedure.
G. Reinelt: The Traveling Salesman, LNCS 840, pp. 73-99, 1994.
ï›™ Springer-Verlag Berlin Heidelberg 199474
Chapter 6. Construction Heuristics
procedure nearest neighbor
(1) Select an arbitrary node j, set l = j and T = {1, 2, . . . , n} \ {j}.
(2) As long as T  = âˆ… do the following.
(2.1) Let j âˆˆ T such that c lj = min{c li | i âˆˆ T }.
(2.2) Connect l to j and set T = T \ {j} and l = j.
(3) Connect l to the first node (selected in Step (1)) to form a tour.
end of nearest neighbor
This procedure runs in time Î©(n 2 ). A possible variation of the standard nearest neighbor
heuristic is the double-sided nearest neighbor heuristic where the current path can
be extended from both of its endnodes.
No constant worst case performance guarantee can be given, since the following theorem
due to Rosenkrantz, Stearns & Lewis (1977) holds.
Theorem 6.1 For every r > 1 and arbitrarily large n there exists a TSP instance on
n cities such that the nearest neighbor tour is at least r times as long as an optimal
tour.
In addition, Rosenkrantz, Stearns and Lewis (1977) show that for arbitrarily
large n there exist TSP instances on n nodes such that the nearest neighbor solution is
Î˜(log n) times as long as an optimal Hamiltonian cycle. This results still holds if the
triangle inequality is satisfied. Therefore it also applies to metric problem instances.
Figure 6.1 A nearest neighbor tour for rd1006.1. Nearest Neighbor Heuristics
75
If one displays nearest neighbor tours one realizes the reason for their poor performance.
The procedure proceeds very well and produces connections with short edges in the
beginning. But, as can be seen from a graphics display, several cities are â€œforgottenâ€
during the course of the algorithm. They have to be inserted at high cost in the end.
Figure 6.1 shows a typical nearest neighbor tour.
Though usually rather bad, nearest neighbor tours have the advantage that they only
contain a few severe mistakes, but there are long segments connecting nodes with short
edges. Therefore, such tours can serve as good starting tours for subsequently performed
improvement methods and it is reasonable to put some effort in designing heuristics that
are based on the nearest neighbor principle. We will comment on improvement methods
in the next chapter. The standard procedure itself is easily implemented with a few lines
of code. But, since running time is quadratic, we describe some variants to speed up
and/or improve the standard nearest neighbor search.
6.1.2 Exploiting the Delaunay Graph
We have seen in Chapter 5 that the Delaunay graph can be used to speed up nearest
neighbor computations. We can apply these results here, too. Namely, when searching
the nearest neighbor of node l in step (2.1) among the nodes which are not yet contained
in the partial tour, we can use the principle of section 5.1 to generate the k-th nearest
neighbor of l for k = 1, 2, . . . , n until a node is found that is not yet connected. Due to
the properties of the Delaunay graph we should find this neighbor examining only a few
edges of the graph in the neighborhood of l. Since in the last steps of the algorithm we
have to collect the forgotten nodes (which are far away from the current node) it makes
no sense to use the Delaunay graph any further. So, for connecting the final nodes we
just use a simple enumeration procedure.
We have conducted several experiments to see how many neighbors of the current node
are examined and to what depth the breadth-first search to find the nearest neighbor is
performed.
Figure 6.2 shows the average depth in the breadth-first search tree needed to find the
next neighbor. The average depth varies between 3 and 5 for real-world problems and
is about 3.5 for random problems.
Figure 6.3 displays the average number of nodes examined to find the next neighbor,
which is then also the number of necessary distance evaluations per node. Here real-
world problems behave better than random problems.
Furthermore, we examined how search depth and number of neighbors to be examined
develop during the heuristic. Figures 6.4 and 6.5 depict the search depth and the number
of examined nodes, respectively, obtained during the execution of the nearest neighbor
heuristic on the problem pr2392.
We see that at the majority of nodes next neighbors can indeed be found in the local
neighborhood with a search depth below five in most cases. Only sometimes a large
part of the Delaunay graph has to be explored to find the next node. Note that we do
not use the Delaunay strategy for inserting the final 100 nodes, since the overhead for
identifying the nearest neighbor increases significantly at the end of the procedure.76
Chapter 6. Construction Heuristics
6.0
5.0
4.0
3.0
2.0
1.0
0.0
0
5000
10000
15000
20000
Figure 6.2 Average search depth
50
45
40
35
30
25
20
15
10
0
5000
10000
15000
20000
Figure 6.3 Average number of examined nodes
The worst case running time of this implementation is still quadratic. The running
times for the sample problems will be given below together with the running times for
all other variants.77
6.1. Nearest Neighbor Heuristics
35
30
25
20
15
10
5
0
0
500
1000
1500
2000
2500
Figure 6.4 Search depth for pr2392
2000
1500
1000
500
0
0
500
1000
1500
2000
Figure 6.5 Number of examined nodes for pr2392
250078
Chapter 6. Construction Heuristics
6.1.3 Precomputed Neighbors
Suppose we have computed a candidate subgraph representing â€œreasonableâ€ connections,
e.g., the k nearest neighbor subgraph. A speed up of the nearest neighbor procedure
is then possible if we first look for nearest neighbors of a node within its adjacent
nodes in the subgraph. This way we reduce the exhaustive neighbor search and the
necessary costly distance computations. If all nodes adjacent in the subgraph are already
contained in the partial tour then we compute the nearest neighbor among all free nodes.
This modification does not improve worst case time complexity but should be faster
when run on practical problems.
Note, that even if the subgraph is obtained by computing the k nearest neighbors this
modified routine and the standard routine will usually come up with different results.
This is due to the fact that we may proceed from the current node l to a node j which
is not its nearest neighbor among all free nodes. This can occur, if edge {l, j} is in the
candidate set and l is among the k nearest neighbors of j, but j is not among the k
nearest neighbors of l.
6.1.4 Neighbors of Predecessors
In this modification we also use a precomputed set of candidate edges but apply the
following variant for the neighbor search. If all nodes adjacent to the current node in
the subgraph are already contained in the partial tour then we look for free neighbors
(in the candidate subgraph) of the predecessor of the current node. If this fails too,
we go back to the predecessor of the predecessor, etc. The backtrack to a predecessor
is only done a limited number of times, say 20 times, because then free neighbors are
usually already far away from the current node and it should be preferrable to look for
the exact nearest neighbor. Again, worst case time complexity is not affected, but a
significant speed up should be possible.
6.1.5 Insertion of Forgotten Nodes
As the main problem with nearest neighbor heuristics is that in the end nodes have to
be connected at high cost, we try to avoid that nodes become isolated. To do this we
first compute the degree of each node in the chosen candidate subgraph. (Without this
subgraph the procedure would not make sense.)
Whenever a node is connected to the current partial tour we decrease the degrees of its
adjacent nodes (in the subgraph) by 1. If the degree of a free node is decreased below
a specified limit (e.g., 2 or 3) this way, we insert that node immediately into the path.
To this end we look for the best insertion point before or after one of its neighbors in
the candidate subgraph. This way more nodes of degree less than or equal to the given
limit may occur which are also inserted rightaway. The selection of the next nodes in
the algorithm is accomplished as in variant 6.1.3. The worst case time complexity is
still quadratic.6.1. Nearest Neighbor Heuristics
79
6.1.6 Using Rotation Operations
The idea of this heuristic is to try to grow the tour within the candidate subgraph. If
at the current node the tour cannot be extended using candidate edges it is tried to
perform a sequence of rotation operations. Such an operation introduces an unused
subgraph edge from the current node. Since this results in a cycle with the partial tour,
another edge has to be eliminated and we obtain a new last node from which we can
try to extend the path.
k
i
l
Figure 6.6 A rotation operation
Figure 6.6 depicts a rotation operation. If at the current last node l the path cannot
be extended within the subgraph we try to use the subgraph edge {l, k}. To break the
resulting cycle we delete the edge {k, i} and try to extend the path now starting at i.
A sequence of rotations can be performed if also the extension from i fails. If the tour
cannot be extended this way using only subgraph edges a neighbor search like in one of
the previous variants can be performed.
6.1.7 Comparison of Variants
We compare the variants with the standard heuristic implemented according to 6.1.2.
It is clear that the tours produced by the algorithms heavily depend on the choice of
the starting point. Since we cannot look for the best starting point we always chose
to start with node  n 2 , thus giving an unbiased starting node to the heuristics. The
chosen candidate subgraph was the 10 nearest neighbor subgraph for variants 6.1.3
through 6.1.6.
In variant 6.1.4 we examined at most 20 predecessors to extend the path. In variant 6.1.5
forgotten nodes were inserted as soon as they were merely connected to at most three
free nodes. In variant 6.1.6 a sequence of rotations was limited to be composed of at
most five single rotations.
Table 6.7 shows the tour lengths (given as deviation in percent from the best known
lower bounds) obtained by applying the different procedures to our set of test problems.
Variants 6.1.2 through 6.1.6 are denoted by Variant 1 through Variant 5 in this table.
The best solution found for every problem instance is marked with a â€˜*â€™.
The results strongly support variant 6.1.5 which avoids adding too many isolated nodes
in the end. Usually, this decreases the tour length considerably. The quality of the
solutions can be expected to be in the range of 15% to 25% above optimality. In
Johnson (1990) an average excess of 24% over an approximation of the Held-Karp
lower bound (see Chapter 10) is reported for randomly generated problems.80
Chapter 6. Construction Heuristics
Problem
d198
lin318
fl417
pcb442
u574
p654
rat783
pr1002
u1060
pcb1173
d1291
rl1323
fl1400
u1432
fl1577
d1655
vm1748
rl1889
u2152
pr2392
pcb3038
fl3795
fnl4461
rl5934
Average
Variant 1
25.79
26.85
21.28
21.36
29.60
31.02
27.13
24.35
30.43
28.18
22.97
22.30
42.42
25.50
27.65
25.99
25.67
28.37
25.80
24.96
23.63
24.44
25.31
22.93
26.27
Variant 2
16.51*
22.52
17.84*
22.91
21.11
18.75*
24.76
25.18
27.14
27.69
15.44*
21.94
42.96
28.04
20.34*
25.06
26.78
25.54
25.85
27.23
23.53
25.92
23.24
23.41
26.58
Variant 3
25.86
33.06
36.42
18.63
25.90
25.80
27.98
28.96
27.33
27.30
25.54
25.51
30.64
31.55
21.97
20.82
31.90
23.85*
23.22
26.09
28.39
32.85
26.98
24.77
27.04
Variant 4
20.88
15.90*
25.36
13.51*
18.67*
25.98
18.86*
18.16*
24.14*
18.09*
17.33
16.81*
30.23*
19.21*
23.30
19.81*
19.11*
25.62
18.97*
22.68*
19.16*
20.09*
19.53*
18.75*
21.45
Variant 5
28.09
27.84
31.80
27.15
29.16
24.43
27.02
27.97
29.42
29.85
16.09
25.10
48.39
25.71
21.55
23.38
31.89
25.41
23.14
26.20
26.92
35.24
24.99
24.38
28.28
Table 6.7 Results of nearest neighbor variants
1.2
1.0
0.8
0.6
0.4
0.2
0.0
0
1000
2000
[S]
[1]
3000
[2]
4000
[3]
[4]
5000
[5]
Figure 6.8 CPU times for nearest neighbor variants
600081
6.1. Nearest Neighbor Heuristics
CPU times for the complete set of instances are shown in Figure 6.8. The running
times for the variants do not include the time to set up the Delaunay graph or the 10
nearest neighbor subgraph. These times were given in Chapters 4 and 5. To document
the speed up obtained by using the different variants we have also included the running
time for the standard implementation. Variant i is indicated by the number [i], the
standard implementation is indicated by [S].
Figure 6.8 clearly visualizes that already simple algorithms of quadratic time complexity
take quite some time when applied to large problems. The comparison of the variants
gives a clear picture. All the variants are much faster than the standard nearest neighbor
algorithm (even if the preprocessing times would be included). When considering larger
random problems (results are not displayed here), variants 2, 3, and 4 seem to exhibit
a quadratic component in their running time while variants 1 and 5 seem to have
subquadratic running times.
6.1.8 Stability of Nearest Neighbor Heuristics
Since we have performed only one run of each heuristic for every sample problem (start-
ing with node  n 2 ) we cannot be absolutely sure that Table 6.7 gives a correct assessment
of the five heuristics. We have therefore examined the average quality of each variant for
three sample problems. To this end we have performed each heuristic for every starting
node l = 1, 2, . . . , n.
Table 6.9 shows the results. Each line corresponds to one variant and gives (in that
sequence) the length of the best, resp. worst tour, the average tour length obtained, the
span between best and worst tour (i.e., worst quality âˆ’ best quality), and the standard
deviation.
Variant
lin318
1
2
3
4
5
pcb442
1
2
3
4
5
u1060
1
2
3
4
5
Minimum Maximum Average Span Deviation
16.89
18.58
20.86
12.96
21.54 39.82
35.96
37.66
29.24
36.94 24.89
25.88
27.12
20.25
29.37 22.92
17.38
16.80
16.28
15.40 2.99
2.96
3.65
3.00
3.07
18.01
16.60
17.19
11.77
15.67 38.15
32.23
34.31
30.03
36.30 29.66
22.11
26.06
16.63
25.81 20.14
15.63
17.12
18.27
20.63 3.63
2.84
3.44
2.74
3.26
22.33
21.25
24.18
19.04
24.98 36.51
39.19
37.99
28.46
37.44 26.11
30.05
28.89
22.95
30.03 14.17
17.94
13.81
9.42
12.46 1.93
3.45
2.21
1.70
2.29
Table 6.9 Sensitivity analysis for nearest neighbor variants
The results verify that insertion of forgotten neighbors leads to the best results. The
average quality of the tours obtained this way is substantially better than for the other82
Chapter 6. Construction Heuristics
four variants. The other variants perform more or less the same. The span is consider-
able, the quality of the tours strongly depends on the choice of the starting node.
6.2 Insertion Heuristics
A further intuitive approach is to start with tours on small subsets (including trivial
â€œtoursâ€ on one or two nodes) and then extend these tours by inserting the remaining
nodes. This principle is realized by the following procedure.
procedure insertion
(1) Select a starting tour through k nodes v 1 , v 2 , . . . , v k (k â‰¥ 1) and set W = V \
{v 1 , v 2 , . . . , v k }.
(2) As long as W  = âˆ… do the following.
(2.1) Select a node j âˆˆ W according to some criterion.
(2.2) Insert j at some position in the tour and set W = W \ {j}.
end of insertion
Using this principle a tour is built containing more and more nodes of the problem until
all nodes are inserted and the final Hamiltonian tour is found.
6.2.1 Standard Versions
Of course, there are several possibilities for implementing such an insertion scheme. The
main difference is the determination of the order in which the nodes are inserted. The
starting tour is usually just some tour on three nodes or, an edge (k = 2), or even a
loop containing only one node (k = 1). We will consider also another type of starting
tour below. The selected node to be inserted is usually inserted into the tour at the
point causing shortest increase in the length of the tour.
We say that a node is a tour node if it is already contained in the partial tour. For
j âˆˆ W 
we define d min (j) = min{c ij | i âˆˆ V \ W }, d max (j) = max{c ij | i âˆˆ V \ W }, and
s(j) = iâˆˆV \W c ij .
The following possibilities for extending the current tour are considered.
6.2.1.1 Nearest Insertion
Insert the node that has the shortest distance to a tour node, i.e., select j with d min (j) =
min{d min (l) | l âˆˆ W }.
6.2.1.2 Farthest Insertion 1
Insert the node whose minimal distance to a tour node is maximal, i.e., select j with
d min (j) = max{d min (l) | l âˆˆ W }.
6.2.1.3 Farthest Insertion 2
Insert the node that has the farthest distance to a tour node, i.e., select j with d max (j) =
max{d max (l) | l âˆˆ W }.6.2. Insertion Heuristics
83
6.2.1.4 Farthest Insertion 3
Insert the node whose maximal distance to a tour node is minimal, i.e., select j with
d max (j) = min{d max (l) | l âˆˆ W }.
6.2.1.5 Cheapest Insertion 1
Among all nodes not inserted so far, choose a node whose insertion causes the lowest
increase in the length of the tour. I.e., among all nodes not inserted so far, choose a
node which can be inserted causing the lowest increase in the length of the tour.
6.2.1.6 Cheapest Insertion 2
In the cheapest insertion heuristic, we have to know for every node not in the tour its
cheapest insertion point. Update of this information is expensive (see below). In this
variant, we only perform a partial update of the best insertion points in the following
sense. Suppose node j has just been inserted into the partial tour. This may have the
effect that the best insertion point changes for a non-tour node, say l. Now, we do not
consider all possibilities to insert l, but only insertions before or after j and a limited
number k of jâ€™s successors and predecessors. This has the consequence, that, for some
nodes, not necessarily the best insertion point is determined.
6.2.1.7 Random Insertion
Select the node to be inserted at random.
6.2.1.8 Largest Sum Insertion
Insert the node whose sum of distances to tour nodes is maximal, i.e., select j with
s(j) = max{s(l) | l âˆˆ W }. This is equivalent to choosing the node with maximal
average distance to tour nodes.
6.2.1.9 Smallest Sum Insertion
Insert the node whose sum of distances to tour nodes is minimal, i.e., select j with
s(j) = min{s(l) | l âˆˆ W }. This is equivalent to choosing the node with minimal average
distance to tour nodes.
There are also variants of these ideas where the node selected is not inserted at cheapest
insertion cost but as a neighbor of that tour node that is nearest to it. These variants
are usually named â€œadditionâ€ instead of insertion. Bentley (1992) reports that the
results are slightly inferior.
All heuristics except for cheapest Insertion have running time O(n 2 ). Cheapest Insertion
can be implemented to be executed in time O(n 2 log n) by storing for each external
node a heap based on the insertion cost at the possible insertion points. Because this
procedure requires O(n 2 ) space it cannot be used for large problem instances. The fast
version of cheapest insertion runs in time O(n 2 ) because of the limited update.
We give an illustration of insertion principles in Figure 6.10 for a Euclidean problem
instance. In the next step nearest insertion adds node i, farthest insertion adds node j,
and cheapest insertion adds node k to the tour.84
Chapter 6. Construction Heuristics
j
i
k
Figure 6.10 Illustration of insertion heuristics
Nearest insertion and cheapest insertion tours are less than twice as long as an optimal
tour if the triangle inequality holds (Rosenkrantz, Stearns & Lewis (1977)). It
can also be shown that there exist classes of problem instances for which the length of
the heuristic solution is 2 âˆ’ n 2 times longer than the optimal tour, thus proving that
these approximation results are tight.
Recently it was shown (Hurkens (1991)) that for random or farthest insertion there
exist examples where these heuristics yield tours that are 13/2 times longer than an
optimal tour (although the triangle inequality is satisfied).
We have compared the nine insertion heuristics for our set of sample problems. Each
heuristic was started with the cycle ( n 2 ,  n 3 ,  n 4 ) to get unbiased starting conditions.
For the variant of cheapest insertion described in 6.2.1.6 we have set k = 30.
Table 6.11 displays the results (headings 1 through 9 corresponding to the insertion
heuristics 6.2.1.1 through 6.2.1.9). The best solution in each row is marked with a â€˜*â€™.
Farthest insertion 1 performs best for our set of problems followed closely by random
insertion. The fast version of cheapest insertion performs as well as the full version, the
time for doing correct cheapest insertion does not pay off. In fact, the results were the
same except for two cases. However, though reasonable at first sight, cheapest insertion
performs significantly worse that farthest insertion. The relatively good performance of
farthest insertion can be explained when observing the development of the generated
tour: after few steps already, a good global outline of the final tour is obtained. Almost
the same is true for random insertion. An average excess over the Held-Karp bound
of 27% for the nearest insertion and of 13.5% for the farthest insertion procedure is
reported in Johnson (1990) for random problem instances.85
6.2. Insertion Heuristics
Problem
d198
lin318
fl417
pcb442
u574
p654
rat783
pr1002
u1060
pcb1173
d1291
rl1323
fl1400
u1432
fl1577
d1655
vm1748
rl1889
u2152
pr2392
pcb3038
fl3795
fnl4461
rl5934
Average
1
13.19
21.62
12.50
20.89
22.33
10.81
23.04
18.57
21.39
25.84
22.90
31.01
20.28
15.26
21.61
20.18
21.26
23.82
21.09
24.70
23.12
19.61
21.10
27.40
20.98
2
3.85*
10.87
5.48
13.83
11.39*
6.89
12.09*
10.85*
12.68
14.22*
23.78
18.89*
8.45*
12.59*
15.17*
17.09*
13.54*
19.10
19.55
14.32*
14.89*
21.97
12.03*
22.17
13.99
3
7.57
18.41
13.37
16.99
22.68
11.33
22.52
24.81
21.52
26.82
27.29
29.30
14.56
20.43
20.04
22.22
25.37
27.74
28.64
28.26
24.54
19.58
27.69
30.12
22.16
4
14.80
24.30
13.37
29.06
26.32
5.94
28.72
27.24
27.55
32.67
29.50
27.80
24.78
20.08
25.21
27.80
33.59
32.70
32.84
33.55
27.84
29.45
28.90
33.42
26.56
5
11.08
18.39
12.39
21.15
19.12
5.79*
16.02
16.61
18.67
21.50
17.01*
24.81
17.98
12.65
17.08
18.83
18.86
21.24
16.12*
20.50
17.08
12.79*
15.97
21.84*
17.23
6
11.08
18.39
12.39
21.15
19.12
5.79*
16.02
16.61
18.67
21.50
17.01*
24.81
17.76
12.65
17.08
18.77
18.86
21.24
16.12*
20.50
17.08
12.79*
15.97
21.84*
17.22
7
8.17
9.18*
3.29*
12.23*
11.64
9.87
13.37
12.50
11.43*
16.58
22.13
20.64
8.47
12.63
18.70
17.69
13.87
17.30*
19.76
16.65
16.69
19.77
12.99
22.71
14.51
8
8.15
20.02
7.84
27.07
23.32
11.30
26.37
23.98
23.94
29.56
31.06
29.30
16.30
23.84
26.66
28.20
29.52
29.99
28.26
31.75
27.57
21.62
28.99
33.56
24.51
9
7.78
16.27
9.04
20.43
22.21
12.64
25.02
25.42
21.58
28.80
18.70
26.56
16.44
20.54
17.97
23.95
24.26
27.53
28.98
28.32
27.28
25.62
28.03
30.36
22.24
Table 6.11 Results of insertion heuristics
Note that the quality of the solutions of the different heuristics is highly problem de-
pendent. Running time will be addressed in the next section.
6.2.2 Fast Versions of Insertion Heuristics
As in the case of the nearest neighbor heuristic we want to give priority to edges from
a candidate set to speed up the insertion heuristics.
To this end we base all our calculations on the edges contained in the candidate set.
E.g., now the distance of a non-tour node v to the current partial tour is infinite if there
is no candidate edge joining v to the tour, otherwise it is the length of the shortest such
edge joining v to the tour. Using this principle the heuristics of the previous chapter
are modified as follows.
6.2.2.1 Nearest Insertion
If there are nodes connected to the current tour by a subgraph edge then insert the node
connected to the tour by the shortest edge. Otherwise insert an arbitrary node.
6.2.2.2 Farthest Insertion 1
Among the nodes that are connected to the tour insert the one whose minimal distance
to the tour is maximal. If all external nodes are not connected to the tour insert an
arbitrary node.86
Chapter 6. Construction Heuristics
6.2.2.3 Farthest Insertion 2
Among the nodes that are connected to the tour insert the one whose distance to the
tour is maximal. If all external nodes are not connected to the tour insert an arbitrary
node.
6.2.2.4 Farthest Insertion 3
Among the nodes that are connected to the tour insert the one whose maximal distance
to the tour is minimal. If all external nodes are not connected to the tour insert an
arbitrary node.
6.2.2.5 Cheapest Insertion 1
Insert a node connected to the current tour by a subgraph edge whose insertion yields
minimal additional length. If no such node exists then compute the cheapest insertion
possibility. Insertion information is only updated for those nodes that are connected to
the node inserted last by a subgraph edge. This way insertion information may become
incorrect for some nodes since it may not be updated.
6.2.2.6 Cheapest Insertion 2
Insert a node connected to the current tour by a subgraph edge whose insertion yields
minimal additional length. If no such node exists then insert an arbitrary node. Update
of insertion information is further simplified as in 6.2.1.6.
6.2.2.7 Random Insertion
Select the node to be inserted at random where priority is given to nodes connected to
the current tour by a subgraph edge.
6.2.2.8 Largest Sum Insertion
For each node compute the sum of lengths of the subgraph edges connecting this node
to the current tour. Insert the node whose sum is maximal. If all external nodes are
not connected to the tour insert an arbitrary node.
6.2.2.9 Smallest Sum Insertion
For each node compute the sum of lengths of the subgraph edges connecting this node
to the current tour. Insert the node whose sum is minimal. If all external nodes are not
connected to the tour insert an arbitrary node.
We have performed the same experiment as for the heuristics in the complete graph.
Results are shown in Table 6.12. Now, the advantages of farthest or random insertion are
lost due to the restricted view. They still perform best but tour quality is significantly
inferior than before. The cheapest insertion variants give some very bad solutions which
is caused by the incomplete update of insertion information.
To visualize the CPU time for insertion heuristics we have compared five variants in
Figure 6.13: Farthest insertion 6.2.1.2 ([1]), Cheapest insertion 6.2.1.5 ([2]), Cheapest
insertion 6.2.1.6 ([3]), Farthest insertion 6.2.2.2 ([4]), Cheapest insertion 6.2.2.5 ([5]).
The diagram shows that standard farthest insertion compares favorably with all cheapest
insertion variants. Speed up using candidate graphs is considerable, but due to inferior
quality there seems to be no point in using these heuristics. This will be further justified
in Chapter 7.87
6.2. Insertion Heuristics
Problem
d198
lin318
fl417
pcb442
u574
p654
rat783
pr1002
u1060
pcb1173
d1291
rl1323
fl1400
u1432
fl1577
d1655
vm1748
rl1889
u2152
pr2392
pcb3038
fl3795
fnl4461
rl5934
Average
1
15.31
25.69
36.20
28.85
22.54
48.59
26.07
19.89*
25.39
28.93
31.24
37.34
30.83
21.61
34.75
28.95
26.05
35.45
28.99
27.01
25.19
35.77
23.47
44.63
29.53
2
7.84*
20.03
31.08
18.59
17.34*
46.22
15.35*
21.30
17.54*
19.28*
25.33
22.46*
31.66
17.81
27.27
23.22
21.07*
25.60*
24.68
23.14*
18.48*
24.96*
16.88*
31.26*
22.85
3
9.41
18.85
33.92
20.16
19.97
36.84*
18.31
29.43
20.42
21.60
26.61
26.82
28.69
20.29
28.95
23.74
21.82
29.58
28.89
27.88
21.47
29.32
17.23
29.81
24.58
4
13.67
23.78
44.57
19.93
18.60
42.62
20.00
20.37
20.78
21.87
27.29
32.97
29.67
20.27
28.19
26.05
23.34
30.32
24.46
28.22
19.67
30.18
20.27
35.55
25.94
5
13.47
42.95
24.36*
29.66
25.28
78.21
24.90
26.54
22.95
34.27
20.91
31.19
85.17
28.08
31.09
33.35
22.90
42.91
21.34*
35.15
25.61
40.31
31.74
51.60
34.33
6
13.47
42.95
24.36*
29.66
25.28
78.81
24.90
26.50
24.07
34.27
20.73*
31.43
94.98
29.89
31.09
35.48
22.90
42.39
21.34*
32.68
25.72
40.62
36.16
48.17
34.91
7
10.42
18.09
26.82
20.07
17.53
49.36
17.47
22.52
18.52
21.84
24.78
26.04
19.07*
20.25
23.67*
22.40*
22.27
31.51
25.03
24.56
20.05
25.80
17.64
32.91
23.28
8
13.80
17.63*
38.90
14.08*
19.02
44.91
16.11
20.74
19.97
22.42
26.81
31.16
27.06
16.51*
29.51
24.38
21.20
28.60
25.06
24.28
20.00
33.85
18.11
33.31
24.48
9
11.13
24.41
31.99
27.33
26.72
54.21
29.58
28.17
25.55
28.86
28.16
35.37
30.59
25.52
36.09
29.23
29.31
35.12
30.82
31.41
28.57
32.41
28.51
37.97
30.29
Table 6.12 Results of fast insertion heuristics
300
250
200
150
100
50
0
0
1000
2000
[1]
3000
[2]
[3]
4000
[4]
5000
[5]
Figure 6.13 CPU times for some insertion heuristics
600088
Chapter 6. Construction Heuristics
6.2.3 Convex Hull Start
The following observation suggests to use a specific starting tour for Euclidean problems.
Let v 1 , v 2 , . . . , v k be located on the boundary of the convex hull of the given points (in
this order). Then, in any optimal tour, this sequence is respected (otherwise the tour
would contain crossing edges and hence could not be optimal). Therefore it is reasonable
to use (v 1 , v 2 , . . . , v k ) as starting tour for the insertion heuristics.
From the results of Chapter 4 we know that convex hulls can be computed very quickly
(in time Î˜(n log n)). Therefore, only negligible additional CPU time is necessary to
compute this type of starting tour for the insertion heuristics in the Euclidean case.
Results with the convex hull start using the standard versions of the insertion heuristics
are displayed in Table 6.14.
Problem
d198
lin318
fl417
pcb442
u574
p654
rat783
pr1002
u1060
pcb1173
d1291
rl1323
fl1400
u1432
fl1577
d1655
vm1748
rl1889
u2152
pr2392
pcb3038
fl3795
fnl4461
rl5934
Average
1
12.86
15.08
14.24
16.52
17.24
17.07
16.90
20.05
22.78
21.61
25.58
25.86
14.04
15.34
20.30
20.94
19.83
25.74
19.03
21.26
22.41
24.06
22.21
26.54
19.90
2
6.73
10.82
5.28
10.37*
9.95*
3.05*
12.72
11.10*
10.69
15.44*
21.80
15.10*
5.79*
12.65
15.18*
15.05
10.77*
17.98*
18.26
15.24*
14.31*
21.60
11.94*
20.28*
13.00
3
6.41
19.70
14.95
17.54
23.55
12.40
24.68
25.65
24.71
26.14
25.52
28.57
14.62
21.06
18.72
21.55
25.31
29.40
29.05
28.88
25.57
16.23
29.49
30.27
22.50
4
6.58
16.82
5.65
18.89
18.47
6.38
23.89
21.66
22.79
26.62
26.22
25.74
12.05
18.73
28.25
27.26
24.04
31.63
27.32
27.07
24.62
27.65
27.94
31.89
22.01
5
8.51
11.42
7.61
11.83
14.67
8.15
15.16
14.23
16.65
19.18
14.69*
20.30
9.73
11.73*
18.09
13.23*
16.94
18.72
13.98*
17.52
16.47
13.81*
15.42
21.24
14.55
6
8.51
11.42
7.61
11.83
14.67
8.15
15.16
14.23
16.65
19.18
14.69*
20.30
9.73
11.73*
18.09
13.23*
16.94
18.72
13.98*
17.52
16.47
13.81*
15.42
21.24
14.55
7
4.37*
7.97*
2.77 *
13.62
10.73
6.49
11.90*
13.27
10.39*
18.25
21.03
20.18
8.35
13.19
15.58
15.99
12.03
18.15
19.73
15.83
15.44
18.35
13.07
21.71
13.68
8
7.03
18.01
6.83
19.17
19.83
13.16
22.31
26.71
23.99
28.45
22.06
27.73
13.69
22.48
37.73
26.67
26.52
30.07
27.73
29.87
25.81
25.40
29.14
29.66
23.34
9
7.34
16.39
8.36
22.71
20.62
10.85
23.80
21.76
22.65
26.35
22.37
27.59
17.90
21.66
24.71
24.03
25.48
27.26
27.05
26.18
27.12
19.04
27.67
29.58
22.02
Table 6.14 Results of insertion heuristics with convex hull start
There is a slight improvement in the quality of tours with respect to the starting tour
( n 2 ,  n 3 ,  n 4 ). Farthest and random insertion do not profit very much from the convex
hull start since they generate good globals tours themselves. For the other heuristics,
this starting variant is more important, but still leading to poor final tours.89
6.3. Heuristics Using Spanning Trees
6.2.4 Stability of Insertion Heuristics
As in the case of the nearest neighbor heuristic we also investigated how strongly our
results depend on the choice of the starting tour.
To get an impression of this, we performed experiments for problems d198, lin318, and
pcb442. Each heuristic was started with all possible tours consisting of just two nodes.
Numbers displayed in Table 6.15 have the same meaning as in Table 6.9 for the nearest
neighbor methods.
Variant
d198
1
2
3
4
5
6
7
8
9
lin318
1
2
3
4
5
6
7
8
9
pcb442
1
2
3
4
5
6
7
8
9
Minimum Maximum Average Span Deviation
8.52
1.53
4.75
6.95
7.57
8.25
2.63
5.87
3.83 17.78
10.72
16.90
17.00
15.65
14.18
8.86
19.99
13.24 12.59
6.08
7.70
12.14
10.95
11.01
5.53
11.55
7.75 9.25
9.18
12.15
10.05
8.08
5.93
6.23
14.13
9.41 1.70
2.11
2.07
1.77
1.40
1.30
1.10
3.56
1.73
17.18
5.47
12.93
20.13
13.81
13.81
6.67
16.48
11.99 25.97
13.24
22.38
29.30
22.15
22.15
14.18
26.60
23.34 22.00
9.00
18.72
24.92
18.61
18.69
10.89
21.78
18.92 8.79
7.77
9.45
9.17
8.35
8.35
7.51
10.12
11.35 1.82
1.50
1.30
1.73
1.46
1.47
1.65
1.69
1.86
13.89
9.05
14.93
21.78
12.73
12.73
10.22
20.13
14.22 23.17
18.03
23.99
33.13
21.38
21.55
17.71
32.93
23.64 18.50
13.00
18.45
27.50
17.76
17.80
14.00
25.30
18.56 9.29
8.97
9.06
11.36
8.66
8.82
7.48
12.80
9.41 1.85
1.49
1.10
1.99
1.47
1.54
1.44
2.44
1.52
Table 6.15 Sensitivity analysis for insertion heuristics
Farthest insertion and random insertion also performed best here. Stability of insertion
heuristics is much better than for the nearest neighbor variants. The table also shows
that performance and stability are highly problem dependent.
6.3 Heuristics Using Spanning Trees
The heuristics considered so far construct in their standard versions the tours â€œfrom
scratchâ€ in the sense that they do not exploit any additional knowledge about the90
Chapter 6. Construction Heuristics
problem instance. In their fast variants they used the presence of a candidate subgraph
to guide the tour construction.
The two heuristics to be described next use a minimum spanning tree as a basis for
generating tours. They are particularly suited for problem instances obeying the triangle
inequality. In this case performance guarantees are possible. Nevertheless, in principle
they can also be applied to general instances.
Before describing these heuristics, consider the following observation. Suppose we are
given some Eulerian tour containing all nodes of the given problem instance. If the
triangle inequality is satisfied we can derive a Hamiltonian tour which is not longer
than the Eulerian tour.
Let v i 0 , v i 1 , . . . , v i k be the sequence in which the nodes (including repetitions) are visited
when traversing the Eulerian tour starting at v i 0 and returning to v i k = v i 0 . The
following procedure obtains a Hamiltonian tour.
procedure obtain tour
(1) Set Q = {v i 0 }, T = âˆ…, v = v i 0 , and l = 1.
(2) As long as |Q| < n perform the following steps.
(2.1) If v i l âˆˆ Q then set Q = Q âˆª {v i l }, T = T âˆª {vv i l }, and v = v i l .
(2.2) Set l = l + 1.
(3) Set T = T âˆª {vv i 0 }.
(4) T is a Hamiltonian tour.
end of obtain tour
Every connection made in this procedure is either an edge of the Eulerian tour or is
a shortcut replacing a subpath of the Eulerian tour by an edge connecting its two
endnodes. This shortcut reduces the length of the tour if the triangle inequality is
satisfied. Hence the resulting Hamiltonian tour cannot be longer than the Eulerian
tour.
Both heuristics start with a minimum spanning tree and differ only in how a Eulerian
graph is generated from the tree.
procedure doubletree
(1) Compute a minimum spanning tree.
(2) Double all edges of the tree to obtain a Eulerian graph.
(3) Compute a Eulerian tour in this graph.
(4) Call obtain tour to get a Hamiltonian tour.
end of doubletree6.3. Heuristics Using Spanning Trees
91
Note that we get multiple edges in Step (2) which we have not allowed in our definition
of graphs in Chapter 2. But it is clear that this does not create any problems. Our graph
data structure is able to handle multiple edges. The running time of the algorithm is
dominated by the time needed to obtain a minimum spanning tree. Therefore we have
time complexity Î˜(n 2 ) for the general TSP and Î˜(n log n) for Euclidean problems.
If we compute the minimum spanning tree with Primâ€™s algorithm (Prim (1957)), we
could as well construct a Hamiltonian cycle along with the tree computation. We always
keep a cycle on the nodes already in the tree (starting with the loop consisting of only
one node) and insert the node into the current cycle which is added to the spanning
tree. If this node is inserted at the best possible position this algorithm is identical
to the nearest insertion heuristic. If it is inserted before or after its nearest neighbor
among the cycle nodes, then we obtain the nearest addition heuristic.
Christofides (1976) suggested a better method to make spanning trees Eulerian.
Namely, it is sufficient to add a perfect matching on the odd-degree nodes of the tree.
(A perfect matching of a node set W , |W | = 2k, is a set of k edges such that each
node of W is incident to exactly one of these edges.) After addition of all edges of this
perfect matching, all node degrees are even and hence the graph is Eulerian.
Figure 6.16 illustrates this idea. The solid edges form a spanning tree and the broken
edges form a perfect matching on the odd-degree nodes of the spanning tree.
Figure 6.16 Illustration of spanning tree heuristic
The cheapest way (with respect to edge weights) to obtain a Eulerian graph is to add
a minimum weight perfect matching.92
Chapter 6. Construction Heuristics
procedure christofides
(1) Compute a minimum spanning tree.
(2) Compute a minimum weight perfect matching on the odd-degree nodes of the
tree and add it to the tree to obtain a Eulerian graph.
(3) Compute a Eulerian tour in this graph.
(4) Call obtain tour to get a Hamiltonian tour.
end of christofides
This procedure takes considerably more time than the previous one. Computation of
a minimum weight perfect matching on k nodes can be performed in time O(k 3 ) (Ed-
monds (1965)). Since a spanning tree may have O(n) odd-degree nodes, Christofidesâ€™
heuristic has cubic worst case time.
The sequence of the edges in the Eulerian tour is not unique. So one can try to find
better solutions by determining different Eulerian tours. We do not elaborate on this
since the gain to be expected is small.
Since a minimum spanning tree is not longer than a shortest Hamiltonian tour and since
the matching computed in Step (2) of Christofidesâ€™ heuristic has weight at most half of
the length of an optimal tour the following theorem holds.
Theorem 6.2 Let an instance of the TSP obeying the triangle inequality be given.
(i) The double tree heuristic produces a tour which is at most twice as long as an
optimal tour.
(ii) Christofidesâ€™ heuristic produces a tour which is at most 1.5 times as long as an
optimal tour.
There are classes of instances (Cornuejols & Nemhauser (1978)) where the Chri-
stofides heuristic yields a tour that is (3n âˆ’ 1)/(2n) times longer than the optimal tour,
thus proving that the above bound is tight.
Because of the cubic worst case running time, we avoid the computation of minimum
weight matchings and simplify the heuristic as follows. First we double all edges to
leaves, and then we compute a farthest insertion tour on the remaining (and newly
introduced) odd-degree nodes. This tour induces two perfect matchings and we add the
shorter one to our subgraph which is then Eulerian. Time complexity of this procedure
is O(n 2 ).
Table 6.17 compares the two heuristics with respect to CPU time and length of the
generated tours.
The double tree heuristic has a very poor performance. But, also the performance
of the Christofides variant is disappointing (coinciding with the findings in Johnson
(1990)), in particular when taking into account that it has the best worst case bound
among the heuristics. This is not due to our simplification, but it was observed in
many experiments that it does not pay off to compute exact minimum weight perfect
matchings in Step (2).93
6.3. Heuristics Using Spanning Trees
Problem Double tree
d198
22.62
lin318
41.32
fl417
36.04
pcb442
39.64
u574
36.29
p654
36.89
rat783
36.75
pr1002
37.29
u1060
34.30
pcb1173
42.29
d1291
48.16
rl1323
39.04
fl1400
39.40
u1432
45.78
fl1577
42.75
d1655
37.47
vm1748
31.68
rl1889
40.50
u2152
48.11
pr2392
37.22
pcb3038
43.23
fl3795
41.38
fnl4461
39.47
rl5934
48.18
Average
39.41
Christofides
15.67*
18.42*
24.52*
18.59*
20.08*
21.73*
21.34*
20.67*
18.97*
18.77*
24.31*
14.05*
22.10*
24.05*
13.27*
18.92*
21.73*
14.00*
22.73*
18.70*
20.58*
17.25*
21.92*
15.17*
19.48
Table 6.17 Comparison of tree heuristics
2.0
1.5
1.0
0.5
0.0
0
1000
2000
3000
[1]
4000
5000
[2]
Figure 6.18 CPU times for tree heuristics
