cema CÃ³mo leer y escribir archivos en Python
abril 26, 2019 Por Daniel RodrÃ­guez Dejar un comentario
Tiempo de lectura: 4 minutos


Comparte
Tuitea
Pinear
Comparte
La importaciÃ³n y exportaciÃ³n de datos desde archivos son tareas que se realizan con bastante asiduidad. Por ello, en Python, librerÃ­as como pandas incorporan herramientas para el manejo de archivos CSV o en formato Microsoft Excel. Pero, si los datos no se encuentran en un formato estÃ¡ndar, estas herramientas pueden no ser las adecuadas. Por lo tanto, puede ser necesario manejar los archivos directamente. En esta entrada se va a mostrar cÃ³mo leer y escribir archivos en Python.

Abrir un archivo para leer o escribir en Python
Antes de leer o escribir archivos con Python es necesario es necesario abrir una conexiÃ³n. Lo que se puede hacer con el comando open(), al que se le ha de indicar el nombre del archivo. Por defecto la conexiÃ³n se abre en modo lectura, con lo que no es posible escribir en el archivo. Para poder escribir es necesario utilizar la opciÃ³n "w" con la que se eliminarÃ¡ cualquier archivo existente y crearÃ¡ uno nuevo. Otra opciÃ³n que se puede utilizar es "a", con la que se aÃ±adirÃ¡ nuevo contenido al archivo existente. Las opciones se pueden ver en el siguiente cÃ³digo.

# Abre el archivo para escribir y elimina los archivos anteriores si existen
fic = open("text.txt", "w")
# Abre el archivo para agregar contenido
fic = open("text.txt", "a")
# Abre el archivo en modo lectura
fic = open("text.txt", "r")
En todos los casos, una vez finalizado las operaciones de lectura y escritura con los archivos, una buena prÃ¡ctica es cerrar el acceso. Para lo que se debe utilizar el mÃ©todo close().

Escribir archivos de texto en Python
Antes guardar un archivo es necesario disponer de un vector con las cadenas de texto que se desean guardar. Para ello se puede crear un vector al que se le puede llamar data.

data = ["LÃ­nea 1", "LÃ­nea 2", "LÃ­nea 3", "LÃ­nea 4", "LÃ­nea 5"]
Para escribir el contenido de este vector en un archivo se puede hacer de dos maneras: lÃ­nea a lÃ­nea o de una sola vez.

Escribir el archivo lÃ­nea a lÃ­nea
El mÃ©todo mÃ¡s fÃ¡cil directo para volcar el vector en un archivo es escribir el contenido lÃ­nea a lÃ­nea. Para ello se puede iterar sobre el archivo y utilizar el mÃ©todo write de archivo. Este proceso es lo que se muestra en el siguiente ejemplo.

fic = open("text_1.txt", "w")
for line in data:
    fic.write(line)
    fic.write("\n")
    
fic.close()
NÃ³tese que los elementos de vector no finalizan con el carÃ¡cter salto de lÃ­nea. Por lo tanto, es necesario aÃ±adir este despuÃ©s de escribir cada lÃ­nea. Ya que, de lo contrario, todos los elementos se escribirÃ­an en una Ãºnica lÃ­nea en el archivo de salida.

Una forma de escribir el archivo lÃ­nea a lÃ­nea sin que sea necesario incluir el salto de lÃ­nea es con la funciÃ³n print. Para lo es necesario incluir la opciÃ³n file con la conexiÃ³n al archivo. Esta opciÃ³n se puede ver en el siguiente ejemplo.

fic = open("text_2.txt", "w")
for line in data:
    print(line, file=fic)
    
fic.close()
Escribir el archivo de una vez
Finalmente, en el caso de que los dato se encuentren en un objeto iterable se puede utilizar el mÃ©todo writelines para volcar este de golpe. Aunque es necesario tener en cuenta que este mÃ©todo no agrega el salto de lÃ­nea, por lo que puede ser necesario agregarlo con antelaciÃ³n.

fic = open("text_3.txt", "w")
fic.writelines("%s\n" % s for s in data)
fic.close()
En el ejemplo se puede apreciar que se ha iterado sobre el vector para agregar el salto de lÃ­nea para cada elemento.

Leer archivos de texto en Python
La lectura de los archivos, al igual que la escritura, se puede hacer de dos maneras: lÃ­nea a lÃ­nea o de una sola vez.

Leer el archivo de una vez
El procedimiento para leer los archivos de texto mÃ¡s sencillo es hacerlo de una vez con el mÃ©todo readlines. Una vez abierto el archivo solamente se ha de llamar a este mÃ©todo para obtener el contenido. Por ejemplo, se puede usar el siguiente cÃ³digo.

fic = open('text_1.txt', "r")
lines = fic.readlines()
fic.close()
En esta ocasiÃ³n lines es un vector en el que cada elemento es una lÃ­nea del archivo. Alternativamente, en lugar del mÃ©todo readlines se puede usar la funciÃ³n list para leer los datos.

fic = open('text_1.txt', "r")
lines = list(fic)
fic.close()
Leer el archivo lÃ­nea a lÃ­nea
En otras ocasiones puede ser necesario leer el archivo lÃ­nea a lÃ­nea. Esto se puede hacer simplemente iterando sobre el fichero una vez abierto. En casa iteraciÃ³n se podrÃ¡ hacer con cada lÃ­nea cualquier operaciÃ³n que sea necesaria. En el siguiente ejemplo cada una de las lÃ­neas se agrega a un vector.

fic = open('text_1.txt', "r")
lines = []
for line in fic:
    lines.append(line)
fic.close()
Eliminar los saltos de lÃ­nea en el archivo importado
Los tres mÃ©todos que se han visto para leer los archivos importan el salto de lÃ­nea. Por lo que puede ser necesario eliminarlo antes de trabajar con los datos. Esto se puede conseguir de forma sencilla con el mÃ©todo rstrip de las cadenas de texto de Python. Lo que se puede hacer iterando sobre el vector.

[s.rstrip('\n') for s in lines]
Conclusiones
Hoy se ha visto cÃ³mo leer y escribir archivos en Python utilizando solamente las funciones estÃ¡ndar del lenguaje. Explicando tres mÃ©todos tanto para escribir los archivos como para leerlos. Aunque normalmente para la lectura de archivos CSV en Python lo mÃ¡s fÃ¡cil es utilizar pandas, puede ser que sea necesario procesar los datos de una forma no estÃ¡ndar. En estas situaciones es cuando los visto en esta entrada es bastante Ãºtil.
En este tutorial aprenderemos a utilizar diccionarios de datos en Python y algunos de sus mÃ©todos mÃ¡s importantes.

Python es un lenguaje de programaciÃ³n interpretado de alto nivel y orientado a objetos, con el cual podemos crear todo tipo de aplicaciones. Entre sus diversos tipos de estructuras de datos, se encuentra "Diccionarios de Datos". En este tutorial aprenderemos a utilizar esta estructura revisando sus mÃ©etodos mÃ¡s utilizados.

Â¿QuÃ© es un Diccionario de datos?
Un Diccionario es una estructura de datos y un tipo de dato en Python con caracterÃ­sticas especiales que nos permite almacenar cualquier tipo de valor como enteros, cadenas, listas e incluso otras funciones. Estos diccionarios nos permiten ademÃ¡s identificar cada elemento por una clave (Key).

Para definir un diccionario, se encierra el listado de valores entre llaves. Las parejas de clave y valor se separan con comas, y la clave y el valor se separan con dos puntos.

diccionario = {'nombre' : 'Carlos', 'edad' : 22, 'cursos': ['Python','Django','JavaScript'] }
Podemos acceder al elemento de un Diccionario mediante la clave de este elemento, como veremos a continuaciÃ³n:

print diccionario['nombre'] #Carlos
print diccionario['edad']#22
print diccionario['cursos'] #['Python','Django','JavaScript']
TambiÃ©n es posible insertar una lista dentro de un diccionario. Para acceder a cada uno de los cursos usamos los Ã­ndices:

print diccionario['cursos'][0]#Python
print diccionario['cursos'][1]#Django
print diccionario['cursos'][2]#JavaScript
Para recorrer todo el Diccionario, podemos hacer uso de la estructura for:

for key in diccionario:
  print key, ":", diccionario[key]
MÃ©todos de los Diccionarios
dict ()

Recibe como parÃ¡metro una representaciÃ³n de un diccionario y si es factible, devuelve un diccionario de datos.

dic =  dict(nombre='nestor', apellido='Plasencia', edad=22)

dic â†’ {â€˜nombreâ€™ : 'nestor', â€˜apellidoâ€™ : 'Plasencia', â€˜edadâ€™ : 22}
zip()

Recibe como parÃ¡metro dos elementos iterables, ya sea una cadena, una lista o una tupla. Ambos parÃ¡metros deben tener el mismo nÃºmero de elementos. Se devolverÃ¡ un diccionario relacionando el elemento i-esimo de cada uno de los iterables.

dic = dict(zip('abcd',[1,2,3,4]))

dic â†’   {â€˜aâ€™ : 1, â€™bâ€™ : 2, â€˜câ€™ : 3 , â€˜dâ€™ : 4}
items()

Devuelve una lista de tuplas, cada tupla se compone de dos elementos: el primero serÃ¡ la clave y el segundo, su valor.

dic =   {â€˜aâ€™ : 1, â€™bâ€™ : 2, â€˜câ€™ : 3 , â€˜dâ€™ : 4}
items = dic.items()

items â†’ [(â€˜aâ€™,1),(â€˜bâ€™,2),(â€˜câ€™,3),(â€˜dâ€™,4)]
keys()

Retorna una lista de elementos, los cuales serÃ¡n las claves de nuestro diccionario.

dic =  {â€˜aâ€™ : 1, â€™bâ€™ : 2, â€˜câ€™ : 3 , â€˜dâ€™ : 4}
keys= dic.keys()

keysâ†’ [â€˜aâ€™,â€™bâ€™,â€™câ€™,â€™dâ€™] 
values()

Retorna una lista de elementos, que serÃ¡n los valores de nuestro diccionario.

dic =  {â€˜aâ€™ : 1, â€™bâ€™ : 2, â€˜câ€™ : 3 , â€˜dâ€™ : 4}
values= dic.values()

valuesâ†’ [1,2,3,4] 
clear()

Elimina todos los Ã­tems del diccionario dejÃ¡ndolo vacÃ­o.

dic 1 =  {â€˜aâ€™ : 1, â€™bâ€™ : 2, â€˜câ€™ : 3 , â€˜dâ€™ : 4}
dic1.clean()

dic1 â†’ { }
copy()

Retorna una copia del diccionario original.

dic = {â€˜aâ€™ : 1, â€™bâ€™ : 2, â€˜câ€™ : 3 , â€˜dâ€™ : 4}
dic1 = dic.copy()

dic1 â†’ {â€˜aâ€™ : 1, â€™bâ€™ : 2, â€˜câ€™ : 3 , â€˜dâ€™ : 4}
fromkeys()

Recibe como parÃ¡metros un iterable y un valor, devolviendo un diccionario que contiene como claves los elementos del iterable con el mismo valor ingresado. Si el valor no es ingresado, devolverÃ¡ none para todas las claves.

dic = dict.fromkeys(['a','b','c','d'],1)

dic â†’  {â€˜aâ€™ : 1, â€™bâ€™ : 1, â€˜câ€™ : 1 , â€˜dâ€™ : 1}
get()

Recibe como parÃ¡metro una clave, devuelve el valor de la clave. Si no lo encuentra, devuelve un objeto none.

dic = {â€˜aâ€™ : 1, â€™bâ€™ : 2, â€˜câ€™ : 3 , â€˜dâ€™ : 4}
valor = dic.get(â€˜bâ€™) 

valor â†’ 2
pop()

Recibe como parÃ¡metro una clave, elimina esta y devuelve su valor. Si no lo encuentra, devuelve error.

dic = {â€˜aâ€™ : 1, â€™bâ€™ : 2, â€˜câ€™ : 3 , â€˜dâ€™ : 4}
valor = dic.pop(â€˜bâ€™) 

valor â†’ 2
dic â†’ {â€˜aâ€™ : 1, â€˜câ€™ : 3 , â€˜dâ€™ : 4}
setdefault()

Funciona de dos formas. En la primera como get

dic = {â€˜aâ€™ : 1, â€™bâ€™ : 2, â€˜câ€™ : 3 , â€˜dâ€™ : 4}
valor = dic.setdefault(â€˜aâ€™)

valor â†’ 1
Y en la segunda forma, nos sirve para agregar un nuevo elemento a nuestro diccionario.

dic = {â€˜aâ€™ : 1, â€™bâ€™ : 2, â€˜câ€™ : 3 , â€˜dâ€™ : 4}
valor = dic.setdefault(â€˜eâ€™,5)

dic â†’ {â€˜aâ€™ : 1, â€™bâ€™ : 2, â€˜câ€™ : 3 , â€˜dâ€™ : 4 , â€˜eâ€™ : 5}
update()

Recibe como parÃ¡metro otro diccionario. Si se tienen claves iguales, actualiza el valor de la clave repetida; si no hay claves iguales, este par clave-valor es agregado al diccionario.

dic 1 = {â€˜aâ€™ : 1, â€™bâ€™ : 2, â€˜câ€™ : 3 , â€˜dâ€™ : 4}
dic 2 = {â€˜câ€™ : 6, â€™bâ€™ : 5, â€˜eâ€™ : 9 , â€˜fâ€™ : 10}
dic1.update(dic 2)

dic 1 â†’ {â€˜aâ€™ : 1, â€™bâ€™ : 5, â€˜câ€™ : 6 , â€˜dâ€™ : 4 , â€˜eâ€™ : 9 , â€˜fâ€™ : 10}
Estos son algunos de los mÃ©todos mÃ¡s Ãºtiles y mÃ¡s utilizados en los Diccionarios. Python es un gran lenguaje de programaciÃ³n que nos permite programar de una manera realmente sencilla. Si deseas conocer mucho mÃ¡s y aprender a profundidad esta tecnologÃ­a, ingresa al Curso de Python que tenemos en Devcode. Â¡Te esperamos!

Â¿Te gusto el tutorial?
AyÃºdanos a llegar a mÃ¡s personas

 
 
user
Carlos Eduardo Plasencia Prado
Backend Developer | Python / Django junior - Javascript / Node.js

@plasenciacar
ython: Agregar y eliminar elementos de un diccionario
Objetivo: mostrar cÃ³mo agregar y eliminar elementos de un diccionario en Python.

En la anterior entrega explicamos el concepto de diccionario en Python y vimos el modo de crearlos. Hoy presentaremos dos tÃ©cnicas bÃ¡sicas: la agregaciÃ³n y eliminaciÃ³n de elementos.

Comencemos con la creaciÃ³n de un par de diccionarios de ejemplo sobre los que practicaremos desde el intÃ©rprete interactivo:

>>> gazpacho = {}
>>> menda = {'Nombre':'Javier', 'Apellido':'Montero'}
El primero, gazpacho, es un simple diccionario vacÃ­o que utilizaremos para almacenar la receta del gazpacho; el otro, menda, recoge algunos datos sobre mi persona. MÃ¡s adelante veremos que podemos crear estructuras de datos mÃ¡s complejas, basadas en los diccionarios, que podremos emplear para mantener una agenda de nuestros contactos personales y sin necesidad de recurrir a un gestor de bases de datos.

Para agregar un par clave-valor a un diccionario, recurrimos a la siguiente sintaxis:

diccionario[clave] = valor

ProbÃ©moslo con gazpacho:

>>> gazpacho['Aceite'] = '300 ml'

Verificamos que gazpacho, que antes estaba vacÃ­o, ahora contiene ese par:

>>> gazpacho
{'Aceite': '300 ml'}
Continuemos con la receta secreta:

>>> gazpacho['Vinagre'] = '100 ml'
>>> gazpacho['Pepino'] = 1
>>> gazpacho['Pimiento'] = 1
Veamos cÃ³mo evoluciona nuestra creaciÃ³n:

>>> gazpacho
{'Pimiento': 1, 'Aceite': '300 ml', 'Vinagre': '100 ml', 'Pepino': 1}
Observa la salida de esta Ãºltima instrucciÃ³n y recuerda lo que dijimos sobre el orden de los diccionarios: no son una estructura ordenada, aunque veremos formas de hacerlos mÃ¡s presentables si lo deseamos.

AÃ±adamos nuevos elemendos al menda:

>>> menda['URL'] = 'http://www.elclubdelautodidacta.es'
>>> menda['Twitter'] = '@pradery'
tras lo cual,

>>> menda
{'URL': 'http://www.elclubdelautodidacta.es', 'Nombre': 'Javier', 'Twitter': '@pradery', 'Apellido': 'Montero'}
Las claves han de ser Ãºnicas. Si tratamos de agregar otra ya existente, simplemente el valor nuevo sustituirÃ¡ al antiguo:

>>> gazpacho['Pimiento'] = 2
>>> menda['URL'] = 'http://elclubdelautodidacta.es/wp/'
Observa cÃ³mo los valores correspondientes son actualizados:

>>> gazpacho
{'Pimiento': 2, 'Aceite': '300 ml', 'Vinagre': '100 ml', 'Pepino': 1}
>>> menda
{'URL': 'http://elclubdelautodidacta.es/wp/', 'Nombre': 'Javier', 'Twitter': '@pradery', 'Apellido': 'Montero'}
Para borrar un par clave-valor de un diccionario disponemos de la sentencia del, que emplearemos del siguiente modo:

del diccionario[clave]

Por ejemplo:

>>> del gazpacho['Aceite']
>>> gazpacho
{'Pimiento': 2, 'Vinagre': '100 ml', 'Pepino': 1}
>>> del menda['URL']
>>> menda
{'Nombre': 'Javier', 'Twitter': '@pradery', 'Apellido': 'Montero'}
Con del podrÃ­amos cargarnos incluso el objeto completo:

>>> del gazpacho

A partir de este momento el gazpacho ha dejado de existir y ha pasado a mejor vida:

>>> gazpacho
Traceback (most recent call last):
  File "<pyshell#28>", line 1, in <module>
    gazpacho
NameError: name 'gazpacho' is not defined
Y con menda mejor no lo hago, no sea que traiga mala suerteâ€¦

Javier Montero GabarrÃ³

Python: Agregar y eliminar elementos de un diccionario

El texto de este artÃ­culo se encuentra sometido a una licencia Creative Commons del tipo CC-BY-NC-ND (reconocimiento, no comercial, sin obra derivada, 3.0 unported)

El Club del Autodidacta

Consulta el Ã­ndice completo de artÃ­culos relacionados con Python.

Tal vez te interese...
Python â€“ Buscando a Wally.txt
Python â€“ Troceando desde el lado izquierdo
Comparando objetos en Python
Python â€“ Sumando objetos
Python â€“ Una tortuga de brocha fina
Â¿Conoces a alguien a quien le pueda interesar este artÃ­culo?
Haz clic para compartir en Facebook (Se abre en una ventana nueva)Haz clic para compartir en Twitter (Se abre en una ventana nueva)
Autor Javier MonteroPublicado el07/08/2012CategorÃ­aspythonEtiquetasdel, diccionarios, python
16 opiniones en â€œPython: Agregar y eliminar elementos de un diccionarioâ€
Isaac Lacoba (@IsaacLacoba)dice:
31/12/2012 a las 13:10
Gracias por tu post. Me ha ahorrado bastante tiempo. ğŸ˜‰

Responder
Javier Monterodice:
02/01/2013 a las 19:39
Me alegro de que te haya sido Ãºtil. Saludos. ğŸ™‚


About
Prensa
Legal
PolÃ­tica de privacidad
TÃ©rminos de Servicio
Cookie Policy
STACK EXCHANGE
NETWORK
Technology
Life / Arts
Culture / Recreation
Science
Other
Blog
Facebook
Twitter
LinkedIn
Instagram
diseÃ±o del sitio / logo Â© 2021 Stack Exchange Inc; contribuciones de los usuarios bajo licencia cc by-sa. rev 2021.3.30.38947
Mujercitas (pelÃ­cula de 2019)
Ir a la navegaciÃ³nIr a la bÃºsqueda
Little Women
Mujercitas 2019.png
TÃ­tulo	Mujercitas
Ficha tÃ©cnica
DirecciÃ³n	
Greta Gerwig
ProducciÃ³n	Denise Di Novi
Amy Pascal
Robin Swicord
Arnon Milchan
Guion	Greta Gerwig
Basada en	Mujercitas, de Louisa May Alcott
MÃºsica	Alexandre Desplat
FotografÃ­a	Yorick Le Saux
Protagonistas	Saoirse Ronan
TimothÃ©e Chalamet
Florence Pugh
Eliza Scanlen
James Norton
Laura Dern
Emma Watson
Louis Garrel
Meryl Streep
Bob Odenkirk
Chris Cooper
Ver todos los crÃ©ditos (IMDb)
Datos y cifras
PaÃ­s	Estados Unidos
AÃ±o	2019
Estreno	
25 de diciembre de 2019 (Estados Unidos)
GÃ©nero	Drama
Romance
DuraciÃ³n	134 minutos
Idioma(s)	InglÃ©s
CompaÃ±Ã­as
Productora	Columbia Pictures
Di Novi Pictures
Pascal Pictures
New Regency Pictures
Sony Pictures Entertainment
DistribuciÃ³n	Sony Pictures Releasing
InterCom
Presupuesto	US$40.000.000
RecaudaciÃ³n	US$205.914.9761â€‹
Ficha en IMDb
Ficha en FilmAffinity
[editar datos en Wikidata]
Mujercitas2â€‹3â€‹ (en inglÃ©s Little Women) es una pelÃ­cula dramÃ¡tica estadounidense de 2019 escrita y dirigida por Greta Gerwig. Es la sÃ©ptima adaptaciÃ³n cinematogrÃ¡fica de la novela de 1868 del mismo nombre de Louisa May Alcott. Narra las vidas de las hermanas March, Jo, Meg, Amy y Beth, en Concord, Massachusetts, durante el siglo XIX. EstÃ¡ protagonizada por un reparto coral compuesto por Saoirse Ronan, Emma Watson, Florence Pugh, Eliza Scanlen, Laura Dern, TimothÃ©e Chalamet, Meryl Streep, Tracy Letts, Bob Odenkirk, James Norton, Louis Garrel y Chris Cooper. Gerwig altera cronolÃ³gicamente la trama de la obra y alterna pasajes de los primeros aÃ±os de las niÃ±as con su vida profesional, con la intenciÃ³n de hacer una nueva visiÃ³n de la novela tras 150 aÃ±os, haciendo hincapiÃ© en el subtexto feminista de la obra4â€‹. En la nueva versiÃ³n, ademÃ¡s, se fusiona el texto original con la vida de Louisa May Alcott, de quien Jo era una especie de alter ego5â€‹.

Sony Pictures iniciÃ³ el desarrollo de la pelÃ­cula en 2013, Amy Pascal se incorporÃ³ para producir en 2015 y Gerwig fue contratada para escribir su guion al aÃ±o siguiente. Usando otros escritos de Alcott como inspiraciÃ³n, Gerwig escribiÃ³ el guion en 2018. Fue nombrada directora ese mismo aÃ±o, tras su triunfo con Lady Bird, y es la segunda pelÃ­cula dirigida por la directora, nominada a los Premios Ã“scar en 2018.6â€‹ La filmaciÃ³n se llevÃ³ a cabo de octubre a diciembre de 2018 en el estado de Massachusetts, y la ediciÃ³n comenzÃ³ el dÃ­a despuÃ©s del final de la filmaciÃ³n.

Mujercitas se estrenÃ³ en el Museo de Arte Moderno de la ciudad de Nueva York el 7 de diciembre de 2019 y se estrenÃ³ en cines en los Estados Unidos el 25 de diciembre de 2019 por Sony Pictures Releasing. La pelÃ­cula recibiÃ³ elogios de la crÃ­tica,7â€‹ con elogios especiales por el guion y la direcciÃ³n de Gerwig, asÃ­ como por las actuaciones del elenco, y recaudÃ³ 216 millones de dÃ³lares en todo el mundo. Entre sus numerosos elogios, la pelÃ­cula recibiÃ³ seis nominaciones a los Premios de la Academia, incluyendo Mejor PelÃ­cula, Mejor Actriz (Ronan), Mejor Actriz de Reparto (Pugh) y Mejor Guion Adaptado, y ganÃ³ el Premio al Mejor DiseÃ±o de Vestuario. TambiÃ©n recibiÃ³ nominaciones para los Premios BAFTA y los Globos de Oro.


Ãndice
1	Argumento
2	Reparto
3	ProducciÃ³n
3.1	Desarrollo y casting
3.2	Escritura
3.3	DiseÃ±o de vestuario
3.4	FilmaciÃ³n y ediciÃ³n
4	Estreno
5	RecepciÃ³n
5.1	Taquilla
5.2	CrÃ­tica
5.3	Premios y nominaciones
6	Referencias
7	Enlaces externos
Argumento
En 1868, Jo March, profesora de la ciudad de Nueva York, acude al Sr. Dashwood, un editor que acepta publicar una historia que ha escrito. Su hermana menor, Amy, que estÃ¡ en ParÃ­s con la tÃ­a March, asiste a una fiesta con su amigo de la escuela y vecino, Laurie. Amy se enoja por el comportamiento de Laurie, que se encuentra ebrio y se burla de ella por su intenciÃ³n de casarse con el rico empresario Fred Vaughn. En Nueva York, Jo se siente herida cuando Friedrich Bhaer, un profesor enamorado de ella, critica constructivamente sus escritos, lo que hace que dÃ© por terminada su amistad. DespuÃ©s de enterarse por una carta de que la enfermedad de su hermana menor Beth ha empeorado, Jo regresa a su casa en Concord, Massachusetts.

Siete aÃ±os antes, en una fiesta con su hermana mayor, Meg, Jo se hace amiga de Laurie. La maÃ±ana de Navidad, la madre de las niÃ±as, "Marmee", las convence para que den su desayuno a su pobre vecina, la seÃ±ora Hummel, y a sus hijos hambrientos. Cuando regresan a casa, encuentran una mesa llena de comida que les trajo su vecino y abuelo de Laurie, el Sr. Laurence. Marmee les a las hermanas una carta de su padre, que lucha en la Guerra Civil Estadounidense. Jo visita regularmente a la tÃ­a March para leerle, con la esperanza de que la tÃ­a March la invite a viajar por Europa.

Cuando Meg, Jo, Laurie y John Brooke -el tutor de Laurie y futuro esposo de Meg-, van al teatro, Amy celosa, quema los manuscritos de Jo. A la maÃ±ana siguiente, Amy, quiere reconciliarse con Jo, la persigue mientras Jo va con Laurie hasta un lago donde patinan. Laurie y Jo salvan a Amy tras hundirse en el agua frÃ­a de invierno. El Sr. Laurence invita a la pequeÃ±a Beth a tocar el piano de su difunta hija en su casa.

De vuelta al presente, Meg habla con su marido, John, tras comprar una tela tan cara que no podÃ­an pagar y le expresa su decepciÃ³n por ser pobre. En Europa, Laurie visita a Amy -quien ha decidido abandonar su carrera como pintora- para disculparse por su comportamiento de la noche anterior y le ruega que no se case con Fred, y que acepte su propia peticiÃ³n de matrimonio. Aunque estÃ¡ enamorada de Laurie, Amy se niega, molesta por ser siempre segundo plato, tras la negativa que Jo le dio a Lauire ante la misma peticiÃ³n. Aun asÃ­, Amy rechaza la propuesta de Fred y acepta la de Laurie.

Jo recuerda cuando tuvo que cortarse el pelo para que su madre pudiese viajar a Nueva York a cuidar de su padre, que habÃ­a sido herido en guerra. Por entonces el Sr. Laurence regalÃ³ su piano a Beth y descubrieron quela niÃ±a habÃ­a contraÃ­do la escarlatina, posiblemente en casa de los Hummel. Para evitar contraer la enfermedad, Amy es enviada a quedarse con la tÃ­a March, quien le aconseja que mantenga a su familia formalizando un buen matrimonio. Beth se recupera a tiempo para la Navidad del pasado, durante la cual su padre tambiÃ©n regresa a casa Meg y John Brooke se comprometen. Jo intenta convencerla de que huya, pero Meg expresa su alegrÃ­a por casarse con John. La tÃ­a March anuncia su viaje por Europa, pero se lleva a Amy en lugar de Jo. Tras la boda, Laurie le propone matrimonio a Jo, quien lo rechaza y le explica que no quiere casarse.

En el presente, John pide a Meg que convierta la cara tela en un vestido si le hace feliz. Meg revela que la habÃ­a vendido y le asegura que es feliz tal y como viven. Tras el empeoramiento, Beth fallece. Marmee informa a la familia de que Amy regresa de Europa con una tÃ­a March enferma. Jo se pregunta a sÃ­ misma si se apresurÃ³ a rechazar la proposiciÃ³n de matrimonio de Laurie y le escribe una carta. Amy se prepara para abandonar Europa y le dice a Laurie que ha rechazado la propuesta de Fred; se besan y luego se casan en el viaje de retorno a casa. Jo y Laurie acuerdan ser solo amigos, y se deshace de la carta que le escribiÃ³ y no llegÃ³ a enviar. Jo comienza a escribir una novela basada en la vida de ella y sus hermanas y envÃ­a los primeros capÃ­tulos a un poco impresionado Sr. Dashwood. Bhaer sorprende a Jo al presentarse en la casa March camino a California.

En Nueva York, el Sr. Dashwood acepta publicar la novela de Jo pues sus propias hijas exigen saber cÃ³mo termina la historia, pero se niega a aceptar que la protagonista permanezca soltera al final. Para apaciguarlo, Jo termina su novela con la protagonista, ella misma, impidiendo que Bhaer se vaya a California. Negocia con Ã©xito derechos de autor y regalÃ­as con el Sr. Dashwood. Tras el fallecimiento de la tÃ­a March, Jo hereda su casa y la abre como una escuela para niÃ±as y niÃ±os, donde enseÃ±an Meg, Amy, John y Bhaer. Jo observa la impresiÃ³n de su novela, titulada Mujercitas.

Reparto
Saoirse Ronan como Josephine "Jo" March.
Emma Watson como Margaret "Meg" March.
Florence Pugh como Amy March.
Eliza Scanlen como Elizabeth "Beth" March.
Laura Dern como Marmee March.
TimothÃ©e Chalamet como Theodore "Laurie" Laurence.
Meryl Streep como TÃ­a March.
Tracy Letts como el SeÃ±or Dashwood.
Bob Odenkirk como el padre de la familia March.
James Norton como John Brooke.
Louis Garrel como Friedrich Bhaer.
Chris Cooper como el SeÃ±or Laurence.
Jayne Houdyshell como Hannah.
Rafael Silva como el amigo de Friedrich.
Dash Barber como Fred Vaughn.
Hadley Robinson como Sallie Gardiner Moffat.
Abby Quinn como Annie Moffat.
Maryann Plunkett como la SeÃ±ora Kirke.
Edward Fletcher como el sirviente del SeÃ±or Laurence.
Sasha Frolova como la SeÃ±ora Hummel.
ProducciÃ³n
Desarrollo y casting
En octubre de 2013, se anunciÃ³ que una nueva adaptaciÃ³n cinematogrÃ¡fica de la novela Mujercitas de Louisa May Alcott estaba en desarrollo en Sony Pictures, con Olivia Milch escribiendo el guion y Robin Swicord y Denise Di Novi como productoras.8â€‹ En marzo de 2015, Amy Pascal se uniÃ³ como productora de la nueva adaptaciÃ³n, y Sarah Polley fue contratada para escribir el guion y potencialmente dirigir.9â€‹ En Ãºltima instancia, la participaciÃ³n de Polley nunca fue mÃ¡s allÃ¡ de las discusiones iniciales.10â€‹ En agosto de 2016, Greta Gerwig fue contratada para escribir el guion.11â€‹ En junio de 2018, Gerwig fue anunciado como el director de la pelÃ­cula ademÃ¡s de ser su guionista.12â€‹ Se habÃ­a enterado de los planes de Sony para adaptar el libro en 2015 e instÃ³ a su agente a que la pusiera en contacto con el estudio, admitiendo que si bien ella "no estaba en la lista de nadie para dirigir esta pelÃ­cula", era algo a lo que aspiraba hacer, citando cÃ³mo el libro la inspirÃ³ a convertirse en escritora y directora.13â€‹ AdemÃ¡s de ser la primera pelÃ­cula de estudio de Gerwig que dirigiÃ³, Mujercitas fue su segundo esfuerzo como directora en solitario.14â€‹15â€‹

TambiÃ©n se anunciÃ³ en junio de 2018 que Meryl Streep, Emma Watson, Saoirse Ronan, TimothÃ©e Chalamet y Florence Pugh habÃ­an sido elegidos para papeles no revelados.16â€‹ Gerwig habÃ­a trabajado con Ronan y Chalamet en su primera pelÃ­cula como directora en solitario, Lady Bird,17â€‹ mientras buscaba elegir a Pugh despuÃ©s de ver su actuaciÃ³n en la pelÃ­cula Lady Macbeth.18â€‹ Eliza Scanlen, a quien Gerwig vio protagonizar la miniserie Sharp Objects,19â€‹ se uniÃ³ al elenco al mes siguiente.20â€‹ James Norton y Laura Dern fueron elegidos en agosto.21â€‹22â€‹ Ese mismo mes, Emma Watson se uniÃ³ al elenco, reemplazando a Stone, quien tuvo que abandonar debido a conflictos de programaciÃ³n con la promociÃ³n de La Favorita.23â€‹ En septiembre, Louis Garrel, Bob Odenkirk y Chris Cooper se unieron al elenco en papeles secundarios.24â€‹25â€‹26â€‹ New Regency Pictures fue anunciado como un financiador adicional de la pelÃ­cula en octubre.27â€‹

Escritura
Gerwig comenzÃ³ a escribir el guion durante un viaje a Big Sur, California, luego de la ceremonia de los Premios de la Academia 2018, utilizando como inspiraciÃ³n las cartas y diarios de Alcott, asÃ­ como "pinturas de mujeres jÃ³venes del siglo XIX".19â€‹ TambiÃ©n se inspirÃ³ en las otras historias de Alcott para los diÃ¡logos.28â€‹ Gerwig escribiÃ³ muchas lÃ­neas de diÃ¡logo superpuestas que se "leerÃ­an una encima de la otra".29â€‹ AdemÃ¡s, afirmÃ³ que un monÃ³logo de la pelÃ­cula se inspirÃ³ en una conversaciÃ³n que tuvo con Streep sobre "los desafÃ­os que enfrentaban las mujeres en el DÃ©cada de 1860".30â€‹ Para "enfocar la pelÃ­cula en [sus personajes] como adultos", Gerwig incorporÃ³ una lÃ­nea de tiempo no lineal.31â€‹ El final difiere del de la novela al describir "los placeres de un romance dentro de una historia sobre Alcott realizando sus ambiciones artÃ­sticas", que Gerwig creÃ­a que honra la verdadera visiÃ³n de Alcott dado que Alcott tenÃ­a que "satisfacer las expectativas narrativas de la Ã©poca".32â€‹33â€‹

DiseÃ±o de vestuario
La pelÃ­cula requiriÃ³ "aproximadamente 75 trajes de Ã©poca principales", cada uno de los cuales tomÃ³ "aproximadamente 40 horas" para crear.19â€‹ La diseÃ±adora de vestuario, Jacqueline Durran, combinÃ³ "un espÃ­ritu de vestuario libre" y "la rigidez victoriana tradicional" al vestir a los personajes.34â€‹ Queriendo hacer que "la ropa vintage pareciera codiciada para el espectador moderno", combinÃ³ "etiquetas de lana" con "faldas a cuadros de muy buen gusto", "capas largas de color carmesÃ­" y "gorras de vendedor de periÃ³dicos".35â€‹ DistinguiÃ³ los guardarropas de la infancia y la edad adulta de los personajes teniendo en cuenta "la lÃ³gica interna de cada uno" y manteniendo "la conexiÃ³n entre los dos", y a cada personaje se le asignÃ³ un "color central", incluido el rojo para el personaje de Ronan, el verde y el lavanda para Watson, marrÃ³n y rosa para Scanlen y azul claro para Pugh.36â€‹ TambiÃ©n hizo que los personajes compartieran y reutilizaran las mismas piezas de vestuario para reforzar sus relaciones entre ellos.37â€‹ AdemÃ¡s de diseÃ±ar el personaje de Ronan con "vestidos holgados de algodÃ³n" y "faldas de lana lisas",35â€‹ incorporÃ³ "referencias modernas" y utilizÃ³ "un joven Bob Dylan", la subcultura Teddy Boy y la pintura del artista francÃ©s James Tissot. El cÃ­rculo de la Rue Royale como inspiraciÃ³n para estilizar el de Chalamet.38â€‹

FilmaciÃ³n y ediciÃ³n

La filmaciÃ³n tuvo lugar principalmente en Harvard, Massachusetts.
El elenco, con la excepciÃ³n de Pugh debido a sus compromisos de filmaciÃ³n con la pelÃ­cula Midsommar, comenzÃ³ los ensayos de la pelÃ­cula dos semanas antes de la filmaciÃ³n.29â€‹ La fotografÃ­a principal comenzÃ³ en Boston en octubre de 2018, 39â€‹ con Harvard, Massachusetts, como ubicaciÃ³n principal.40â€‹ Las ubicaciones adicionales incluyeron Lancaster, la Universidad de Harvard en Cambridge, Crane Beach en Ipswich y Concord, todas en el estado de Massachusetts.41â€‹42â€‹ La casa de la familia March se construyÃ³ desde cero en un terreno en Concord,19â€‹ mientras que el Arnold Arboretum de Harvard se utilizÃ³ para rodar una escena ambientada en un parque de ParÃ­s del siglo XIX con Pugh, Chalamet y Streep.43â€‹ Castle Hill en Ipswich tambiÃ©n se utilizÃ³ para duplicar las escenas europeas.44â€‹ El director de fotografÃ­a Yorick Le Saux filmÃ³ la pelÃ­cula en formato de 35 mm.45â€‹

Gerwig descubriÃ³ que estaba embarazada durante la producciÃ³n y lo mantuvo en privado durante todo el proceso.14â€‹ TambiÃ©n impuso la prohibiciÃ³n de los telÃ©fonos mÃ³viles en el set durante el rodaje.46â€‹ DespuÃ©s de terminar la fotografÃ­a principal el 16 de diciembre de 2018, Gerwig comenzÃ³ a editar la pelÃ­cula junto con el editor Nick Houy al dÃ­a siguiente y luego la proyectÃ³ para los ejecutivos de Sony Pictures en la ciudad de Nueva York el 10 de marzo de 2019, tres dÃ­as antes de dar a luz a un hijo.33â€‹

Estreno
El 19 de junio de 2019, Vanity Fair lanzÃ³ las primeras imÃ¡genes fijas,47â€‹ y el avance oficial se lanzÃ³ el 13 de agosto.48â€‹ Mujercitas tuvo su estreno mundial en el Museo de Arte Moderno de la ciudad de Nueva York el 7 de diciembre de 2019,49â€‹ y tambiÃ©n se proyectÃ³ para inaugurar el Festival Internacional de Cine de RÃ­o de Janeiro el 9 de diciembre.50â€‹ Fue estrenada en cines en los Estados Unidos el 25 de diciembre de 2019 por Sony Pictures Releasing.51â€‹52â€‹ Deadline Hollywood informÃ³ que Sony gastÃ³ aproximadamente $70 millones en la promociÃ³n de la pelÃ­cula.53â€‹

Mujercitas estaba programada originalmente para un estreno en cines en China el 14 de febrero de 2020, pero esto se descartÃ³ debido a la pandemia de COVID-19.54â€‹ La pelÃ­cula fue lanzada digitalmente el 10 de marzo de 2020 y en DVD y Blu-ray el 7 de abril.55â€‹56â€‹ En mayo, Variety informÃ³ que una vez mÃ¡s estaba destinado a ser lanzado en China en una fecha no especificada despuÃ©s de la pandemia.57â€‹ La pelÃ­cula se estrenÃ³ en Dinamarca y JapÃ³n en junio despuÃ©s de que ambos paÃ­ses reabrieran sus salas de cine luego de cierres pandÃ©micos.58â€‹ Finalmente fue lanzada en China el 25 de agosto.59â€‹

RecepciÃ³n
Taquilla
Mujercitas recaudÃ³ $108.1 millones en los Estados Unidos y CanadÃ¡, y $108.5 millones en otros paÃ­ses, para un total mundial de $216.6 millones, contra un presupuesto de producciÃ³n de $40 millones.60â€‹61â€‹ En abril de 2020, Deadline Hollywood calculÃ³ su beneficio neto en 56 millones de dÃ³lares.53â€‹

En los Estados Unidos y CanadÃ¡, la pelÃ­cula se estrenÃ³ junto con Spies in Disguise y la expansiÃ³n de Diamantes en Bruto, y se proyectaba que recaudarÃ­a entre 18 y 22 millones de dÃ³lares en 3308 salas durante su fin de semana de estreno de cinco dÃ­as. GanÃ³ $6.4 millones el dÃ­a de Navidad y $6 millones en su segundo dÃ­a,62â€‹ y pasÃ³ a debutar con $16.8 millones (un total de $29.2 millones durante el perÃ­odo de cinco dÃ­as de Navidad), terminando en cuarto lugar.63â€‹64â€‹ En su segundo fin de semana, la pelÃ­cula recaudÃ³ $13.6 millones, terminando tercero.65â€‹ Luego ganÃ³ $7.8 millones y $6.4 millones, respectivamente, los siguientes fines de semana.66â€‹67â€‹

En junio de 2020, la pelÃ­cula recaudÃ³ 495000 dÃ³lares y 255000 dÃ³lares durante su primer fin de semana en JapÃ³n y su segundo fin de semana en Dinamarca, respectivamente.68â€‹ Ese mismo mes, superÃ³ los $100 millones en la taquilla internacional luego de lanzamientos en otros 12 mercados.69â€‹ La pelÃ­cula ganÃ³ $4.7 millones durante los primeros seis dÃ­as de su lanzamiento en agosto de 2020 en China.70â€‹

CrÃ­tica


Saoirse Ronan (arriba) y Florence Pugh fueron nominadas al Premio de la Academia a Mejor Actriz y Mejor Actriz de Reparto, respectivamente.
En el sitio web del agregador de reseÃ±as Rotten Tomatoes, la pelÃ­cula tiene una calificaciÃ³n de aprobaciÃ³n del 95% basada en 403 reseÃ±as, con una calificaciÃ³n promedio de 8.54/10. El consenso de los crÃ­ticos del sitio web dice: "Con un elenco estelar y un recuento inteligente y sensible de su material original clÃ¡sico, Mujercitas de Greta Gerwig demuestra que algunas historias son verdaderamente atemporales".71â€‹ En Metacritic, tiene una puntuaciÃ³n media ponderada de 91 sobre de 100, sobre la base de 57 crÃ­ticas, lo que indica "aclamaciÃ³n universal".72â€‹ Las audiencias encuestadas por CinemaScore le dieron a la pelÃ­cula una calificaciÃ³n promedio de "Aâ€“" en una escala de A+ a F, y los espectadores encuestados por PostTrak le dieron un promedio de cinco de cinco.63â€‹

Escribiendo para IndieWire, Kate Erbland destacÃ³ la "ambiciosa narraciÃ³n elÃ­ptica" de Gerwig y elogiÃ³ su direcciÃ³n por no ser ni "torpe" ni "sermoneadora".73â€‹ Anthony Lane de The New Yorker dijo que "puede que sea la mejor pelÃ­cula hecha hasta ahora por una mujer estadounidense".74â€‹ Lindsey Bahr, de Associated Press, tambiÃ©n elogiÃ³ la direcciÃ³n de Gerwig, considerÃ¡ndola un "logro asombroso" y una "declaraciÃ³n de artista".75â€‹ Al premiar la pelÃ­cula con tres y medio de cuatro, Brian Truitt de USA Today elogiÃ³ la escritura de Gerwig como "magnÃ­fica" y dijo que "hace que el tiempo y el lenguaje de Alcott se sientan efervescentemente modernos y autÃ©nticamente nostÃ¡lgicos".76â€‹ Mick LaSalle, que escribe para el San Francisco Chronicle, le dio a la pelÃ­cula una crÃ­tica mixta, en la que elogiÃ³ la direcciÃ³n de Gerwig pero criticÃ³ la lÃ­nea de tiempo no lineal y los personajes "presumidos".77â€‹

Los crÃ­ticos elogiaron las actuaciones del elenco, con David Rooney de The Hollywood Reporter destacando su "encantador trabajo de conjunto", y Alonso Duralde de TheWrap diciendo que no hubo "un solo momento artificial" de ninguno de los actores.78â€‹79â€‹ Caryn James de BBC Online calificÃ³ la actuaciÃ³n de Ronan de "luminosa",80â€‹ y Leah Greenblatt de Entertainment Weekly sugiriÃ³ que "lleva casi todas las escenas en las que se encuentra".81â€‹ David Sims de The Atlantic destacÃ³ la actuaciÃ³n de Pugh, escribiendo que convirtiÃ³ a su personaje en "una heroÃ­na tan rica y convincente como [Ronan]",82â€‹ mientras que Clarisse Loughrey de The Independent declarÃ³ que Pugh "se las arregla para robar el show".83â€‹ En su reseÃ±a para NPR, Justin Chang elogiÃ³ las actuaciones de Ronan y Pugh como "increÃ­blemente buenas".84â€‹ Chalamet tambiÃ©n fue elogiado por Peter Travers de Rolling Stone y Ann Hornaday de The Washington Post por el "encanto innato y la vulnerabilidad conmovedora", asÃ­ como la "fisicalidad lÃºdica" en su actuaciÃ³n.85â€‹86â€‹

Si bien la pelÃ­cula en general recibiÃ³ seis nominaciones al Premio de la Academia, Gerwig no fue nominada a Mejor Director, lo que se considerÃ³ un desaire.87â€‹88â€‹ Allison Pearson de The Daily Telegraph calificÃ³ esto como un "estÃ¡ndar completamente nuevo de idiotez", y opinÃ³ que "menosprecia la experiencia de las mujeres",89â€‹ mientras que Dana Stevens de Slate teorizÃ³ que los miembros de la Academia creen que "las mujeres solo pueden tener un pequeÃ±o reconocimiento, como un regalo "y que Gerwig" ahora puede ser ignorada con seguridad "ya que anteriormente habÃ­a sido nominada para Lady Bird.90â€‹ Escribiendo para Los Angeles Times, los psicÃ³logos sociales Devon Proudfoot y Aaron Kay concluyeron que el desaire se debiÃ³ a una "tendencia psicolÃ³gica general a ver sin saberlo el trabajo de las mujeres como menos creativo que el de los hombres".91â€‹

Premios y nominaciones
En la 92Âª ediciÃ³n de los Premios de la Academia, Mujercitas recibiÃ³ seis nominaciones, incluyendo Mejor PelÃ­cula, Mejor Actriz (Ronan), Mejor Actriz de Reparto (Pugh) y Mejor Guion Adaptado,92â€‹93â€‹ y ganÃ³ el Premio al Mejor DiseÃ±o de Vestuario.7â€‹94â€‹ La pelÃ­cula tambiÃ©n recibiÃ³ nueve nominaciones en los Premios de la CrÃ­tica CinematogrÃ¡fica 2019, ganando como Mejor Guion Adaptado,95â€‹96â€‹ cinco nominaciones en los Premios BAFTA 2019,97â€‹ y dos en los Premios Globo de Oro 2019.98â€‹ Fue elegida por el American Film Institute como una de las diez mejores pelÃ­culas del aÃ±o.99â€‹

Referencias
 Â«'Little Women' (2019)Â». Box Office Mojo. Consultado el 18 de marzo de 2020.
 https://www.ecartelera.com/pelÃ­culas/mujercitas-2019/
 https://www.elle.com/es/living/ocio-cultura/a19594863/nuevo-mujercitas-reparto/
 Â«Entrevista a Greta GerwigÂ».
 Â«La nueva â€œMujercitasâ€: una mirada real sobre el deseo y las ambiciones del universo femeninoÂ».
 Â«Â«Pensaba que solo los hombres podÃ­an dirigirÂ»: Greta Gerwig, la realizadora de â€˜Mujercitasâ€™ explica su pasiÃ³n por las mujeres que escribenÂ».
 Carras, Christi (9 de febrero de 2020). Â«The only Oscar 'Little Women' won was for costume designÂ». Los Angeles Times. Archivado desde el original el 10 de febrero de 2020. Consultado el 15 de junio de 2020. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 Kroll, Justin (18 de octubre de 2013). Â«Sony Sets Up 'Little Women' Adaptation with Olivia Milch Writing (EXCLUSIVE)Â». Variety. Archivado desde el original el 12 de enero de 2020. Consultado el 8 de enero de 2020. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 Sneider, Jeff (18 de marzo de 2015). Â«Amy Pascal, Sarah Polley Team on 'Little Women' Remake at SonyÂ». TheWrap. Archivado desde el original el 5 de octubre de 2018. Consultado el 29 de junio de 2018. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 Whipp, Glenn (5 de julio de 2018). Â«Why it's a perfect time for Greta Gerwig's version of 'Little Women'Â». Los Angeles Times. Archivado desde el original el 9 de noviembre de 2019. Consultado el 10 de noviembre de 2019. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 Calvario, Liz (6 de agosto de 2016). Â«'Little Women': Greta Gerwig Will Rewrite Sony's Remake of Louisa May Alcott NovelÂ». IndieWire. Archivado desde el original el 11 de abril de 2019. Consultado el 15 de junio de 2020. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 Kroll, Justin (29 de junio de 2018). Â«Greta Gerwig Eyes 'Little Women' With Meryl Streep, Emma Watson, Saoirse Ronan, Timothee Chalamet CirclingÂ». Variety. Archivado desde el original el 29 de junio de 2018. Consultado el 29 de junio de 2018. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 Salisbury, Mark (17 de enero de 2020). Â«Greta Gerwig on fighting to make 'Little Women': "I was not on anybody's list to direct this film"Â». Screendaily.com. Screen International. Archivado desde el original el 29 de mayo de 2020. Consultado el 16 de junio de 2020. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 Kaufman, Amy (24 de diciembre de 2019). Â«'Little Women' director Greta Gerwig didn't just make a 'women's movie'Â». Los Angeles Times. Archivado desde el original el 24 de diciembre de 2019. Consultado el 31 de julio de 2020. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 Miller, Jenni (25 de diciembre de 2019). Â«Greta Gerwig's 'Little Women' is the adaptation every Jo March always neededÂ». NBC News. Archivado desde el original el 27 de diciembre de 2019. Consultado el 1 de agosto de 2020. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 N'Duka, Amanda (29 de junio de 2018). Â«Greta Gerwig To Helm 'Little Women' At Sony; Meryl Streep, Emma Stone, TimothÃ©e Chalamet, Saoirse Ronan In TalksÂ». Deadline Hollywood. Archivado desde el original el 10 de julio de 2018. Consultado el 29 de junio de 2018. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 Canfield, David (17 de octubre de 2019). Â«Little Women: Timothee Chalamet and Saoirse Ronan talk reunionÂ». Entertainment Weekly. Archivado desde el original el 18 de octubre de 2019. Consultado el 15 de junio de 2020. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 Keegan, Rebecca (5 de enero de 2020). Â«The Season of Florence PughÂ». The Hollywood Reporter. Archivado desde el original el 5 de enero de 2020. Consultado el 15 de junio de 2020. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 Sandberg, Bryn Elise (29 de noviembre de 2019). Â«Making of 'Little Women': Greta Gerwig Gives Modern Take on 1868 Novel for Big ScreenÂ». The Hollywood Reporter. Archivado desde el original el 30 de noviembre de 2019. Consultado el 15 de junio de 2020. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 N'Duka, Amanda (24 de julio de 2018). Â«'Little Women': 'Sharp Objects' Actress In Talks For The Role Of Beth MarchÂ». Deadline Hollywood. Archivado desde el original el 27 de julio de 2018. Consultado el 24 de julio de 2018. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 N'Duka, Amanda (2 de agosto de 2018). Â«Sony's 'Little Women' Adaptation Adds 'Flatliners' Actor James NortonÂ». Deadline Hollywood. Archivado desde el original el 17 de junio de 2019. Consultado el 15 de junio de 2020. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 Galuppo, Mia (14 de agosto de 2018). Â«Laura Dern in Talks to Join Meryl Streep in 'Little Women'Â». The Hollywood Reporter. Archivado desde el original el 15 de agosto de 2018. Consultado el 14 de agosto de 2018. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 Kroll, Justin (24 de agosto de 2018). Â«Emma Watson Joins Greta Gerwig's Adaptation of 'Little Women'Â». Variety. Archivado desde el original el 24 de agosto de 2018. Consultado el 24 de agosto de 2018. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 N'Duka, Amanda (5 de septiembre de 2018). Â«Louis Garrel Cast In 'Little Women' Movie At SonyÂ». Deadline Hollywood. Archivado desde el original el 6 de septiembre de 2018. Consultado el 5 de septiembre de 2018. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 N'Duka, Amanda (24 de septiembre de 2018). Â«'Better Call Saul's Bob Odenkirk Joins Greta Gerwig's 'Little Women' RemakeÂ». Deadline Hollywood. Archivado desde el original el 25 de septiembre de 2018. Consultado el 24 de septiembre de 2018. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 N'Duka, Amanda (28 de septiembre de 2018). Â«Oscar Winner Chris Cooper Boards Greta Gerwig's 'Little Women' AdaptationÂ». Deadline Hollywood. Archivado desde el original el 29 de septiembre de 2018. Consultado el 28 de septiembre de 2018. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 Fleming Jr, Mike (2 de octubre de 2018). Â«New Regency Co-Finances Two Sony Films: 'Little Women' & 'Girl In The Spider's Web'Â». Deadline Hollywood. Archivado desde el original el 3 de octubre de 2018. Consultado el 2 de octubre de 2018. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 White, Abbey (23 de diciembre de 2019). Â«Greta Gerwig on How Her 'Little Women' Adaptation Became "A Movie About Making Movies"Â». The Hollywood Reporter. Archivado desde el original el 23 de diciembre de 2019. Consultado el 15 de junio de 2020. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 Kaufman, Amy (31 de octubre de 2019). Â«How Saoirse Ronan and Florence Pugh updated 'Little Women' for modern feministsÂ». Los Angeles Times. Archivado desde el original el 9 de diciembre de 2019. Consultado el 9 de diciembre de 2019. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 Weinberg, Lindsay (2 de noviembre de 2019). Â«Greta Gerwig Says Meryl Streep Inspired a Powerful Scene in 'Little Women'Â». The Hollywood Reporter. Archivado desde el original el 4 de noviembre de 2019. Consultado el 15 de junio de 2020. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 Topel, Fred (30 de diciembre de 2019). Â«[WATCH] Greta Gerwig And Cast Discuss Focusing On 'Little Women' As AdultsÂ». Deadline Hollywood. Archivado desde el original el 15 de junio de 2020. Consultado el 15 de junio de 2020. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 Nicolaou, Elena (27 de diciembre de 2019). Â«Why Greta Gerwig's Little Women Movie Radically Changed the Book's EndingÂ». O, The Oprah Magazine. Archivado desde el original el 15 de junio de 2020. Consultado el 15 de junio de 2020. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 Whipp, Glenn (16 de diciembre de 2019). Â«Why Greta Gerwig kept her perfect 'Little Women' ending a secretÂ». Los Angeles Times. Archivado desde el original el 20 de diciembre de 2019. Consultado el 15 de junio de 2020. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 Kinosian, Janet (25 de diciembre de 2019). Â«Designing 'Women' lets Jacqueline Durran get a little freer with Victorian costumesÂ». Los Angeles Times. Archivado desde el original el 25 de diciembre de 2019. Consultado el 15 de junio de 2020. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 Syme, Rachel (13 de enero de 2020). Â«How Jacqueline Durran, the "Little Women" Costume Designer, Remixes Styles and ErasÂ». The New Yorker. Archivado desde el original el 13 de enero de 2020. Consultado el 4 de agosto de 2020. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 Grobar, Matt (2 de enero de 2020). Â«Costume Designer Jacqueline Durran Talks 'Little Women' Timelines, '1917' Military Attire & Entering Domain Of Superheroes With 'The Batman'Â». Deadline Hollywood. Archivado desde el original el 2 de enero de 2020. Consultado el 15 de junio de 2020. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 Gonzales, Erica (9 de febrero de 2020). Â«Jo and Laurie Shared Clothes on Purpose in Little WomenÂ». Harper's Bazaar. Archivado desde el original el 29 de febrero de 2020. Consultado el 15 de junio de 2020. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 Ivie, Devon (18 de diciembre de 2019). Â«TimothÃ©e Chalamet Little Women Outfits and Fashion InterviewÂ». Vulture. Archivado desde el original el 18 de diciembre de 2019. Consultado el 4 de agosto de 2020. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 Nechamkin, Sarah (9 de octubre de 2018). Â«Everything We Know About Greta Gerwig's Little Women AdaptationÂ». The Cut. Archivado desde el original el 14 de agosto de 2019. Consultado el 15 de junio de 2020. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 Schaffstall, Katherine (8 de febrero de 2020). Â«Oscars: 10 Things to Know About Best Picture Nominee 'Little Women'Â». The Hollywood Reporter. Archivado desde el original el 15 de junio de 2020. Consultado el 15 de junio de 2020. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 Slane, Kevin (24 de diciembre de 2019). Â«'Little Women' was filmed entirely in Massachusetts. Here are the historic, picturesque locations from the movie.Â». Boston.com. Archivado desde el original el 26 de diciembre de 2019. Consultado el 15 de junio de 2020. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 Goldstein, Meredith (21 de diciembre de 2019). Â«A big stage for Concord in 'Little Women'Â». The Boston Globe. Archivado desde el original el 22 de diciembre de 2019. Consultado el 15 de junio de 2020. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 Blackwell, Deborah (1 de noviembre de 2018). Â«Harvard's Arnold Arboretum attracts 'Little Women' with Meryl StreepÂ». The Harvard Gazette. Archivado desde el original el 18 de noviembre de 2018. Consultado el 17 de noviembre de 2018. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 Gale, Natalie (19 de diciembre de 2019). Â«Inside the Filming of Greta Gerwig's Little WomenÂ». Northshore Magazine. Archivado desde el original el 17 de junio de 2020. Consultado el 16 de junio de 2020. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 Thompson, Anne (23 de diciembre de 2019). Â«Little Women: 10 Decisions That Turned It Into a Modern Movie ClassicÂ». IndieWire. Archivado desde el original el 23 de diciembre de 2019. Consultado el 4 de agosto de 2020. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 Ford, Rebecca (3 de enero de 2020). Â«Why Quentin Tarantino and More Directors Are Banning Cellphones on SetÂ». The Hollywood Reporter. Archivado desde el original el 4 de enero de 2020. Consultado el 15 de junio de 2020. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 Saraiya, Sonia (19 de junio de 2019). Â«Exclusive First Look: Greta Gerwig and Saoirse Ronan's 'Little Women'Â». Vanity Fair. Archivado desde el original el 19 de junio de 2019. Consultado el 19 de junio de 2019. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 Beresford, Trilby (13 de agosto de 2019). Â«Greta Gerwig's 'Little Women' Releases First TrailerÂ». The Hollywood Reporter. Consultado el 13 de agosto de 2019.
 Bell, Keaton (9 de diciembre de 2019). Â«All the Photos From Inside the New York Premiere of Little WomenÂ». Vogue. Archivado desde el original el 10 de diciembre de 2019. Consultado el 15 de junio de 2020. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 Cajueiro, Marcelo (7 de diciembre de 2019). Â«Rio Fest's Compact Edition Opens Amidst Sectorial CrisisÂ». Variety. Archivado desde el original el 7 de diciembre de 2019. Consultado el 15 de junio de 2020. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 McClintock, Pamela (18 de julio de 2018). Â«Quentin Tarantino's Manson Movie Shifts Off Sharon Tate Murder Anniversary DateÂ». The Hollywood Reporter. Archivado desde el original el 19 de julio de 2018. Consultado el 18 de julio de 2018. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 Eldredge, Kristy (27 de diciembre de 2019). Â«Opinion â€“ Men Are Dismissing 'Little Women.' What a Surprise.Â». The New York Times. Archivado desde el original el 27 de diciembre de 2019. Consultado el 27 de diciembre de 2019. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 D'Alessandro, Anthony (7 de abril de 2019). Â«'Little Women,' Big Profits: Remake Lands At No. 24 In Deadline's 2019 Most Valuable Blockbuster TournamentÂ». Deadline Hollywood. Archivado desde el original el 7 de abril de 2020. Consultado el 7 de abril de 2019. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 Brzeski, Patrick (3 de febrero de 2020). Â«China Releases for 'Dolittle,' '1917,' 'Jojo Rabbit' Canceled Amid Coronavirus CrisisÂ». The Hollywood Reporter. Archivado desde el original el 5 de febrero de 2020. Consultado el 15 de junio de 2020. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 Russian, Ale (9 de marzo de 2020). Â«See Saoirse Ronan, TimothÃ©e Chalamet and Emma Watson Get Silly Behind-the-Scenes on Little WomenÂ». People. Archivado desde el original el 10 de marzo de 2020. Consultado el 15 de junio de 2020. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 West, Amy (2 de abril de 2020). Â«Little Women made a Game of Thrones mistake that was just spotted by fansÂ». Digital Spy. Archivado desde el original el 5 de abril de 2020. Consultado el 15 de junio de 2020. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 Davis, Rebecca (13 de mayo de 2020). Â«'Little Women,' '1917' Likely Among First Films to Hit Reopened Chinese TheatersÂ». Variety. Archivado desde el original el 14 de mayo de 2020. Consultado el 15 de junio de 2020. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 Roxborough, Scott (15 de junio de 2020). Â«Little Women: Denmark, Japan Give Cinemas Hope for Post-Virus RecoveryÂ». The Hollywood Reporter. Archivado desde el original el 16 de junio de 2020. Consultado el 16 de junio de 2020. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 Grater, Tom (6 de agosto de 2020). Â«'Little Women' Locks China Release For August 25Â». Deadline Hollywood. Archivado desde el original el 7 de agosto de 2020. Consultado el 7 de agosto de 2020. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 Â«Little Women (2019)Â». Box Office Mojo. Amazon. Archivado desde el original el 13 de abril de 2020. Consultado el 31 de agosto de 2020. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 Â«Little Women (2019)Â». The Numbers. Archivado desde el original el 3 de enero de 2020. Consultado el 31 de agosto de 2020. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 McClintock, Pamela (25 de diciembre de 2019). Â«Box Office: 'Star Wars: Rise of Skywalker' Unwraps Huge $32M on Christmas DayÂ». The Hollywood Reporter. Archivado desde el original el 26 de diciembre de 2019. Consultado el 26 de diciembre de 2019. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 D'Alessandro, Anthony (28 de diciembre de 2019). Â«'Star Wars: Rise Of Skywalker' Chasing 'Last Jedi' With $76M 2nd Weekend; 'Little Women' Not So Tiny With $29M 5-DayÂ». Deadline Hollywood. Archivado desde el original el 29 de diciembre de 2019. Consultado el 29 de diciembre de 2019. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 Â«Domestic 2019 Weekend 52Â». Box Office Mojo. Amazon. Archivado desde el original el 30 de diciembre de 2019. Consultado el 3 de enero de 2020. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 D'Alessandro, Anthony (5 de enero de 2020). Â«'Star Wars: Rise Of Skywalker' Dips To $34M+ Third Weekend; 'Grudge' Doesn't Scream With $11M+ & 'F' CinemaScoreÂ». Deadline Hollywood. Archivado desde el original el 5 de enero de 2020. Consultado el 5 de enero de 2020. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 D'Alessandro, Anthony (12 de enero de 2020). Â«'1917' Strong With $36M+, But 'Like A Boss' & 'Just Mercy' Fighting Over 4th With $10M; Why Kristen Stewart's 'Underwater' Went Kerplunk With $6M+Â». Deadline Hollywood. Archivado desde el original el 12 de enero de 2020. Consultado el 12 de enero de 2020. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 D'Alessandro, Anthony (19 de enero de 2020). Â«'Bad Boys For Life' So Great With $100M+ Worldwide; 'Dolittle' Still A Dud With $57M+ Global â€“ Box Office UpdateÂ». Deadline Hollywood. Archivado desde el original el 20 de enero de 2020. Consultado el 19 de enero de 2020. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 Tartaglione, Nancy (14 de junio de 2020). Â«Little Women Marches Towards $100M Overseas â€“ International Box OfficeÂ». Deadline Hollywood. Archivado desde el original el 14 de junio de 2020. Consultado el 15 de junio de 2020. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 Moreau, Jordan (21 de junio de 2020). Â«'Little Women' Crosses $100 Million at the International Box OfficeÂ». Variety. Archivado desde el original el 22 de junio de 2020. Consultado el 22 de junio de 2020. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 Tartaglione, Nancy (30 de agosto de 2020). Â«'Tenet' Triumphs With $53M Worldwide Launch From 40 Offshore Markets & Canada â€“ International Box OfficeÂ». Deadline Hollywood. Archivado desde el original el 27 de agosto de 2020. Consultado el 30 de agosto de 2020. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 Â«Little Women (2019)Â». Rotten Tomatoes. Fandango Media. Archivado desde el original el 18 de diciembre de 2019. Consultado el 19 de agosto de 2020. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 Â«'Little Women' (2019) ReviewsÂ». Metacritic. CBS Interactive. Archivado desde el original el 18 de diciembre de 2019. Consultado el 25 de diciembre de 2019. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 Erbland, Kate (25 de noviembre de 2019). Â«'Little Women' Review: Greta Gerwig Marries Tradition With Meta Modernity in Stunning AdaptationÂ». IndieWire. Archivado desde el original el 29 de febrero de 2020. Consultado el 29 de febrero de 2020. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 Lane, Anthony (6 de enero de 2020). Â«Greta Gerwig's "Little Women," ReviewedÂ». The New Yorker. Archivado desde el original el 25 de diciembre de 2019. Consultado el 15 de junio de 2020. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 Bahr, Lindsey (16 de diciembre de 2019). Â«Review: Greta Gerwig's 'Little Women' is a new classicÂ». Associated Press. Archivado desde el original el 15 de junio de 2020. Consultado el 15 de junio de 2020. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 Truitt, Brian (25 de noviembre de 2019). Â«Review: Greta Gerwig's all-star 'Little Women' adapts a classic with modern wit, resonanceÂ». USA Today. Archivado desde el original el 11 de febrero de 2020. Consultado el 29 de febrero de 2020. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 LaSalle, Mick (19 de diciembre de 2019). Â«Gerwig's 'Little Women' are snootier than we rememberÂ». San Francisco Chronicle. Archivado desde el original el 20 de diciembre de 2019. Consultado el 6 de julio de 2020. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 Rooney, David (25 de noviembre de 2019). Â«'Little Women': Film ReviewÂ». The Hollywood Reporter. Archivado desde el original el 26 de noviembre de 2019. Consultado el 15 de junio de 2020. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 Duralde, Alonso (25 de noviembre de 2019). Â«'Little Women' Film Review: Greta Gerwig's New Spin on a Beloved TaleÂ». TheWrap. Archivado desde el original el 28 de noviembre de 2019. Consultado el 15 de junio de 2020. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 James, Caryn (16 de diciembre de 2019). Â«Why Little Women is a triumphÂ». BBC Online. Archivado desde el original el 15 de junio de 2020. Consultado el 15 de junio de 2020. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 Greenblatt, Leah (25 de noviembre de 2019). Â«Little Women review: Greta Gerwig's remake is a warm blanket in a cold worldÂ». Entertainment Weekly. Archivado desde el original el 26 de enero de 2020. Consultado el 15 de junio de 2020. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 Sims, David (25 de diciembre de 2019). Â«Greta Gerwig Captures the Poignancy of 'Little Women'Â». The Atlantic. Archivado desde el original el 26 de diciembre de 2019. Consultado el 15 de junio de 2020. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 Loughrey, Clarisse (27 de diciembre de 2019). Â«Little Women review: Greta Gerwig's loving adaptation waltzes with a literary ghostÂ». The Independent. Archivado desde el original el 25 de diciembre de 2019. Consultado el 15 de junio de 2020. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 Chang, Justin (20 de diciembre de 2019). Â«'Little Women' Again? Greta Gerwig's Adaptation Is Both Faithful And RadicalÂ». NPR. Archivado desde el original el 20 de diciembre de 2019. Consultado el 15 de junio de 2020. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 Travers, Peter (23 de diciembre de 2019). Â«Greta Gerwig Delivers a 'Little Women' for a New GenerationÂ». Rolling Stone. Archivado desde el original el 8 de abril de 2020. Consultado el 14 de junio de 2020. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 Hornaday, Ann (17 de diciembre de 2019). Â«Part Alcott, part Gerwig, 'Little Women' is a very nearly perfect filmÂ». The Washington Post. Archivado desde el original el 18 de diciembre de 2019. Consultado el 14 de junio de 2020. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 Aurthur, Kate (4 de febrero de 2020). Â«Greta Gerwig on 'Little Women's' Oscar Nominations â€” and That One Big SnubÂ». Variety. Archivado desde el original el 9 de febrero de 2020. Consultado el 24 de marzo de 2020. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 Butler, Bethonie (8 de febrero de 2020). Â«The biggest female director Oscar snubs of the past decadeÂ». The Washington Post. Archivado desde el original el 8 de febrero de 2020. Consultado el 24 de marzo de 2020. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 Pearson, Allison (14 de enero de 2020). Â«Greta Gerwig's Oscars snub proves Hollywood is still pale, male and staleÂ». The Daily Telegraph. Archivado desde el original el 15 de enero de 2020. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 Stevens, Dana (13 de enero de 2020). Â«2020 Oscar nominations snub Greta Gerwig for Best Director: Does the Academy think Little Women directed itself?Â». Slate. Archivado desde el original el 14 de enero de 2020. Consultado el 16 de junio de 2020. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 Proudfoot, Devon; Kay, Aaron (8 de febrero de 2020). Â«Op-Ed: A scientific reason for Greta Gerwig's Oscar snub: The creativity of women is judged more harshlyÂ». Los Angeles Times. Archivado desde el original el 8 de febrero de 2020. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 Nordyke, Kimberly; Konerman, Jennifer; Strause, Jackie; Howard, Annie (13 de enero de 2020). Â«Oscar Nominations 2020: The Complete List of NomineesÂ». The Hollywood Reporter. Archivado desde el original el 13 de enero de 2020. Consultado el 15 de enero de 2020. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 Wilson, Jordan (13 de enero de 2020). Â«Oscars: Greta Gerwig's Adaptation Brings 'Little Women' Noms Tally to 14Â». The Hollywood Reporter. Archivado desde el original el 13 de enero de 2020. Consultado el 13 de enero de 2020. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 N'Duka, Amanda (9 de febrero de 2020). Â«Jacqueline Durran Nabs Second Career Oscar Award For Costume Design For 'Little Women'Â». Deadline Hollywood. Archivado desde el original el 10 de febrero de 2020. Consultado el 10 de febrero de 2020. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 Malkin, Marc (8 de diciembre de 2019). Â«Critics' Choice: 'The Irishman,' 'Once Upon a Time in Hollywood' Lead Movie NominationsÂ». Variety. Archivado desde el original el 9 de diciembre de 2019. Consultado el 8 de diciembre de 2019. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 Ramos, Dino-Ray (12 de enero de 2020). Â«Critics' Choice Awards: 'Once Upon A Time In Hollywood' Wins Best Picture, Netflix And HBO Among Top Honorees â€“ Full Winners ListÂ». Deadline Hollywood. Archivado desde el original el 14 de enero de 2020. Consultado el 15 de enero de 2020. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 Tartaglione, Nancy (7 de enero de 2020). Â«BAFTA Film Awards Nominations: 'Joker', 'The Irishman', 'Once Upon A Time In Hollywood' Lead â€“ Full ListÂ». Deadline Hollywood. Archivado desde el original el 8 de enero de 2020. Consultado el 7 de enero de 2020. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 Nordyke, Kimberly; Konerman, Jennifer; Howard, Annie (9 de diciembre de 2019). Â«Golden Globes: Full List of NominationsÂ». The Hollywood Reporter. Archivado desde el original el 10 de diciembre de 2019. Consultado el 9 de diciembre de 2019. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
 Hipes, Patrick (4 de diciembre de 2019). Â«AFI Awards Film: 'The Irishman', '1917', 'Little Women' Among Top 10Â». Deadline Hollywood. Archivado desde el original el 4 de diciembre de 2019. Consultado el 4 de diciembre de 2019. ParÃ¡metro desconocido |url-status= ignorado (ayuda)
Enlaces externos
PÃ¡gina web oficial
Mujercitas en Internet Movie Database (en inglÃ©s).
Little Women en Metacritic (en inglÃ©s).
Little Women en Rotten Tomatoes (en inglÃ©s).
Little Women en Box Office Mojo (en inglÃ©s)
Control de autoridades	
Proyectos WikimediaWd Datos: Q56881140IdentificadoresWorldCatVIAF: 69155707017022410579LCCN: no2019055758
CategorÃ­as: PelÃ­culas en inglÃ©sPelÃ­culas de Estados UnidosPelÃ­culas de 2019PelÃ­culas de Columbia PicturesPelÃ­culas dramÃ¡ticasPelÃ­culas romÃ¡nticasPelÃ­culas de Regency EnterprisesPelÃ­culas sobre hermanasPelÃ­culas sobre la guerra de SecesiÃ³nPelÃ­culas dramÃ¡ticas de Estados UnidosPelÃ­culas dramÃ¡ticas de los aÃ±os 2010PelÃ­culas candidatas al premio Ã“scar a la mejor pelÃ­culaAdaptaciones cinematogrÃ¡ficas de Mujercitas
MenÃº de navegaciÃ³n
No has accedido
DiscusiÃ³n
Contribuciones
Crear una cuenta
Acceder
ArtÃ­culoDiscusiÃ³n
LeerEditarVer historialBuscar
Buscar en Wikipedia
Portada
Portal de la comunidad
Actualidad
Cambios recientes
PÃ¡ginas nuevas
PÃ¡gina aleatoria
Ayuda
Donaciones
Notificar un error
Herramientas
Lo que enlaza aquÃ­
Cambios en enlazadas
Subir archivo
PÃ¡ginas especiales
Enlace permanente
InformaciÃ³n de la pÃ¡gina
Citar esta pÃ¡gina
Elemento de Wikidata
Imprimir/exportar
Crear un libro
Descargar como PDF
VersiÃ³n para imprimir

En otros idiomas
Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©
Deutsch
English
FranÃ§ais
Bahasa Indonesia
PortuguÃªs
Ğ ÑƒÑÑĞºĞ¸Ğ¹
Tiáº¿ng Viá»‡t
ä¸­æ–‡	!"#$%&%()(=()/=)=/)?/=?/=Ã‘Ã‘))=Ã‘/)99
21 mÃ¡s
Editar enlaces
Esta pÃ¡gina se editÃ³ por Ãºltima vez el Â°22 mar 2021 a las 18:14.
El texto estÃ¡ disponible bajo la Lic	|encia Creative Commons AtribuciÃ³n Compartir Igual 3.0; pueden aplicarse clÃ¡usulas adicionales. Al usar este sitio, usted acepta nuestros tÃ©rminos de uso y nuestra polÃ­tica de privacidad.
WikipediaÂ® es una marca registrada de la FundaciÃ³n Wikimedia, Inc., una organizaciÃ³n sin Ã¡nimo de lucro.
	"#import string as st
import pmli as pm 
import sys
import operator as op

class crearAyudante():
    def __init__(self):
        self.MAX = 27
        self.lista1 = [i for i in st.ascii_lowercase]
        self.lista1.insert(14,"Ã±")
        
        self.dicc = [pm.crearPMLI(i) for i in self.lista1]

    def esPalabraValida(self, s):
        assert(type(s) == str)
        for letra in s:
            if not (letra in self.lista1):
                return False
        return True
        
    def distance(self, s1, s2):
        d=dict()
        for i in range(len(s1)+1):
           d[i]=dict()
           d[i][0]=i
        for i in range(len(s2)+1):
           d[0][i] = i
        for i in range(1, len(s1)+1):
            for j in range(1, len(s2)+1):
                d[i][j] = min(d[i][j-1]+1, d[i-1][j]+1, d[i-1][j-1]+(not s1[i-1] == s2[j-1]))
        return d[len(s1)][len(s2)]    

    def cargarDiccionario(self, fname):
        with open(fname) as archivo:
            for linea in archivo:
                linea = linea[:-1].lower()
                # print(linea)
                if self.esPalabraValida(linea) is True:
                    for i in range(self.MAX):
                        if self.dicc[i].l == linea[0] and  not linea.isspace():
                            #print(i, self.dicc[i].l, len(self.dicc))
                            #resolver las lineas en blamco
                            self.dicc[i].agregarPalabra(linea)
                            break              
                else:
                    return

    def borrarPalabra(self, p):
        assert(self.esPalabraValida(p) is True)
        for i in range(self.MAX):
            if self.dicc[i].l == p[0] and self.dicc[i].eliminarPalabra(p) is True:
                return           

    def buscarPalabra(self, p):
        assert(self.esPalabraValida(p) is True)
        for i in range(self.MAX):
            if self.dicc[i].l == p[0] and self.dicc[i].buscarPalabra(p) is True:
                return True

    def mostrar(self):
        for i in range(self.MAX):
            print(self.dicc[i].mostrar(), "\n")

    def corregirTexto(self, finput):
        with open(finput) as archivo:
            for linea in archivo:
                lista = [i for i in linea.split()
                         if (self.esPalabraValida(i) is True) and (self.buscarPalabra(i) is not True)]
                
                for palabra in lista:
                    # print(palabra)
                    dlev = {}
                    for i in range(self.MAX):
                        aux = self.dicc[i].pal.tabla
                        dlev1 = {}
                        for j in range(len(aux)):
                            dlev1 = {}
                            if aux[j] is not None:
                                if len(dlev) < 4:
                                    dlev[aux[j]] = self.distance(aux[j], palabra)

                                else:    
                                    # print(dlev, palabra, dlev1,"\n")
                                    pal_dicc = (aux[j], self.distance(aux[j], palabra))

                                    dlev = dict(sorted([i for i in dlev.items()], key=lambda x: x[1]))
                                    for i in dlev.items():

                                        if i[1] <= pal_dicc[1] and len(dlev1) < 4:
                                            dlev1[i[0]] = i[1]  

                                        elif i[1] > pal_dicc[1] and len(dlev1) < 4:       
                                            dlev1[pal_dicc[0]] = pal_dicc[1]    
                                            pal_dicc = i

                                    dlev = dlev1
                    
                    with open("foutput.txt", "a") as archivo:
                        salida = dlev
                        salida = [clave for clave in salida.keys()]
                        salida.insert(0, palabra)
                        salida = ",".join(salida)
                        archivo.write(salida)
                        archivo.write("\n")



                                
                        
                        

                
                # for i in d.items():
                # print("*", lista,"\n")


helpo = crearAyudante()
helpo.cargarDiccionario(sys.argv[1])

# print(helpo.mostrar())

# elpo.borrarPalabra(sys.argv[2])
# helpo.buscarPalabra(sys.argv[3])
helpo.corregirTexto(sys.argv[4])
helpo.mostrar()




# with open(sys.argv[4]) as archivo:
#    for linea in archivo:
#        lista = linea.split()
#        print(lista)
Series Editors
Gerhard Goos
Universitat Karlsruhe
Postfach 69 80
Vincenz-Priessnitz-StraBe 1
D-76131 Karlsruhe, Germany
Juris Hartmanis
Cornell University
Department of Computer Science
4130 Upson Hall
Ithaca, NY 14853, USA
Author
Gerhard Reinelt
Institut fur Angewandte Mathematik, Universitat Heidelberg
Im Neuenheimer Feld 294, D-69120 Heidelberg, Germany
CR Subject Classification (1991): G.2,1.3.5, G.4,1.2.8, J.l-2
ISBN 3-540-58334-3 Springer-Verlag Berlin Heidelberg New York
ISBN 0-387-58334-3 Springer-Verlag New York Berlin Heidelberg
CIP data applied for
This work is subject to copyright. All rights are reserved, whether the whole or part
of the material is concerned, specifically the rights of translation, reprinting, re-use
of illustrations, recitation, broadcasting, reproduction on microfilms or in any other
way, and storage in data banks. Duplication of this publication or parts thereof is
permitted only under the provisions of the German Copyright Law of September 9,
1965, in its current version, and permission for use must always be obtained from
Springer-Verlag. Violations are liable for prosecution under the German Copyright
Law.
Â© Springer-Verlag Berlin Heidelberg 1994
Printed in Germany
Typesetting: Camera-ready by author
SPIN: 10475419
45/3140-543210 - Printed on acid-free paperPreface to the Online Edition
Still today I am receiving requests for reprints of the book, but unfortunately it
is out of print. Therefore, since the book still seems to receive some attention, I pro-
posed to Springer Verlag to provide a free online edition. I am very happy that Springer
agreed. Except for the correction of some typographical errors, the online edition is just
a copy of the printed version, no updates have been made. In particular, Table 13.1
gives the status of TSPLIB at the time of publishing the book. For accessing TSPLIB the
link http://www.iwr.uni-heidelberg.de/iwr/comopt/software/TSPLIB95/ should be
used instead of following the procedure described in Chapter 13.
Heidelberg, January 2001
Gerhard ReineltPreface
More than fifteen years ago, I was faced with the following problem in an assignment
for a class in computer science. A brewery had to deliver beer to five stores, and the task
was to write a computer program for determining the shortest route for the truck driver to
visit all stores and return to the brewery. All my attemps to find a reasonable algorithm
failed, I could not help enumerating all possible routes and then select the best one.
Frustrated at that point, I learnt later that there was no fast algorithm for solving this
problem. Moreover, I found that this problem was well known as the traveling salesman
problem and that there existed a host of published work on finding solutions. Though
no efficient algorithm was developed, there was a tremendous progress in designing fast
approximate solutions and even in solving ever larger problem instances to optimality. I
started some work on the traveling salesman problem several years ago, first just writing
demos for student classes, but then trying to find good and better solutions more effec-
tively. I experienced the fascination of problem solving that, I think, everyone studying
the traveling salesman problem will experience. In addition, I found that the problem has
relevance in practice and that there is need for fast algorithms.
The present monograph documents my experiments with algorithms for finding good
approximate solutions to practical traveling salesman problems. The work presented here
profited from discussions and meetings with several people, among them Thomas Christof,
Meinrad Funke, Martin GrÃ¶tschel, Michael JÃ¼nger, Manfred Padberg, Giovanni Rinaldi,
and Stefan Thienel, not naming dozens of further international researchers.
It is the aim of this text to serve as a guide for practitioners, but also to show that
the work on the traveling salesman problem is not at all finished. The TSP will stimulate
further efforts and continue to serve as the classical benchmark problem for algorithmic
ideas.
Heidelberg, June 1994
Gerhard ReineltContents
1 Introduction
2 Basic Concepts
2.1
2.2
2.3
2.4
2.5
. . . . . . . . . . . . . . . . . . . . . . . . . . 1
. . . . . . . . . . . . . . . . . . . . . . . . . 4
Graph Theory . . . . . . . .
Complexity Theory . . . . . .
Linear and Integer Programming
Data Structures
. . . . . . .
Some Fundamental Algorithms .
.
.
.
.
. 4
7
12
14
25
. . . . . . . . . . . . . . . 31
Some Related Problems . . . . . . . . . . . . . . . . . . . .
Practical Applications of the TSP . . . . . . . . . . . . . . . .
The Test Problem Instances . . . . . . . . . . . . . . . . . . 31
35
40
3 Related Problems and Applications
3.1
3.2
3.3
4 Geometric Concepts
4.1
4.2
4.3
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
42
Voronoi Diagrams . . . . . . . . . . . . . . . . . . . . . . .
Delaunay Triangulations . . . . . . . . . . . . . . . . . . . .
Convex Hulls . . . . . . . . . . . . . . . . . . . . . . . . . 42
48
54
. . . . . . . . . . . . . . . . . . . . . . . . . 64
Nearest Neighbors
. . . . . . . . . . . . . . . . . . . . . .
Candidates Based on the Delaunay Graph . . . . . . . . . . . .
Other Candidate Sets . . . . . . . . . . . . . . . . . . . . . 64
67
70
6 Construction Heuristics
6.1
6.2
6.3
6.4
6.5
.
.
.
.
.
. . . . . . . . . . . . . . . . . . . . . .
5 Candidate Sets
5.1
5.2
5.3
.
.
.
.
.
. . . . . . . . . . . . . . . . . . . . .
Nearest Neighbor Heuristics . . . . .
Insertion Heuristics . . . . . . . . .
Heuristics Using Spanning Trees . . .
Savings Methods and Greedy Algorithm
Comparison of Construction Heuristics
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
73
73
82
89
94
97VIII
7 Improving Solutions
7.1
7.2
7.3
7.4
7.5
7.6
. . . . . . . . . . . . . . . . . . . . . . . 100
Node and Edge Insertion . . . . . .
2-Opt Exchange
. . . . . . . . .
Crossing Elimination . . . . . . .
The 3-Opt Heuristic and Variants . .
Lin-Kernighan Type Heuristics . . .
Comparison of Improvement Heuristics
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
8 Fast Heuristics for Large Geometric Problems
8.1
8.2
8.3
8.4
Space Filling Curves
. .
Strip Heuristics . . . . .
Partial Representation . .
Decomposition Approaches
9 Further Heuristic Approaches
9.1
9.2
9.3
9.4
Simulated Annealing
Evolutionary Strategies
Tabu Search . . . .
Neural Networks . .
10 Lower Bounds
10.1
10.2
10.3
10.4
. .
and
. .
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
100
105
115
117
123
130
. . . . . . . . . 133
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
133
137
139
145
. . . . . . . . . . . . . . . . . . 153
. . . .
Genetic
. . . .
. . . .
. . . . . . . . .
Algorithms
. . .
. . . . . . . . .
. . . . . . . . .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
153
157
158
159
. . . . . . . . . . . . . . . . . . . . . . . . . . 161
Bounds from Linear Programming
. .
Simple Lower Bounds . . . . . . . .
Lagrangean Relaxation . . . . . . .
Comparison of Lower Bounds . . . . .
11 A Case Study: TSPs in PCB Production
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
161
163
172
184
. . . . . . . . . . . . 187
11.1 Drilling of Printed Circuit Boards . . . . . . . . . . . . . . . . 187
11.2 Plotting of PCB Production Masks . . . . . . . . . . . . . . . 193
12 Practical TSP Solving
. . . . . . . . . . . . . . . . . . . . . . 200
12.1 Determining Optimal Solutions . . . . . . . . . . . . . . . . . 200
12.2 An Implementation Concept . . . . . . . . . . . . . . . . . . 204
12.3 Interdependence of Algorithms . . . . . . . . . . . . . . . . . 207
Appendix: TSPLIB
References
Index
. . . . . . . . . . . . . . . . . . . . . . . 211
. . . . . . . . . . . . . . . . . . . . . . . . . . . 214
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 222Chapter 1
Introduction
The most prominent member of the rich set of combinatorial optimization problems is
undoubtly the traveling salesman problem (TSP), the task of finding a route through
a given set of cities with shortest possible length. It is one of the few mathematical
problems that frequently appear in the popular scientific press (Cipra (1993)) or even
in newspapers (Kolata (1991)). It has a long history, dating back to the 19th century
(Hoffman & Wolfe (1985)).
The study of this problem has attracted many researchers from different fields, e.g.,
Mathematics, Operations Research, Physics, Biology, or Artificial Intelligence, and there
is a vast amount of literature on it. This is due to the fact that, although it is easily
formulated, it exhibits all aspects of combinatorial optimization and has served and
continues to serve as the benchmark problem for new algorithmic ideas like simulated
annealing, tabu search, neural networks, simulated tunneling or evolutionary methods
(to name only a few of them).
On the other hand, the TSP is interesting not only from a theoretical point of view.
Many practical applications can be modeled as a traveling salesman problem or as
variants of it. Therefore, there is a tremendous need for algorithms. The number of
cities in practical applications ranges from some dozens up to even millions (in VLSI
design). Due to this manifold area of applications there also has to be a broad collection
of algorithms to treat the various special cases.
In the last two decades an enormous progress has been made with respect to solving
traveling salesman problems to optimality which, of course, is the ultimate goal of every
researcher. Landmarks in the search for optimal solutions are the solution of a 48-city
problem (Dantzig, Fulkerson & Johnson (1954)), a 120-city problem (GrÃ¶tschel
(1980)), a 318-city problem (Crowder & Padberg (1980)), a 532-city problem (Pad-
berg & Rinaldi (1987)), a 666-city problem (GrÃ¶tschel & Holland (1991)), a
2392-city problem (Padberg & Rinaldi (1991)), a 3038-city problem (Applegate,
Bixby, ChvÃ tal & Cook (1991)), and of a 4461-city problem (Applegate, Bixby,
ChvÃ tal & Cook (1993)). This progress is only partly due to the increasing hardware
power of computers. Above all, it was made possible by the development of mathemat-
ical theory (in particular polyhedral combinatorics) and of efficient algorithms. But,
despite of these achievements, the traveling salesman problem is far from being solved.
Many aspects of the problem still need to be considered and questions are still left to
be answered satisfactorily.
First, the algorithms that are able to solve the largest (with respect to the number of
cities) problems to optimality are not stable in the following sense: solution times vary
strongly for different problems with the same number of cities and there is no function
G. Reinelt: The Traveling Salesman, LNCS 840, pp. 1-3, 1994.
ï›™ Springer-Verlag Berlin Heidelberg 19942
Chapter 1. Introduction
depending on the number of cities that only gives a slight idea of the time necessary
to solve a particular problem. Already problems with some hundred nodes can be very
hard for these algorithms and require hours of CPU time on supercomputers. And,
there is a lot of theoretical knowledge that has not yet gone into implementations.
Second, problems arising in practice may have a number of cities that is far beyond
the capabilities of any exact algorithm available today. There are very good heuristics
yielding solutions which are only a few percent above optimality. However, they still
can be improved with respect to running time or quality of the computed solutions.
Third, requirements in the production environment may make many algorithms or
heuristics unsuitable. Possible reasons are that not enough real time or CPU time is
available to apply certain algorithms, that the problem instances are simply too large,
or that not enough real time or man power is at hand to code a method one would like
to apply.
These arguments visualize the potential that is still inherent in the traveling salesman
problem.
The present monograph is meant to be a contribution to practical traveling salesman
problem solving. Main emphasis will be laid on the question of how to find good or
acceptable tours for large problem instances in short time. We will discuss variants
and extensions of known approaches and discuss some new ideas that have proved to
be useful. Furthermore we will indicate some directions of future research. Literature
will be reviewed to some extent, but a complete coverage of the knowledge about the
TSP is beyond the purpose and available space of this tract. For an introduction we
recommend the book Lawler, Lenstra, Rinnooy Kan & Shmoys (1985) and the
survey article JÃ¼nger, Reinelt & Rinaldi (1994).
Nevertheless, even without consulting further references the present text is meant to
be a guide for readers who are concerned with applications of the TSP and aims at
providing sufficient information for their successful treatment.
We give a short survey of the topics that will be addressed. Chapter 2 covers basic con-
cepts that we need throughout this monograph. This chapter contains an introduction
to complexity theory and describes some fundamental data structures and algorithms.
The many possible applications of the TSP are indicated in Chapter 3. Of particular
importance are Euclidean instances. To exploit the underlying geometric structure we
use Voronoi diagrams and convex hulls which are discussed in Chapter 4. A basic ingre-
dient of fast heuristics will be a limitation of the scope of search for good tours. This is
accomplished by candidate sets which restrict algorithms to certain subsets of promising
connections. The construction of reasonable candidate sets is the topic of Chapter 5.
Construction heuristics to find starting tours are given in Chapter 6. Emphasis is laid
on improving standard approaches and making them applicable for larger problems.
Many of these heuristics are also useful in later chapters. Chapter 7 is concerned with
the improvement of given tours. It is shown how data structures can be successfully
employed to come up with very efficient implementations. An important issue is cov-
ered in Chapter 8: the treatment of very large problem instances in short time. Several
types of approaches are presented. A short survey of recent heuristic methods is con-
tained in Chapter 9. Lower bounds are the topic of Chapter 10. Besides variants of
known approaches we comment on heuristics for computing Lagrange multipliers. The
algorithms described in this text have been successfully applied in an industry project.Chapter 1. Introduction
3
We discuss this project in depth in Chapter 11. Chapter 12 addresses the question of
computing optimal solutions as well as solutions with quality guarantee and discusses
some lines of future research. In particular, a proposal for a hardware and software
setup for the effective treatment of traveling salesman problems in practice is presented.
The appendix gives information of how getting access to TSPLIB, a publicly available
collection of TSP instances, and lists the current status of these problem instances.
In this monograph we will not describe the approaches down to the implementation
level. But, we will give enough information to facilitate implementations and point
out possible problems. Algorithms are presented on a level that is sufficient for their
understanding and for guiding practical realizations.
Extensive room is spent for computational experiments. Implementations were done
carefully, however, due to limited time for coding the software, not always the absolutely
fastest algorithm could be used. The main point is the discussion of various algorithmic
ideas and their comparison using reasonable implementations. We have not restricted
ourselves to only tell â€œsuccess storiesâ€, but we rather point out that sometimes even
elaborate approaches fail in practice.
Summarizing, it is the aim of this monograph to give a comprehensive survey on heuristic
approaches to traveling salesman problem solving and to motivate the development and
implementation of further and possibly better algorithms.Chapter 2
Basic Concepts
The purpose of this chapter is to survey some basic knowledge from computer science
and mathematics that we need in this monograph. It is intended to provide the reader
with some fundamental concepts and results. For a more detailed representation of the
various subjects we shall refer to appropriate textbooks.
2.1 Graph Theory
Many combinatorial optimization problems can be formulated as problems in graphs.
We will therefore review some basic definitions from graph theory.
An undirected graph (or graph) G = (V, E) consists of a finite set of nodes V and a
finite set of edges E. Each edge e has two endnodes u, v and is denoted by e = uv or
e = {u, v}. We call such a graph undirected because we do not distinguish between the
edges uv and vu. However, we will sometimes speak about head and tail of an edge.
If e = uv then e is incident to v and to u. The set of edges incident to a node v is
denoted by Î´(v). The number |Î´(v)| is the degree of v.
A graph G  = (V  , E  ) is called a subgraph of G = (V, E) if V  âŠ† V and E  âŠ† E. For an
edge set E âŠ† E we define V (E) := {u, v âˆˆ V |uv âˆˆ E}. Conversely, for a node set V âŠ† V
we define E(V ) := {uv âˆˆ E | u âˆˆ V and v âˆˆ V }. We call the subgraph G  = (V (E), E)
edge induced (by E) and the subgraph G  = (V , E(V )) node induced (by V ).
A graph G = (V, E) is said to be complete if for all u, v âˆˆ V it contains edge uv. We
denote the complete graph on n nodes by K n = (V n , E n ) and assume unless otherwise
stated that V n = {1, 2, . . . , n}.
Two graphs G  = (V  , E  ) and G  = (V  , E  ) are isomorphic if there exists a bijective
mapping f : V  â†’ V  such that uv âˆˆ E  if and only if f (u)f (v) âˆˆ E  , e.g., the complete
graph K n is unique up to isomorphism.
A graph G = (V, E) is called bipartite if its node set V can be partitioned into two
nonempty disjoint sets V 1 , V 2 with V 1 âˆªV 2 = V such that no two nodes in V 1 and no two
nodes in V 2 are connected by an edge. If |V 1 | = m, |V 2 | = n and E = {ij | i âˆˆ V 1 , j âˆˆ V 2 }
then we call G the complete bipartite graph K m,n .
An edge set P = {v 1 v 2 , v 2 v 3 , . . . , v kâˆ’1 v k } is called a walk or more precisely a [v 1 , v k ]â€“
walk. If v i  = v j for all i  = j then P is called path or [v 1 , v k ]â€“path. The length of a
walk or path is the number of its edges and is denoted by |P |. If in a walk v 1 = v k we
speak of a closed walk.
A set of edges C = {v 1 v 2 , v 2 v 3 , . . . , v kâˆ’1 v k , v k v 1 } with v i  = v j for i  = j is called a cycle
(or k-cycle). An edge v i v j , 1 â‰¤ i  = j â‰¤ k, not in C is called chord of C. The length of
G. Reinelt: The Traveling Salesman, LNCS 840, pp. 4-30, 1994.
ï›™ Springer-Verlag Berlin Heidelberg 19942.1. Graph Theory
5
a cycle C is denoted by |C|. For convenience we shall sometimes abbreviate the cycle
{v 1 v 2 , v 2 v 3 , . . . , v k v 1 } by (v 1 , v 2 , . . . v k ) and also say that a graph G is a cycle if its edge
set forms a cycle. A graph or edge set is called acyclic if it contains no cycle. An
acyclic graph is also called forest.
A graph G = (V, E) is said to be connected if it contains for every pair of nodes a
path connecting them; otherwise G is called disconnected. A spanning tree is a
connected forest containing all nodes of the graph.
A nonempty edge set F âŠ† E is said to be a cut of the graph G = (V, E) if V can be
partitioned into two nonempty disjoint subsets V 1 , V 2 with V 1 âˆª V 2 = V such that the
following holds: F = {uv âˆˆ E | u âˆˆ V 1 , v âˆˆ V 2 }. Equivalently, F is a cut if there exists
a node set W âŠ† V such that F = Î´(W ).
Sometimes it is useful to associate a direction with the edges of a graph. A directed
graph (or digraph) D = (V, A) consists of a finite set of nodes V and a set of arcs
A âŠ† V Ã— V \ {(v, v) | v âˆˆ V } (we do not consider loops or multiple arcs). If e = (u, v) is
an arc of D with endnodes u and v then we call u its tail and v its head. The arc e is
said to be directed from u to v, incident from u and incident to v. The number
of arcs incident to a node v is called the indegree of v and the number of arcs incident
from v is called the outdegree of v. The degree of v is the sum of its indegree and
outdegree. For a node v the sets of arcs incident from v, incident to v, and incident from
or to v are denoted by Î´ + (v), Î´ âˆ’ (v), and Î´(v), respectively. Two nodes are adjacent if
there is an arc connecting them.
Most of the definitions for undirected graphs carry over in a straightforward way to
directed graphs. For example, diwalks, dipaths and dicycles are defined analogously
to walks, paths, and cycles with the additional requirement that the arcs are directed
in the same direction.
A digraph D = (V, A) is said to be complete if for all u, v âˆˆ V it contains both arcs
(u, v) and (v, u). We denote the complete digraph on n nodes by D n = (V n , A n ).
For each digraph D = (V, A) we can construct its underlying graph G = (V, E) by
setting E = {uv | u and v are adjacent in D}.
A digraph D = (V, A) is called connected (disconnected) if its underlying graph is
connected (disconnected). D is called diconnected if for each pair u, v of its nodes
there are a [u, v]â€“ and a [v, u]â€“dipath in D. A node v âˆˆ V is called articulation node
or cutnode of a digraph (graph) if the removal of v and all arcs (edges) having v as an
endnode disconnects the digraph (graph). A connected digraph (graph) is said to be
2-connected if it contains no articulation node.
To avoid degenerate situations we assume that, unless otherwise noted, all graphs and
digraphs contain at least one edge, respectively arc.
A walk (diwalk) that traverses every edge (arc) of a graph (digraph) exactly once is
called Eulerian trail (Eulerian ditrail). If such a walk (diwalk) is closed we speak of
a Eulerian tour. A graph (digraph) is Eulerian if its edge (arc) set can be traversed
by a Eulerian tour.
A cycle (dicycle) of length n in a graph (digraph) on n nodes is called Hamiltonian
cycle (Hamiltonian dicycle) or Hamiltonian tour. A path (dipath) of length n is
called Hamiltonian path (Hamiltonian dipath). A graph (digraph) containing a
Hamiltonian tour is called Hamiltonian.6
Chapter 2. Basic Concepts
Often we have to deal with graphs where a rational number (edge weight) is associated
with each edge. We call a function c : E â†’ Q (where Q denotes the set of rational
numbers) a weight function defining a weight c(e) (or c e , or c uv ) for every edge e =
uv âˆˆ E. (In the context of practical computations it makes no sense to admit arbitrary
real-valued functions since only rational numbers are representable on a computer.) The
weight of a set of edges F âŠ† E is defined as

c uv .
c(F ) :=
uvâˆˆF
The weight of a tour is usually called its length, a tour of smallest weight is called
shortest tour. The problem in the focus of this monograph is the so-called (symmetric)
traveling salesman problem.
(Symmetric) Traveling Salesman Problem
Given the complete graph K n with edge weights c uv find a shortest Hamiltonian tour
in K n .
A symmetric TSP is said to satisfy the triangle inequality, if c uv â‰¤ c uw + c wv for
all distinct nodes u, v, w âˆˆ V . Of particular interest are metric traveling salesman
problems. These are problems where the nodes correspond to points in some space and
where the edge weights are given by evaluating some metric distance between corre-
sponding points. For example, a Euclidean TSP is defined by a set of points in the
plane. The corresponding graph contains a node for every point and edge weights are
given by the Euclidean distance of the points associated with the end nodes.
We list some problems on graphs related to the traveling salesman problem which will
be referred to at some places.
Asymmetric Traveling Salesman Problem
Given the complete digraph D n with arc weights c uv find a shortest Hamiltonian tour
in D n .
Chinese Postman Problem
Given a graph G = (V, E) with edge weights c uv for uv âˆˆ E find a shortest closed walk
in G containing all edges at least once.
Hamiltonian Cycle Problem
Given a graph G = (V, E) decide if G contains a Hamiltonian cycle.
Eulerian Tour Problem
Given a graph G = (V, E) decide if G is Eulerian.
Though quite similar, these problems are very different with respect to their hardness.
It is a topic of the next section to give a short introduction into complexity theory and
its impact on the traveling salesman problem.2.2. Complexity Theory
7
2.2 Complexity Theory
When dealing with combinatorial problems or algorithms one is often interested in
comparing problems with respect to their hardness and algorithms with respect to their
efficiency. Often it is intuitively clear that some problem is more difficult to solve than
another problem and that one algorithm takes longer than another algorithm. The work
of Cook (Cook (1971)) laid the foundation for putting these questions into an exact
mathematical framework. Based on the notion of deterministic and nondeterministic
Turing machines it makes the classification of problems as â€œhardâ€ or â€œeasyâ€ possible and
allows the measurement of efficiency of algorithms. Although being only a theoretical
model this concept had a great impact on the design and the analysis of algorithms.
For our purposes it is sufficient to introduce the concepts of complexity theory in a more
informal manner. If we omit certain subtleties we can take a real-world computer as our
computational model and think of an algorithm as a procedure written in some high-
level programming language. For a thorough study of complexity issues we recommend
Garey & Johnson (1979).
For reasons of exactness we distinguish in this section between â€œproblemsâ€ and â€œin-
stances of problemsâ€. A problem or problem class is a question defined on several
formal parameters, e.g., determine whether a graph contains a Hamiltonian cycle or
compute a shortest Hamiltonian tour in a weighted graph. If we associate concrete
values to these formal parameters we create a particular instance of the problem. A
particular graph defines an instance of the Hamiltonian cycle problem and a particular
weight function c : E n â†’ Q gives an instance of the traveling salesman problem for the
complete graph K n = (V n , E n ).
These two examples show in addition that we have to distinguish between two types
of problems. One type is the so-called decision problem which requires a â€œyesâ€ or
â€œnoâ€ answer and the other type is the optimization problem demanding to exhibit a
solution which optimizes some objective function. We first give the basic terminology
which has been defined for decision problems and then show how optimization problems
can also be handled within this concept.
The performance of an algorithm has to be measured in some way depending on the
â€œsizesâ€ of the problem instances to be solved. Therefore we associate with each instance
I of a certain problem class S a size or encoding length l(I) which is defined as
the number of bits required to represent the actual parameters in the usual binary
encoding scheme. If A is an algorithm for the solution of problem S then we define
its running time (for instance I) as the number of elementary operations (addition,
multiplication, etc.) which have to be executed on a computer to solve instance I. The
time complexity or running time of an algorithm A for a problem class S is then
defined as a function t A : N â†’ N giving for each natural number n the number t A (n)
of elementary operations that the algorithm has to execute at most to solve an instance
of size n.
Note, that we assume that arithmetic operations are executed in constant time, i.e.,
independent of the size of the numbers involved. This is not correct in general, but is
feasible in our context.
Of course, in the usual case we will not be able to derive an explicit formula for evaluating
t A (n). On the other hand, we are not interested in concrete values of t A but rather in8
Chapter 2. Basic Concepts
the rate of growth of t A with increasing n. If it is not possible to give the exact rate
of growth we are interested in lower and upper bounds for this rate. In the case of a
problem we are also interested in bounds for the running time of algorithms that are
able to solve the problem. We introduce some notations to express knowledge about
the rate of growth or bounds on this rate.
Definition 2.1 Let f : N â†’ N and g : N â†’ N be given.
(i) We say that f is O(g) if there exist positive constants c and n 0 such that 0 â‰¤
f (n) â‰¤ c Â· g(n) for all n â‰¥ n 0 .
(ii) We say that f is Î©(g) if there exist positive constants c and n 0 such that 0 â‰¤
c Â· g(n) â‰¤ f (n) for all n â‰¥ n 0 .
(iii) We say that f is Î˜(g) if there exist positive constants c 1 , c 2 , and n 0 such that
0 â‰¤ c 1 Â· g(n) â‰¤ f (n) â‰¤ c 2 Â· g(n) for all n â‰¥ n 0 .
The three notations define asymptotic upper, lower, and tight bounds, respectively, on
the rate of growth of f . An alternate definition of an asymptotic lower bound is obtained
by replacing â€œfor all n â‰¥ n 0 â€ in 2.1 (iii) by â€œfor infinitely many nâ€. Asymptotic upper
bounds are of practical interest since they give a worst case running time of an
algorithm. It is usually harder to derive nontrivial asymptotic lower bounds, but we
will occasionally be able to give such bounds.
We also use the Î©- and Î˜-notation for problems. With the first notation we indicate
lower bounds on the running time of any algorithm that solves the problem, with the
second notation we indicate that an algorithm with best possible time complexity exists
to solve the problem.
An algorithm A is said to have polynomial time complexity if there exists a polyno-
mial p such that t A (n) = O(p(n)). All other algorithms are said to be of exponential
time complexity (although there are superpolynomial functions not being exponen-
tial). Edmonds (1965) was the first to emphasize the difference between polynomial and
nonpolynomial algorithms. It is now commonly accepted that only algorithms having a
polynomial worst case time complexity should be termed efficient algorithms.
We denote by P the class of decision problems which can be solved by polynomial time
algorithms.
The Eulerian tour problem can easily be solved in polynomial time. Using a result from
graph theory the following algorithm tests whether a connected graph G = (V, E) is
Eulerian.
procedure eulerian(G)
(1) For every v âˆˆ V compute its degree |Î´(v)|.
(2) If all node degrees are even then the graph is Eulerian. If the degree of at least
one node is odd then the graph is not Eulerian.
end of eulerian2.2. Complexity Theory
9
This algorithm runs in time Î˜(n + m). Surprisingly, also the Chinese postman problem
can be solved in polynomial time (Edmonds & Johnson (1973)).
However, many problems (in fact, most of the interesting problems in combinatorial
optimization) can up to date not be solved (and probably are not solvable) by polynomial
time algorithms. From a theoretical viewpoint they could be solved in the following way.
If the answer to an instance I (of a decision problem) is â€œyesâ€, then in a first step some
string s whose length is polynomial in the input size is guessed nondeterministically.
In a second step it is verified that s proves that the problem has a â€œyesâ€ answer. The
verification step is performed (deterministically) in time polynomial both in the length
of s and in the size of I. If the answer to I is â€œnoâ€ then there exists no such string
and the algorithm is assumed to run forever. E.g., in the Hamiltonian cycle problem
the string s could be the encoding of a Hamiltonian cycle (if the graph contains such
a cycle); the length of s is polynomial in the input length, and it can be easily verified
whether s is indeed the encoding of a Hamiltonian cycle.
Obviously this procedure cannot be realized in practice. The formal model enabling
such computations is the so-called nondeterministic Turing machine. For our pur-
poses we can think of the instruction set of an ordinary computer enhanced by the
instruction â€œInvestigate the following two branches in parallelâ€. The time complexity of
a nondeterministic algorithm is the maximal number of elementary steps that is required
to solve a decision problem if it has a â€œyesâ€ answer.
The class of decision problems that can be solved in polynomial time using such non-
deterministic algorithms is called NP.
Note the important point that there is an asymmetry between â€œyesâ€ and â€œnoâ€ answers
here. The question of how to show that a decision problem has a â€œnoâ€ answer is not
considered in this concept.
An important subclass of NP consists of the NP-complete problems. These are the
hardest problems in NP in the sense that if one of them is shown to be in P then
P=NP. Let A be an algorithm for the solution of problem B. We say that a problem
C is polynomially reducible to problem B if it can be solved in polynomial time by
an algorithm that uses A as a subroutine provided that each subroutine call of A only
counts as one step. A problem is then called NP-complete if every problem in NP is
polynomially reducible to it.
The Hamiltonian cycle problem is one member of the broad collection of NP-complete
problems (for a derivation of this result see Johnson & Papadimitriou (1985)).
The question P=NP? is one of the most famous unsolved questions in complexity theory.
Since this question has now been attacked for two decades and since NP-complete
problems proved to be substantially hard in practice it is commonly accepted that
P = NP should be the probable answer to this question (if it can be decided at all).
We want to emphasize again that our representation is kept on an informal level, and it
is intended to give just an idea of the concepts of complexity theory. Especially we have
not considered space complexity which measures the amount of storage an algorithm
requires.
We now discuss how optimization problems like the traveling salesman problem can be
dealt with. With the TSP we associate the following decision problem which can be
analyzed using the above concepts.10
Chapter 2. Basic Concepts
Traveling Salesman Decision Problem
Given the complete graph K n with edge weights c uv and a number b decide if there
exists a Hamiltonian tour in K n with length at most b.
This decision problem is NP-complete (Johnson & Papadimitriou (1985)).
If the traveling salesman problem is in P then obviously also the corresponding decision
problem is in P. An optimization problem having the property that the existence of
a polynomial time algorithm for the solution of an associated decision problem implies
the polynomial solvability of an NP-complete problem, is said to be NP-hard.
On the other hand, assume there exists a polynomial time algorithm for the solution
of the TSP decision problem. If all edge weights are integral and the largest weight in
absolute value of an edge is c then clearly the optimal solution of the traveling salesman
problem is not smaller than âˆ’c Â· n and not larger than c Â· n. Using the algorithm to solve
the decision problem we can find the shortest tour length using the following approach.
procedure tsplength(G)
(1) Set L = âˆ’c Â· n and U = c Â· n.
(2) As long as L < U perform the following steps.
(2.1) Set b =  L+U
2 .
(2.2) If there exists a Hamiltonian tour of length at most b then set U = b, otherwise
set L = b + 1.
end of tsplength
Applying this binary search technique we can find the length of the shortest tour by
at most log(c Â· n) + 1 calls of the solution algorithm for the TSP decision problem.
(Throughout this text we will use log to denote the logarithm with base 2).
To completely solve the optimization problem we have to exhibit an optimal solution.
This is now easily done once the shortest length is known.
procedure tsptour(G)
(1) Let U be the optimal tour length found by algorithm tsplength.
(2) For all u = 1, 2, . . . , n and all v = 1, 2, . . . , n perform the following steps.
(2.1) Set s uv = c uv and c uv = c Â· n + 1.
(2.2) If there does not exist a Hamiltonian tour of length U in the modified graph
then restore c uv = s uv .
end of tsptour2.2. Complexity Theory
11
After execution of this procedure the edges whose weights have not been altered give
the edges of an optimal tour.
The procedures tsplength and tsptour call a polynomial number (in n and log c) of times
the algorithm for the solution of the traveling salesman decision problem. Optimization
problems with the property that they can be polynomially reduced to a decision problem
in NP are called NP-easy. Problems which are both NP-easy and NP-hard (like the
traveling salesman problem) are called NP-equivalent. If P  = NP then no NP-hard
problem can be solved in polynomial time, if P=NP then every NP-easy problem is in
P.
So far we have considered the general traveling salesman problem. One might hope
that there are special cases where the problem can be solved in polynomial time. Un-
fortunately, such cases rarely have practical importance (Burkard (1990), van Dal
(1992), van der Veen (1992), Warren (1993)). For most practical situations, namely
for symmetric distances with triangle inequality, for Euclidean instances, for bipartite
planar graphs, or even for grid graphs, the traveling salesman problem remains NP-hard.
A different important issue is the question of whether algorithms can be designed which
deliver solutions with requested or at least guaranteed quality in polynomial time (poly-
nomial in the problem size and in the desired accuracy). Whereas for other NP-hard
problems such possibilities do exist, there are only discouraging results for the general
TSP. For a particular problem instance let c opt denote the length of a shortest tour and
c H denote the length of a tour computed by heuristic H. There are two basic results
relating these two values.
Theorem 2.2 Unless P=NP there does not exist for any constant r â‰¥ 1 a polynomial
time heuristic H such that c H â‰¤ r Â· c opt for all problem instances.
A proof is given in Sahni & Gonzales (1976).
A fully polynomial approximation scheme for a minimization problem is a heuristic
H which computes for a given problem instance and any Îµ > 0 a feasible solution
satisfying c H â‰¤ (1 + Îµ) Â· c opt in time polynomial in the size of the instance and in Îµ âˆ’1 .
It is an easy exercise to prove that to require polynomiality also in the encoding length
of Îµ is equivalent to require a polynomial algorithm for the exact solution. It is very
unlikely that fully polynomial approximation schemes exist for the traveling salesman
problem since the following result holds.
Theorem 2.3 Unless P=NP there does not exist a fully polynomial approximation
scheme for the Euclidean traveling salesman problem.
A proof can be found in Johnson & Papadimitriou (1985). The result holds in
general for TSPs with triangle inequality.
Despite these theoretical results we can nevertheless design heuristics that determine
good or very good tours in practice. The theorems tell us that for every heuristic there
are however problem instances where it fails badly. There are a few approximation
results for problems with triangle inequality which will be addressed in Chapter 6.
It should be pointed out that the complexity of an algorithm derived by theoretical
analysis might be insufficient to predict its behaviour when applied to real-world in-
stances of a problem. This is mainly due to the fact that only worst case analysis is12
Chapter 2. Basic Concepts
performed which may be different from average behaviour, and, in addition, polynomial
algorithms can cause a large amount of CPU time if the polynomial is not of low degree.
In fact, for practical applications only algorithms having running time at most O(n 3 )
would be rated efficient, but even algorithms with running times as low as O(n 2 ) may
not be applicable in certain situations.
Another point is, that the proof of NP-hardness of a problem does not imply the nonex-
istence of a reasonable algorithm for the solution of problem instances arising in practice.
It is the aim of this study to show that in the case of the traveling salesman problem
algorithms can be designed which are capable of finding good approximate solutions to
even large sized real-world instances within moderate time limits.
2.3 Linear and Integer Programming
Linear and integer programming is not a central topic of this tract. However, at some
points we will make references to concepts and results of linear and integer programming.
We give a short survey on these. Highly recommendable references in this area are the
prize-winning books Schrijver (1986) and Nemhauser & Wolsey (1988).
Let A be an m Ã— n-matrix (constraint matrix), b be an m-vector (right hand side) and c
be an n-vector (objective function), where all entries of A, b, and c are rational numbers.
Given these data the linear programming problem is defined as follows.
Linear Programming Problem
Find a vector x âˆ— maximizing the objective function c T x over the set {x âˆˆ Q | Ax â‰¤ b}.
A linear program may be given in various forms which can all be transformed to the
above. For example we may have equality constraints, nonnegativity conditions for
some variables, or the objective function is to be minimized.
In its general form a linear programming problem is given as
max c T x + d T y
Ax + By â‰¤ a
Cx + Dy = b
x â‰¥ 0
with appropriately dimensioned matrices and vectors.
A fundamental concept of linear programming is duality. The dual linear program
to the program given above (which is then called the primal linear program) is
defined as
min u T a + v T b
u T A + v T C â‰¥ c T
u T B + v T D = d T
u â‰¥ 0.
It is easily verified that the dual of the dual problem is again the primal problem.
One important aspect of the duality concept is stated in the following theorem.2.3. Linear and Integer Programming
13
Theorem 2.4 Let P and D be a pair of dual linear programs as defined above.
Suppose there exist vectors (x âˆ— , y âˆ— ) and (u âˆ— , v âˆ— ) satisfying the constraints of P , resp.,
D. Then we have
(i) The objective function value of (x âˆ— , y âˆ— ) (in problem P ) is less than or equal to
the objective function value of (u âˆ— , v âˆ— ) (in problem D).
(ii) Both problems have optimal solutions and their objective function values are
equal.
Duality exhibits further relations between primal and dual problem. But since they
are not important in the sequel we omit them here. Note in particular, that the dual
problem can be used to give bounds for the optimal value of the primal problem (and
vice versa).
The first algorithm for solving linear programming problems was the Simplex method
invented by Dantzig (1963). Since then implementations of this method have been
considerably improved. Today, even very large sized linear programming problems with
several ten thousands of variables and constraints can be solved routinely in moderate
CPU time (Bixby (1994)). The running time of the Simplex method cannot be bounded
by a polynomial, in fact there are examples where exponential running time is necessary
to solve a problem.
However, the linear programming problem, i.e., the problem of maximizing a linear ob-
jective function subject to linear constraints is in P. This was proved in the famous
papers Khachian (1979) (using the Ellipsoid method) and Karmarkar (1984) (us-
ing an Interior-point method). Though both of these algorithms are polynomial, only
interior point methods are competitive with the Simplex method (Lustig, Marsten
& Shanno (1994)).
These facts illustrate again that complexity analysis is in the first place only a theoretical
tool to assess hardness of problems and running time of algorithms.
A step beyond polynomial solvability is taken if we require feasible solutions to have
integral entries. The integer linear programming problem is defined as follows.
Integer Linear Programming Problem
Let A, b, and c be appropriately dimensioned with rational entries. Find a vector x âˆ—
maximizing the objective function c T x over the set {x âˆˆ Q | Ax â‰¤ b, x integer}.
This problem is NP-complete and no duality results are available. We show that the
traveling salesman problem can be formulated as an integer linear program.
To be able to apply methods of linear algebra to graph theory we associate vectors to
edge sets in the following way. Let G = (V, E) be a graph. If |E| = m then we denote
by Q E the m-dimensional rational vector space where the components of the vectors
x âˆˆ Q E are indexed by the edges uv âˆˆ E. We denote a component by x uv or x e if
e = uv.
The incidence vector x F âˆˆ Q E of an edge set F âŠ† E is defined by setting x F
uv = 1 if
F
uv âˆˆ F and by setting x uv = 0 otherwise. Similarly, if we associate a variable x uv to
each edge uv we denote by x(F ) the formal sum of the variables belonging to the edges
of F .14
Chapter 2. Basic Concepts
Now consider the TSP for the complete graph K n with edge weights c uv . With the
interpretation that x uv = 1 if edge uv is contained in a tour and x uv = 0 otherwise, the
following is a formulation of the TSP as an integer linear program.

min
c uv x uv
uvâˆˆE
x(Î´(v)) = 2,
x(C) â‰¤ |C| âˆ’ 1,
x uv âˆˆ {0, 1},
for all u âˆˆ V,
for all cycles C âŠ† E n , |C| < n,
for all u, v âˆˆ V.
The TSP can be successfully attacked within the framework of linear and integer pro-
gramming for surprisingly large problem sizes. We will comment on this issue in Chap-
ter 12.
2.4 Data Structures
In this section we discuss some data structures that are useful for implementing traveling
salesman problem algorithms. They are all used in the software package TSPX (Reinelt
(1991b)) with which all experiments in this monograph were conducted. The exposition
is based on the books Tarjan (1983) and Cormen, Leiserson & Rivest (1989) on
algorithms and data structures. A further reference on the foundations of algorithms is
Knuth (1973).
2.4.1 Binary Search Trees
A rooted tree is a connected acyclic graph with one distinguished node, the so-called
root or root node of the tree. Therefore, if the graph has n nodes a tree consists of
n âˆ’ 1 edges. The depth of the tree is the length (number of edges) of the longest path
from the root to any other node. Every node v is connected to the root by a unique
path. The length of this path is said to be the depth or the level of v. We say that
a tree is binary if at most two edges are incident to the root node and at most three
edges are incident to every other node.
If node u is the first node encountered when traversing the unique path from v to the
root then u is called father of v, denoted by f [v] in the following. By definition f [r] = 0
if r is the root node. If v is a node in a binary tree then there are at most two nodes
with v as their father. These nodes are called sons of v, and we define one of them to
be the right son and the other one to be the left son of v. Either of them may be
missing. A node without sons is called leaf of the tree. To represent a binary tree we
store for every node v its father f [v], its right son r[v] and its left son l[v]. If one of
them is missing we assign 0 to the respective entry.
Figure 2.1 shows a binary tree with root 5 and leaves 2, 7, 3, and 9.15
2.4. Data Structures
5
9
8
10
4
2
1
6
7
3
Figure 2.1 A binary tree on 10 nodes
By assigning to each node v a number k[v], the key of the node, we can store information
in such a tree.
Let a 1 , a 2 , . . . , a n be a sequence of keys assigned to a set of n nodes.
Definition 2.5 A binary tree on these nodes is called search tree if it satisfies the
following conditions.
(i) If u = r[v] then k[u] â‰¥ k[v].
(ii) If w = l[v] then k[w] â‰¤ k[v].
If the following procedure is called with the root of the search tree as parameter then
it prints the stored numbers in increasing order.
procedure inorder(v)
(1) If v = 0 then return.
(2) Call inorder(l[v]).
(3) Print k[v].
(4) Call inorder(r[v]).
end of inorder
Algorithms to find the smallest key stored in a binary search tree or to check if some
key is present are obvious.
The depth of an arbitrary binary search tree can be as large as n âˆ’ 1 in which case the
tree is a path. Hence checking if a key is present in a binary search tree can take time
O(n) in the worst case. This is also the worst case time for inserting a new key into the
tree.
On the other hand, if we build the tree in a clever way, we could realize it with depth
log n. In such a tree searching can be performed much faster.
This observation leads us to the concept of balanced binary search trees which allow
searching in worst case time O(log n). There are many possibilities for implementing
balanced trees. We describe the so-called red-black trees.16
Chapter 2. Basic Concepts
For the proper definition we have to change our notion of binary search trees. Now we
distinguish between internal nodes (having a key associated with them) and external
nodes (leafs of the tree). Every internal node is required to have two sons, no internal
node is a leaf of the tree.
Definition 2.6 A binary tree is called red-black tree if it satisfies the following
conditions.
(i) Every node is either red or black.
(ii) Every external node (leaf) is black.
(iii) If a node is red then its sons are black.
(iv) Every path from a node down to a leaf contains the same number of black nodes.
It can be shown that a red-black tree with n non-leaf nodes has depth at most 2 log(n+1).
Therefore searching in a red-black tree takes time O(log n).
Insertion of a new key can also be performed in time O(log n) because of the small
depth of the tree. But we have to ensure that the red-black property still holds after
having inserted a key. To do this we have to perform some additional fixing operations.
Basically, we color the newly inserted node red and then reinstall the red-black condition
on the path from the new node to the root. At every node on this path we spend constant
time to check correctness and to fix node colors and keys if necessary. So the overall time
spent for inserting a new node and reestablishing the red-black condition is O(log n).
2.4.2 Disjoint Sets Representation
Very frequently we need to manage a partition of some ground set V = {1, 2, . . . , n} into
disjoint subsets S 1 , S 2 , . . . , S k , i.e., sets satisfying âˆª ki=1 S i = V and S i âˆ© S j = âˆ… for all
i  = j. Operations to be performed are merging two subsets and identifying the subset
containing a given element.
The data structure to store such a partition consists of a collection of trees each repre-
senting one subset. The nodes of each tree correspond to those elements of the ground
set which belong to the respective subset, and the root node of each tree is said to be
the representative of the respective set. For simplification of algorithms we define
here the father of the root to be the root itself.
Suppose we have an initialized data structure representing some partition. Identification
of the subset, i.e., the representative of the subset, to which an element v âˆˆ V belongs
is achieved by the following function.
function find(v)
(1) While f [v]  = v set v = f [v].
(2) Return v.
end of find
Joining two subsets S and T where we are given elements x âˆˆ S and y âˆˆ T is accom-
plished by the following procedure.2.4. Data Structures
17
procedure union(x, y)
(1) Set u = find(x).
(2) Set v = find(y).
(3) Set f [u] = v.
end of union
The representative of the new set is v.
We will exclusively use the disjoint set representation in the following context. The
ground set V is the node set of some graph with edge set E. Initially, the ground set
is partitioned into n sets containing one element each. We then scan the edges of E in
some order depending on the application. If the endnodes of the current edge satisfy
some condition then the sets containing these endnodes are merged. Usually n âˆ’ 1
merge operations are performed so that the final partition consists just of the set V .
The number of find operations is bounded by 2|E|. An application of this principle is
used for implementing Kruskalâ€™s spanning tree algorithm to be discussed in section 2.5.
Without further modifications the above implementation can result in trees which are
paths. This is the worst case for performing find operations. Ideally, we would like to
have trees of depth 1 for set representation. But to achieve this we have to traverse one
of the trees participating in a union operation. On the other hand, this can result in a
running time of O(m 2 ) for m union operations if the wrong trees are chosen.
Fortunately, there are two improvements to overcome these problems. We implement
the find operations with additional path compression and union operations as union
by rank. In addition we now store for each node r the number of nodes n[r] in the tree
rooted at r. The modified procedures are now.
function find and compress(v)
(1) If v  = f [v] then set f [v] = find and compress(f [v]).
(2) Return f [v].
end of find and compress
After execution of this procedure for a node v all nodes on the path from v to the root
(including v) in the previous tree now have the root node as their father.
procedure union by rank(x, y)
(1) Set u = find and compress(x).
(2) Set v = find and compress(y).
(3) If n[u] < n[v] then set f [u] = v and n[v] = n[v] + n[u]. Otherwise set f [v] = u
and n[u] = n[u] + n[v].
end of union by rank18
Chapter 2. Basic Concepts
This procedure makes the root of the larger tree the father of the root of the smaller
tree after the union operation. Looking more closely at this principle one realizes that
it is not necessary to know the exact number of nodes in the trees. It suffices to store
a rank at the root node which is incremented by one if trees of equal rank are merged.
This way one can avoid additions of integers.
Initialization of a single element set is simply done by the following code.
procedure make set(v)
(1) Set f [v] = v and n[v] = 1.
end of make set
The modified implementation of union/find turns out to perform very efficiently for our
purposes.
Theorem 2.7 If m operations are performed using disjoint sets representation by
trees where n of them are make set operations and the other ones are union operations
(by rank) for disjoint sets and find operations (with path compression) then this can be
performed in time O(m log âˆ— n).
A proof of this instructive theorem can be found in Tarjan (1983) or Cormen, Leis-
erson & Rivest (1989). The number log âˆ— n is defined via log (i) n as follows.

n
if i = 0,
(i)
(iâˆ’1)
n) if i > 0 and log (iâˆ’1) n > 0,
log n = log(log
undefined
otherwise,
and then
log âˆ— n = min{i â‰¥ 0 | log (i) n â‰¤ 1}.
In fact, in the above theorem a slightly better bound of O(mÎ±(m, n)) where Î± denotes
the inverse of the Ackermann function can be proved. But this is of no importance for
practical computations since already log âˆ— n â‰¤ 5 for n â‰¤ 2 65536 . So we can speak of
linear running time of the fast union-find algorithm in practice (for our applications).
2.4.3 Heaps and Priority Queues
A heap is a data structure to store special binary trees satisfying an additional heap
condition. These binary trees have the property that except for the deepest level all
levels contain the maximal possible number of nodes. The deepest level is filled from
â€œleft to rightâ€ if we imagine the tree drawn in the plane. To every tree node there is an
associated key, a real number. These keys are stored in a linear array A according to a
special numbering that is assumed for the tree nodes. The root receives number 1 and if
a node has number i then its left son has number 2i and its right son has number 2i + 1.
If a node has number i then its key is stored in A[i]. Therefore, if such a binary tree
has k nodes then the corresponding keys are stored in A[1] through A[k]. The special
property that array A has to have is the following.2.4. Data Structures
19
Definition 2.8 An array A of length n satisfies the heap property if for all 1 < i â‰¤ n
we have A[ 2 i ] â‰¤ A[i].
Stated in terms of binary trees this condition means that the key of the father of a node
is not larger than the key of the node. The heap property implies that the root has the
smallest key among all tree nodes, or equivalently, A[1] is the smallest element of the
array.
Alternatively, we can define the heap property as â€œA[ 2 i ] â‰¥ A[i]â€. This does not make
an essential difference for the following, since only the relative order of the keys is
reversed.
As a first basic operation we have to be able to turn an array filled with arbitrary keys
into a heap. To do this we have to use the subroutine heapify with argument i which
fixes the heap property for the subtree rooted at node i (where it is assumed that the
two subtrees rooted at the left, resp. right, son of i are already heaps).
procedure heapify(i)
(1) Let n be the number of elements in heap A (nodes in the binary tree). If 2i > n
or 2i + 1 > n the array entries A[2i], resp. A[2i + 1] are assumed to be +âˆ.
(2) Let k be the index of i, 2i, and 2i + 1 whose array entry is the smallest.
(3) If k  = i then exchange A[i] and A[k] and perform heapify(k).
end of heapify
It is easy to see that heapify(i) takes time O(h) if h is the length of the longest path
from node i down to a leaf in the search tree. Therefore heapify(1) takes time O(log n).
Suppose we are given an array A of length n to be turned into a heap. Since all leaves
represent 1-element heaps the following procedure does the job.
procedure build heap(A)
(1) For i =  n 2  downto 1 perform heapify(i).
end of build heap
A careful analysis of this procedure shows that an arbitrary array of length n can be
turned into a heap in time O(n). Note that the binary tree represented by the heap is
not necessarily a search tree.
Except for sorting (see section 2.5) we use the heap data structure for implementing
priority queues. As the name suggests such a structure is a queue of elements such
that elements can be accessed one after the other according to their priority. The
first element of such a queue will always be the element with highest priority. For the
following we assume that an element has higher priority than another element if its key
is smaller.
The top element of a heap is therefore the element of highest priority. The most fre-
quently applied operation on a priority queue is to extract the top-priority element and
assure that afterwards the element of the remaining ones with highest priority is in the
first position. This operation is implemented using the heap data structure. We assume
that the current size of the heap is n.20
Chapter 2. Basic Concepts
function extract top(A)
(1) Set t = A[1].
(2) Set A[1] = A[n] and decrease the heap size by 1.
(3) Call heapify(1).
(4) Return t.
end of extract top
Because of the call of heapify(1), extracting the top element and fixing the heap property
needs time O(log n). Inserting a new element is accomplished as follows.
procedure insert key(k)
(1) Increase the heap size by 1 and set i to the new size.
(2) While i > 1 and A[ 2 i ] > k
(2.1) Set A[i] = A[ 2 i ] and i =  2 i .
(3) Set A[i] = k.
end of insert key
Since in the worst case we have to scan the path from the new leaf to the root, a call of
insert key takes time O(log n).
2.4.4 Graph Data Structures
Very frequently we have to store undirected graphs G = (V, E) with |V | = n nodes and
|E| = m edges where n is large, say in the range of 1,000 to 100,000. The number of
edges to be stored depends on the application.
Matrix type data structures are the (node-edge) incidence matrix and the (node-node)
adjacency matrix. The incidence matrix A is an n Ã— m-matrix whose entries a ie are
defined by

1 if i is an endnode of edge e,
a ie =
0 otherwise.
The adjacency matrix is an n Ã— n-matrix B whose entries b ij are defined by

1 if ij is an edge of G,
b ij =
0 otherwise.
Since we need O(nm) or O(n 2 ) storage for these matrices they cannot be used for large
graphs. This also limits the use of distance matrices for the definition of edge weights
in large problem instances. Fortunately, for large TSPs, distances between nodes are
usually given by a distance function.
If we just want to store a graph we can use an edge list consisting of two arrays tail
and head such that tail[e] and head[e] give the two endnodes of the edge e. This is21
2.4. Data Structures
appropriate if some graph is generated edge by edge and no specific operation has to be
performed on it.
Another possibility is to use a system of adjacency lists. Here we store for each node
a list of its adjacent nodes. This is done by an array adj of length 2m containing the
adjacent nodes and an array ap of length n. These arrays are initialized such that the
neighbors of node i are given in adj[ap[i]], adj[ap[i] + 1], through adj[ap[i + 1] âˆ’ 1]. This
data structure is suitable if we have to scan neighbors of a node and if the graph remains
unchanged. Adding edges is time consuming since large parts of the array may have to
be moved.
If we have to add edges and want to avoid moving parts of arrays, then we have to
use linked lists. Since this is our most frequent operation on graphs we have used the
following data structure to store an undirected graph. The two arrays tail and head
contain the endnodes of the edges. The arrays nxtt and nxth are initialized such that
nxtt[e] gives the number of a further edge in this structure having tail[e] as one endnode
and nxth[e] gives the number of a further edge having head[e] as an endnode. An entry 0
terminates a linked list of edges for a node. For each node v the array entry first[v]
gives the number of the first edge having v as an endnode.
To get used to this form of storing a graph we give an example here. Suppose we
have a subgraph of the complete graph on six nodes consisting of the edges {1, 2},
{1, 5}, {2, 5}, {2, 3}, {3, 5}, {4, 5} and {4, 6}. This graph could be stored e.g., by the
assignment shown in Table 2.2.
Index first Index head tail nxth nxtt
1
2
3
4
5
6 6
5
5
7
7
4 1
2
3
4
5
6
7 1
3
2
4
2
1
4 2
5
5
6
3
5
5 0
0
1
0
3
1
4 0
0
2
0
2
3
6
Table 2.2 Example for subgraph data structure
Suppose the current graph has m edges and is stored using this data structure. Adding
a new edge can then be performed in constant time using the following piece of code.
procedure add edge(i, j)
(1) Set tail[m + 1]=i, nxtt[m + 1]=first[i], and first[i] = m + 1.
(2) Set head[m + 1]=j, nxth[m + 1]=first[j], and first[j] = m + 1.
(3) Set m = m + 1.
end of add edge
Of course, if we add more edges than dynamic memory space has been allocated for we
have to allocate additional space.22
Chapter 2. Basic Concepts
2.4.5 Representing Tours
An easy way to store a tour is to use an array t of length n and let t[k] be the k-th node
visited in the tour. However, this is not sufficient as we shall see below. We also have
to impose a direction on the tour. We therefore store with each node i its predecessor
pred[i] and its successor succ[i] in the tour with respect to the chosen orientation.
When using heuristics to find short tours one has to perform a sequence of local modi-
fications of the current tour to improve its length. We explain our method to perform
modifications in an efficient way using the example of 2-opt moves. A 2-opt move
consists of removing two edges from the current tour and reconnecting the resulting
paths in the best possible way. This operation is depicted in Figure 2.3 where broken
arcs correspond to directed paths.
j l j l
k i k i
Figure 2.3 A 2-opt move
Note that we have to have an imposed direction of the tour to be able to decide which
pair of new edges can be added to form a new tour. Adding edges jk and il would result
in an invalid subgraph consisting of two subtours. Furthermore, the direction on one of
the paths has to be reversed which takes time O(n).
Since we have to make a sequence of such 2-opt moves we do not update the tour
structure as in Figure 2.3 but rather store the current tour as a sequence of unchanged
intervals of the starting tour. This is of particular importance for some heuristics where
2-opt moves are only tentative and might not be realized to form the next tour. For
each interval we store the direction in which it is traversed in the current tour. The
result of the 2-opt move in our example would be a doubly linked sequence of intervals
as in Figure 2.4 where we also indicate that the path represented by the interval [k, j]
has been reversed.
[ l , i ]
[ k , j ]
Figure 2.4 Result of the 2-opt move
To make the approach clearer we use concrete node numbers l = 4, i = 7, k = 3, j = 10
and perform another 2-opt move involving nodes 11 and 5 on the path from 4 to 7 and
nodes 8 and 16 on the path from 3 to 10 as in Figure 2.5.23
2.4. Data Structures
10
4
10
4
16 11 16 11
8 5 8 5
3
3
7
7
Figure 2.5 A second 2-opt move
After execution of this move we have the interval sequence shown in Figure 2.6. Note
that the segment between nodes 3 and 8 was reversed before the 2-opt move, so it is
not reversed any more after the move.
[ 4 , 11 ]
[ 8 , 3 ]
[ 7 , 5 ]
[ 16 , 10 ]
Figure 2.6 Result of the second 2-opt move
The new interval sequence was obtained by splitting two intervals and reconnecting the
intervals in an appropriate way. Note that also in the interval representation of a tour
we have to reorient paths of the sequence. But, if we limit the number of moves in such
a sequence by k this can be performed in O(k).
One difficulty has been omitted so far. If we want to perform a 2-opt move involving
four nodes we have to know the intervals in which they are contained to be able to
choose the correct links. We cannot do this efficiently without additional information.
We store with each node its rank in the starting tour. This rank is defined as follows:
An arbitrary node gets rank 1, its successor gets rank 2, etc., until the predecessor of the
rank 1 node receives rank n. Since we know for each interval the ranks of its endnodes
and whether it has been reversed with respect to the starting tour or not, we can check
which interval contains a given node if we store the intervals in a balanced binary search
tree.
Applying this technique the interval containing a given node can be identified in time
O(log k) if we have k intervals.
This way of maintaining tour modifications is extensively used in the implementation
of a Lin-Kernighan type improvement method (see Chapter 7). Experience with this
data structure was also reported in Applegate, ChvÃ¡tal & Cook (1990). They used
splay trees (Tarjan (1983)) instead of red-black trees in their implementation.
Of course, the number of intervals should not become too large because the savings in
execution time decreases with the number of intervals.
Finally, or if we have too many intervals, we have to clear the interval structure and
generate correct successor and predecessor pointers to represent the current tour. This24
Chapter 2. Basic Concepts
is done in the obvious way. Note that the longest path represented as an interval can
remain unchanged, i.e., for its interior nodes successors, predecessors, and ranks do not
have to be altered.
[11] [12] [1] [2]
1 3 5 8
[10] 6 10 [3]
[9] 9 11 [4]
1 (12)
[ 5 , 3 ]
2 7 12 4
[8] [7] [6] [5]
1 3 5 8
6 10
9 11
2 7 12 4
1 3 5 8
9 11
12
4
[ 3 , 12 ]
3 (3)
10
7
6 (7)
[ 5 , 4 ]
6
2
1 (5)
1 (2)
6 (2)
8 (5)
[ 5 , 8 ]
[ 2 , 3 ]
[ 4 , 10 ]
[ 7 , 12 ]
Figure 2.7 An example for representing a tour
To visualize this approach we give a sequence of 2-opt moves starting from a basic tour
together with the resulting interval sequences and search trees in Figure 2.7. Ranks of
nodes are listed in brackets. Each node of the search tree represents an interval. We give
for each node the rank of the endnode with lower rank and in parentheses the number
of nodes in the interval.2.5. Some Fundamental Algorithms
25
2.5 Some Fundamental Algorithms
In this chapter we review some basic algorithms which are not traveling salesman prob-
lem specific but are used as building blocks in many heuristics. For an extensive dis-
cussion we refer again to Cormen, Leiserson & Rivest (1989).
2.5.1 Sorting
Given a set A = {a 1 , a 2 , . . . , a n } of n integer or rational numbers, the sorting problem
consists of finding the sequence of these numbers in increasing (or decreasing) order. In
many cases the numbers will correspond to the weights of the edges of a subgraph.
There is a variety of algorithms we cannot discuss here. One example of a very sim-
ple sorting algorithm shall be given first. This algorithm recursively subdivides a set
into two halves, sorts the subsets and then merges the sorted sequences. Suppose the
numbers are stored in B[1], B[2], through B[n].
procedure mergesort(B, l, u)
(1) If l â‰¥ u âˆ’ 1 sort B[l] through B[u] by comparisons and return.
).
(2) Perform mergesort(B, l,  l+u
2
 + 1, u).
(3) Perform mergesort(B,  l+u
2
(4) Rearrange B to represent the sorted sequence.
end of mergesort
The call mergesort(B, 1, n) sorts the array in time O(n log n). This will follow from
considerations in section 2.5.3 since Step (1) is executed in constant time and Step (4)
can be performed in time O(u âˆ’ l).
Another sorting algorithm makes use of the heap data structure presented in the pre-
vious chapter. Since in a heap the top element is always the smallest one we can
successively generate the sorted sequence of the elements of A using the heap.
procedure heapsort(B)
(1) Call build heap(B).
(2) For i = n downto 1 perform the following steps.
(2.1) Exchange B[1] and B[i].
(2.2) Decrement the heap size by 1.
(2.3) Call heapify(1).
end of heapsort26
Chapter 2. Basic Concepts
After execution of heapsort(A) we have the elements of A sorted in increasing order in
A[n], A[n âˆ’ 1], through A[1].
The running time is easily derived from the discussion in the section on heaps. Step (1)
takes time O(n),  and, since Step (2.3) takes time O(log i), we obtain the overall running
n
time as O(n) + i=1 O(log i) = O(n log n).
Therefore both merge sort and heap sort seem to be more or less equivalent. However,
heap sort has an important advantage. It is able to generate the sorted sequence as
long as needed. If for some reason the remaining sequence is not of interest any more
at some point we can exit from heapsort prematurely.
A final remark is in order. It can be shown that sorting based on the comparison of two
elements cannot be performed faster than in O(n log n) time. So the above discussion
shows that the sorting problem has time complexity Î˜(n log n) (in this computational
model).
Faster sorting algorithms can only be achieved if some assumptions on the input can
be made, e.g., that all numbers are integers between 1 and n. Expected linear running
time of some sorting algorithms can be shown for specific input distributions.
2.5.2 Median Finding
We could also have implemented a sorting algorithm by recursively doing the following
for a set A = {a 1 , a 2 , . . . , a n }. First identify a median of A, i.e., a value a such that half
of the elements of A are below a and half of the elements are above a. More precisely,
we identify a number a such that we can partition A into two sets A 1 and A 2 satisfying
A 1 âŠ† {a i | a i â‰¤ a}, A 2 âŠ† {a i | a i â‰¥ a}, |A 1 | =  n 2 , and |A 2 | =  n 2 . We then sort A 1
and A 2 separately. The concatenation of the respective sorted sequences give a sorting
of A.
In particular for geometric problems defined on points in the plane we often need to
compute horizontal or vertical lines separating the point set into two (approximately
equally sized) parts. For this we need medians with respect to the x- or y-coordinates
of the points.
A natural way to find a median is to sort the n points. The element at position  n 2 
gives a median. However, one can do better as is shown in the following sophisticated
algorithm which is also very instructive. The algorithm requires a subroutine that for a
given input b rearranges an array such that in the first part all elements are at most as
large as b while in the second part all elements are at least as large as b. Assume again
that array B contains n numbers in B[1] through B[n].
function partition(B, b)
(1) Set i = 1 and j = n.
(2) Repeat the following steps until i â‰¥ j.
(2.1) Decrement j by 1 until B[j] â‰¤ b.
(2.2) Increment i by 1 until B[i] â‰¥ b.
(2.3) If i < j exchange B[i] and B[j].27
2.5. Some Fundamental Algorithms
(3) Return j.
end of partition
After execution of this algorithm we have B[l] â‰¤ b for all l = 1, 2, . . . , i and B[l] â‰¥ b for
all l = j, j + 1, . . . , n.
The procedure for finding a median can now be given. In fact, the procedure does a
little bit more. It finds the i-th smallest element of a set.
procedure find ith element(B, i)
(1) Partition B into  n 5  groups of 5 elements each and one last group containing the
remaining elements.
(2) Sort each set to find its â€œmiddleâ€ element. If the last set has even cardinality l
we take the element at position 2 l + 1.
(3) Apply find ith element to find the median b of the set of medians found in Step (2).
(4) Let k = partition(B, b).
(5) If i â‰¤ k then find the i-th smallest element of the lower side of the partition,
otherwise find the (i âˆ’ k)-th smallest element of the higher side of the partition.
end of find ith element
The call find ith element(B,  n+1
2 ) now determines a median of B.
A running time analysis shows that this algorithm runs in linear time which is best
possible since every element of B has to be considered in a median finding procedure.
Hence medians can be found in time Î˜(n).
2.5.3 Divide and Conquer
We have already applied the divide and conquer principle without having named it
explicitly. The basic idea is to divide a problem into two (or more) subproblems, solve
the subproblems, and then construct a solution of the original problem by combining
the solutions of the subproblems in an appropriate way.
Since this is a very important and powerful principle, we want to cite the main result for
deriving the running time of a divide and conquer algorithm of Cormen, Leiserson
& Rivest (1989). Such an algorithm has the following structure where we assume that
it is applied to some set S, |S| = n, to solve some (unspecified) problem.
procedure divide and conquer(S)
(1) Partition S into subproblems S 1 , S 2 , . . . , S a of size less than
n
.
b
(2) For each subproblem S i perform divide and conquer(S i ).
(3) Combine the subproblem solutions to solve the problem for S.
end of divide and conquer28
Chapter 2. Basic Concepts
In the following we assume that f is the running time function for performing Steps (1)
and (3).
Theorem 2.9 gives a formula to compute the running time of divide and conquer algo-
rithms. (It does not matter that nb is not necessarily integral, we can both substitute
 nb  or  nb ).
Theorem 2.9 Let a â‰¥ 1 and b > 1 be constants and let T (n) be defined by the
recurrence T (n) = aT ( nb )+f (n). Then T (n) can be bounded asymptotically (depending
on f ) as follows.
(i) If f (n) = Î˜(n log b aâˆ’Îµ ) for some constant Îµ > 0, then T (n) = Î˜(n log b a ).
(ii) If f (n) = O(n log b a ), then T (n) = Î˜(n log b a log n).
(iii) If f (n) = Î©(n log b a+Îµ ) for some constant Îµ > 0 and if a Â· f ( nb ) â‰¤ c Â· f (n) for some
constant c < 1 and all sufficiently large n, then T (n) = Î˜(f (n)).
This theorem provides a very useful tool. Take as an example the merge sort algorithm.
Step (1) is performed in constant time and Step (4) is performed in time O(u âˆ’ l), hence
f (n) = O(n). For the theorem we have a = 2 and b = 2. Therefore case (ii) applies and
we obtain a running time of Î˜(n log n) for merge sort.
2.5.4 Minimum Spanning Trees
Let G = (V, E), |V | = n, |E| = m, be a connected graph with edge weights c uv for all
uv âˆˆ E. A minimum spanning tree of G is an acyclic connected subset T âŠ† E such
that c(T ) is minimal among these edge sets. Clearly a spanning tree has n âˆ’ 1 edges.
The following algorithm computes a minimum length spanning tree of G (Prim (1957)).
procedure prim(G)
(1) Set T = âˆ… and Q = {1}.
(2) For all i = 2, 3, . . . , n set d[i] = c 1i and p[i] = 1 if {1, i} âˆˆ E, resp., d[i] = âˆ and
p[i] = 0 if {1, i} âˆˆ
/ E.
(3) As long as |T | < n âˆ’ 1 perform the following.
(3.1) Let d[j] = min{d[l] | l âˆˆ V \ Q}.
(3.2) Add edge {j, p[j]} to T and set Q = Q âˆª {j}.
(3.3) For all l âˆˆ V \ Q check if c jl < d[l]. If this is the case set d[l] = c jl and
p[l] = j.
(4) T is a minimum spanning tree of G.
end of prim2.5. Some Fundamental Algorithms
29
The running time of this algorithm is Î˜(n 2 ). It computes in each iteration a mini-
mum spanning tree for the subgraph G Q = (Q, E(Q)), and so upon termination of the
algorithm we have a minimum spanning tree of G.
The implementation of Primâ€™s algorithm given here is best possible if we have complete
or almost complete graphs G. If we want to compute minimum spanning trees for graphs
with fewer edges then other implementations are superior. The idea is to maintain the
nodes not yet connected to the tree in a binary heap (where the keys are the shortest
distances to the tree). Since keys are only decreased, every update in Step (3.3) can
be performed in time O(log n). Because we have to scan the adjacency list of j to see
which distances might be updated, Step (3.3) takes altogether time O(m log n). Finding
the minimum of a heap and updating it has to be performed n âˆ’ 1 times requiring
time O(log n) each time. Therefore we obtain running time O(m log n) for the heap
implementation of Primâ€™s algorithm.
Using more advanced techniques (as e.g., binomial heaps or Fibonacci heaps) one can
even achieve time O(m + n log n).
A different approach to finding a minimum spanning tree was given in Kruskal (1956).
It is particularly suited for sparse graphs.
procedure kruskal(G)
(1) Build a heap of the edges of G (with respect to smaller weights).
(2) Set T = âˆ….
(3) As long as |T | < n âˆ’ 1 perform the following.
(3.1) Get the top edge {u, v} from the heap and update the heap.
(3.2) If u and v belong to different connected components of the graph (V, T ) then
add edge {u, v} to T .
(4) T is a minimum spanning tree of G.
end of kruskal
The idea of this algorithm is to generate minimum weight acyclic edge sets with in-
creasing cardinality until such an edge set of cardinality n âˆ’ 1 is found (which is then a
minimum spanning tree of G).
This algorithm is implemented using fast union-find techniques. Combining results for
maintaining heaps and for applying fast union-find operations we obtain a running time
of O(m log m) for this algorithm.
Instead of using a heap we could sort in a first step all edges with respect to increasing
length and then use only union-find to identify different components. Using a heap we
can stop as soon as |T | = n âˆ’ 1 and do not necessarily have to sort all edges.
If G is not connected a slight modification of the above computes the minimum weight
acyclic subgraph of cardinality n âˆ’ k where k is the number of connected components
of G.30
Chapter 2. Basic Concepts
2.5.5 Greedy Algorithms
The greedy algorithm is a general algorithmic principle for finding feasible solutions of
combinatorial optimization problems. It is characterized by a myopic view, performing
the construction of a feasible solution step by step based only on local knowledge of the
problem.
To state it in general, we need a suitable definition of a combinatorial optimization
problem. Let E = {e 1 , e 2 , . . . , e m } be a finite set where each element has an associated
the set of so-called feasible solutions. For a set
weight c i , 1 â‰¤ i â‰¤ m, and I âŠ† 2 E be 
I âŠ† E, its weight is given as c(I) = e i âˆˆI c i . The optimization problem consists of
finding a feasible solution I âˆ— âˆˆ I such that c(I âˆ— ) = min{c(I) | I âˆˆ I}.
The greedy algorithm works as follows.
procedure greedy(I, c)
(1) Sort E such that c 1 â‰¤ c 2 â‰¤ . . . , c m .
(2) Set I = âˆ….
(3) For i = 1, 2, . . . , m:
(3.1) If I âˆª {e i } âˆˆ I, then set I = I âˆª {e i }.
end of greedy
Due to the sorting in Step (1), the algorithm needs time Î©(m log m). It does not need
more time, if the test â€œI âˆª {e i } âˆˆ I?â€ can be performed in time O(log m).
Inspite of its simplicity, the greedy principle has some important aspects, in particular
in the context of matroids and independence systems, but we do not elaborate on this.
In general, however, it delivers feasible solutions of only moderate quality. Note, that
Kruskalâ€™s algorithm for computing minimum spanning trees is nothing but the greedy
algorithm. Because the spanning tree problem is essentially an optimization problem
over a matroid, the greedy algorithm is guaranteed to find an optimal solution in this
case.Chapter 3
Related Problems and Applications
For several practical problems it is immediately seen that the TSP provides the suit-
able optimization model. In many cases, however, this is either not straightforward
or the pure TSP has to be augmented by further constraints. In this chapter we first
discuss some optimization problems that are related to the TSP. Some of them can
be transformed to a pure TSP in a reasonable way, others are at least related in the
sense that algorithms developed for the TSP can be adapted to their solution. Then
we survey some application areas where the TSP or its relatives can be used to treat
practical problems. Further aspects are found in Garfinkel (1985). Finally, we intro-
duce the collection of sample problem instances that we will use in the sequel for testing
algorithms.
3.1 Some Related Problems
Note that we can assume without loss of generality that a symmetric TSP is always a
minimization problem and that all distances c ij are positive. First, if we are looking
for the longest Hamiltonian cycle we can multiply all edge weights by âˆ’1 and solve
a minimization problem. Second, we can add a constant to all edge weights without
affecting the ranking of tours with respect to their lengths. Hence all edge weights can
be made positive. In the sequel we will not explicitly mention this fact. Without loss
of generality we can also assume that all edge lengths are integer numbers.
Observe, however, that approximation results may be no longer valid after having mod-
ified edge weights. For example, if a very large constant is added to each edge weight,
then every tour is near optimal.
3.1.1 Traveling Salesman Problems in General Graphs
There may be situations where we want to find shortest Hamiltonian tours in arbitrary
graphs G = (V, E), in particular in graphs which are not complete. If it is required that
each node is visited exactly once and that only edges of the given graph must be used
then we can do the following.
Add all missing edges giving them a sufficiently large

c
weight M (e.g., M =
ijâˆˆE ij ) and apply an algorithm for the symmetric TSP in
complete graphs. If this algorithm terminates with an optimal tour containing none of
the edges with weight M , then this tour is also optimal for the original problem. If an
edge with weight M is contained in the optimal tour then the original graph does not
G. Reinelt: The Traveling Salesman, LNCS 840, pp. 31-41, 1994.
ï›™ Springer-Verlag Berlin Heidelberg 199432
Chapter 3. Related Problems and Applications
contain a Hamiltonian cycle. Note however, that heuristics cannot guarantee to find a
tour in G even if one exists.
The second way to treat such problems is to allow that nodes may be visited more than
once and edges be traversed more than once. If the given graph is connected we can
always find a feasible roundtrip under this relaxation. This leads us to the so-called
graphical traveling salesman problem.
3.1.2 The Graphical Traveling Salesman Problem
For an arbitrary connected graph G with edge weights, the graphical traveling sales-
man problem (GTSP) consists of finding a closed walk in G for the salesman to visit
every city requiring the least possible total distance. The salesman may only use edges
of G, but is allowed to visit a city or to traverse an edge more than once. This is
sometimes a more practical definition of the TSP because we may have cases where the
underlying graph of connections does not even contain a Hamiltonian cycle and where
some direct transitions from a city i to a city j are not possible. In the formulation as
a GTSP we explicitly stick to the given graph.
To avoid degenerate situations we have to have nonnegative edge weights. Otherwise
we could use an edge as often as we like in both directions to achieve an arbitrarily
small length of the solution. We transform a GTSP to a symmetric TSP as follows.
Let K n = (V n , E n ) be the complete graph on n nodes. For every pair i, j of nodes we
compute the shortest path from i to j in the graph G. The length d ij of this path is
taken as the weight of edge ij in K n . Now the shortest Hamiltonian tour in K n can be
transformed to a shortest closed walk in G visiting all nodes.
Naddef & Rinaldi (1993) discuss relation between the TSP and the GTSP in detail.
3.1.3 The Shortest Hamiltonian Path Problem
We are given a graph G = (V, E) with edge weights c ij . Two special nodes, say v s and
v t , of V are also given. The task is to find a path from v s to v t visiting each node of V
exactly once with minimal length, i.e., to find the shortest Hamiltonian path in G from
v s to v t .
This problem can be solved as a standard TSP in two ways.
a) Choose M sufficiently large and assign weight âˆ’M to the edge from v s to v t .
Then compute the optimal traveling salesman tour in this graph. This tour has
to contain edge v s v t and thus solves the Hamiltonian path problem.
b) Add a new node 0 to V and edges from 0 to v s and to v t with weight 0. Each
Hamiltonian tour in this new graph corresponds to a Hamiltonian path from v s
to v t in the original graph with the same length.
If the terminating point of the Hamiltonian path is not fixed, then we can solve the
problem by introducing a new node 0 and adding edges from all nodes v âˆˆ V \ {v s } to 0
with zero length. Now we can solve the Hamiltonian path problem with starting point
v s and terminating point v t = 0 which solves the original problem.
If neither starting point nor terminating point are specified, then we just add node 0
and connect all other nodes to 0 with edges of length zero. In this new graph we solve
the standard TSP.3.1. Some Related Problems
33
3.1.4 Hamiltonian Path and Cycle Problems
Sometimes it has to be checked if a given graph G = (V, E) contains a Hamiltonian
cycle or path at all. This question can be answered by solving a symmetric TSP in the
complete graph K n (with n = |V |) where all edges of the original graph obtain weight
1 and all other edges obtain weight 2. Then G contains a Hamiltonian cycle if and only
if the shortest Hamiltonian cycle in K n has length n. If this shortest cycle has length
n + 1, then G is not Hamiltonian, but contains a Hamiltonian path.
3.1.5 The Asymmetric Traveling Salesman Problem
If the cost of traveling from city i to city j is not necessarily the same as of traveling
from city j to city i, then an asymmetric traveling salesman problem has to be solved.
Let D = (W, A), W = {1, 2, . . . , n}, A âŠ† W Ã— W , and d ij be the arc weight of (i, j) âˆˆ A.
We define a graph G = (V, E) by
V = W âˆª {n + 1, n + 2, . . . , 2n},
E = {(i, n + i) | i = 1, 2, . . . , n}
âˆª {(n + i, j) | (i, j) âˆˆ A}.
Edge weights are assigned as follows
c i,n+i = âˆ’ M for i = 1, 2, . . . , n,
c n+i,j = d ij for (i, j) âˆˆ A,

where M is a sufficiently large number, e.g., M = (i,j)âˆˆA d ij . It is easy to see that
for each directed Hamiltonian cycle in D with length d D there is a Hamiltonian cycle
in G with length c G = d D âˆ’ nM . In addition, since an optimal tour in G contains all
edges with weight âˆ’M , it induces a directed Hamiltonian cycle in D. Hence we can
solve asymmetric TSPs as symmetric TSPs.
3.1.6 The Multisalesmen Problem
We give the asymmetric version of this problem. Instead of just one salesman now there
are m salesmen available who are all located in a city n + 1 and have to visit cities
1, 2, . . . , n. The task is to select some (or all) of these salesmen and assign tours to
them such that in the collection of all these tours together each city is visited exactly
once. The activation of salesman j incurs a fixed cost w j . The cost of the tour of
salesman j is the sum of the intercity distances of his tour (starting at and returning
to city n + 1). The m-salesmen problem (m-TSP) now consists of selecting a subset
of the salesmen and assigning a tour to each of them such that each city is visited by
exactly one salesman and such that the total cost of visiting all cities this way is as
small as possible.
In Bellmore & Hong (1974) it was observed that this problem can be transformed
to an asymmetric TSP involving only one salesman. We give their construction.34
Chapter 3. Related Problems and Applications
Let D = (V, A) be the digraph where V = {1, 2, . . . , n, n + 1} and A âŠ† V Ã— V gives the
possible transitions between cities. Let d ij be the distance from city i to city j.
We construct a new digraph D = (V  , A  ) as follows.
V  = V âˆª {n + 2, n + 3, . . . , n + m},
A  = A
âˆª {(n + i, j) | 2 â‰¤ i â‰¤ m, (n + 1, j) âˆˆ A}
âˆª {(j, n + i) | 2 â‰¤ i â‰¤ m, (j, n + 1) âˆˆ A}
âˆª {(n + i, n + i âˆ’ 1) | 2 â‰¤ i â‰¤ m}.
The weights d  ij for the arcs (i, j) in A  are defined via
d  ij =
d  n+i,j
d  j,n+i
= d n+1,j +
= d j,n+1 +
d ij , for 1 â‰¤ i â‰¤ n, 1 â‰¤ j â‰¤ n, (i, j) âˆˆ A,
1
w ,
2 i
1
2 w i ,
for 1 â‰¤ i â‰¤ m, 1 â‰¤ j â‰¤ n, (n + 1, j) âˆˆ A,
for 1 â‰¤ i â‰¤ m, 1 â‰¤ j â‰¤ n, (j, n + 1) âˆˆ A,
d  n+i,n+iâˆ’1 = 12 w iâˆ’1 âˆ’ 12 w i , for 2 â‰¤ i â‰¤ m.
It is not difficult to verify that the shortest tour in D  relates to an optimal solution of
the corresponding m-salesmen problem for D.
Observe in addition, that with an easy modification we can require that every salesman
is to be activated. We simply eliminate all edges (n + i, n + i âˆ’ 1) for 2 â‰¤ i â‰¤ m. Of
course, the fixed costs w j can now be ignored.
A different transformation is given in Jonker & Volgenant (1988). Solution algo-
rithms are discussed in Gavish & Srikanth (1986).
3.1.7 The Rural Postman Problem
We are given a graph G = (V, E) with edge weights c ij and a subset F âŠ† E. The Rural
Postman Problem consists of finding a shortest closed walk in G containing all edges
in F . If F = E we have the special case of a Chinese postman problem which can be
solved in polynomial time using matching techniques (Edmonds & Johnson (1973)).
The standard symmetric TSP can easily be transformed to a rural postman problem.
Therefore, in general the rural postman problem is NP-hard. In Chapter 11 we will en-
counter a special version of the rural postman problem and find approximative solutions
for it using TSP methods.
3.1.8 The Bottleneck Traveling Salesman Problem
Instead of tours with minimal total length one searches in this problem for tours whose
longest edge is as short as possible. This bottleneck traveling salesman problem
can be solved by a sequence of TSPs. To see this observe that the absolute values of the
distances are not of interest under this objective function. We may reduce distances as
long as they compare exactly as before. Hence we may assume that we have at most
1
1
2 n(n âˆ’ 1) different distances and that the largest of them is not greater than 2 n(n âˆ’ 1).
We now solve problems of the following kind for some parameter b.3.2. Practical Applications of the TSP
35
â€œIs there a Hamiltonian cycle of the original graph consisting only of edges with length
at most b?â€
This problem can be transformed to a standard TSP. By performing a binary search
on the parameter b (starting with b = 14 n(n âˆ’ 1)) we can identify the smallest such b
leading to a â€œyesâ€ answer by solving at most O(log n) TSPs.
3.1.9 The Prize Collecting Traveling Salesman Problem
We are given a graph G = (V, E) with edge weights c ij , node weights u i (representing
benefits received when visiting the respective city), and a special base node v 0 (with
c v 0 = 0). The Prize Collecting Traveling Salesman Problem consists of finding
a cycle in G containing the node v 0 such that the sum of the edge weights of the cycle
minus the sum of the benefits of the nodes of the cycle is minimized. We can get rid of
the node weights if we substitute the edge weights c ij by c ij âˆ’ 12 v i âˆ’ 12 v j . Now the prize
collection TSP amounts to finding a shortest cycle in G containing v 0 . More details are
given in Balas (1989) and Ramesh, Yoon & Karwan (1992).
We have seen that a variety of problems can be transformed to symmetric TSPs or are
at least related to it. However, each such transformation has to be considered with
some care before actually trying to use it for practical problem solving. E. g., the
shortest path computations necessary to treat a GTSP as a TSP take time O(n 3 ) which
might not be acceptable in practice. Most transformations require the introduction of
a large number M . This can lead to numerical problems or may even prevent finding
feasible solutions at all using only heuristics. In particular, for LP-based approaches,
the usage of the â€œbig M â€ cannot be recommended in general. But, in any case, these
transformations provide a basic means for using a TSP code to treat related problems.
3.2 Practical Applications of the TSP
Since we are aiming at the development of algorithms and heuristics for practical travel-
ing salesman problem solving we give a survey on some of the possible applications. The
list is not complete but covers the most important cases. In addition we have included
problems which cannot be transformed to pure TSPs, but which can be attacked using
variants of the methods to be described later.
3.2.1 Drilling of Printed Circuit Boards
The drilling problem for printed circuit boards (PCBs) is a standard application of the
symmetric traveling salesman problem. To connect a conductor on one layer with a
conductor on another layer or to position (in a later stage of the PCB production) the
pins of integrated circuits, holes have to be drilled through the board. The holes may
be of different diameters. To drill two holes of different diameters consecutively, the
head of the machine has to move to a tool box and change the drilling equipment. This36
Chapter 3. Related Problems and Applications
is quite time consuming. Thus it is clear at the outset that one has to choose some
diameter, drill all holes of the same diameter, change the drill, drill the holes of the
next diameter etc.
Thus, this drilling problem can be viewed as a sequence of symmetric traveling salesman
problems, one for each diameter resp. drill, where the â€œcitiesâ€ are the initial position
and the set of all holes that can be drilled with one and the same drill. The â€œdistanceâ€
between two cities is the time it takes to move the head from one position to the other.
The goal is to minimize the travel time for the head of the machine.
We will discuss an application of the drilling problem in depth in Chapter 11.
3.2.2 X-Ray Crystallography
A further direct application of the TSP occurs in the analysis of the structure of crystals
(Bland & Shallcross (1989), Dreissig & Uebach (1990)). Here an X-ray diffrac-
tometer is used to obtain information about the structure of crystalline material. To
this end a detector measures the intensity of X-ray reflections of the crystal from various
positions. Whereas the measurement itself can be accomplished quite fast there is a con-
siderable overhead in positioning time since up to 30,000 positions have to be realized
for some experiments. In the two examples that we refer to, the positioning involves
moving four motors. The time needed to move from one position to the other can be
computed very accurately. For the experiment the sequence in which the measurements
at the various positions are taken is irrelevant. Therefore, in order to minimize the total
positioning time the best sequence for the measurements has to be determined. This
problem can be modeled as a symmetric TSP.
3.2.3 Overhauling Gas Turbine Engines
This application was reported by Plante, Lowe & Chandrasekaran (1987) and
occurs when gas turbine engines of aircrafts have to be overhauled. To guarantee a
uniform gas flow through the turbines there are so-called nozzle-guide vane assemblies
located at each turbine stage. Such an assembly basically consists of a number of
nozzle guide vanes affixed about its circumference. All these vanes have individual
characteristics and the correct placement of the vanes can result in substantial benefits
(reducing vibration, increasing uniformity of flow, reducing fuel consumption). The
problem of placing the vanes in the best possible way can be modeled as a symmetric
TSP.
3.2.4 The Order-Picking Problem in Warehouses
This problem is associated with material handling in a warehouse (Ratliff & Rosen-
thal (1981)). Assume that at a warehouse an order arrives for a certain subset of the
items stored in the warehouse. Some vehicle has to collect all items of this order to
ship them to the customer. The relation to the TSP is immediately seen. The storage
locations of the items correspond to the nodes of the graph. The distance between two
nodes is given by the time needed to move the vehicle from one location to the other.3.2. Practical Applications of the TSP
37
The problem of finding a shortest route for the vehicle with minimal pickup time can
now be solved as a TSP. In special cases this problem can be solved easily (see van Dal
(1992) for an extensive discussion).
3.2.5 Computer Wiring
A special case of connecting components on a computer board is reported in Lenstra
& Rinnooy Kan (1974). Modules are located on a computer board and a given subset
of pins has to be connected. In contrast to the usual case where a Steiner tree connection
is desired, here the requirement is that no more than two wires are attached to each
pin. Hence we have the problem of finding shortest Hamiltonian paths with unspecified
starting and terminating points.
A similar situation occurs for the so-called testbus wiring. To test the manufactured
board one has to realize a connection which enters the board at some specified point,
runs through all the modules, and terminates at some specified point. For each module
we also have a specified entering and leaving point for this test wiring. This problem also
amounts to solving a Hamiltonian path problem with the difference that the distances
are not symmetric and that starting and terminating point are specified.
3.2.6 Clustering of a Data Array
This application is also reported in Lenstra & Rinnooy Kan (1974). An (r, s)-
matrix A = (a ij ) is given representing relationships between two finite sets of elements
R = {R 1 , R 2 , . . . , R r } and S = {S 1 , S 2 , . . . , S s }. The entry a ij gives the strength of
the relationship between R i âˆˆ R and S j âˆˆ S. The task is to identify clusters of highly
related elements.
To this end, a permutation of the rows and columns of A has to be found which max-
imizes the sum of all products of horizontally or vertically adjacent pairs of entries of
A. We transform this problem as follows.
If Ï and Ïƒ are permutations of R and S, respectively, then the corresponding measure
of effectiveness is
ME(Ï, Ïƒ) =
sâˆ’1 
r

a i,Ïƒ(j) a i,Ïƒ(j+1) +
j=1 i=1
râˆ’1 
s

a Ï(i),j a Ï(i+1),j .
i=1 j=1
The two terms can be evaluated separately. We only consider the first one. Let V =
{1, 2, . . . , s} and E = V Ã— V . We define weights for the edges of G = (V, E) by
c ij = âˆ’
r

a ki a kj .
k=1
Then the problem of maximizing the first term amounts to finding a shortest Hamilto-
nian path in G with arbitrary starting and terminating node. The clustering problem
is then solved as two separate such problems, one for the rows and one for the columns.
The special cases where A is symmetric or where A is a square matrix and where we
only allow simultaneous permutations of rows and columns lead to a single Hamiltonian
path problem.38
Chapter 3. Related Problems and Applications
3.2.7 Seriation in Archeology
Suppose archeologists have discovered a graveyard and would like to determine the
chronological sequence of the various gravesites. To this end each gravesite is classi-
fied according to the types of items contained in it. A distance measure between two
gravesites is introduced reflecting the diversity between their respective contents. A very
likely chronological sequence can be found by computing the shortest Hamiltonian path
in the graph whose nodes correspond to the gravesites and where distances are given
due to the criterion above. In fact, this was one of the earliest applications mentioned
for the TSP.
3.2.8 Vehicle Routing
Suppose that in a city n mail boxes have to be emptied every day within a certain
period of time, say 1 hour. The problem is to find the minimal number of trucks to do
this and the shortest time to do the collections using this number of trucks. As another
example, suppose that customers require certain amounts of some commodities and a
supplier has to satisfy all demands with a fleet of trucks. Here we have the additional
problem to assign customers to trucks and to find a delivery schedule for each truck so
that its capacity is not exceeded and the total travel cost is minimized.
The vehicle routing problem is solvable as a TSP if there is no time constraint or if
the number of trucks is fixed (say m). In this case we obtain an m-salesmen problem.
Nevertheless, one can apply methods for the TSP to find good feasible solutions for this
problem (see Lenstra & Rinnooy Kan (1974)).
3.2.9 Scheduling
We are given n jobs that have to be performed on some machine. The time to process
job j is t ij if i is the job performed immediately before j (if j is the first job then its
processing time is t 0j ). The task is to find an execution sequence for the jobs such that
the total processing time is as short as possible.
We define the directed graph D = (V, A) with node set V = {0, 1, 2, . . . , n} and arc
set A = {1, 2, . . . , n} Ã— {1, 2, . . . , n} âˆª {(0, i) | i = 1, 2, . . . , n}. Arc weights are t ij for
(i, j) âˆˆ A. The scheduling problem can now be solved by finding a shortest (directed)
Hamiltonian path starting at node 0 with arbitrary terminating node.
Sometimes it is requested that the machine returns to its initial state after having
performed the last job. In this case we add arcs (i, 0) for every i = 1, 2 . . . , n where t i0
is the time needed to return to the initial state if job i was performed last. Now the
scheduling problem amounts to solving the asymmetric TSP in the new digraph.
Suppose the machine in question is an assembly line and that the jobs correspond to
operations which have to be performed on some product at the workstations of the line.
In such a case the primary interest would lie in balancing the line. Therefore instead of
the shortest possible time to perform all operations on a product the longest individual
processing time needed on a workstation is important. To model this requirement a
bottleneck TSP is more appropriate.3.2. Practical Applications of the TSP
39
In Lenstra & Rinnooy Kan (1974) it is shown that the following job-shop scheduling
problem can be transformed to an asymmetric TSP. We are given n jobs that have to
be processed on m machines. Each job consists of a sequence of operations (possibly
more than m) where each operation has to be performed on one of the machines. The
operations have to be performed one after the other in a sequence which is given in ad-
vance. As a restriction we have that no passing is allowed (we have the same processing
order of jobs on every machine) and that each job visits each machine at least once.
The problem is to find a schedule for the jobs that minimizes the total processing time.
3.2.10 Mask Plotting in PCB Production
For the production of each layer of a printed circuit board, as well as for layers of
integrated semiconductor devices, a photographic mask has to be produced. In our case
for printed circuit boards this is done by a mechanical plotting device. The plotter
moves a lens over a photosensitive coated glass plate. The shutter may be opened or
closed to expose specific parts of the plate. There are different apertures available to
be able to generate different structures on the board. Two types of structures have
to be considered. A line is exposed on the plate by moving the closed shutter to one
endpoint of the line, then opening the shutter and moving it to the other endpoint of
the line. Then the shutter is closed. A point type structure is generated by moving the
appropriate aperture to the position of that point then opening the shutter just to make
a short flash, and then closing it again. Exact modeling of the plotter control problem
leads to a problem more complicated than the TSP and also more complicated than the
rural postman problem.
We will discuss an application of the plotting problem in Chapter 11.
3.2.11 Control of Robots
In order to manufacture some workpiece a robot has to perform a sequence of operations
on it (drilling of holes of different diameters, cutting of slots, planishing, etc.). The
task is to determine a sequence to perform the necessary operations that leads to the
shortest overall processing time. A difficulty in this application arises because there
are precedence constraints that have to be observed. So here we have the problem of
finding the shortest Hamiltonian path (where distances correspond to times needed for
positioning and possible tool changes) that satisfies certain precedence relations between
the operations. This problem cannot be formulated as a TSP in a straightforward way,
but can be treated by applying methods similar to those presented in the forthcoming
chapters.40
Chapter 3. Related Problems and Applications
3.3 The Test Problem Instances
Throughout this tract we will use a set of sample problems compiled from various sources
to compare the different approaches and to examine the behaviour of algorithms with
respect to problem sizes.
To choose a suitable test set one has to decide between contradicting goals.
â€“ Too many computational results might bore the reader.
â€“ Too few results may not exhibit too much insight.
â€“ Problems from different sources might not be comparable in a fair way because
distance computations may be more complicated and time consuming in one case.
â€“ Not all problems are suitable for every approach.
To overcome these difficulties we have chosen to proceed as follows.
â€“ We have selected a set of sample problem instances which can always be treated
by our methods and which are of the same type. This set consists of twenty-four
Euclidean problems in the plane of sizes from 198 to 5934 nodes and is given
in Table 3.1. This table also gives the currently best known upper and lower
bounds for the respective problems. A number in boldface indicates that an
optimal solution is known (and proved!).
Problem
d198
lin318
fl417
pcb442
u574
p654
rat783
pr1002
u1060
pcb1173
d1291
rl1323
fl1400
u1432
fl1577
d1655
vm1748
rl1889
u2152
pr2392
pcb3038
fl3795
fnl4461
rl5934
Size
198
318
417
442
574
654
783
1002
1060
1173
1291
1323
1400
1432
1577
1655
1748
1889
2152
2392
3038
3795
4461
5934
Bounds
15780
42029
11861
50778
36905
34643
8806
259045
224094
56892
50801
270199
[19849,20127]
152970
[22137,22249]
62128
336556
316536
[64163,64294]
378032
137694
[28594,28772]
182566
[554070,556146]
Table 3.1 Bounds for sample problems
â€“ In addition we sometimes report about results on other problems. Most of these
problem instances are contained in the library TSPLIB of traveling salesman3.3. The Test Problem Instances
41
problem instances (see Reinelt (1991a)) and are therefore at the disposal of the
reader to conduct own experiments.
In our experiments, we have almost completely dispensed of random problem instances.
Only at some points we have included a few random problems for extrapolating CPU
times. For these problem instances, the points are located in a square and are drawn
independently from a uniform distribution. To our opinion, this should be the primary
reason for considering random problems. With respect to the assessment of algorithms
one should prefer the treatment of real instances (if available) because these are the
problems whose solution is of interest. Moreover, real problems have properties that
cannot be modeled by random distributions in an appropriate way.
Concerning the CPU times that are either explicitly given or presented in a graphical
display the following remarks apply.
â€“ All CPU times are given in seconds on the SUN SPARCstation 10/20. All software
(unless otherwise noted) has been written in C and was compiled using cc with
option -O4 under the operating system SUNOS 4.1.2. The function get rusage
was used for measuring running times.
â€“ Distance matrices are not used. Except for precomputed distances for candidate
sets, distances are always computed by evaluating the Euclidean distance function
and not by a matrix-lookup.
â€“ If a figure displays CPU times for problems up to size 6,000 then these are times
for the 24 standard problem instances listed in Table 3.1.
â€“ If a figure displays CPU times for problems up to size 20,000 then this includes
in addition the times for further problem instances, in particular for the real
problems rl11849, brd14051, and d18512, and for random problems of size 8,000
to 20,000.
Usually, we will not give the explicit length of tours produced by the various heuristics.
Rather we give their quality with respect to the lower bounds of Table 3.1. More
precisely, if c H is the length of a tour computed by heuristic H and if c L is the lower
bound of Table 3.1 for the respective problem instance, we say that the heuristic tour
has quality 100 Â· (c H /c L âˆ’ 1) percent.Chapter 4
Geometric Concepts
Many traveling salesman problem instances arising in practice have the property that
they are defined by point sets in the 2-dimensional plane (e.g., drilling and plotting
problems to be discussed in Chapter 11). Though true distances can usually be not
given exactly or only by very complicated formulae, they can very well be approxi-
mated by metric distances. To speed up computations we can therefore make use of
geometric properties of the point set. In this chapter we introduce concepts for deriv-
ing informations about the structure of point sets in the plane and review some basic
algorithms. A textbook on computational geometry is Edelsbrunner (1987).
4.1 Voronoi Diagrams
Although known since quite some time (Voronoi (1908)) the Voronoi diagram has
only recently received the attention of many researchers in the field of computational
geometry. It has several attractive features and puts particular emphasis on proximity
relations between points.
Let S = {P 1 , P 2 , . . . , P n } be a finite subset of R m and let d : R m Ã— R m âˆ’â†’ R be a
metric. We define the Voronoi region VR(P i ) of a point P i via
VR(P i ) = {P âˆˆ R m | d(P, P i ) â‰¤ d(P, P j ) for all j = 1, 2, . . . , n, j = i},
i.e., VR(P i ) is the set of all points that are at least as close to P i as to any other point
of S. The set of all n Voronoi regions is called the Voronoi diagram VD(S) of S.
Other names are Dirichlet tessellation or Thiessen tessellation. In the following
we call the elements of S generators.
We consider only the 2-dimensional case. With each generator P i we associate its
Cartesian coordinates (x i , y i ). For the first part we assume that d is the Euclidean
metric (L 2 ), i.e.,

d((x 1 , y 1 ), (x 2 , y 2 )) = (x 1 âˆ’ x 2 ) 2 + (y 1 âˆ’ y 2 ) 2 .
For two generators P i and P j we define the perpendicular bisector B(P i , P j ) = {P âˆˆ
R 2 | d(P, P i ) = d(P, P j )}. If we define the half space B ij = {x âˆˆ R 2 | d(P i , x) â‰¤
d(P j , x)} then we see that
n

VR(P i ) =
B ij .
j=1
j = i
G. Reinelt: The Traveling Salesman, LNCS 840, pp. 42-63, 1994.
ï›™ Springer-Verlag Berlin Heidelberg 19944.1. Voronoi Diagrams
43
This implies that in the case of the Euclidean metric the Voronoi regions are convex
polygons with at most n âˆ’ 1 vertices. Figure 4.1 shows the Voronoi diagram for the
points defining the traveling salesman problem instance rd100, a random problem on
100 points.
Figure 4.1 Voronoi diagram for rd100 (L 2 -metric)
We call nonempty intersections of two or more Voronoi regions Voronoi points if they
have cardinality 1 and Voronoi edges otherwise. Note that every Voronoi point is
the center of a circle through (at least) three generators and every Voronoi edge is a
segment of a perpendicular bisector of two generators.
A generator configuration is called degenerate if there are four generators lying on a
common circle. We say that two Voronoi regions are adjacent if they intersect. Note
that only in degenerate cases two regions can intersect in just one point. Degeneracy
does not occur in Figure 4.1 as is usually the case for randomly generated points.
Some observations are important.
Proposition 4.1 The Voronoi diagram of a generator P i is unbounded if and only if
P i lies on the boundary of the convex hull of S.
Proof. If VR(P i ) is bounded then P i is contained in the interior of the convex hull of
those generators whose regions are adjacent to VR(P i ). On the other hand, if VR(P i )
is unbounded it cannot be situated in the interior of conv(S).44
Chapter 4. Geometric Concepts
An unbounded region does not necessarily imply that a generator is a vertex of the
convex hull. Consider for example the case where all generators are located on a line.
Then all regions are unbounded, but only two generators define the convex hull of the
set.
Proposition 4.2 If d(P i , P j ) â‰¤ d(P i , P k ) for all k = i, j then VR(P i ) and VR(P j )
intersect in a Voronoi edge.
Proof. If P j is a nearest neighbor of P i then VR(P j ) and VR(P i ) intersect by definition.
If they intersect in only one point then this Voronoi point is the center of a circle through
P i , P j , and at least two more generators. At least two of these generators must be nearer
to P i than P j .
Proposition 4.3 The Voronoi diagram of n generators has at most 2n âˆ’ 4 Voronoi
points and at most 3n âˆ’ 6 Voronoi edges.
Proof. This property follows easily from Eulerâ€™s formula stating the relation between
the number of vertices, edges and facets of a convex polygon in the 3-dimensional
Euclidean space. Namely, if n v , n e , and n f denote the respective numbers we have
n f + n v = n e + 2.
We can apply this formula to a Voronoi diagram if we connect all Voronoi edges ex-
tending to infinity to a common imaginary Voronoi point (edges extending to infinity
in both directions are ignored here). We have n f = n Voronoi regions. Every Voronoi
point is the endnode of at least three Voronoi edges. Hence we obtain n v â‰¤ 23 n e and
therefore n e â‰¤ 3n âˆ’ 6. This implies n v â‰¤ 2n âˆ’ 4.
The Voronoi diagram gives a concise description of proximity relations between the
generators and also exhibits the information about which generators lie on the boundary
of the convex hull of the set S.
Before talking about time complexity of Voronoi diagram computations we have to
specify what the result of such a computation has to be. As the result of a Voronoi
diagram algorithm we require a data structure that allows for an easy access to the
Voronoi edges forming a specific region as well as to all Voronoi edges containing a given
Voronoi point. This can, e.g., be achieved by a data structure proposed in Ottmann
& Widmayer (1990). We store a doubly linked list of the Voronoi edges together with
the following information for every edge:
â€“ tail and head nodes, u and v, of the edge (the number âˆ’1 indicates that the edge
extends to infinity),
â€“ the Voronoi regions V l and V r to the left and to the right of the edge (where left
and right are with respect to some arbitrarily imposed direction of the edge),
â€“ pointers to the next Voronoi edge of region V l (resp. V r ) having u (resp. v) as one
endnode.
A straightforward algorithm for computing the Voronoi diagram takes time O(n 2 ). We
just compute each VR(P i ) as the intersection of the halfspaces B ij for j = i.
A lower bound on the running time of every Voronoi diagram algorithm is established
as follows. Consider the situation that all generators are located on the x-axis, i.e.,
their y-coordinates are 0. If we have the Voronoi diagram of this set we easily obtain
the sorted sequence of the generators with respect to their x-coordinates. Since sorting4.1. Voronoi Diagrams
45
cannot be performed in less than O(n log n) time we have a lower bound on the running
time for Voronoi diagram computations. Note that sorting and computing the Voronoi
diagram are essentially equivalent in this 1-dimensional case. The following section will
describe an algorithm that achieves this best possible running time.
In the present and in the following sections which also deals with geometric problems
we will not discuss the algorithms in full detail. When solving geometric problems
very often certain â€œdegenerateâ€ situations can occur that may have the consequence
that some object or some operation is not uniquely specified. Explaining the idea of a
geometric algorithm is often easy but when it comes to implementing the algorithm one
will usually face severe difficulties. It is a commonly observed fact that, when coding
algorithms for geometric problems, about 10â€“20% of the code account for the â€œrealâ€
algorithm while the remaining part is only necessary to deal with degenerate situations.
Almost the same is true when describing such algorithms in detail. We will therefore not
discuss degenerate situations explicitly, but will assume that points defining a geometric
problem are in general position, i.e., no degeneracy occurs. The reader should have in
mind, however, that an implementation must take care of these cases.
The O(n log n) algorithm to be described now was given in Shamos and Hoey (1975).
It is a divide and conquer approach that divides the set of generators recursively into
two halves, computes the respective Voronoi diagram for the two parts, and merges
them to obtain the Voronoi diagram of the whole set S.
procedure divide and conquer
(1) If the current set has at most three elements compute its Voronoi diagram and
return.
(2) Partition S into two sets S 1 = {P i 1 , P i 2 , . . . P i l } and S 2 = {P i l+1 , P i l+2 , . . . , P i n }
where l = n 2 such that there is a vertical line separating S 1 and S 2 .
(3) Perform the divide and conquer algorithm recursively to compute VD(S 1 ) and
VD(S 2 ).
(4) Obtain VD(S) from VD(S 1 ) and VD(S 2 ).
end of divide and conquer
The partition in Step (2) can be found in linear time if we have sorted the elements of
S in a preprocessing step with respect to their x-coordinates. There is also a more com-
plicated algorithm achieving linear running time without preprocessing (see Chapter 2
for median finding).
The critical step is Step (4). If this step can be performed in linear time (linear in
|S 1 | + |S 2 |) then the basic recurrence relation for divide and conquer algorithms gives a
time bound of O(n log n) for the overall Voronoi computation.
Merging two Voronoi diagrams (where one set of generators is to the left of the other
set of generators) basically consists of identifying the thick â€œmerge lineâ€ in the center
of Figure 4.2.
We cannot go into detail here and only discuss the principle of the construction of
this merge line. The line is constructed from the bottom to the top. One first has to
identify the respective generators in VD(S 1 ) and VD(S 2 ) with minimal y-coordinates46
Chapter 4. Geometric Concepts
(and therefore unbounded Voronoi regions). The perpendicular bisector between these
two generators then gives the lower part of the merge line. The process of merging the
two diagrams can now be visualized as extending the merge line upwards until an edge
of either of the two Voronoi diagrams is hit. At such a point the merge line may change
direction because it will now be part of the perpendicular bisector between two other
generators. This process is continued until the final part of the merge line is part of the
bisector of the two generators in the respective diagrams with maximal y-coordinates.
Figure 4.2 The merge line for merging two Voronoi diagrams
Implementing these steps carefully results in a running time of O(|S 1 | + |S 2 |) for the
merge process. This gives the desired optimal running time of the divide and conquer
algorithm.
The principle of the next algorithm (see Green & Sibson (1978)) is to start out with
the Voronoi diagram for three generators and then to successively take into account the
remaining generators and update the current Voronoi diagram accordingly.
procedure incremental algorithm
(1) Compute the Voronoi diagram VD({P 1 , P 2 , P 3 }).
(2) For t = 4, 5, . . . , n compute the diagram VD({P 1 , P 2 , . . . , P t }) as follows.
(2.1) Find P s , 1 â‰¤ s â‰¤ t âˆ’ 1, such that P t âˆˆ VR(P s ), i.e., find the nearest neighbor
of P t among the generators already considered.47
4.1. Voronoi Diagrams
(2.2) Start with the perpendicular bisector of P t and P s and find its intersection
with a boundary edge of VR(P s ). Suppose this edge is also the boundary
edge of VR(P i ). Find the other intersection of the bisector between P t and
P i with the boundary of VR(P i ). Proceed this way by entering neighboring
regions and computing intersections between the current bisector B(P t , P j )
and the boundary of VR(P j ) until the starting region VR(P s ) is reached
again. Eliminate everything within the resulting closed walk. (In the case of
an unbounded region VR(P t ) some further details have to be observed.)
end of incremental algorithm
Figure 4.3 shows a typical step of the incremental algorithm. The broken lines indicate
the bisectors that are followed in Step (2.2). The new diagram is obtained by deleting
everything inside the convex polygon determined by the broken lines.
P
v
P
r
P u
P t
P w
P s
Figure 4.3 A step of the incremental algorithm
The running time of this algorithm can only be bounded above by O(n 2 ). In Ohya,
Iri & Murota (1984) the algorithm is examined for application in practice. It turns
out that an enormous speed up can be obtained if the generators are considered in a
clever sequence. Ohya et al. use a bucketing approach which allows the nearest neighbor
in Step (2.1) to be guessed with high probability in constant time. Step (2.2) is not
that critical because usually only very local updates have to be performed to obtain the
new Voronoi diagram. In practical experiments linear running time on several classes of
randomly generated point sets was observed. In particular, the incremental algorithm
outperformed the divide and conquer method.
It is interesting to note, that selecting the next generator at random results in observed
3
O(n 2 ) running time.48
Chapter 4. Geometric Concepts
Expected linear running time is not proven in Ohya, Iri & Murota (1984), but some
evidence is given for it. For generators that are independently drawn from a uniform
distribution on the unit square, the expected number of generators to be examined
in Step (2.1) to find the nearest neighbor is bounded by a constant. In addition, the
expected number of Voronoi edges of a Voronoi region in any intermediate diagram is
bounded by a constant. These are two basic results that suggest that also a rigorous
mathematical proof of linear expected running time for this algorithm can be obtained.
A further algorithm for Voronoi diagram construction has been given in Fortune
(1987). This algorithm uses a sweep-line principle to compute the diagram in time
O(n log n).
Cronin (1990) and RujÃ¡n, Evertsz and Lyklema (1988) employ the Voronoi dia-
gram for generating traveling salesman tours.
4.2 Delaunay Triangulations
For most applications it is not the Voronoi diagram itself that is of interest. More
important are the proximity relations that it exhibits and not the concrete specification
of the Voronoi points and edges. To represent the topology of the diagram it suffices to
consider the â€œdualâ€ of the diagram.
Given the Voronoi diagram of S, its dual D(S) is the undirected graph G(S) = (S, D)
where D = {{P 1 , P 2 } | VR(P 1 ) âˆ© VR(P 2 ) = âˆ…}. It is easy to observe that G(S) is a
triangulated graph, i.e., every cycle of length at least four contains a chord. This graph
is called Delaunay triangulation (Delaunay (1934)).
An alternative definition excludes those edges {P 1 , P 2 } for which |VR(P 1 )âˆ©VR(P 2 )| = 1.
In this case the name is misleading, because we do not necessarily have a triangulation
anymore, but the resulting graph is planar (implying |D| = O(|S|)). We will use this
definition in the sequel and speak about the Delaunay graph or the straight line
dual of the Voronoi diagram.
Note that in the case of nondegeneracy (no four generators lie on a common circle) both
definitions coincide and D(S) is a planar graph.
Besides planarity D(S) (according to the modified definition) has additional important
properties.
Proposition 4.4
(i) If P i and P j are generators such that d(P i , P j ) â‰¤ d(P i , P k ) for all k = i, j then
{P i , P j } is an edge of D(S).
(ii) D(S) has at most 3n âˆ’ 6 edges.
(iii) D(S) contains a minimum spanning tree of the complete graph on n nodes where
the nodes correspond to the generators and the edge weights are respective Eu-
clidean distances.
Proof. Part (i) is clear because of Proposition 4.2 and part (ii) follows from Proposi-
tion 4.3.
For part (iii) consider Primâ€™s algorithm to compute a minimum spanning tree. In each
step we have a set V of nodes that are already connected by a spanning tree and the4.2. Delaunay Triangulations
49
set S \ V (consisting of isolated nodes). The next edge to be added to the tree is the
shortest edge connecting a node in V to a node in S \ V . This edge must be contained
in D(S) since it connects two generators whose Voronoi regions intersect in a Voronoi
edge.
Figure 4.4 shows the Delaunay triangulation corresponding to the diagram of Figure 4.1.
Figure 4.4 The Delaunay triangulation for rd100
Note that Proposition 4.4 (ii) does not hold for the general Delaunay triangulation but
only for the Delaunay graph. For example, if all generators are located on a circle then
all Voronoi regions intersect in a common Voronoi point and the Delaunay triangulation
is the complete graph on n nodes. The Delaunay graph is only a cycle of length n.
Straightforward implementations of algorithms for computing the Voronoi diagram (or
the Delaunay triangulation), in which all numerical computations are carried out in
floating point arithmetic, run into numerical problems.
Voronoi points are given as intersection points of bisectors. Due to insufficient accuracy
it may not be possible to safely decide whether two lines are parallel or almost parallel.
Moreover, the intersection points may be located â€œfar awayâ€ from the generators leading
to imprecise computations of Voronoi points.
The consequence is that due to incorrect decisions the algorithm may not work because
computed data is contradictory.
Consider the following example (Figure 4.5). We have three generators that are located
at three corners of a square. Depending on whether the fourth generator is at the fourth50
Chapter 4. Geometric Concepts
corner, or inside, or outside the square different Voronoi diagrams arise. If it cannot
be exactly differentiated between these three cases, then the correct computation of the
Voronoi diagram and hence of the Delaunay triangulation fails.
Figure 4.5 Inconsistent decisions due to round-off errors
The question of how to obtain correct results and avoid numerical difficulties is consid-
ered in Sugihara & Iri (1988), Sugihara (1988), and JÃ¼nger, Reinelt & Zepf
(1991). The principle idea is to not compute an explicit representation of the Voronoi
diagram, but to base the computation of the Delaunay graph on different logical tests.
Details are given in JÃ¼nger, Reinelt & Zepf (1991), we review the main results.
If all generators have integral coordinates between 0 and M , then one can compute
the Delaunay graph using integer numbers of value at most 6M 4 . On a computer
representing integers with binary 1-complement numbers having b bits, integers in the
interval [âˆ’2 bâˆ’1 , 2 bâˆ’1 âˆ’ 1] are available. The inequality 6M 4 â‰¤ 2 bâˆ’1 âˆ’ 1 implies
 

bâˆ’1
âˆ’ 1
4 2
M â‰¤
.
6
For the usual word length of real-world computers that means that we can allow

8 if b = 16
M â‰¤
137 if b = 32
35, 211 if b = 64 .
So, only 32-bit integer arithmetic is not enough for computing correct Delaunay trian-
gulations in practical applications. Only by using at least 64-bit arithmetic we can treat
reasonable inputs.
Of special interest are also the two metrics
â€“ Manhattan metric (L 1 ): d((x 1 , y 1 ), (x 2 , y 2 )) = |x 1 âˆ’ x 2 | + |y 1 âˆ’ y 2 |, and
â€“ Maximum metric (L âˆ ): d((x 1 , y 1 ), (x 2 , y 2 )) = max{|x 1 âˆ’ x 2 |, |y 1 âˆ’ y 2 |}.
This is due to the fact that very often distances correspond to the time a mechanical
device needs to travel from one point to the other. If this movement is performed
first in the horizontal direction and then in the vertical direction the L 1 -metric should
be chosen to approximate travel times. If the movement is performed by two motors
working simultaneously in horizontal and vertical directions then the L âˆ -metric is the
appropriate choice for modeling the movement times.4.2. Delaunay Triangulations
51
For these metrics, bisectors are no longer necessarily straight lines. They may consist of
three line segments and we can also have degenerate situations as shown in Figure 4.6.
Figure 4.6 Bisectors for L 1 - and L âˆ -metric
Here the bisectors include the shaded regions. In the L 1 -metric (left) this situation
occurs when the coordinate differences in both coordinates are the same. In the L âˆ -
metric (right picture) we have this situation if both points coincide in one of the two
coordinates. It is convenient to restrict our definition of bisectors to the bold line
segments in Figure 4.6 (the definition of Voronoi regions is changed accordingly). The
L 1 -metric Voronoi diagram for problem rd100 is shown in Figure 4.7.
Figure 4.7 Voronoi diagram for rd100 (L 1 -metric)52
Chapter 4. Geometric Concepts
The numerical analysis of the L 1 -case shows that we can compute an explicit representa-
tion of the Voronoi diagram itself using only one additional bit of accuracy than needed
to input the data. It follows that we can carry out all computations with numbers of
size at most 2M , and depending on the word length b we have the following constraints
for M

16, 383 if b = 16
1, 073, 741, 823 if b = 32
M â‰¤
4, 611, 686, 018, 427, 387, 903 if b = 64 .
Observe that also in diagrams for the Manhattan as well as for the maximum metric
the vertices of the convex hull of the generators have unbounded Voronoi regions. But
there may be further points with unbounded regions lying in the interior of the convex
hull. This is also reflected by the shape of the Delaunay graph in Figure 4.8 which
corresponds to the the Voronoi diagram of Figure 4.7.
Figure 4.8 Delaunay graph for rd100 (L 1 -metric)
Finally, we want to give an indication that Delaunay graphs can indeed be computed
very fast. We have used an implementation of the incremental Voronoi diagram algo-
rithm described in Ohya, Iri & Murota (1984) by M. JÃ¼nger and D. Zepf for the
L 2 -metric. This implementation uses floating point arithmetic, but was able to solve all
sample problem instances. Details for implementing an algorithm to compute Voronoi
diagrams for further metrics are discussed in Kaibel (1993).53
4.2. Delaunay Triangulations
Figure 4.9 shows the running time of this implementation on our set of sample problems.
CPU times are given in seconds on a SUN SPARCstation 10/20.
1.5
1.0
0.5
0.0
0
1000
2000
3000
4000
5000
6000
Figure 4.9 CPU times for computing Delaunay graphs
The figure seems to suggest indeed a linear increase of the running times with the
problem size. But, as we shall see throughout this monograph, we have to accept that
real-world problems do not behave well in the sense that smooth running time functions
can be obtained. Just the number of nodes of a problem is not sufficient to characterize
it. Real problems have certain structural properties that cannot be modeled by random
problem instances and can lead to quite different running times of the same algorithm
for different problem instances of the same size.
The number of edges of the respective Delaunay graphs is shown in Figure 4.10.
For random problems the expected number of edges forming a Voronoi region is six,
which is hence also the expected degree of a node in the Delaunay graph. Therefore we
would expect about 3n edges in a Delaunay graph which is quite closely achieved for
our sample problems.
We can conclude that Delaunay graphs can be computed very efficiently. For practical
purposes it is also important that running times are quite stable even though we have
problem instances from various sources and with different structural properties.54
Chapter 4. Geometric Concepts
20000
15000
10000
5000
0
0
1000
2000
3000
4000
5000
6000
Figure 4.10 Number of edges of the Delaunay graphs
CPU times are very well predictable (in our case as approximately n/5000 seconds on the
SPARCstation). Computing the Delaunay graph for problem d18512 took 19.4 seconds,
the number of edges of this graph was 61,126.
4.3 Convex Hulls
The convex hull of a set of points is the smallest convex set containing these points. It is
a convenient means for representing point sets. If the point set is dense then the convex
hull may very well reflect its shape. Large instances of traveling salesman problems
in the plane usually exhibit several clusters. Building the convex hull of these clusters
can result in a concise representation of the whole point set still exhibiting many of its
geometric properties.
A short description of complicated objects is also important in other areas, for example
in computer graphics or control of robots. Here movements of objects in space have
to be traced in order to avoid collisions. In such cases convex hulls can be applied to
represent the objects approximately.
We define the problem to compute the convex hull as follows. We are given a finite set
A = {a 1 , a 2 , . . . , a n } of n points in the plane where a i = (x i , y i ). The task is to identify4.3. Convex Hulls
55
those points that constitute the vertices of the convex hull conv(A) of the n points,
i.e., which are not representable as a convex combination of other points. Moreover,
we require that the computation also delivers the sequence of these vertices in correct
order. This means that if the computation outputs the vertices v 1 , v 2 , . . . , v t then the
convex hull is the (convex) polygon that is obtained by drawing edges from v 1 to v 2 ,
from v 2 to v 3 , etc., and finally by drawing the edges from v tâˆ’1 to v t and from v t to v 1 .
We will review some algorithms for computing the convex hull that also visualize some
fundamental principles in the design of geometric algorithms.
Before starting to describe algorithms we want to make some considerations concerning
the time that is at least necessary to find convex hulls. Of course, since all n input
points have to be taken into account we must spend at least time of linear order in n.
It is possible to establish a better lower bound. Let B = {b 1 , b 2 , . . . , b n } be a set of
distinct positive real numbers. Consider the set A âŠ† R 2 defined by A = {(b i , b 2 i ) | 1 â‰¤
i â‰¤ n}. Since the function f : R â†’ R with f (x) = x 2 is strictly convex none of the
points of A is a convex combination of other points. Hence computing the convex hull
of A also sorts the numbers b i . It is known that in many computational models sorting
of n numbers needs worst case time Î©(n log n).
A second way to derive a lower bound is based on the notion of maxima of vectors. Let
A = {a 1 , a 2 , . . . , a n } be a finite subset of R 2 . We define a partial ordering â€œâ€ on A by
a i = (x i , y i )  a j = (x j , y j ) if and only if x i â‰¥ x j and y i â‰¥ y j .
The maxima with respect to this ordering are called maximal vectors. In Kung,
Luccio & Preparata (1975) the worst case time lower bound of Î©(n log n) for identi-
fying the maximal vectors of A is proved. This observation is exploited in Preparata
& Hong (1977) for the convex hull problem. Suppose A is such that every a i is a vertex
of the convex hull. Identify the (w.l.o.g.) four points with maximal, resp. minimal x- or
y-coordinate. Let a j be the vertex with maximal y-coordinate and a k be the vertex with
maximal x-coordinate. The number of vertices between a j and a k may be of order n.
They are all maximal elements. Since convex hull computations can identify maximal
vectors it cannot be faster in the worst case than O(n log n). Note that this lower bound
is also valid if we do not require that the vertices of the convex hull are output in their
correct sequence.
Though the lower bound derived here may seem to be weak there are many algorithms
that compute the convex hull in worst case time O(n log n).
According to Toussaint (1985) the first efficient convex hull algorithm has been out-
lined in Bass & Schubert (1967). Their algorithm was designed to be the first step
for computing the smallest circle containing a given set of points in the plane. Though
the algorithm is not completely correct it already exhibits some of the powerful ideas
used in convex hull algorithms. It consists of an elimination step as in the throw-away
algorithm of section 4.3.3 and afterwards basically performs a scan similar to Grahamâ€™s
scan (to be described next). When corrected appropriately a worst case running time
of O(n log n) can be shown. Therefore, this algorithm can be considered as the first
O(n log n) convex hull algorithm.56
Chapter 4. Geometric Concepts
4.3.1 Grahamâ€™s Scan
This algorithm was given by Graham (1972). Its main step consists of computing a
suitable ordering of the points. Then the convex hull is built by successively scanning
the points in this order. The algorithm works as follows.
procedure graham scan
(1) Identify an interior point of conv(A), say P 0 . This can be done by finding three
points of A that are not collinear and by taking their center of gravity as P 0 .
(2) Compute polar coordinates for the points of A with respect to the center P 0 and
some arbitrary direction representing the angle zero.
(3) Sort the points with respect to their angles.
(4) If there are points with the same angle then eliminate all of them but the one
with largest radius. Let a i 1 , a i 2 , . . . , a i t be the sorted sequence of the remaining
points.
(5) Start with three consecutive points P r , P m , and P l , i.e., P r = a i k , P m = a i k+1 ,
P l = a i k+2 for some index k where indices are taken modulo t.
(6) Perform the following step for the current three points until the same triple of
points occurs for the second time.
a) If P m lies on the same side of the segment [P l , P r ] as P 0 or lies on the segment
then delete P m and set P m = P r and P r to its predecessor in the current sorted
list.
b) If P m lies on the side of the segment [P l , P r ] opposite to P 0 then set P r = P m ,
P m = P l , and P l to its successor in the current sorted list.
end of graham scan
P l
P r
P m
P 0
Figure 4.11 Grahamâ€™s scan4.3. Convex Hulls
57
Correctness of this algorithm is easily verified. Step (6) has to be performed at most
2t times for scanning the necessary triples of nodes. In Step (6a) a point is discarded
so it cannot be performed more than t âˆ’ 3 times. If Step (6b) is executed t times we
have scanned the points â€œonce around the clockâ€ and no further changes are possible.
Therefore the worst case running time is dominated by the sorting in Step (3) and we
obtain the worst case running time O(n log n).
This way we have established the worst case time complexity Î˜(n log n) for computing
convex hulls in the plane.
4.3.2 Divide and Conquer
The divide and conquer principle also applies in the case of convex hull computations
(Bentley & Shamos (1978)). In this case, the basic step consists of partitioning a
point set according to some rule into two sets of about equal size, computing their
respective convex hulls and merging them to obtain the convex hull of the whole set.
procedure divide and conquer
(1) If the current set has at most three elements compute its convex hull and return.
(2) Partition A into two sets A 1 = {a i 1 , a i 2 , . . . , a i l } and A 2 = {a i l+1 , a i l+2 , . . . , a i n }
where l = n 2 such that there is a vertical line separating A 1 and A 2 .
(3) Perform the divide and conquer algorithm recursively to compute conv(A 1 ) and
conv(A 2 ).
(4) Merge the two convex hulls to obtain the convex hull of A.
end of divide and conquer
Figure 4.12 The divide and conquer algorithm58
Chapter 4. Geometric Concepts
The partition required in Step (2) can be easily computed if the points are presorted
(which takes time O(n log n)). So, the only critical step to be examined is Step (4).
When merging two hulls we have to add two edges (the so-called upper and lower
bridges) and eliminate all edges of conv(A 1 ) and conv(A 2 ) that are not edges of
conv(A). To find these bridges we can exploit the fact that due to the sorting step
we know the leftmost point of A 1 and the rightmost point of A 2 (w.l.o.g., A 1 is left of
A 2 ). Starting at these points it is fairly simple to see that the edges to be eliminated
can be readily identified and that no other edges are considered for finding the bridges.
Therefore, the overall time needed to merge convex hulls during the algorithm is linear
in n. Due to Theorem 2.9, this establishes the O(n log n) worst case time bound for the
divide and conquer approach.
Kirkpatrick & Seidel (1986) describe a refinement of the divide and conquer ap-
proach to derive a convex hull algorithm which has worst case running time O(n log v)
where v is the number of vertices of the convex hull.
4.3.3 Throw-Away Principles
It is intuitively clear that when computing the convex hull of a set A not all points
are equally important. With high probability, points in the â€œinteriorâ€ of A will not
contribute to the convex hull whereas points near the â€œboundaryâ€ of A are very likely
vertices of the convex hull. Several approaches make use of this observation in that they
eliminate points before starting the true convex hull computation.
If we consider a convex polygon whose vertices are contained in the set A then all points
inside this polygon can be discarded since they cannot contribute to the convex hull. In
Akl & Toussaint (1978) an algorithm is given that makes use of this fact.
procedure throw away
(1) Compute the points a xmax , a xmin , a ymax , and a ymin with maximal (minimal) x-,
resp. y-coordinate.
(2) Discard all points inside the convex polygon given by these four points and identify
four regions of points to be considered. The regions are associated with the four
edges of the polygon.
(3) For each subregion, determine the convex hull of points contained in it.
(4) Construct conv(A) from these four convex hulls.
end of throw away
Any of the convex hull algorithms could be used in Step (3). Akl and Toussaint basically
use Grahamâ€™s scan modified in a way that no angles have to be computed.
In a refined version Devroye & Toussaint (1981) compute four additional points,
namely those with maximal (minimal) coordinate sum x i +y i , resp. coordinate difference
x i âˆ’ y i . Elimination is now performed using the convex polygon given by these eight
points.59
4.3. Convex Hulls
a ymax
a xmin
a xmax
a ymin
Figure 4.13 A throw-away principle
4.3.4 Convex Hulls from Maximal Vectors
Every point in R 2 can be viewed as the origin of a coordinate system with axes parallel
to the x- and y-directions. This coordinate system induces four quadrants. A point is
called maximal with respect to the set A if at least one of these quadrants (including
the axes) does not contain any other point of A. It is easily seen that every vertex of
conv(A) is maximal. Namely, let x be a vertex of conv(A) and assume that each of the
corresponding quadrants contains a point of A. Then x is contained in the convex hull of
these points and therefore cannot be a vertex of the convex hull of A. Kung, Luccio
& Preparata (1975) give an algorithm to compute the maximal vectors of a point
set in the plane in worst case time O(n log n). This leads to the following O(n log n)
algorithm for computing the convex hull.
procedure maximal vector hull
(1) Compute the set S of maximal vectors with respect to A.
(2) Let A  = S.
(3) Compute the convex hull of A  using any of the O(n log n) worst case time algo-
rithms.
(4) conv(A) = conv(A  ).
end of maximal vector hull
The expectation is that very many points can be discarded in Step (1).60
Chapter 4. Geometric Concepts
Figure 4.14 Maximal vectors
4.3.5 A New Elimination Type Algorithm
We are now going to discuss a further elimination type algorithm that uses a particularly
simple discarding mechanism. This algorithm is best suited for large dense point sets
distributed uniformly in a rectangle. It is discussed in full detail in Borgwardt,
Gaffke, JÃ¼nger & Reinelt (1991).
Assume that the points of A are contained in the unit sqaure, i.e., their coordinates are
between 0 and 1. The function h : [0, 1] Ã— [0, 1] â†’ R is defined by h(x) = h(x 1 , x 2 ) =
min{x 1 , 1 âˆ’ x 1 } Â· min{x 2 , 1 âˆ’ x 2 }.
The basic idea is to compute the convex hull of a small subset S of A such that conv(S) =
conv(A) with high probability. The value h(x) will express whether x is likely to be an
interior point of conv(A). With increasing h(x) the probability that x can be eliminated
as a candidate for a vertex of the convex hull increases.
We will discard points based on this function in a first phase and compute the convex
hull for the remaining points. It will turn out that we cannot guarantee that we have
obtained conv(A) this way. Therefore, in a second phase we have to check correctness
and possibly correct the results.
The following is a sketch of our elimination type algorithm where CH is some algorithm
to compute the convex hull of a set of points in the plane.
procedure elim hull
Phase1
(1) Choose a suitable parameter Î±.
(2) Let S Î± = {a i âˆˆ A | h(a i ) â‰¤ Î±}.
(3) Apply CH to compute the convex hull of S Î± .4.3. Convex Hulls
61
(4) Compute the minimum Î³ such that conv(S Î± ) âŠ‡ {x âˆˆ [0, 1] Ã— [0, 1] | h(x) > Î³}.
(5) If Î± â‰¥ Î³ then STOP (conv(S Î± ) = conv(A)), otherwise perform Phase 2.
Phase2
(6) Compute conv(S Î³ ) where S Î³ = {a i âˆˆ A | h(a i ) â‰¤ Î³}. Now conv(S Î³ ) = conv(A).
end of elim hull
Figure 4.15 gives an illustration of the algorithm. The set S Î± is given by the solid
points (â€˜â€¢â€™) and its convex hull by solid lines. The broken curve defines the set S Î³ and
the additional points to be considered in Phase 2 are shown as small circles (â€˜â—¦â€™). The
extreme points of the correct convex hull resulting from Phase 2 are obtained by adding
one point in the north-east corner.
A detailed analysis shows that Step (4) can be performed in linear time. Therefore the
worst case running time of this algorithm is given by the worst case running time of CH
(independent of Î±).
Figure 4.15 Illustration of the algorithm
The analysis of the average time complexity of this algorithm exhibits some interesting
consequences. If Î± is chosen carefully, then in certain random models only very few
points are contained in S Î± and with very high probability Phase 2 is not needed. In
particular, one can obtain a speed-up theorem for convex hull algorithms in the following
sense.62
Chapter 4. Geometric Concepts
Theorem 4.5 Let A be a set of n random points generated independently from the
uniform distribution on the unit square [0, 1]Ã—[0, 1]. For any algorithm CH with polyno-
mial worst-case running time the two-phase method has linear expected running time.
For a detailed analysis of the algorithm and the proper choice of Î± as well as for
a discussion of the computation of convex hulls for random points in the unit disk
D(0, 1) = {x = (x 1 , x 2 ) âˆˆ R 2 | x 21 + x 22 â‰¤ 1} we refer to Borgwardt, Gaffke,
JÃ¼nger & Reinelt (1991). Our approach can be generalized to higher dimensions. A
complete coverage of the 3-dimensional case can be found in Thienel (1991).
Assuming a uniform distribution of the n independent points over the unit square, linear
expected time algorithms have been given by Bentley & Shamos (1978), Akl &
Toussaint (1978), Devroye (1980), Devroye & Toussaint (1981), Kirkpatrick
& Seidel (1986), and Golin & Sedgewick (1988). For a survey on these and related
subjects see Lee & Preparata (1984).
We have compared the practical behaviour of five linear expected time algorithms,
namely
[1] the divide and conquer algorithm (4.3.2),
[2] the maximal vectors approach (4.3.4),
[3] the throw-away principle based on eight points (4.3.3),
[4] the throw-away principle based on four points (4.3.3),
[5] the new algorithm.
For algorithms [2], [3], [4], and [5] we apply Grahamâ€™s scan to the selected points.
All five algorithms have been implemented as Pascal programs on a SUN SPARCsta-
tion SLC which is about four times slower than the SPARCstation 10/20. We have tried
to put the same efforts into all five programs to make the comparison as fair as possible.
For instance, in all throw-away type algorithms we found that the following trick helped
to reduce the computation time. As soon as the elimination area has been determined
(a closed polygon in case [3] and [4] and the curve defined by the function h in [5]) we
inscribe the biggest possible rectangle with vertical and horizontal sides into this area.
Assume that this rectangle (always a square in case [5]) has vertices (x 1 , y 1 ), (x 2 , y 1 ),
(x 2 , y 2 ) and (x 1 , y 2 ). The elimination criterion is satisfied for a point with coordinates
(x, y) if x 1 â‰¤ x â‰¤ x 2 and y 1 â‰¤ y â‰¤ y 2 which takes only four comparisons to check. Only
if this criterion fails, we have to check the actual elimination criterion which, e.g., in
case [5] amounts to checking which quadrant (x, y) lies in, and depending on this, up
to two additions, one multiplication, and one comparison.
Figure 4.16 shows the computation times of the algorithms for computing the convex
hull of point sets in the unit square drawn independently form a uniform distribution.
The curves are based on 10 sample problems for each problem size n = 1 000, 2 000, . . .,
10 000, 20 000, . . . , 100 000, 200 000, . . . , 1 000 000. In our opinion, these curves should
be interpreted as follows. When doing practical computations, the throw-away principle
is superior compared to the divide and conquer algorithms. The four point method is
slightly better than the eight point method.
For our experiments with practical traveling salesman problems in the plane we have
coded the algorithm as follows. The points are mapped to the unit square by horizontal63
4.3. Convex Hulls
20
15
10
5
0
0
200000
400000
[1]
[2]
600000
[3]
[4]
800000
1000000
[5]
Figure 4.16 Comparison of convex hull algorithms
and vertical scaling and appropriate offsets. This is done in such a way that each of
the four sides of the square contains at least one problem point. Then the elimination
algorithm is performed using Î± = 4 log n/n. However, in the range of problem sizes
we had available, the new algorithm does not pay off since the convex hull of some
ten thousands of points can be computed in so little time that it is negligibly small
compared to the other algorithms employed for problem solving. Therefore we do not
report computing times for TSPLIB problems here.Chapter 5
Candidate Sets
In many practical applications it is required to find reasonably good tours for a traveling
salesman problem in short time. When designing fast heuristics, one is faced with the
problem that, in principle, very many connections need to be considered. For example,
in the traveling salesman problem fnl4461, tours have to be constructed by selecting
4461 out of 9,948,030 possible connections. Standard implementations of heuristics
consider all these connections which leads to substantial running times.
On the other hand, it is intuitively clear, that most of the possible connections will not
occur in short tours because they are too long. It is therefore a reasonable idea, which
we will exploit extensively in the sequel, to restrict attention to â€œpromisingâ€ edges and
to avoid considering long edges too frequently. To this end we employ several types of
candidate sets from which edges are taken with priority in the computations.
In geometric problem instances one has immediate access to long edges because their
length is related to the location of the points. In general, for problems given by a
distance matrix, already time Î©(n 2 ) has to be spent to scan all edge lengths. We will
discuss three types of candidate sets in this chapter. The first one is applicable in
general, but can be computed very fast for geometric instances. The other two sets can
only be computed for geometric instances.
5.1 Nearest Neighbors
It can be observed that most of the edges in good or optimal tours connect nodes to near
neighbors. For a TSP on n nodes and k â‰¥ 1, we define the corresponding k nearest
neighbor subgraph G k = (V, E) by setting
V ={1, 2 . . . , n},
E ={uv | v is among the k nearest neighbors of u}.
For example, an optimal solution for the problem pr2392 can be found within the
8 nearest neighbor subgraph and for pcb442 even within the subgraph of the 6 nearest
neighbors.
Figure 5.1 shows the 10 nearest neighbor subgraph for the problem u159. This subgraph
contains an optimal tour.
G. Reinelt: The Traveling Salesman, LNCS 840, pp. 64-72, 1994.
ï›™ Springer-Verlag Berlin Heidelberg 19945.1. Nearest Neighbors
65
Figure 5.1 The 10 nearest neighbor subgraph for u159
A straightforward computation of the k nearest neighbors by enumeration takes time
Î©(n 2 ) for fixed k. The following proposition shows that for Euclidean problem instances
we can exploit the Delaunay graph for nearest neighbor computations. The discussion
applies to other metrics as well.
Proposition 5.1 Let P i and P j be two generators.
(i) If the straight line connecting P i and P j intersects the interior of the Voronoi
region of a generator P l different from P i and P j then d(P i , P l ) < d(P i , P j ).
(ii) If the smallest number of edges on a path from P i to P j in the Delaunay graph
is k then there exist at least k âˆ’ 1 generators P l different from P i and P j with
d(P i , P l ) < d(P i , P j ).
Proof. For part (i) suppose that the line intersects the boundary of VR(P l ) in the
points T 1 and T 2 where w. l. o. g. d(P i , T 1 ) < d(P i , T 2 ). By definition we have d(P l , T 1 ) â‰¤
d(P i , T 1 ) and d(P l , T 2 ) â‰¤ d(P j , T 2 ). Since P i , P l , and T 1 are distinct we obtain
d(P i , P l ) â‰¤ d(P i , T 1 ) + d(P l , T 1 )
< d(P i , T 1 ) + d(T 1 , P j )
= d(P i , P j ).
Part (ii) is an immediate corollary.
Therefore, to compute the k nearest neighbors of some generator P i we only have to
examine generators which are connected to P i in the Delaunay graph by a path of length66
Chapter 5. Candidate Sets
at most k âˆ’ 1. Since, for random instances, the expected degree of a node in this graph
is six, we can expect linear running time of this procedure for fixed k.
The fast algorithm for Euclidean problem instances is the following.
procedure nearest neighbors
(1) Compute the Delaunay graph and initialize the empty candidate list.
(2) For i = 1, 2, . . . , n compute the k nearest neighbors of node i by breadth-first
search in the Delaunay graph starting at node i. Add the corresponding edges to
the candidate set.
end of nearest neighbors
5.0
4.0
3.0
2.0
1.0
0.0
0
1000
2000
3000
4000
5000
6000
Figure 5.2 CPU times for computing 10 nearest neighbor graph
Figures 5.2 and 5.3 show the running times for the 10 nearest neighbor computations
for our set of sample problems as well as the number of edges in the resulting candidate
sets. A good approximation for the cardinality of the 10 nearest neighbor candidate set
is 6n.
It is interesting to note that computation of the 10 nearest neighbor set for problem
pr2392 takes 22.3 seconds if the trivial algorithm is used. If the complete distance
matrix is stored this time reduces to 18.8 seconds which is still substantially more than
the 0.5 seconds needed with the Delaunay graph. It should be kept in mind that we
do not use complete distance matrices in this tract, but that much CPU time can be
gained if they are available.67
5.2. Candidates Based on the Delaunay Graph
70000
60000
50000
40000
30000
20000
10000
0
0
1000
2000
3000
4000
5000
6000
Figure 5.3 Number of edges of the 10 nearest neighbor graphs
There are further approaches for an efficient computation of nearest neighbors for geo-
metric problems instances, e.g., probabilistic algorithms and algorithms based on k-d-
trees (Bentley (1990)).
5.2 Candidates Based on the Delaunay Graph
In particular if point sets exhibit several clusters, the k nearest neighbor subgraph is not
connected and many edges to form good tours are missing. Here the Delaunay graph
should help since it contains important connections between the clusters.
Though it may seem to be true at a first glance, the Delaunay graph itself does not
necessarily contain a Hamiltonian tour. For example, in the case where all points
are on a line the Delaunay graph is just a path. There are also examples where the
Delaunay graph is a triangulation, but does not contain a tour or even a Hamiltonian
path (Dillencourt (1987a,1987b)).
First experiments have indicated that the Delaunay graph provides a candidate set too
small. We therefore decided to augment it using transitive relations in the following
way. If the edges {i, j} and {j, k} are contained in the Delaunay graph then we also
add edge {i, k} to the candidate set. We call this set Delaunay candidate set. The
cardinality of this set can be quite large. For example, if n âˆ’ 1 generators are located
on a circle and one generator is at the center of this circle then the Delaunay candidate
set is the complete graph on n nodes. Figure 5.4 shows the Delaunay candidate set for
problem u159.68
Chapter 5. Candidate Sets
Figure 5.4 The Delaunay candidate set for u159
The following procedure computes the Delaunay candidate set.
procedure Delaunay candidates
(1) Compute the Delaunay graph and initialize the candidate set with the edges of
the Delaunay graph.
(2) For every node i = 1, 2, . . . , n do
(2.1) For every two nodes j and k adjacent to i in the Delaunay graph add edge
{j, k} to the candidate set if it was not a candidate edge before.
end of Delaunay candidates
For random points in the plane, empirical observations show that we can expect about
9n to 10n edges in this candidate set.
Figure 5.4 illustrates that the candidate set is rather dense. Therefore we have to expect
more than 9n edges in this candidate set for practical problem instances. Furthermore,
due to long edges in the Delaunay graph, the candidate set may contain many long
edges. To avoid long edges, we usually first run a fast heuristic to compute an initial
tour (as the space filling curves heuristic described in Chapter 8) and then eliminate
all edges from the candidate set that are longer than the longest edge in this tour. For
dense point sets most long edges will be eliminated, for clustered point sets the edges
connecting the clusters will be kept. In general, however, elimination of long edges is
not critical for the performance of our heuristics.69
5.2. Candidates Based on the Delaunay Graph
5.0
4.0
3.0
2.0
1.0
0.0
0
1000
2000
3000
4000
5000
6000
Figure 5.5 CPU times for computing Delaunay candidate sets
70000
60000
50000
40000
30000
20000
10000
0
0
1000
2000
3000
4000
5000
6000
Figure 5.6 Number of edges of Delaunay candidate sets
Figures 5.5 and 5.6 give the CPU times necessary for computing the Delaunay candidate
set and the cardinality of this set. CPU time and size of the candidate set depend highly
on the point configuration and not only on the problem size. For a random problem on70
Chapter 5. Candidate Sets
15,000 nodes we obtained 44,971 (i.e., 2.99 Â· n) edges in the Delaunay graph and 147,845
(i.e., 9.86 Â· n) edges in the Delaunay candidate set.
5.3 Other Candidate Sets
We discuss a further candidate set that is easily computed and does not require such
sophisticated computations as the Delaunay graph.
The idea is to obtain near neighbors without too much effort just based on coordinates.
We outline the procedure for horizontal coordinates. We sort the points with respect to
their x-coordinates. For every point i we consider the points that appear right before or
after i in this sorted sequence. To limit the amount of work we only take those points
into account which appear at most w positions before or at most w positions after i.
Among these points we compute the k points nearest to i and choose the corresponding
edges as candidate edges.
The complete heuristic also takes the vertical coordinates into account. This is accom-
plished as follows. The parameter w specifies a search width as sketched above, the
parameter k gives the number of candidate edges that are selected from each node.
procedure candidate heuristic
(1) Initialize two sorted lists of the points by sorting them with respect to their x-
coordinates and with respect to their y-coordinates. For every point i let i x and
i y be its respective positions in the lists.
(2) Initialize the empty candidate list.
(3) For every node i = 1, 2, . . . , n do
(3.1) Let Q 1 = {j|j x âˆˆ {i x + 1, . . . , i x + w}, j y âˆˆ {i y + 1, . . . , i y + w}}, Q 2 = {j|j x âˆˆ
{i x + 1, . . . , i x + w}, j y âˆˆ {i y âˆ’ 1, . . . , i y âˆ’ w}}, Q 3 = {j|j x âˆˆ {i x âˆ’ 1, . . . , i x âˆ’
w}, j y âˆˆ {i y âˆ’ 1, . . . , i y âˆ’ w}}, and Q 4 = {j|j x âˆˆ {i x âˆ’ 1, . . . , i x âˆ’ w}, j y âˆˆ
{i y + 1, . . . , i y + w}}.
(3.2) Add edges from node i to its two nearest neighbors in every set Q j , j =
1, 2, 3, 4 (or less, if Q j contains fewer than two elements) to the candidate set
and remove the corresponding nodes from the sets. (We incorporate this step
to have candidate edges connecting i to each of the sets Q j .)
(3.3) Compute the k âˆ’ l nearest neighbors of i in the reduced set Q 1 âˆª Q 2 âˆª Q 3 âˆª Q 4
(where l is the number of edges selected in (3.2)) and add the corresponding
edges to the candidate set.
end of candidate heuristic
Figure 5.7 shows the candidate set obtained with this heuristic for problem instance u159
(parameters were k = 10 and w = 20). It contains 94% of the edges of the 10 nearest
neighbor graph. Because of pathological conditions at the border of point sets we may
also incur a number of long edges in this heuristic. These could be eliminated as above,
but keeping them has no significant effects as our experiments showed.71
5.3. Other Candidate Sets
Figure 5.7 Result of candidate heuristic for u159
5.0
4.0
3.0
2.0
1.0
0.0
0
1000
2000
3000
4000
5000
Figure 5.8 CPU times for candidate heuristic
600072
Chapter 5. Candidate Sets
70000
60000
50000
40000
30000
20000
10000
0
0
1000
2000
3000
4000
5000
6000
Figure 5.9 Number of edges of heuristic candidate sets
Figures 5.8 and 5.9 display the CPU times for computing this candidate set and the
number of its edges, respectively. Both, sizes of the candidate sets and the times to
compute them, are very well predictable depending on the size of the problem instance.
An interesting further candidate set can be obtained by taking the union of the Delaunay
graph and an appropriate nearest neighbor graph. Here we have the short connections
as well as the important connections between distant clusters.
We will investigate the usefulness of our candidate sets in the subsequent chapters.Chapter 6
Construction Heuristics
Starting with this chapter we will now consider computational aspects of the traveling
salesman problem. For the beginning we shall consider pure construction procedures,
i.e., heuristics that determine a tour according to some construction rule, but do not
try to improve upon this tour. In other words, a tour is successively built and parts
already built remain in a certain sense unchanged throughout the algorithm.
Many of the construction heuristics presented here are known and computational results
are available (Golden & Stewart (1985), Arthur & Frendeway (1985), Johnson
(1990), Bentley (1992)) We include them for the sake of completeness of this tract
and for having a reference to be compared with other algorithms on our sample problem
instances. Moreover, most evaluations of heuristics performed in the literature lack
from the fact that either fairly small problem instances or only random instances were
examined.
The following types of algorithms will be discussed:
â€“ nearest neighbor heuristics,
â€“ insertion heuristics,
â€“ heuristics based on spanning trees, and
â€“ savings heuristics.
This does not cover by far all the approaches that have been proposed. But we think
that the ideas presented here provide the reader with the basic principles that can also
be adapted to other combinatorial optimization problems.
For the following, we will always assume that we are given the complete undirected
graph K n with edge weights c uv for every pair u and v of nodes. For ease of notation
we will denote the node set by V and assume that V = {1, 2, . . . , n}. The question to
be addressed is to find good Hamiltonian tours in this graph.
6.1 Nearest Neighbor Heuristics
This heuristic for constructing a traveling salesman tour is near at hand. The salesman
starts at some city and then visits the city nearest to the starting city. From there he
visits the nearest city that was not visited so far, etc., until all cities are visited, and
the salesman returns to the start,
6.1.1 The Standard Version
Formulated as an algorithm we obtain the following procedure.
G. Reinelt: The Traveling Salesman, LNCS 840, pp. 73-99, 1994.
ï›™ Springer-Verlag Berlin Heidelberg 199474
Chapter 6. Construction Heuristics
procedure nearest neighbor
(1) Select an arbitrary node j, set l = j and T = {1, 2, . . . , n} \ {j}.
(2) As long as T  = âˆ… do the following.
(2.1) Let j âˆˆ T such that c lj = min{c li | i âˆˆ T }.
(2.2) Connect l to j and set T = T \ {j} and l = j.
(3) Connect l to the first node (selected in Step (1)) to form a tour.
end of nearest neighbor
This procedure runs in time Î©(n 2 ). A possible variation of the standard nearest neighbor
heuristic is the double-sided nearest neighbor heuristic where the current path can
be extended from both of its endnodes.
No constant worst case performance guarantee can be given, since the following theorem
due to Rosenkrantz, Stearns & Lewis (1977) holds.
Theorem 6.1 For every r > 1 and arbitrarily large n there exists a TSP instance on
n cities such that the nearest neighbor tour is at least r times as long as an optimal
tour.
In addition, Rosenkrantz, Stearns and Lewis (1977) show that for arbitrarily
large n there exist TSP instances on n nodes such that the nearest neighbor solution is
Î˜(log n) times as long as an optimal Hamiltonian cycle. This results still holds if the
triangle inequality is satisfied. Therefore it also applies to metric problem instances.
Figure 6.1 A nearest neighbor tour for rd1006.1. Nearest Neighbor Heuristics
75
If one displays nearest neighbor tours one realizes the reason for their poor performance.
The procedure proceeds very well and produces connections with short edges in the
beginning. But, as can be seen from a graphics display, several cities are â€œforgottenâ€
during the course of the algorithm. They have to be inserted at high cost in the end.
Figure 6.1 shows a typical nearest neighbor tour.
Though usually rather bad, nearest neighbor tours have the advantage that they only
contain a few severe mistakes, but there are long segments connecting nodes with short
edges. Therefore, such tours can serve as good starting tours for subsequently performed
improvement methods and it is reasonable to put some effort in designing heuristics that
are based on the nearest neighbor principle. We will comment on improvement methods
in the next chapter. The standard procedure itself is easily implemented with a few lines
of code. But, since running time is quadratic, we describe some variants to speed up
and/or improve the standard nearest neighbor search.
6.1.2 Exploiting the Delaunay Graph
We have seen in Chapter 5 that the Delaunay graph can be used to speed up nearest
neighbor computations. We can apply these results here, too. Namely, when searching
the nearest neighbor of node l in step (2.1) among the nodes which are not yet contained
in the partial tour, we can use the principle of section 5.1 to generate the k-th nearest
neighbor of l for k = 1, 2, . . . , n until a node is found that is not yet connected. Due to
the properties of the Delaunay graph we should find this neighbor examining only a few
edges of the graph in the neighborhood of l. Since in the last steps of the algorithm we
have to collect the forgotten nodes (which are far away from the current node) it makes
no sense to use the Delaunay graph any further. So, for connecting the final nodes we
just use a simple enumeration procedure.
We have conducted several experiments to see how many neighbors of the current node
are examined and to what depth the breadth-first search to find the nearest neighbor is
performed.
Figure 6.2 shows the average depth in the breadth-first search tree needed to find the
next neighbor. The average depth varies between 3 and 5 for real-world problems and
is about 3.5 for random problems.
Figure 6.3 displays the average number of nodes examined to find the next neighbor,
which is then also the number of necessary distance evaluations per node. Here real-
world problems behave better than random problems.
Furthermore, we examined how search depth and number of neighbors to be examined
develop during the heuristic. Figures 6.4 and 6.5 depict the search depth and the number
of examined nodes, respectively, obtained during the execution of the nearest neighbor
heuristic on the problem pr2392.
We see that at the majority of nodes next neighbors can indeed be found in the local
neighborhood with a search depth below five in most cases. Only sometimes a large
part of the Delaunay graph has to be explored to find the next node. Note that we do
not use the Delaunay strategy for inserting the final 100 nodes, since the overhead for
identifying the nearest neighbor increases significantly at the end of the procedure.76
Chapter 6. Construction Heuristics
6.0
5.0
4.0
3.0
2.0
1.0
0.0
0
5000
10000
15000
20000
Figure 6.2 Average search depth
50
45
40
35
30
25
20
15
10
0
5000
10000
15000
20000
Figure 6.3 Average number of examined nodes
The worst case running time of this implementation is still quadratic. The running
times for the sample problems will be given below together with the running times for
all other variants.77
6.1. Nearest Neighbor Heuristics
35
30
25
20
15
10
5
0
0
500
1000
1500
2000
2500
Figure 6.4 Search depth for pr2392
2000
1500
1000
500
0
0
500
1000
1500
2000
Figure 6.5 Number of examined nodes for pr2392
250078
Chapter 6. Construction Heuristics
6.1.3 Precomputed Neighbors
Suppose we have computed a candidate subgraph representing â€œreasonableâ€ connections,
e.g., the k nearest neighbor subgraph. A speed up of the nearest neighbor procedure
is then possible if we first look for nearest neighbors of a node within its adjacent
nodes in the subgraph. This way we reduce the exhaustive neighbor search and the
necessary costly distance computations. If all nodes adjacent in the subgraph are already
contained in the partial tour then we compute the nearest neighbor among all free nodes.
This modification does not improve worst case time complexity but should be faster
when run on practical problems.
Note, that even if the subgraph is obtained by computing the k nearest neighbors this
modified routine and the standard routine will usually come up with different results.
This is due to the fact that we may proceed from the current node l to a node j which
is not its nearest neighbor among all free nodes. This can occur, if edge {l, j} is in the
candidate set and l is among the k nearest neighbors of j, but j is not among the k
nearest neighbors of l.
6.1.4 Neighbors of Predecessors
In this modification we also use a precomputed set of candidate edges but apply the
following variant for the neighbor search. If all nodes adjacent to the current node in
the subgraph are already contained in the partial tour then we look for free neighbors
(in the candidate subgraph) of the predecessor of the current node. If this fails too,
we go back to the predecessor of the predecessor, etc. The backtrack to a predecessor
is only done a limited number of times, say 20 times, because then free neighbors are
usually already far away from the current node and it should be preferrable to look for
the exact nearest neighbor. Again, worst case time complexity is not affected, but a
significant speed up should be possible.
6.1.5 Insertion of Forgotten Nodes
As the main problem with nearest neighbor heuristics is that in the end nodes have to
be connected at high cost, we try to avoid that nodes become isolated. To do this we
first compute the degree of each node in the chosen candidate subgraph. (Without this
subgraph the procedure would not make sense.)
Whenever a node is connected to the current partial tour we decrease the degrees of its
adjacent nodes (in the subgraph) by 1. If the degree of a free node is decreased below
a specified limit (e.g., 2 or 3) this way, we insert that node immediately into the path.
To this end we look for the best insertion point before or after one of its neighbors in
the candidate subgraph. This way more nodes of degree less than or equal to the given
limit may occur which are also inserted rightaway. The selection of the next nodes in
the algorithm is accomplished as in variant 6.1.3. The worst case time complexity is
still quadratic.6.1. Nearest Neighbor Heuristics
79
6.1.6 Using Rotation Operations
The idea of this heuristic is to try to grow the tour within the candidate subgraph. If
at the current node the tour cannot be extended using candidate edges it is tried to
perform a sequence of rotation operations. Such an operation introduces an unused
subgraph edge from the current node. Since this results in a cycle with the partial tour,
another edge has to be eliminated and we obtain a new last node from which we can
try to extend the path.
k
i
l
Figure 6.6 A rotation operation
Figure 6.6 depicts a rotation operation. If at the current last node l the path cannot
be extended within the subgraph we try to use the subgraph edge {l, k}. To break the
resulting cycle we delete the edge {k, i} and try to extend the path now starting at i.
A sequence of rotations can be performed if also the extension from i fails. If the tour
cannot be extended this way using only subgraph edges a neighbor search like in one of
the previous variants can be performed.
6.1.7 Comparison of Variants
We compare the variants with the standard heuristic implemented according to 6.1.2.
It is clear that the tours produced by the algorithms heavily depend on the choice of
the starting point. Since we cannot look for the best starting point we always chose
to start with node  n 2 , thus giving an unbiased starting node to the heuristics. The
chosen candidate subgraph was the 10 nearest neighbor subgraph for variants 6.1.3
through 6.1.6.
In variant 6.1.4 we examined at most 20 predecessors to extend the path. In variant 6.1.5
forgotten nodes were inserted as soon as they were merely connected to at most three
free nodes. In variant 6.1.6 a sequence of rotations was limited to be composed of at
most five single rotations.
Table 6.7 shows the tour lengths (given as deviation in percent from the best known
lower bounds) obtained by applying the different procedures to our set of test problems.
Variants 6.1.2 through 6.1.6 are denoted by Variant 1 through Variant 5 in this table.
The best solution found for every problem instance is marked with a â€˜*â€™.
The results strongly support variant 6.1.5 which avoids adding too many isolated nodes
in the end. Usually, this decreases the tour length considerably. The quality of the
solutions can be expected to be in the range of 15% to 25% above optimality. In
Johnson (1990) an average excess of 24% over an approximation of the Held-Karp
lower bound (see Chapter 10) is reported for randomly generated problems.80
Chapter 6. Construction Heuristics
Problem
d198
lin318
fl417
pcb442
u574
p654
rat783
pr1002
u1060
pcb1173
d1291
rl1323
fl1400
u1432
fl1577
d1655
vm1748
rl1889
u2152
pr2392
pcb3038
fl3795
fnl4461
rl5934
Average
Variant 1
25.79
26.85
21.28
21.36
29.60
31.02
27.13
24.35
30.43
28.18
22.97
22.30
42.42
25.50
27.65
25.99
25.67
28.37
25.80
24.96
23.63
24.44
25.31
22.93
26.27
Variant 2
16.51*
22.52
17.84*
22.91
21.11
18.75*
24.76
25.18
27.14
27.69
15.44*
21.94
42.96
28.04
20.34*
25.06
26.78
25.54
25.85
27.23
23.53
25.92
23.24
23.41
26.58
Variant 3
25.86
33.06
36.42
18.63
25.90
25.80
27.98
28.96
27.33
27.30
25.54
25.51
30.64
31.55
21.97
20.82
31.90
23.85*
23.22
26.09
28.39
32.85
26.98
24.77
27.04
Variant 4
20.88
15.90*
25.36
13.51*
18.67*
25.98
18.86*
18.16*
24.14*
18.09*
17.33
16.81*
30.23*
19.21*
23.30
19.81*
19.11*
25.62
18.97*
22.68*
19.16*
20.09*
19.53*
18.75*
21.45
Variant 5
28.09
27.84
31.80
27.15
29.16
24.43
27.02
27.97
29.42
29.85
16.09
25.10
48.39
25.71
21.55
23.38
31.89
25.41
23.14
26.20
26.92
35.24
24.99
24.38
28.28
Table 6.7 Results of nearest neighbor variants
1.2
1.0
0.8
0.6
0.4
0.2
0.0
0
1000
2000
[S]
[1]
3000
[2]
4000
[3]
[4]
5000
[5]
Figure 6.8 CPU times for nearest neighbor variants
600081
6.1. Nearest Neighbor Heuristics
CPU times for the complete set of instances are shown in Figure 6.8. The running
times for the variants do not include the time to set up the Delaunay graph or the 10
nearest neighbor subgraph. These times were given in Chapters 4 and 5. To document
the speed up obtained by using the different variants we have also included the running
time for the standard implementation. Variant i is indicated by the number [i], the
standard implementation is indicated by [S].
Figure 6.8 clearly visualizes that already simple algorithms of quadratic time complexity
take quite some time when applied to large problems. The comparison of the variants
gives a clear picture. All the variants are much faster than the standard nearest neighbor
algorithm (even if the preprocessing times would be included). When considering larger
random problems (results are not displayed here), variants 2, 3, and 4 seem to exhibit
a quadratic component in their running time while variants 1 and 5 seem to have
subquadratic running times.
6.1.8 Stability of Nearest Neighbor Heuristics
Since we have performed only one run of each heuristic for every sample problem (start-
ing with node  n 2 ) we cannot be absolutely sure that Table 6.7 gives a correct assessment
of the five heuristics. We have therefore examined the average quality of each variant for
three sample problems. To this end we have performed each heuristic for every starting
node l = 1, 2, . . . , n.
Table 6.9 shows the results. Each line corresponds to one variant and gives (in that
sequence) the length of the best, resp. worst tour, the average tour length obtained, the
span between best and worst tour (i.e., worst quality âˆ’ best quality), and the standard
deviation.
Variant
lin318
1
2
3
4
5
pcb442
1
2
3
4
5
u1060
1
2
3
4
5
Minimum Maximum Average Span Deviation
16.89
18.58
20.86
12.96
21.54 39.82
35.96
37.66
29.24
36.94 24.89
25.88
27.12
20.25
29.37 22.92
17.38
16.80
16.28
15.40 2.99
2.96
3.65
3.00
3.07
18.01
16.60
17.19
11.77
15.67 38.15
32.23
34.31
30.03
36.30 29.66
22.11
26.06
16.63
25.81 20.14
15.63
17.12
18.27
20.63 3.63
2.84
3.44
2.74
3.26
22.33
21.25
24.18
19.04
24.98 36.51
39.19
37.99
28.46
37.44 26.11
30.05
28.89
22.95
30.03 14.17
17.94
13.81
9.42
12.46 1.93
3.45
2.21
1.70
2.29
Table 6.9 Sensitivity analysis for nearest neighbor variants
The results verify that insertion of forgotten neighbors leads to the best results. The
average quality of the tours obtained this way is substantially better than for the other82
Chapter 6. Construction Heuristics
four variants. The other variants perform more or less the same. The span is consider-
able, the quality of the tours strongly depends on the choice of the starting node.
6.2 Insertion Heuristics
A further intuitive approach is to start with tours on small subsets (including trivial
â€œtoursâ€ on one or two nodes) and then extend these tours by inserting the remaining
nodes. This principle is realized by the following procedure.
procedure insertion
(1) Select a starting tour through k nodes v 1 , v 2 , . . . , v k (k â‰¥ 1) and set W = V \
{v 1 , v 2 , . . . , v k }.
(2) As long as W  = âˆ… do the following.
(2.1) Select a node j âˆˆ W according to some criterion.
(2.2) Insert j at some position in the tour and set W = W \ {j}.
end of insertion
Using this principle a tour is built containing more and more nodes of the problem until
all nodes are inserted and the final Hamiltonian tour is found.
6.2.1 Standard Versions
Of course, there are several possibilities for implementing such an insertion scheme. The
main difference is the determination of the order in which the nodes are inserted. The
starting tour is usually just some tour on three nodes or, an edge (k = 2), or even a
loop containing only one node (k = 1). We will consider also another type of starting
tour below. The selected node to be inserted is usually inserted into the tour at the
point causing shortest increase in the length of the tour.
We say that a node is a tour node if it is already contained in the partial tour. For
j âˆˆ W 
we define d min (j) = min{c ij | i âˆˆ V \ W }, d max (j) = max{c ij | i âˆˆ V \ W }, and
s(j) = iâˆˆV \W c ij .
The following possibilities for extending the current tour are considered.
6.2.1.1 Nearest Insertion
Insert the node that has the shortest distance to a tour node, i.e., select j with d min (j) =
min{d min (l) | l âˆˆ W }.
6.2.1.2 Farthest Insertion 1
Insert the node whose minimal distance to a tour node is maximal, i.e., select j with
d min (j) = max{d min (l) | l âˆˆ W }.
6.2.1.3 Farthest Insertion 2
Insert the node that has the farthest distance to a tour node, i.e., select j with d max (j) =
max{d max (l) | l âˆˆ W }.6.2. Insertion Heuristics
83
6.2.1.4 Farthest Insertion 3
Insert the node whose maximal distance to a tour node is minimal, i.e., select j with
d max (j) = min{d max (l) | l âˆˆ W }.
6.2.1.5 Cheapest Insertion 1
Among all nodes not inserted so far, choose a node whose insertion causes the lowest
increase in the length of the tour. I.e., among all nodes not inserted so far, choose a
node which can be inserted causing the lowest increase in the length of the tour.
6.2.1.6 Cheapest Insertion 2
In the cheapest insertion heuristic, we have to know for every node not in the tour its
cheapest insertion point. Update of this information is expensive (see below). In this
variant, we only perform a partial update of the best insertion points in the following
sense. Suppose node j has just been inserted into the partial tour. This may have the
effect that the best insertion point changes for a non-tour node, say l. Now, we do not
consider all possibilities to insert l, but only insertions before or after j and a limited
number k of jâ€™s successors and predecessors. This has the consequence, that, for some
nodes, not necessarily the best insertion point is determined.
6.2.1.7 Random Insertion
Select the node to be inserted at random.
6.2.1.8 Largest Sum Insertion
Insert the node whose sum of distances to tour nodes is maximal, i.e., select j with
s(j) = max{s(l) | l âˆˆ W }. This is equivalent to choosing the node with maximal
average distance to tour nodes.
6.2.1.9 Smallest Sum Insertion
Insert the node whose sum of distances to tour nodes is minimal, i.e., select j with
s(j) = min{s(l) | l âˆˆ W }. This is equivalent to choosing the node with minimal average
distance to tour nodes.
There are also variants of these ideas where the node selected is not inserted at cheapest
insertion cost but as a neighbor of that tour node that is nearest to it. These variants
are usually named â€œadditionâ€ instead of insertion. Bentley (1992) reports that the
results are slightly inferior.
All heuristics except for cheapest Insertion have running time O(n 2 ). Cheapest Insertion
can be implemented to be executed in time O(n 2 log n) by storing for each external
node a heap based on the insertion cost at the possible insertion points. Because this
procedure requires O(n 2 ) space it cannot be used for large problem instances. The fast
version of cheapest insertion runs in time O(n 2 ) because of the limited update.
We give an illustration of insertion principles in Figure 6.10 for a Euclidean problem
instance. In the next step nearest insertion adds node i, farthest insertion adds node j,
and cheapest insertion adds node k to the tour.84
Chapter 6. Construction Heuristics
j
i
k
Figure 6.10 Illustration of insertion heuristics
Nearest insertion and cheapest insertion tours are less than twice as long as an optimal
tour if the triangle inequality holds (Rosenkrantz, Stearns & Lewis (1977)). It
can also be shown that there exist classes of problem instances for which the length of
the heuristic solution is 2 âˆ’ n 2 times longer than the optimal tour, thus proving that
these approximation results are tight.
Recently it was shown (Hurkens (1991)) that for random or farthest insertion there
exist examples where these heuristics yield tours that are 13/2 times longer than an
optimal tour (although the triangle inequality is satisfied).
We have compared the nine insertion heuristics for our set of sample problems. Each
heuristic was started with the cycle ( n 2 ,  n 3 ,  n 4 ) to get unbiased starting conditions.
For the variant of cheapest insertion described in 6.2.1.6 we have set k = 30.
Table 6.11 displays the results (headings 1 through 9 corresponding to the insertion
heuristics 6.2.1.1 through 6.2.1.9). The best solution in each row is marked with a â€˜*â€™.
Farthest insertion 1 performs best for our set of problems followed closely by random
insertion. The fast version of cheapest insertion performs as well as the full version, the
time for doing correct cheapest insertion does not pay off. In fact, the results were the
same except for two cases. However, though reasonable at first sight, cheapest insertion
performs significantly worse that farthest insertion. The relatively good performance of
farthest insertion can be explained when observing the development of the generated
tour: after few steps already, a good global outline of the final tour is obtained. Almost
the same is true for random insertion. An average excess over the Held-Karp bound
of 27% for the nearest insertion and of 13.5% for the farthest insertion procedure is
reported in Johnson (1990) for random problem instances.85
6.2. Insertion Heuristics
Problem
d198
lin318
fl417
pcb442
u574
p654
rat783
pr1002
u1060
pcb1173
d1291
rl1323
fl1400
u1432
fl1577
d1655
vm1748
rl1889
u2152
pr2392
pcb3038
fl3795
fnl4461
rl5934
Average
1
13.19
21.62
12.50
20.89
22.33
10.81
23.04
18.57
21.39
25.84
22.90
31.01
20.28
15.26
21.61
20.18
21.26
23.82
21.09
24.70
23.12
19.61
21.10
27.40
20.98
2
3.85*
10.87
5.48
13.83
11.39*
6.89
12.09*
10.85*
12.68
14.22*
23.78
18.89*
8.45*
12.59*
15.17*
17.09*
13.54*
19.10
19.55
14.32*
14.89*
21.97
12.03*
22.17
13.99
3
7.57
18.41
13.37
16.99
22.68
11.33
22.52
24.81
21.52
26.82
27.29
29.30
14.56
20.43
20.04
22.22
25.37
27.74
28.64
28.26
24.54
19.58
27.69
30.12
22.16
4
14.80
24.30
13.37
29.06
26.32
5.94
28.72
27.24
27.55
32.67
29.50
27.80
24.78
20.08
25.21
27.80
33.59
32.70
32.84
33.55
27.84
29.45
28.90
33.42
26.56
5
11.08
18.39
12.39
21.15
19.12
5.79*
16.02
16.61
18.67
21.50
17.01*
24.81
17.98
12.65
17.08
18.83
18.86
21.24
16.12*
20.50
17.08
12.79*
15.97
21.84*
17.23
6
11.08
18.39
12.39
21.15
19.12
5.79*
16.02
16.61
18.67
21.50
17.01*
24.81
17.76
12.65
17.08
18.77
18.86
21.24
16.12*
20.50
17.08
12.79*
15.97
21.84*
17.22
7
8.17
9.18*
3.29*
12.23*
11.64
9.87
13.37
12.50
11.43*
16.58
22.13
20.64
8.47
12.63
18.70
17.69
13.87
17.30*
19.76
16.65
16.69
19.77
12.99
22.71
14.51
8
8.15
20.02
7.84
27.07
23.32
11.30
26.37
23.98
23.94
29.56
31.06
29.30
16.30
23.84
26.66
28.20
29.52
29.99
28.26
31.75
27.57
21.62
28.99
33.56
24.51
9
7.78
16.27
9.04
20.43
22.21
12.64
25.02
25.42
21.58
28.80
18.70
26.56
16.44
20.54
17.97
23.95
24.26
27.53
28.98
28.32
27.28
25.62
28.03
30.36
22.24
Table 6.11 Results of insertion heuristics
Note that the quality of the solutions of the different heuristics is highly problem de-
pendent. Running time will be addressed in the next section.
6.2.2 Fast Versions of Insertion Heuristics
As in the case of the nearest neighbor heuristic we want to give priority to edges from
a candidate set to speed up the insertion heuristics.
To this end we base all our calculations on the edges contained in the candidate set.
E.g., now the distance of a non-tour node v to the current partial tour is infinite if there
is no candidate edge joining v to the tour, otherwise it is the length of the shortest such
edge joining v to the tour. Using this principle the heuristics of the previous chapter
are modified as follows.
6.2.2.1 Nearest Insertion
If there are nodes connected to the current tour by a subgraph edge then insert the node
connected to the tour by the shortest edge. Otherwise insert an arbitrary node.
6.2.2.2 Farthest Insertion 1
Among the nodes that are connected to the tour insert the one whose minimal distance
to the tour is maximal. If all external nodes are not connected to the tour insert an
arbitrary node.86
Chapter 6. Construction Heuristics
6.2.2.3 Farthest Insertion 2
Among the nodes that are connected to the tour insert the one whose distance to the
tour is maximal. If all external nodes are not connected to the tour insert an arbitrary
node.
6.2.2.4 Farthest Insertion 3
Among the nodes that are connected to the tour insert the one whose maximal distance
to the tour is minimal. If all external nodes are not connected to the tour insert an
arbitrary node.
6.2.2.5 Cheapest Insertion 1
Insert a node connected to the current tour by a subgraph edge whose insertion yields
minimal additional length. If no such node exists then compute the cheapest insertion
possibility. Insertion information is only updated for those nodes that are connected to
the node inserted last by a subgraph edge. This way insertion information may become
incorrect for some nodes since it may not be updated.
6.2.2.6 Cheapest Insertion 2
Insert a node connected to the current tour by a subgraph edge whose insertion yields
minimal additional length. If no such node exists then insert an arbitrary node. Update
of insertion information is further simplified as in 6.2.1.6.
6.2.2.7 Random Insertion
Select the node to be inserted at random where priority is given to nodes connected to
the current tour by a subgraph edge.
6.2.2.8 Largest Sum Insertion
For each node compute the sum of lengths of the subgraph edges connecting this node
to the current tour. Insert the node whose sum is maximal. If all external nodes are
not connected to the tour insert an arbitrary node.
6.2.2.9 Smallest Sum Insertion
For each node compute the sum of lengths of the subgraph edges connecting this node
to the current tour. Insert the node whose sum is minimal. If all external nodes are not
connected to the tour insert an arbitrary node.
We have performed the same experiment as for the heuristics in the complete graph.
Results are shown in Table 6.12. Now, the advantages of farthest or random insertion are
lost due to the restricted view. They still perform best but tour quality is significantly
inferior than before. The cheapest insertion variants give some very bad solutions which
is caused by the incomplete update of insertion information.
To visualize the CPU time for insertion heuristics we have compared five variants in
Figure 6.13: Farthest insertion 6.2.1.2 ([1]), Cheapest insertion 6.2.1.5 ([2]), Cheapest
insertion 6.2.1.6 ([3]), Farthest insertion 6.2.2.2 ([4]), Cheapest insertion 6.2.2.5 ([5]).
The diagram shows that standard farthest insertion compares favorably with all cheapest
insertion variants. Speed up using candidate graphs is considerable, but due to inferior
quality there seems to be no point in using these heuristics. This will be further justified
in Chapter 7.87
6.2. Insertion Heuristics
Problem
d198
lin318
fl417
pcb442
u574
p654
rat783
pr1002
u1060
pcb1173
d1291
rl1323
fl1400
u1432
fl1577
d1655
vm1748
rl1889
u2152
pr2392
pcb3038
fl3795
fnl4461
rl5934
Average
1
15.31
25.69
36.20
28.85
22.54
48.59
26.07
19.89*
25.39
28.93
31.24
37.34
30.83
21.61
34.75
28.95
26.05
35.45
28.99
27.01
25.19
35.77
23.47
44.63
29.53
2
7.84*
20.03
31.08
18.59
17.34*
46.22
15.35*
21.30
17.54*
19.28*
25.33
22.46*
31.66
17.81
27.27
23.22
21.07*
25.60*
24.68
23.14*
18.48*
24.96*
16.88*
31.26*
22.85
3
9.41
18.85
33.92
20.16
19.97
36.84*
18.31
29.43
20.42
21.60
26.61
26.82
28.69
20.29
28.95
23.74
21.82
29.58
28.89
27.88
21.47
29.32
17.23
29.81
24.58
4
13.67
23.78
44.57
19.93
18.60
42.62
20.00
20.37
20.78
21.87
27.29
32.97
29.67
20.27
28.19
26.05
23.34
30.32
24.46
28.22
19.67
30.18
20.27
35.55
25.94
5
13.47
42.95
24.36*
29.66
25.28
78.21
24.90
26.54
22.95
34.27
20.91
31.19
85.17
28.08
31.09
33.35
22.90
42.91
21.34*
35.15
25.61
40.31
31.74
51.60
34.33
6
13.47
42.95
24.36*
29.66
25.28
78.81
24.90
26.50
24.07
34.27
20.73*
31.43
94.98
29.89
31.09
35.48
22.90
42.39
21.34*
32.68
25.72
40.62
36.16
48.17
34.91
7
10.42
18.09
26.82
20.07
17.53
49.36
17.47
22.52
18.52
21.84
24.78
26.04
19.07*
20.25
23.67*
22.40*
22.27
31.51
25.03
24.56
20.05
25.80
17.64
32.91
23.28
8
13.80
17.63*
38.90
14.08*
19.02
44.91
16.11
20.74
19.97
22.42
26.81
31.16
27.06
16.51*
29.51
24.38
21.20
28.60
25.06
24.28
20.00
33.85
18.11
33.31
24.48
9
11.13
24.41
31.99
27.33
26.72
54.21
29.58
28.17
25.55
28.86
28.16
35.37
30.59
25.52
36.09
29.23
29.31
35.12
30.82
31.41
28.57
32.41
28.51
37.97
30.29
Table 6.12 Results of fast insertion heuristics
300
250
200
150
100
50
0
0
1000
2000
[1]
3000
[2]
[3]
4000
[4]
5000
[5]
Figure 6.13 CPU times for some insertion heuristics
600088
Chapter 6. Construction Heuristics
6.2.3 Convex Hull Start
The following observation suggests to use a specific starting tour for Euclidean problems.
Let v 1 , v 2 , . . . , v k be located on the boundary of the convex hull of the given points (in
this order). Then, in any optimal tour, this sequence is respected (otherwise the tour
would contain crossing edges and hence could not be optimal). Therefore it is reasonable
to use (v 1 , v 2 , . . . , v k ) as starting tour for the insertion heuristics.
From the results of Chapter 4 we know that convex hulls can be computed very quickly
(in time Î˜(n log n)). Therefore, only negligible additional CPU time is necessary to
compute this type of starting tour for the insertion heuristics in the Euclidean case.
Results with the convex hull start using the standard versions of the insertion heuristics
are displayed in Table 6.14.
Problem
d198
lin318
fl417
pcb442
u574
p654
rat783
pr1002
u1060
pcb1173
d1291
rl1323
fl1400
u1432
fl1577
d1655
vm1748
rl1889
u2152
pr2392
pcb3038
fl3795
fnl4461
rl5934
Average
1
12.86
15.08
14.24
16.52
17.24
17.07
16.90
20.05
22.78
21.61
25.58
25.86
14.04
15.34
20.30
20.94
19.83
25.74
19.03
21.26
22.41
24.06
22.21
26.54
19.90
2
6.73
10.82
5.28
10.37*
9.95*
3.05*
12.72
11.10*
10.69
15.44*
21.80
15.10*
5.79*
12.65
15.18*
15.05
10.77*
17.98*
18.26
15.24*
14.31*
21.60
11.94*
20.28*
13.00
3
6.41
19.70
14.95
17.54
23.55
12.40
24.68
25.65
24.71
26.14
25.52
28.57
14.62
21.06
18.72
21.55
25.31
29.40
29.05
28.88
25.57
16.23
29.49
30.27
22.50
4
6.58
16.82
5.65
18.89
18.47
6.38
23.89
21.66
22.79
26.62
26.22
25.74
12.05
18.73
28.25
27.26
24.04
31.63
27.32
27.07
24.62
27.65
27.94
31.89
22.01
5
8.51
11.42
7.61
11.83
14.67
8.15
15.16
14.23
16.65
19.18
14.69*
20.30
9.73
11.73*
18.09
13.23*
16.94
18.72
13.98*
17.52
16.47
13.81*
15.42
21.24
14.55
6
8.51
11.42
7.61
11.83
14.67
8.15
15.16
14.23
16.65
19.18
14.69*
20.30
9.73
11.73*
18.09
13.23*
16.94
18.72
13.98*
17.52
16.47
13.81*
15.42
21.24
14.55
7
4.37*
7.97*
2.77 *
13.62
10.73
6.49
11.90*
13.27
10.39*
18.25
21.03
20.18
8.35
13.19
15.58
15.99
12.03
18.15
19.73
15.83
15.44
18.35
13.07
21.71
13.68
8
7.03
18.01
6.83
19.17
19.83
13.16
22.31
26.71
23.99
28.45
22.06
27.73
13.69
22.48
37.73
26.67
26.52
30.07
27.73
29.87
25.81
25.40
29.14
29.66
23.34
9
7.34
16.39
8.36
22.71
20.62
10.85
23.80
21.76
22.65
26.35
22.37
27.59
17.90
21.66
24.71
24.03
25.48
27.26
27.05
26.18
27.12
19.04
27.67
29.58
22.02
Table 6.14 Results of insertion heuristics with convex hull start
There is a slight improvement in the quality of tours with respect to the starting tour
( n 2 ,  n 3 ,  n 4 ). Farthest and random insertion do not profit very much from the convex
hull start since they generate good globals tours themselves. For the other heuristics,
this starting variant is more important, but still leading to poor final tours.89
6.3. Heuristics Using Spanning Trees
6.2.4 Stability of Insertion Heuristics
As in the case of the nearest neighbor heuristic we also investigated how strongly our
results depend on the choice of the starting tour.
To get an impression of this, we performed experiments for problems d198, lin318, and
pcb442. Each heuristic was started with all possible tours consisting of just two nodes.
Numbers displayed in Table 6.15 have the same meaning as in Table 6.9 for the nearest
neighbor methods.
Variant
d198
1
2
3
4
5
6
7
8
9
lin318
1
2
3
4
5
6
7
8
9
pcb442
1
2
3
4
5
6
7
8
9
Minimum Maximum Average Span Deviation
8.52
1.53
4.75
6.95
7.57
8.25
2.63
5.87
3.83 17.78
10.72
16.90
17.00
15.65
14.18
8.86
19.99
13.24 12.59
6.08
7.70
12.14
10.95
11.01
5.53
11.55
7.75 9.25
9.18
12.15
10.05
8.08
5.93
6.23
14.13
9.41 1.70
2.11
2.07
1.77
1.40
1.30
1.10
3.56
1.73
17.18
5.47
12.93
20.13
13.81
13.81
6.67
16.48
11.99 25.97
13.24
22.38
29.30
22.15
22.15
14.18
26.60
23.34 22.00
9.00
18.72
24.92
18.61
18.69
10.89
21.78
18.92 8.79
7.77
9.45
9.17
8.35
8.35
7.51
10.12
11.35 1.82
1.50
1.30
1.73
1.46
1.47
1.65
1.69
1.86
13.89
9.05
14.93
21.78
12.73
12.73
10.22
20.13
14.22 23.17
18.03
23.99
33.13
21.38
21.55
17.71
32.93
23.64 18.50
13.00
18.45
27.50
17.76
17.80
14.00
25.30
18.56 9.29
8.97
9.06
11.36
8.66
8.82
7.48
12.80
9.41 1.85
1.49
1.10
1.99
1.47
1.54
1.44
2.44
1.52
Table 6.15 Sensitivity analysis for insertion heuristics
Farthest insertion and random insertion also performed best here. Stability of insertion
heuristics is much better than for the nearest neighbor variants. The table also shows
that performance and stability are highly problem dependent.
6.3 Heuristics Using Spanning Trees
The heuristics considered so far construct in their standard versions the tours â€œfrom
scratchâ€ in the sense that they do not exploit any additional knowledge about the90
Chapter 6. Construction Heuristics
problem instance. In their fast variants they used the presence of a candidate subgraph
to guide the tour construction.
The two heuristics to be described next use a minimum spanning tree as a basis for
generating tours. They are particularly suited for problem instances obeying the triangle
inequality. In this case performance guarantees are possible. Nevertheless, in principle
they can also be applied to general instances.
Before describing these heuristics, consider the following observation. Suppose we are
given some Eulerian tour containing all nodes of the given problem instance. If the
triangle inequality is satisfied we can derive a Hamiltonian tour which is not longer
than the Eulerian tour.
Let v i 0 , v i 1 , . . . , v i k be the sequence in which the nodes (including repetitions) are visited
when traversing the Eulerian tour starting at v i 0 and returning to v i k = v i 0 . The
following procedure obtains a Hamiltonian tour.
procedure obtain tour
(1) Set Q = {v i 0 }, T = âˆ…, v = v i 0 , and l = 1.
(2) As long as |Q| < n perform the following steps.
(2.1) If v i l âˆˆ Q then set Q = Q âˆª {v i l }, T = T âˆª {vv i l }, and v = v i l .
(2.2) Set l = l + 1.
(3) Set T = T âˆª {vv i 0 }.
(4) T is a Hamiltonian tour.
end of obtain tour
Every connection made in this procedure is either an edge of the Eulerian tour or is
a shortcut replacing a subpath of the Eulerian tour by an edge connecting its two
endnodes. This shortcut reduces the length of the tour if the triangle inequality is
satisfied. Hence the resulting Hamiltonian tour cannot be longer than the Eulerian
tour.
Both heuristics start with a minimum spanning tree and differ only in how a Eulerian
graph is generated from the tree.
procedure doubletree
(1) Compute a minimum spanning tree.
(2) Double all edges of the tree to obtain a Eulerian graph.
(3) Compute a Eulerian tour in this graph.
(4) Call obtain tour to get a Hamiltonian tour.
end of doubletree6.3. Heuristics Using Spanning Trees
91
Note that we get multiple edges in Step (2) which we have not allowed in our definition
of graphs in Chapter 2. But it is clear that this does not create any problems. Our graph
data structure is able to handle multiple edges. The running time of the algorithm is
dominated by the time needed to obtain a minimum spanning tree. Therefore we have
time complexity Î˜(n 2 ) for the general TSP and Î˜(n log n) for Euclidean problems.
If we compute the minimum spanning tree with Primâ€™s algorithm (Prim (1957)), we
could as well construct a Hamiltonian cycle along with the tree computation. We always
keep a cycle on the nodes already in the tree (starting with the loop consisting of only
one node) and insert the node into the current cycle which is added to the spanning
tree. If this node is inserted at the best possible position this algorithm is identical
to the nearest insertion heuristic. If it is inserted before or after its nearest neighbor
among the cycle nodes, then we obtain the nearest addition heuristic.
Christofides (1976) suggested a better method to make spanning trees Eulerian.
Namely, it is sufficient to add a perfect matching on the odd-degree nodes of the tree.
(A perfect matching of a node set W , |W | = 2k, is a set of k edges such that each
node of W is incident to exactly one of these edges.) After addition of all edges of this
perfect matching, all node degrees are even and hence the graph is Eulerian.
Figure 6.16 illustrates this idea. The solid edges form a spanning tree and the broken
edges form a perfect matching on the odd-degree nodes of the spanning tree.
Figure 6.16 Illustration of spanning tree heuristic
The cheapest way (with respect to edge weights) to obtain a Eulerian graph is to add
a minimum weight perfect matching.92
Chapter 6. Construction Heuristics
procedure christofides
(1) Compute a minimum spanning tree.
(2) Compute a minimum weight perfect matching on the odd-degree nodes of the
tree and add it to the tree to obtain a Eulerian graph.
(3) Compute a Eulerian tour in this graph.
(4) Call obtain tour to get a Hamiltonian tour.
end of christofides
This procedure takes considerably more time than the previous one. Computation of
a minimum weight perfect matching on k nodes can be performed in time O(k 3 ) (Ed-
monds (1965)). Since a spanning tree may have O(n) odd-degree nodes, Christofidesâ€™
heuristic has cubic worst case time.
The sequence of the edges in the Eulerian tour is not unique. So one can try to find
better solutions by determining different Eulerian tours. We do not elaborate on this
since the gain to be expected is small.
Since a minimum spanning tree is not longer than a shortest Hamiltonian tour and since
the matching computed in Step (2) of Christofidesâ€™ heuristic has weight at most half of
the length of an optimal tour the following theorem holds.
Theorem 6.2 Let an instance of the TSP obeying the triangle inequality be given.
(i) The double tree heuristic produces a tour which is at most twice as long as an
optimal tour.
(ii) Christofidesâ€™ heuristic produces a tour which is at most 1.5 times as long as an
optimal tour.
There are classes of instances (Cornuejols & Nemhauser (1978)) where the Chri-
stofides heuristic yields a tour that is (3n âˆ’ 1)/(2n) times longer than the optimal tour,
thus proving that the above bound is tight.
Because of the cubic worst case running time, we avoid the computation of minimum
weight matchings and simplify the heuristic as follows. First we double all edges to
leaves, and then we compute a farthest insertion tour on the remaining (and newly
introduced) odd-degree nodes. This tour induces two perfect matchings and we add the
shorter one to our subgraph which is then Eulerian. Time complexity of this procedure
is O(n 2 ).
Table 6.17 compares the two heuristics with respect to CPU time and length of the
generated tours.
The double tree heuristic has a very poor performance. But, also the performance
of the Christofides variant is disappointing (coinciding with the findings in Johnson
(1990)), in particular when taking into account that it has the best worst case bound
among the heuristics. This is not due to our simplification, but it was observed in
many experiments that it does not pay off to compute exact minimum weight perfect
matchings in Step (2).93
6.3. Heuristics Using Spanning Trees
Problem Double tree
d198
22.62
lin318
41.32
fl417
36.04
pcb442
39.64
u574
36.29
p654
36.89
rat783
36.75
pr1002
37.29
u1060
34.30
pcb1173
42.29
d1291
48.16
rl1323
39.04
fl1400
39.40
u1432
45.78
fl1577
42.75
d1655
37.47
vm1748
31.68
rl1889
40.50
u2152
48.11
pr2392
37.22
pcb3038
43.23
fl3795
41.38
fnl4461
39.47
rl5934
48.18
Average
39.41
Christofides
15.67*
18.42*
24.52*
18.59*
20.08*
21.73*
21.34*
20.67*
18.97*
18.77*
24.31*
14.05*
22.10*
24.05*
13.27*
18.92*
21.73*
14.00*
22.73*
18.70*
20.58*
17.25*
21.92*
15.17*
19.48
Table 6.17 Comparison of tree heuristics
2.0
1.5
1.0
0.5
0.0
0
1000
2000
3000
[1]
4000
5000
[2]
Figure 6.18 CPU times for tree heuristics
Universidad Sim Ìon Bol ÌÄ±var
Departamento de Computaci Ìon y Tecnolog ÌÄ±a de la Informaci Ìon
CI-2692 - Laboratorio de Algoritmos y Estructuras II
Trimestre enero-marzo 2021

Proyecto Ayudante Ortogr Ìafico

El objetivo del proyecto es la implementaci Ìon de un ayudante ortogr Ìafico. Esta herramienta carga un
diccionario desde un archivo y adem Ìas el ayudante debe ser capaz de revisar un texto y detectar las palabras
que no se encuentren en el diccionario. A cada una de las estas palabras que no est Ìan en el diccionario, se les
va a buscar las palabras que les sean m Ìas cercanas para presentarlas al usuario como recomendaciones.
1. Preliminares
1.1. Funci Ìon palabra v Ìalida

Consideramos que un elemento de tipo String es una palabra v Ìalida si est Ìa formado por caracteres al-
fab Ìeticos en min Ìusculas, esto es los caracteres entre â€˜aâ€™y â€˜zâ€™, y agregando el car Ìacter e Ìƒne (â€˜ Ìƒnâ€™). Para determinar

si un String es una palabra v Ìalida definimos la funci Ìon esPalabraValida como sigue:
esPalabraValida(s) â‰¡ (âˆ€ i : 0 6 i < len(s) : â€˜aâ€™ 6 s[i] 6 â€˜zâ€™ âˆ¨ s[i] = â€˜~nâ€™)
Se tiene que len una la funci Ìon que retorna la longitud del elemento de tipo String y que s[i] denota el
i- Ìesimo car Ìacter del String s.
1.2. Distancia entre palabras
Para poder determinar la similitud entre dos palabras en necesario contar con un algoritmo que mida
la distancia entre ellas. Para medir la distancia entre dos elementos de tipo String se debe utilizar una
m Ìetrica de comparaci Ìon de cadenas de caracteres. En espec ÌÄ±fico, debe usar la m Ìetrica conocida como la
distancia Levenshtein. En la secci Ìon 3.3.3 del libro [2], se encuentra una descripci Ìon y el algoritmo de la
distancia de Levenshtein. El libro [2] se encuentra disponible en http://nlp.stanford.edu/IR-book/pdf/
irbookonlinereading.pdf
2. Tipos de datos
2.1. TAD Palabras con la misma letra inicial (PMLI)
Este tipo de dato contiene a un conjunto de palabras, las cuales comienzan por una misma letra. A
continuaci Ìon se presenta la especificaci Ìon del TAD.

1

Especificaci Ìon del TAD PMLI
Modelo de Representaci Ìon
var letra : Char
var palabras : Conjunto de Strings
Invariante de Representaci Ìon
(letra = â€˜aâ€™ âˆ¨ letra = â€˜bâ€™ âˆ¨ . . . âˆ¨ letra = â€˜zâ€™ âˆ¨ letra = â€˜~nâ€™) âˆ§
(âˆ€ p)(p âˆˆ palabras â‡’ (p[0] = letra âˆ§ esPalabraValida(p)))
Operaciones
fun crearPMLI (in l : Char ) â†’ PMLI
{ Pre: True }
{ Post: letra = l âˆ§ palabras = âˆ… }
proc agregarPalabra (in p : String )
{ Pre: p[0] = letra }
{ Post: palabras = palabras0 âˆª {p} }
proc eliminarPalabra (in p : String )
{ Pre: p[0] = letra }
{ Post: palabras = palabras0 âˆ’ {p} }
proc mostrarPalabras ( )
{ Pre: True }
{ Post: Muestra por la salida est Ìandar los elementos en palabras en orden lexicogr Ìafico. }
fun buscarPalabra (in p : String ) â†’ Boolean
{ Pre: p[0] = letra }
{ Post: buscarPalabra â‰¡ (p âˆˆ palabras) }
Fin TAD
2.2. TAD Ayudante Ortogr Ìafico
Este TAD posee como principal estructura de datos un arreglo de PMLI de tama Ìƒno 27. Cada casilla del
arreglo corresponde a una letra del alfabeto latino, incluyendo al car Ìacter â€™ Ìƒnâ€™. Esta estructura de datos es
donde usted debe almacenar las palabras del diccionario o diccionarios que va a usar el TAD. A continuaci Ìon
se presenta el modelo de representaci Ìon y el invariante del TAD.
Especificaci Ìon del TAD Ayudante Ortogr Ìafico
Modelo de Representaci Ìon
const MAX : int
var dicc : arreglo de PMLI
Invariante de Representaci Ìon
MAX = 27 âˆ§ #dicc = 27 âˆ§
dicc[0].letra = â€˜aâ€™ âˆ§ dicc[1].letra = â€˜bâ€™ âˆ§ . . . âˆ§ dicc[26].letra = â€˜zâ€™
Operaciones
. . .
Fin TAD
A continuaci Ìon se describen las operaciones operaciones del TAD Ayudante Ortogr Ìafico.

2

2.2.1. crearAyudante
Crea un nuevo TAD Ayudante Ortogr Ìafico en donde la estructura de datos dicc se inicializa, generando
un arreglo, donde se crean 27 instancias del TAD PMLI, una por cada letra del alfabeto.

fun crearAyudante () â†’ AyudanteOrtografico
{ Pre: True }
{ Post: Queda inicializa la estructura dicc
creando las 27 instancias de TAD PMLI, una para cada letra del alfabeto. }

2.2.2. cargarDiccionario
Este procedimiento lee un archivo de texto que contiene las palabras de un diccionario, las cuales van a
ser almacenadas en la estructura de datos dicc. La entrada del procedimiento es el nombre del archivo con
las palabras del diccionario. El formato del archivo de diccionario es el siguiente: solo debe contener una
palabra por l ÌÄ±nea y todas las palabras deben cumplir la definici Ìon de esPalabraValida. Despu Ìes de finalizar
la lectura del archivo y de haber verificado que el mismo tiene un formato v Ìalido, las palabras son cargadas
en el estructura dicc, siguiendo las especificaciones del TAD PMLI. Observe que con este procedimiento es
posible cargar varios archivos con diccionarios.
proc cargarDiccionario (in fname : String )
{ Pre: El archivo fname debe cumplir con el formato preestablecido. }
{ Post: Quedan agregadas en dicc las palabras de fname
cumpliendo la especificaci Ìon del TAD PMLI. }

2.2.3. borrarPalabra
Este procedimiento recibe como entrada una palabra, si la misma se encuentra en dicc la elimina. Si la
palabra no se encuentra en dicc la estructura no sufre ninguna modificaci Ìon.

proc borrarPalabra (in p : String )
{ Pre: esPalabraValida(p) }
{ Post: dicc[x ].palabras = dicc0

[x ].palabras, donde dicc0

[x ].palabras es el conjunto
resultante de aplicar dicc0[x ].eliminarPalabra(p) en la instancia dicc0[x ] del TAD PMLI,
que cumple el invariante de representaci Ìon dicc[x ]0.letra = p[0], para 0 â‰¤ x â‰¤ 26 }

2.2.4. corregirTexto
Este procedimiento recibe como entrada un archivo de texto con palabras a revisar. Este archivo adem Ìas de
palabras v Ìalidas, puede contener palabras inv Ìalidas, espacios en blanco, saltos de l ÌÄ±nea, espacio de tabulaciones
y signos de puntuaci Ìon [1]. El procedimiento corregirTexto debe procesar el archivo de entrada
para extraer  Ìunicamente las palabras v Ìalidas que contenga, ignorando todos los dem Ìas elementos
mencionados anteriormente. Luego debe determinar cuales son las palabras v Ìalidas que no es encuentran en
el diccionario. Despu Ìes para cada una de estas palabras, debe calcular las cuatro palabras en el diccionario
de palabras (estructura dicc), con menor valor de distancia Levenshtein. Las cuatro palabras con menor
distancia, ser Ìan las palabras va a sugerir el corrector ortogr Ìafico. Finalmente debe imprimir los resultados
en el archivo de salida. El archivo de salida debe tener el siguiente formato. Cada palabra v Ìalida que no
se encuentre en el diccionario debe comenzar una l ÌÄ±nea y luego le siguen las cuatros palabras con menor
distancia, separadas por un espacio una coma.

3

proc corregirTexto (in finput : String ; in-out foutput : String)
{ Pre: finput es un archivo de texto v Ìalido }
{ Post: Imprime en el archivo foutput cada una de las palabras v Ìalidas contenidas
en el archivo finput que no se encuentren en dicc, seguidas de las cuatro palabras
con menor distancia. }
2.2.5. imprimirDiccionario
Imprime por la salida est Ìandar la estructura dicc, en un formato que permita entender f Ìacilmente el
contendido de la misma.

proc imprimirDiccionario ()
{ Pre: True }
{ Post: Imprime dicc por la salida est Ìandar mostrando las palabras en orden lexicogr Ìafico }

3. Requerimientos de la implementaci Ìon
El conjunto de Strings del TAD PMLI, llamado palabras, debe ser implementado como una tabla de hash
basada en direccionamiento abierto usando como funci Ìon de hash sondeo lineal. Para el sondeo lineal se debe
usar como funci Ìon de hash interna (h1) la funci Ìon hash de Python. Cuando al insertar un elemento en la tabla
de hash, se tiene un factor de carga igual o mayor a 0.7, entonces se aplica la operaci Ìon rehashing, en donde
se duplica el tama Ìƒno de la tabla. La estructura de datos tabla de hash debe ser creada en una clase llamada
OpenHtable con sus operaciones. El m Ìodulo que contiene esta tabla de hash debe llamarse open htable.py.
La estructura dicc del TAD Ayudante Ortogr Ìafico, debe ser implementada como una lista de Python.
Una vez creada la estructura dicc, se deben agregar las 27 instancias del TAD PMLI. Luego esta estructura
no cambia de tama Ìƒno.
Debe implementar una aplicaci Ìon cliente llamada prueba ortografia.py, que al ejecutarla, proporciona
al usuario de un men Ìu simple que permite llevar a cabo todas las operaciones del TAD Ayudante Ortogr Ìafico.
Al iniciar el cliente, entrar Ìa en una iteraci Ìon de men Ìu con las siguiente 6 opciones:
1. Crear un nuevo ayudante ortogr Ìafico.
2. Cargar un diccionario.
3. Eliminar palabra.
4. Corregir texto.
5. Mostrar diccionario.
6. Salir de la aplicaci Ìon.
Este men Ìu se mostrar Ìa por la salida est Ìandar y el usuario terminar Ìa la ejecuci Ìon del cliente cuando
seleccione la opci Ìon 6 â€œSalir de la aplicaci Ìonâ€. Si alguna de las precondiciones de los procedimientos de
los TADs no se cumple, entonces se le debe indicar al usuario que la operaci Ìon no pudo ser efectuada porque
no se cumple la precondici Ìon, indicando cual es la precondici Ìon.
Cada uno de los TADs debe ser implementado como una clase de Python. Debe hacer entrega de por lo
menos cuatro archivos:
pmli.py: Implementaci Ìon del TAD PMLI.
ayudante ortografico.py: Implementaci Ìon del TAD Ayudante Ortogr Ìafico.
open htable.py: Tabla se hash basada en direccionamiento abierto y con sondeo lineal.

4

prueba ortografia.py: Cliente que permite interactuar con el TAD Ayudante Ortogr Ìafico.
Como los TADs se deben a implementar como clases de Python, la funci Ìon crearPMLI del TAD PMLI
va a corresponder al constructor de la clase, por lo no debe ser implementado como un m Ìetodo aparte. De
la misma forma la funci Ìon crearAyudante corresponde al constructor de la clase que implementa al TAD
Ayudante Ortogr Ìafico, por lo que no debe ser creada como un m Ìetodo nuevo.
El c Ìodigo debe estar debidamente documentado y cada uno de los m Ìetodos de los TADs deben tener los

siguientes elementos: descripci Ìon, descripci Ìon de los par Ìametros, efectos secundarios, precondici Ìon y postcon-
dici Ìon. Adem Ìas debe hacer uso de la gu ÌÄ±a de estilo de Python.

4. Condiciones de entrega
Debe entregar los c Ìodigos fuentes en un archivo comprimido llamado Proy2ci2692em21-X.tar.xz donde
X es el n Ìumero de carn Ìe del estudiante autor del proyecto. La entrega se realizar Ìa por medio de la plataforma
Classroom antes de las 8:00 am del viernes 16 de abril de 2021.
Referencias
[1] Colaboradores de Wikipedia. Signos de puntuaci Ìon â€” Wikipedia, la enciclopedia libre, 2021. [Online;
revisado el 25-marzo-2021].
[2] Manning, C., Raghavan, P., and Schutze, H.  Ìˆ Introduction to Information Retrieval. Cambridge
University Press, 2009.

Guillermo Palma / gvpalma@usb.ve / Marzo 2021
Sea S una superficie regular orientada y Ï† : U âŠ‚ R

2 â†’ S una parametrizaci Ìon en
la orientaci Ìon de S. A cada punto de Ï†(U) le podemos asignar un triedro formado
por los vectores Ï†u, Ï†v y N. Expresando las derivadas de estos vectores en la base
{Ï†u, Ï†v, N} obtenemos:
Ï†uu = Î“1
11 Ï†u + Î“2
11 Ï†v + L1 N

Ï†uv = Î“1
12 Ï†u + Î“2
12 Ï†v + L2 N
Definici Ìon. Sea Ï‰ un campo vectorial diferenciable sobre un abierto U âŠ‚ S y
p âˆˆ U. Sea y âˆˆ TpS. Consideremos una curva parametrizada Î± : (âˆ’, ) â†’ U
con Î±(0) = p y Î±
0
(0) = y, y sea Ï‰(t) con t âˆˆ (âˆ’, ) la restricci Ìon del campo
vectorial Ï‰ a la curva Î±. El vector que se obtiene tomando la proyecci Ìon normal
del vector
dÏ‰
dt

(0) sobre el plano TpS se llama derivada covariante en p del
campo vectorial Ï‰ relativa al vector y. Denotaremos esta derivada covariante por
DÏ‰
dt

(0) o
DyÏ‰

(p).

Veamos que la definici Ìon anterior no depende de la escogencia de la curva Î±.
Sea Ï†(u, v) una parametrizaci Ìon de S en p; escribamos Ï†
u(t), v(t)

= Î±(t) y

Ï‰(t) = a
u(t), v(t)

Ï†u + b
u(t), v(t)

Ï†v = a(t) Ï†u + b(t) Ï†v. Entonces,

dÏ‰
dt
= a
Ï†uu u
0 + Ï†uv v
0

+ b
Ï†vu u
0 + Ï†vv v
0

= a
0 Ï†u + b
0 Ï†v.

Como DÏ‰
dt
es la componente de dÏ‰
dt
sobre el plano tangente, usamos las expresiones
que aparecen en (??) para Ï†uu, Ï†uv y Ï†vv, y simplemente quitamos la componente
que cae sobre N. As ÌÄ±,
DÏ‰
dt
=
a
0 + Î“1
11 au0 + Î“1
12 av0 + Î“1
12 bu0 + Î“1
22 bv0

Ï†u

+
b
0 + Î“2
11 au0 + Î“2
12 av0 + Î“2
12 bu0 + Î“2
22 bv0

Ï†v.

Notemos que esta expresi Ìon depende s Ìolo del vector (u
0
, v0
) = y y no de la curva Î±.

Observaci Ìon. Si S es un plano, sabemos que es posible encontrar una parametri-
zaci Ìon tal que E = G = 1 y F = 0. En tal caso, todos los s ÌÄ±mbolos de Christoffel,

Î“
k
ij , son iguales a cero. Notemos que, en tal caso, la derivada covariante coincide
con la derivada usual para vectores en el plano.
1

2 Transporte paralelo y Geod Ìesicas
Definici Ìon. Una curva parametrizada Î± : [0, l] â†’ S es la restricci Ìon a [0, l] de un
mapa diferenciable de (0 âˆ’ , l + ) en S ( > 0). Si Î±(0) = p y Î±(l) = q, diremos
que Î± une a p con q. Diremos que Î± es regular si Î±
0
(t) 6= 0 para t âˆˆ [0, l].

(Denotemos al intervalo [0, l] por I.)
Definici Ìon. Sea Î± : I â†’ S una curva parametrizada en S. Un campo vectorial
Ï‰ a lo largo de Î± es una correspondencia que le asigna a cada t âˆˆ I un vector
Ï‰(t) âˆˆ TÎ±(t)S. Diremos que el campo vectorial Ï‰ es diferenciable en t0 âˆˆ I si para
alguna parametrizaci Ìon Ï†(u, v) en Î±(t0) las componentes a(t) y b(t) de Ï‰(t) =
a Ï†u + b Ï†v son funciones diferenciables de t en t0. Diremos que Ï‰ es diferenciable
en I si lo es para cada t âˆˆ I.
Definici Ìon. Sea Ï‰ un campo diferenciable a lo largo de Î± : I â†’ S. La expresi Ìon

a
0 + Î“1
11 au0 + Î“1
12 av0 + Î“1
12 bu0 + Î“1
22 bv0

Ï†u

+
b
0 + Î“2
11 au0 + Î“2
12 av0 + Î“2
12 bu0 + Î“2
22 bv0

Ï†v

de
DÏ‰
dt

(t), tâˆˆI, est Ìa bien definida y se llama la derivada covariante de Ï‰ en t.
Si Î±(t) es una curva sobre la superficie S, pensamos en ella como la trayectoria de
un punto movi Ìendose sobre S, Î±
0
(t) es entonces la velocidad y Î±

00 la aceleraci Ìon

de Î±. La derivada covariante DÎ±
0
dt
, del campo Î±
0
(t), es la componente tangencial

de la aceleraci Ìon Î±
00(t).

Definici Ìon. Diremos que un campo vectorial Ï‰ es paralelo a lo largo de una
curva parametrizada Î± : I â†’ S si DÏ‰

dt = 0 âˆ€ t âˆˆ I.

Proposici Ìon. Sean w y v campos vectoriales paralelos a lo largo de Î± : I â†’ S.
Entonces, hw(t), v(t)i es constante. En particular, |w(t)|, |v(t)| y el  Ìangulo entre
w(t) y v(t) tambi Ìen son constantes.
Demostraci Ìon. Como w es paralelo a lo largo de Î± entonces dw
dt
es normal a

TÎ±(t)S. Luego, hv(t), w0

(t)i = 0, âˆ€ t âˆˆ I, ya que v(t) âˆˆ TÎ±(t)S. An Ìalogamente,

hv
0
(t), w(t)i = 0 âˆ€ t âˆˆ I. As ÌÄ±,
hv(t), w(t)i
0 = hv(t), w0
(t)i + hv
0
(t), w(t)i = 0

para todo t âˆˆ I, por lo que hv(t), w(t)i es constante. Como |w(t)|, |v(t)| y el
 Ìangulo entre w(t) y v(t) se expresan en t Ìerminos de hw(t), w(t)i y hv(t), v(t)i y
hv(t), w(t)i (solamente) entonces ellos tambi Ìen son constantes.

Transporte paralelo y Geod Ìesicas 3
Definici Ìon. Sea Ï‰ un campo diferenciable de vectores unitarios a lo largo de la
curva parametrizada Î± : I â†’ S sobre una superficie orientada S. Como Ï‰(t),
t âˆˆ I, es un campo vectorial unitario,
dÏ‰
dt

(t) es normal a Ï‰(t) y, por lo tanto,

DÏ‰
dt
= Î»
N âˆ§ Ï‰(t)

.
El n Ìumero real Î» = Î»(t), denotado por  DÏ‰
dt

, se llama valor algebraico de la

derivada covariante de Ï‰ en t.
Notemos que el signo de  DÏ‰
dt

depende de la orientaci Ìon de S y
 DÏ‰
dt

=
dÏ‰
dt
, N âˆ§Ï‰
.

Sean v y w dos campos vectoriales diferenciables a lo largo de la curva parame-
trizada Î± : I â†’ S con |v(t)| = |w(t)| = 1, t âˆˆ I. Queremos definir una funci Ìon

diferenciable Î˜ : I â†’ R tal que Î˜(t) es la determinaci Ìon del  Ìangulo de v(t) a w(t)
en la orientaci Ìon de S. Consideremos el campo vectorial diferenciable  Ì„v a lo largo
de Î± de manera tal que {v(t), v Ì„(t)} es una base ortonormal positiva para cada
t âˆˆ I. As ÌÄ±, podemos expresar

w(t) = a(t) v(t) + b(t)  Ì„v(t)
donde a y b son funciones diferenciables en I y a
2 + b
2 = 1.
Lema. Sean a y b funciones diferenciables en I tales que a
2 + b
2 = 1, Î˜0 tal que

a(t0) = cos(Î˜0) y b(t0) = sen(Î˜0). Entonces, la funci Ìon diferenciable

Î˜ = Î˜0 +
Z t
t0
(ab0 âˆ’ ba0
) dt

es tal que cos
Î˜(t)

= a(t), sen
Î˜(t)

= b(t), para t âˆˆ I, y Î˜(t0) = Î˜0.

Demostraci Ìon. Queremos ver que
0 â‰¡
a âˆ’ cos(Î˜)2
+
b âˆ’ sen(Î˜)2
= 2 âˆ’ 2
a cos(Î˜) + b sen(Î˜)
o equivalentemente, que A = a cos(Î˜) + b sen(Î˜) = 1. Luego,
A
0 = a
0
cos(Î˜) âˆ’ a sen(Î˜) Î˜0 + b
0
sen(Î˜) + b cos(Î˜) Î˜0

= a
0
cos(Î˜) âˆ’ b
0
sen(Î˜)
a
2 + b
2

+ b
0
sen(Î˜) âˆ’ a
0
cos(Î˜)
a
2 + b
2


= 0
ya que a
2 + b
2 = 1 â‡’ aa0 = âˆ’bb0

. Entonces A(t) es constante y como A(t0) = 1

entonces A â‰¡ 1.

4 Transporte paralelo y Geod Ìesicas
Lema. Sean v y w dos campos vectoriales diferenciables a lo largo de la curva
Î± : I â†’ S con |w(t)| = |v(t)| = 1, t âˆˆ I. Entonces,


Dw
dt

âˆ’

Dv
dt

=
dÎ˜
dt

donde Î˜ es una de las determinaciones diferenciables del  Ìangulo desde v hasta w
como se tiene en el lema anterior.
Demostraci Ìon. Sean  Ì„v = N âˆ§ v y  Ì„w = N âˆ§ w. Entonces

w = cos(Î˜) v + sen(Î˜)  Ì„v (âˆ—)

y
w Ì„ = N âˆ§ w = cos(Î˜) N âˆ§ v + sen(Î˜) N âˆ§ v Ì„ = cos(Î˜)  Ì„v âˆ’ sen(Î˜) v. (âˆ—âˆ—)
Derivando a (âˆ—) respecto de t obtenemos,

w
0 = âˆ’sen(Î˜) Î˜0

v + cos(Î˜) v

0 + cos(Î˜) Î˜0

v Ì„ + sen(Î˜)  Ì„v
0
.
Tomando el producto interno de esta  Ìultima expresi Ìon con (âˆ—âˆ—) obtenemos,
w
0
, w Ì„
= sen2
(Î˜) Î˜0 + cos2
(Î˜)
v
0
, v Ì„
+ cos2
(Î˜) Î˜0 âˆ’ sen2
(Î˜)
v Ì„
0
, v

= Î˜0 + cos2
(Î˜)
v
0
, v Ì„
âˆ’ sen2
(Î˜)
v Ì„
0
, v

= Î˜0 +

cos2
(Î˜) + sen2
(Î˜)
v
0
, v Ì„

= Î˜0 +
v
0
, v Ì„
ya que hv, v Ì„i = 0 
â‡’
v
0
, v Ì„
= âˆ’
v, v Ì„
0

y
v, v0
= 0. Luego,


Dw
dt

=
w
0
, w Ì„
= Î˜0 +
v
0
, v Ì„
=
dÎ˜
dt
+

Dv
dt


ya que
w
0
, w Ì„
=
dw
dt
, w Ì„
=
 Dw
dt

hN âˆ§ w, w Ì„i =
 Dw
dt

.

Proposici Ìon. Sea Ï†(u, v) una parametrizaci Ìon ortogonal (es decir, F = 0) de un
entorno de una superficie orientada S, y sea Ï‰(t) un campo vectorial diferenciable
de vectores unitarios a lo largo de la curva Ï†
u(t), v(t)

. Entonces


DÏ‰
dt

=
1
2
âˆš
EG 
Gu
dv
dt
âˆ’ Ev
du
dt

+
dÎ˜
dt

donde Î˜(t) es el  Ìangulo formado desde Ï†u hasta Ï‰(t) en la orientaci Ìon dada.

Transporte paralelo y Geod Ìesicas 5
Demostraci Ìon. Sean e1 = âˆš
1
E
Ï†u y e2 = âˆš
1
G
Ï†v los vectores unitarios tangentes
a las curvas coordenadas. Notemos que e1 âˆ§ e2 = N, donde N es la orientaci Ìon
dada para S. Por segundo lema, tenemos que


DÏ‰
dt

=

De1
dt

+
dÎ˜
dt

donde e1(t) = e1
u(t), v(t)

es el campo vectorial e1 restringido a la curva

Ï†
u(t), v(t)

. Luego

De1
dt

=

de1
dt
, N âˆ§ e1

=

de1
dt
, e2

=
(e1)u, e2
du
dt
+
(e1)v, e2
dv
dt
.

Como F = 0 entonces hÏ†uu, Ï†vi = 1 1

2Ev (recordemos que hÏ†uu, Ï†vi = Fu1
1
2Ev y

hÏ†uv, Ï†vi =
1
2Gu). As ÌÄ±,
(e1)u, e2
=
 Ï†u âˆš
E

u
,
Ï†v âˆš
G

=
*
Ï†uu
âˆš
E âˆ’
Ï†uEu
2
âˆš
E
E
,
Ï†v âˆš
G
+
= âˆ’
1
2
Ev âˆš
EG
ya que Ï† es una parametrizaci Ìon ortogonal (0 = F = hÏ†u, Ï†vi). An Ìalogamente,
(e1)v, e2
=
1
2
Gu âˆš
EG
.

Combinando lo anterior se obtiene el resultado.
Proposici Ìon. Sea Î± : I â†’ S una curva parametrizada en S y Ï‰0 âˆˆ TÎ±(t0)S,
t0 âˆˆ I. Entonces existe un  Ìunico campo vectorial paralelo Ï‰(t) a lo largo de Î±(t)
con Ï‰(t0) = Ï‰0.
Demostraci Ìon. Supongamos que la curva parametrizada Î± : I â†’ S est Ìa contenida
en un entorno coordenado de una parametrizaci Ìon ortogonal Ï†(u, v). Siguiendo la
notaci Ìon de la proposici Ìon anterior, la condici Ìon de paralelismo para el campo Ï‰
es

dÎ˜
dt
= âˆ’
1
2
âˆš
EG 
Gu
dv
dt
âˆ’ Ev
du
dt

= B(t).

Denotando por Î˜0 la determinaci Ìon del  Ìangulo orientado de Ï†u a Ï‰0, el campo Ï‰
queda determinado por

Î˜ = Î˜0 +
Z t
t0
B(t) dt

lo que demuestra la existencia y unicidad de Ï‰ para este caso. Si Î±(I) no est Ìa
contenido en un entorno coordenado, usamos la compacidad de I para dividir a

6 Transporte paralelo y Geod Ìesicas
Î±(I) en un n Ìumero finito de partes tal que cada una est Ìa contenida en un entorno
coordenado. Como la soluci Ìon es  Ìunica en las intersecciones de estas partes, el
resultado se puede extender.
Definici Ìon. Sea Î± : I â†’ S una curva parametrizada y Ï‰0 âˆˆ TÎ±(t0)S, t0 âˆˆ I. Sea
Ï‰ el campo vectorial paralelo a lo largo de Î± con Ï‰(t0) = Ï‰0. El vector Ï‰(t1),
t1 âˆˆ I, se llama el transporte paralelo de Ï‰0 a lo largo de Î± en el punto t1.
Observaci Ìon. Si Î± : I â†’ S es regular entonces el transporte paralelo no depende
de la parametrizaci Ìon de Î±(I). Si Î² : J â†’ S (t âˆˆ I, Ïƒ âˆˆ J) es otra parametrizaci Ìon
regular para Î±(I) entonces DÏ‰
dÏƒ =
DÏ‰
dt
dt
dÏƒ
. Como dt
dÏƒ
6= 0, Ï‰(t) es paralelo si, y s Ìolo

si, Ï‰(Ïƒ) es paralelo.
Observaci Ìon. Dados los puntos p, q, âˆˆ S y una curva parametrizada Î±: [0, 1]â†’S
con Î±(0) = p y Î±(1) = q, denotemos por PÎ± : TpS â†’ TqS el mapa que le asigna
a cada v âˆˆ TpS su transporte paralelo a lo largo de Î± en q. Como el producto
interno es constante a lo largo de campos paralelos (ver primera proposici Ìon en
esta secci Ìon), este mapa es una isometr ÌÄ±a.
Observaci Ìon. Si dos superficies, S y S Ì„, son tangentes a lo largo de una curva
parametrizada Î± y Ï‰0 es un vector de TÎ±(t0)S = TÎ±(t0)S Ì„, entonces Ï‰(t) es el
transporte paralelo de Ï‰0 relativo a la superficie S si, y s Ìolo si, Ï‰(t) es el transporte
paralelo de Ï‰0 relativo a S Ì„. (La derivada covariante DÏ‰
dt
de Ï‰ es igual para ambas

superficies y el transporte paralelo es  Ìunico.)
Definici Ìon. Diremos que una curva parametrizada (y no constante) Î³ : I â†’ S es
geod Ìesica en t âˆˆ I si su campo de vectores tangentes Î³
0
(t) es paralelo a lo largo

de Î³ en t. Es decir, si DÎ³
0
(t)
d( t) = 0. Diremos que Î³ es una geod Ìesica parametrizada

si es geod Ìesica para todo t âˆˆ I.
Notemos que la definici Ìon anterior equivale a decir que Î±

00(s) = Îºn es normal
al plano tangente (es decir, paralelo a la normal de la superficie). Entonces, una
curva regular C âŠ‚ S (Îº 6= 0) es una geod Ìesica si, y s Ìolo si, su normal principal en
cada punto p âˆˆ C es paralela a la normal de S en p.
Ejemplo. Los c ÌÄ±rculos maximales de una esfera son geod Ìesicas.

Transporte paralelo y Geod Ìesicas 7
Ejemplo. Consideremos el cilindro recto sobre la circunferencia x
2 +y
2 = 1. Los
c ÌÄ±rculos que se obtienen intersectanto al cilindro con planos perpendiculares al eje
de rotaci Ìon son geod Ìesicas. Por otra parte, las rectas (generadoras) del cilindro
tambi Ìen son geod Ìesicas. Para determinar las otras geod Ìesicas del cilindro, C,
consideremos la parametrizaci Ìon Ï†(u, v) =

cos(u),sen(u), v

en el punto p âˆˆ C,
con Ï†(0, 0) = p. En esta parametrizaci Ìon, un entorno de p en C es expresado por
Ï†
u(s), v(s)

, donde s es la longitud de arco de C. Recordemos que este mapa Ï†
es una isometr ÌÄ±a local. Como la condici Ìon de ser geod Ìesica es local e invariante
bajo isometr ÌÄ±as, la curva
u(s), v(s)

debe ser una geod Ìesica en U que pase por
(0, 0). Pero las geod Ìesicas del plano son l ÌÄ±neas rectas. As ÌÄ±, adem Ìas de los casos
anteriores,
u(s) = as, v(s) = ba, a2 + b

2 = 1 (para que est Ìe parametrizada
por longitud de arco).
Luego, para que una curva regular C (que no sea ni un c ÌÄ±rculo ni una l ÌÄ±nea) sea
geod Ìesica del cilindro debe ser localmente de la forma

cos(as),sen(as), bs

que resulta ser una h Ìelice.
Observaci Ìon. Dados dos puntos sobre el cilindro, que no

est Ìen en un c ÌÄ±rculo paralelo al plano xy, pueden existir infi-
nitas geod Ìesicas que los una (es necesario que las geod Ìesicas

den al menos una vuelta completa alrededor del cilindro).

q

p

Definici Ìon. Sea C una curva regular orientada contenida en una superficie orien-
tada S, y sea Î±(s) una parametrizaci Ìon de C en un entorno de p âˆˆ S parametrizada

por longitud de arco s. El valor algebraico de la derivada covariante de Î±
0
(s) en

p,
h
DÎ±
0
(s)
ds
i
= Îºg, se llama la curvatura geod Ìesica de C en p.
Observaci Ìon. Geod Ìesicas que son curvas regulares est Ìan caracterizadas como
curvas cuya curvatura geod Ìesica es cero.
Observaci Ìon. El valor absoluto de la curvatura geod Ìesica, Kg, de C en p es igual
al valor absoluto de la componente tangencial del vector Î±

00(s) = Îºn, donde Îº es
la curvatura de C en p y n es el vector normal de C en p. Recordando que el valor

8 Transporte paralelo y Geod Ìesicas
absoluto de la componente normal del vector Îºn es igual al valor absoluto de la
curvatura normal Îºn de C âŠ‚ S en p, tenemos que K2 = (Îºg)
2 + (Îºn)
2
.
Observaci Ìon. La curvatura geod Ìesica de C âŠ‚ S cambia de signo cuando se
cambia la orientaci Ìon o bien de C o bien de S.
Proposici Ìon (F Ìormula de Liouville). Sea Î±(s) una parametrizaci Ìon por longitud
de arco, de un entorno de un punto p âˆˆ S, de una curva regular orientada C sobre
una superficie orientada S. Sea Ï†(u, v) una parametrizaci Ìon ortogonal de S en p
y Î˜(s) el  Ìangulo que hace Ï†u con Î±
0
(s) en la orientaci Ìon dada. Entonces,

Îºg =
Îºg

1
cos(Î˜) +
Îºg

2
sen(Î˜) + dÎ˜
ds

donde
Îºg

1
y
Îºg

2
son las curvaturas geod Ìesicas de las curvas coordenadas

v = constante y u = constante, respectivamente.
Demostraci Ìon. Por una proposici Ìon anterior (considerando Ï‰ = Î±
0
(s)) tenemos

que

Îºg =
1
2
âˆš
EG 
Gu
dv
ds
âˆ’ Ev
du
ds

+
dÎ˜
ds
.
A lo largo de la curva v = constante u = u(s), tenemos que dv
ds = 0 y du
ds = âˆš
1
E
;

luego,
Îºg

1
= âˆ’
Ev
2E
âˆš
G
. An Ìalogamente,
Îºg

2
= âˆ’
Gu
2G
âˆš
E
. As ÌÄ±,

Îºg =
Îºg

1
âˆš
E
du
ds
+
Îºg

2
âˆš
G
dv
ds
+
dÎ˜
ds
.

Como âˆš
E
du
ds =
D
Î±
0
(s), âˆš
Ï†u
E
E
= cos(Î˜) y âˆš
G
dv
ds =
D
Î±
0
(s), âˆš
Ï†v
G
E
= sen(Î˜), el

resultado sigue.
Sea Î³ : I â†’ S una curva parametrizada de S y Ï†(u, v) una parametrizaci Ìon de
S en un entorno V de Î³(t0), t0 âˆˆ I. Sea J âŠ‚ I un intervalo abierto que contenga
a t0 tal que Î³(J) âŠ‚ v. Sea Ï†
u(t), v(t)

, t âˆˆ J, la expresi Ìon de Î³ : J â†’ S en

la parametrizaci Ìon Ï†. El campo de vectores tangentes, Î³
0
(t), t âˆˆ J es dado por

Ï‰ = u
0
(t) Ï†u + v
0
(t) Ï†v. As ÌÄ±, el hecho que Ï‰ es paralelo es equivalente al sistema

de ecuaciones diferenciales:
u
00 + Î“1
11(u
0
)
2 + 2Î“1
12u
0v
0 + Î“1
22(v
0
)
2 = 0

v
00 + Î“2
11(u
0
)
2 + 2Î“2
12u
0v
0 + Î“2
22(v
0
)
2 = 0

(âˆ—)

obtenido de la definici Ìon de DÏ‰
dt
(con a = u
0 y b = v
0
) e igualando a cero.

Transporte paralelo y Geod Ìesicas 9
Es decir, Î³ : I â†’ S es una geod Ìesica si, y s Ìolo si, el sistema (âˆ—) se satisface para
cada intervalo J âŠ‚ I tal que Î³(J) est Ìe contenido en un entorno coordenado. El
sistema (âˆ—) se conoce como las ecuaciones diferenciales de las geod Ìesicas
de S.
Al caracterizar las geod Ìesicas mediante el sistema (âˆ—) tenemos la siguiente:
Proposici Ìon. Dado un punto p âˆˆ S y un vector Ï‰ âˆˆ TpS, Ï‰ 6= 0, existe un  > 0
y una  Ìunica geod Ìesica parametrizada Î³ : (âˆ’, ) â†’ S tal que Î³(0) = p y Î³
0
(0) = Ï‰.
Observaci Ìon. Pedimos Ï‰ 6= 0 ya que las curvas constantes est Ìan excluidas de la
definici Ìon de geod Ìesica parametrizada.
Ejemplo. Para un s Ìolido de revoluci Ìon, parametrizado por

Ï†(u, v) =
f(v) cos(u), f(v) sen(u), g(v)


con f(v) 6= 0, como
Î“
1
11 = 0 Î“1
12 =
ff0
f
2
Î“
1
22 = 0

Î“
2
11 = âˆ’
ff0
(f
0)
2 + (g
0)
2
Î“
2
12 = 0 Î“2
22 =
f
0f
00 + g
0
g
00
(f
0)
2 + (g
0)
2

el sistema (âˆ—) se reduce a

u
00 +
2ff0
f
2
u
0
v
0 = 0 (i)

v
00 âˆ’
ff0
(f
0)
2 + (g
0)
2
(u
0
)
2 +
f
0f
00 + g
0
g
00
(f
0)
2 + (g
0)
2
(v
0
)
2 = 0 (ii)
Notemos que los meridianos u = const. y v = v(s), parametrizados por longitud
de arco, son geod Ìesicas:
(i) se satisface trivialmente
(ii) se convierte en v
00 +
f
0
f
00+g
0
g
00
(f
0)
2+(g
0)
2 (v
0
)
2 = 0 pero la primera forma fundamental

(a lo largo de un meridiano) nos dice que
(f
0
)
2 + (g
0
)
2

(v
0
)
2 = 1. Derivando y

usando que v

0 6= 0 se tiene que se satisface la ecuaci Ìon.

10 Transporte paralelo y Geod Ìesicas
Miremos ahora los paralelos u = u(s) y v = const., parametrizados por longitud
de arco s. La ecuaci Ìon (i) nos dice que u

0 = const. y la ecuaci Ìon (ii) se convierte en

f f0
(f
0)
2+(g
0)
2 (u
0
)
2 = 0. Como u

0 6= 0 (para que pueda ser geod Ìesica), (f
0
)
2+(g
0
)
2 6= 0

y f 6= 0 entonces, necesariamente, f

0 = 0. Es decir, para que un paralelo sea una
geod Ìesica es necesario que sea generado por la rotaci Ìon de un punto de la curva
generadora donde la recta tangente es paralela al eje de rotaci Ìon de la superficie.
Observaci Ìon. Notemos que la ecuaci Ìon (i) se puede reescribir como (f
2u
0
)
0
f
2 = 0

y, por lo tanto, f
2u
0 = const. = c. Sea Î¸ âˆˆ [0, 2Ï€] el  Ìangulo entre una geod Ìesica y

el paralelo con el cual se est Ìa intersectando. Luego,

cos(Î¸) =
Ï†u, Ï†u u
0 + Ï†v v
0
|Ï†u|
=
fu0
.

Como f = r es el radio del paralelo en el punto de intersecci Ìon, tenemos la
relaci Ìon de Clairaut: r cos(Î¸) = const. = |c|.
Volviendo al ejemplo, sea u = u(s) y v = v(s) una geod Ìesica, parametrizada por
longitud de arco, que no sea ni un meridiano ni un paralelo de la superficie. Ya
vimos que la ecuaci Ìon (i) es equivalente a f
2u
0 = const. = c 6= 0. La primera

forma fundamental, a lo largo de
u(s), v(s)

Las ecuaciones (iii) y f
2u
0 = c 6= 0 producen la ecuaci Ìon (ii) ya que al reemplazar

y podemos asumir que v

0 6= 0 porque Ï†
u(s), v(s)

no es un paralelo y si la
geod Ìesica llegase a ser tangente a un paralelo que no es una geod Ìesica (v
0 = 0),
la relaci Ìon de Clairaut establece que esto s Ìolo ocurre en puntos aislados. Como
c 6= 0 (pues la geod Ìesica no es un meridiano) tenemos que u
0
(s) 6= 0. As ÌÄ±, podemos

invertir u = u(s) y obtener s = s(u), de donde v = v
s(u)

. Multiplicando a (iii)


Transporte paralelo y Geod Ìesicas 11
de donde,
dv
du
=
Ï†vv = Î“1
22 Ï†u + Î“2
22 Ï†v + L3 N
Nu = a11 Ï†u + a21 Ï†v
Nv = a12 Ï†u + a22 Ï†v.

(âˆ—)

Los coeficientes aij se determinan igual que antes. Los coeficientes Î“k
ij se llaman
los s ÌÄ±mbolos de Christoffel de S en la parametrizaci Ìon Ï†. Como Ï†uv = Ï†vu,
tenemos que Î“1
12 = Î“1
21 y Î“2
12 = Î“2
21. Tomando el producto interno con N en

las primeras cuatro ecuaciones obtenemos que L1 = e, L2 = L Ì„

2 = f y L3 = g,
donde e, f y g son los coeficientes de la segunda forma fundamental sobre S. Por
otra parte, tomando el producto interno con los vectores Ï†u y Ï†v, en las primeras
cuatro ecuaciones, obtenemos:


2 El Teorema de Gauss y las ecuaciones de compatibilidad
Notemos que el determinante del sistema para cada par de ecuaciones, como han
sido agrupadas, es EG âˆ’ F

2 6= 0; por lo que el sistema admite soluci Ìon. As ÌÄ±,
podemos calcular los s ÌÄ±mbolos de Christoffel en t Ìerminos de los coeficientes de la
primera forma fundamental (E, F y G) y sus derivadas.
Observaci Ìon. Todos los conceptos y propiedades geom Ìetricas expresadas en
t Ìerminos de los s ÌÄ±mbolos de Christoffel son invariantes bajo isometr ÌÄ±as.
Ejemplo. Consideremos la superficie de revoluci Ìon parametrizada por

Ï†(u, v) =
f(v) cos(u), f(v) sen(u), g(v)

, f(v) 6= 0.



Notemos que
(Ï†uu)v âˆ’ (Ï†uv)u = 0 , (Ï†vv)u âˆ’ (Ï†vu)v = 0 y Nuv âˆ’ Nvu = 0. (âˆ—âˆ—)
Introduciendo los valores de (âˆ—) en (âˆ—âˆ—) obtenemos
A1 Ï†u + B1 Ï†v + C1 N = 0
A2 Ï†u + B2 Ï†v + C2 N = 0
A3 Ï†u + B3 Ï†v + C3 N = 0

(âˆ—âˆ—âˆ—)

donde Ai
, Bi y Ci (i = 1, 2, 3) son funciones de E, F, G, e, f, g y sus derivadas.
Como los vectores Ï†u, Ï†v y N son linealmente independientes, entonces
Ai = 0 , Bi = 0 y Ci = 0 para i = 1, 2, 3.

Usando los valores de (âˆ—) en la primera relaci Ìon de (âˆ—âˆ—), volviendo a usar (âˆ—),
mirando el coeficiente de Ï†v y reemplazando los valores de aij , obtenemos
Î“
2
conocida como la f Ìormula de Gauss.

El Teorema de Gauss y las ecuaciones de compatibilidad 3
Teorema (Egregium de Gauss). La curvatura Gaussiana, K, de una superficie
es invariante bajo isometr ÌÄ±as locales.
De hecho, si Ï† : U âŠ‚ R

2 â†’ S es una parametrizaci Ìon en p âˆˆ S y si Ïˆ : V âŠ‚ S â†’ S,
donde V âŠ‚ Ï†(U) es un entorno de p, es una isometr ÌÄ±a local en p, entonces Ïˆ â—¦ Ï†
es una parametrizaci Ìon en Ïˆ(p). Como Ïˆ es una isometr ÌÄ±a, los coeficientes de la
primera forma fundamental en las parametrizaciones Ï† y Ïˆ â—¦ Ï† coinciden en los

puntos correspondientes q y Ïˆ(q) (q âˆˆ V ). Luego, los s ÌÄ±mbolos de Christoffel co-
rrespondientes tambi Ìen coinciden. Usando la f Ìormula de Gauss, podemos calcular

a K en t Ìerminos de los s ÌÄ±mbolos de Christoffel en una parametrizaci Ìon dada. As ÌÄ±,
K(q) = K
Ïˆ(q)

pata todo q âˆˆ V .

Ejemplo. Recordemos que el catenoide es localmente isom Ìetrico al helicoide.
Por el Teorema de Gauss, las curvaturas Gaussianas son iguales en los puntos
correspondientes.
Observaci Ìon. La curvatura Gaussiana no depende de la posici Ìon en el espacio
de la superficie sino de la estructura m Ìetrica de la superficie (primera forma
fundamental).
Siguiendo el procedimiento anterior, si miramos el coeficiente de Ï†u obtenemos

Mirando el coeficiente de N obtenemos
el procedimiento ahora con la segunda expresi Ìon en (âˆ—âˆ—) y mirando el
coeficiente de N obtenemos

Los dem Ìas coeficientes (A2 = 0, B2 = 0, A3 = 0, B3 = 0 y C3 = 0) no producen

ecuaciones nuevas. Las ecuaciones (MC.1) y (MC.2) se conocen como las ecua-
ciones de Mainardi-Codazzi.

La f Ìormula de Gauss y las ecuaciones de Mainardi-Codazzi se conocen como las
ecuaciones de compatibilidad de la teor ÌÄ±a de superficies.
arras de progreso, hÃ©lices y contadores




Las barras de progreso, las hÃ©lices y los contadores son elementos que se utilizan en un programa para mostrar grÃ¡ficamente el avance de un proceso. Generalmente, sirven para informar a los usuarios sobre su evoluciÃ³n cuando duran mÃ¡s de unos pocos segundos. Por ello, tienen por objeto dar certidumbre a la persona que espera sobre que un proceso sigue en ejecuciÃ³n informando, a veces, del tiempo transcurrido y del que resta para su finalizaciÃ³n. Es un recurso apropiado para mostrar el progreso de unos cÃ¡lculos prolongados; de una transferencia de archivos; de la instalaciÃ³n de una aplicaciÃ³n, etc.

A continuaciÃ³n, mostraremos algunas de las posibilidades que existen para incorporar este prÃ¡ctico y vistoso elemento a un programa Python. Nos centraremos en los paquetes Progress y TQDM, ambos disponibles en nuestro repositorio favorito. 


Progress

El paquete Progress desarrollado por el griego Georgios Verigakis (Verigak) consta de varios mÃ³dulos que agrupan un conjunto de clases que permiten declarar barras de progreso, hÃ©lices, contadores, etc. en programas para la consola. Son objetos de distintos tipos que tienen la misma finalidad y de los cuales existen algunas variantes.


InstalaciÃ³n de Progress

Para instalar Progress con PIP desde la lÃ­nea de comandos:

$ pip install progress


Objetos Bar, ChargingBar, Spynner y Countdown de Progress

Normalmente los objetos se declaran antes del inicio de un bucle y, despuÃ©s, en cada ciclo del bucle se utiliza el mÃ©todo Next() para hacer que avance.

A continuaciÃ³n, varios ejemplos que muestran cÃ³mo declarar y usar los objetos: dos tipos de barras, una hÃ©lice y un contador. La funciÃ³n sleep() de time se utiliza a veces para introducir un pequeÃ±o retardo de tiempo para que se pueda observar el progreso de los distintos elementos usados.




from progress.bar import Bar, ChargingBar
import os, time, random

# Declara un objeto de la clase Bar(). En cada ciclo la barra
# muestra una porciÃ³n hasta llegar a su mÃ¡xima longitud en el 
# ciclo 20. La barra se representa con el carÃ¡cter #

bar1 = Bar('Procesando:', max=20)
for num in range(20):
    time.sleep(0.2)
    bar1.next()
bar1.finish()


# Declara un objeto de la clase ChargingBar(). Cuando comienza
# el bucle aparece una barra punteada y durante los ciclos los 
# puntos "âˆ™" son sustituidos por el carÃ¡cter "â–ˆ" hasta alcazar
# el 100%.

bar2 = ChargingBar('Instalando:', max=100)
for num in range(100):
    time.sleep(random.uniform(0, 0.2))
    bar2.next()
bar2.finish()


# Declara un objeto de la clase Bar(). En cada ciclo la barra
# muestra el carÃ¡cter del atributo "fill" (Â·) hasta alcanzar 
# el 100%.
# Durante el proceso se escribe un archivo de texto con 
# 5000 caracteres ('X' y 'O') que son generados por una funciÃ³n 
# aleatoria. En este caso el retardo de tiempo es real.
# En el mÃ³dulo hay otras clases para declarar objetos similares:
# FillingSquaresBar, FillingCirclesBar, IncrementalBar, PixelBar
# y ShadyBar

bar3 = Bar('Escribiendo:', fill='Â·', suffix='%(percent)d%%')
caracteres = ['X', 'O']
datos = os.getcwd()+os.sep+"datos.txt"
archivo = open(datos, "w")
for i in range(100):
    cadena = ""
    longitud = 5000
    for num in range(longitud):
        cadena += caracteres[random.randint(0, 1)]
    
    archivo.write(cadena + "\n")
    bar3.next()
    
bar3.finish()
archivo.close


# Declara un objeto de la clase Spinner(). En cada ciclo una hÃ©lice
# gira hasta que se completa la lectura del archivo creado en el
# ejemplo anterior. Cuando se completa la lectura se muestra el 
# nÃºmero total de caracteres encontrados de cada tipo ('X'/'O').
# Una hÃ©lice muestra que un proceso se estÃ¡ ejecutando pero no 
# no ayuda a prever su final.
# El mÃ³dulo tienes otras clase para declarar objetos similares:
# PieSpinner, MoonSpinner, LineSpinner y PixelSpinner.

from progress.spinner import Spinner
import time

spinner = Spinner('Leyendo: ')
cuenta_X = 0
cuenta_O = 0
archivo = open(datos, "r")
while True: 
    linea = archivo.readline()
    if linea:
        for caracter in linea:
            if caracter == 'X':
                cuenta_X+=1
            elif caracter == 'O':
                cuenta_O+=1
    else:
        break
    time.sleep(0.1)
    spinner.next()
    
print(' X=', cuenta_X, 'O=', cuenta_O)
archivo.close


# Declara un objeto de la clase Countdown(). En cada ciclo un 
# contador que comienza en 100 va disminuyendo su valor hasta
# alcanzar 0, que marca el fin del bucle.
# El mÃ³dulo tiene otras clases para declarar objetos 
# similares: Counter, Pie y Stack

from progress.counter import Countdown
import time

contador = Countdown("Contador: ")
for num in range(100):
    contador.next()
    time.sleep(0.05)

print()

TQDM

El mÃ³dulo TQDM de Noam Yorav-Raphael y un grupo de colaboradores a diferencia de progress sÃ³lo permite implementar barras de progreso, pero mÃ¡s sofisticadas porque son capaces de ofrecer mÃ¡s informaciÃ³n mientras se ejecuta un proceso: el porcentaje ejecutado, el tiempo trascurrido, una estimaciÃ³n del tiempo que resta para su finalizaciÃ³n y el nÃºmero de iteraciones por segundo.

Algunas caracterÃ­sticas interesantes de TQDM son que se puede utilizar en el entorno interactivo IPython, dentro de un cuaderno Jupyter y desde la lÃ­nea de comandos.


InstalaciÃ³n de TQDM

Para instalar TQDM con PIP desde la lÃ­nea de comandos:

$ pip install tqdm


Barras de progreso con TQDM

A continuaciÃ³n, varios ejemplos que muestran el uso de las barras de progreso TQDM:




# Declara una barra de progreso tqdm. En cada ciclo la barra
# muestra una porciÃ³n hasta llegar a 100.
# La barra es representada con el carÃ¡cter "â–ˆ": a su izquierda
# aparece el porcentaje procesado y a su derecha el nÃºmero de ciclo
# actual, el total de ciclos, el tiempo transcurrido de proceso,
# una estimaciÃ³n del tiempo pendiente y el nÃºmero de ciclos o 
# iteraciones por segundo.

from tqdm import tqdm, trange
import time

for num in tqdm(range(100)):
    time.sleep(0.01)


# Declara una barra de progreso con trange() que es equivalente a
# tqdm(range()) del ejemplo anterior

for num in trange(100):
    time.sleep(0.01)


# Declara una barra de progreso tqdm utilizando una lista,
# correspondiÃ©ndose en este caso el nÃºmero de ciclos con el 
# nÃºmero de elementos de la lista.

texto = ""
for caracter in tqdm(["a", "b", "c", "d"]):
    texto = texto + caracter


# Declara una barra de progreso tqdm con una lista y
# se establece una descripciÃ³n para mostrar el elemento
# en curso a la izquierda de la barra.

barra1 = tqdm(["a", "b", "c", "d"])
for caracter in barra1:
    barra1.set_description("Procesando %s" % caracter)
    time.sleep(0.5)


# Declara una barra de progreso tqdm para un bucle de
# 100 ciclos que se actualiza cada 10 ciclos.
# El carÃ¡cter utilizado para construir la barra es '#'
# y lo establece el atributo ascii con el valor True.

with tqdm(total=100, ascii=True) as barra2:
    for num in range(10):
        barra2.update(10)
        time.sleep(0.5)


# Declara una barra de progreso tqdm en un bucle para
# recorrer y acumular con accumulate() los valores de una lista

from itertools import *
barra3 = tqdm(accumulate([0, 1, 2, 3, 4, 5]))
for acumulado in barra3:
    barra3.set_description("Acumulado %i" % acumulado)
    time.sleep(0.5)


# Declara una barra de progreso tqdm en un bucle para
# recorrer y acumular con accumulate() los valores de una lista.
# En ese ejemplo el atributo disable se establece con el valor
# True para no mostrar la barra de progreso

desactivado = True
barra4 = tqdm(accumulate([0, 1, 2, 3, 4, 5]), disable=desactivado)
for acumulado in barra4:
    barra4.set_description("Acumulado %i" % acumulado)
    if desactivado:
        print(acumulado, end=' ')
    time.sleep(0.5)


# Ejecutado desde la lÃ­nea de comandos lista los archivos y carpetas
# del directorio /usr/ redirigiendo la salida a un archivo de texto,
# mostrando el nÃºmero de elementos encontrados y el nÃºmero de
# iteraciones por segundo.

$ ls /usr/ -R | tqdm >>usr.txt


Ir al Ã­ndice del tutorial de Python

Publicado por Pherkad en 14:09 
Enviar por correo electrÃ³nico
Escribe un blog
Compartir con Twitter
Compartir con Facebook
Compartir en Pinterest
Etiquetas: IPython, Jupyter, Python3
Entrada mÃ¡s recienteEntrada antiguaInicio
Buscar
Python para impacientes
Python
IPython
EasyGUI
Tkinter
JupyterLab
Numpy
Anexos
GuÃ­a urgente de MySQL
GuÃ­a rÃ¡pida de SQLite3
Entradas + populares
InstalaciÃ³n de Python, paso a paso
InstalaciÃ³n de Python 3.6 A finales de 2016 se produjo el lanzamiento de Python 3.6 . El propÃ³sito de esta entrada es mostrar, pas...
GrÃ¡ficos en IPython
Unos de los motivos que inspiraron el desarrollo de IPython fue contar con una herramienta que uniera la posibilidad de realizar cÃ¡lcu...
Operaciones con fechas y horas. Calendarios
Los mÃ³dulos datetime y calendar amplÃ­an las posibilidades del mÃ³dulo time que provee funciones para manipular expresiones de ti...
CÃ¡lculo estadÃ­stico
El mÃ³dulo statistics agrupa un conjunto de funciones para cÃ¡lculo estadÃ­stico. Las funciones estÃ¡n organizadas en dos grupos: las...
Tkinter: interfaces grÃ¡ficas en Python
IntroducciÃ³n Con Python hay muchas posibilidades para programar una interfaz grÃ¡fica de usuario ( GUI ) pero Tkinter es fÃ¡cil d...
CÃ¡lculo con arrays Numpy
Numpy ofrece todo lo necesario para obtener un buen rendimiento cuando se trata de hacer cÃ¡lculos con arrays. Por como estÃ¡ concebid...
Dar color a las salidas en la consola
En Python para dar color a las salidas en la consola (o en la terminal de texto) existen varias posibilidades. Hay un mÃ©todo basado ...
AÃ±adir, consultar, modificar y suprimir elementos en Numpy
Acceder a los elementos de un array. [], [,], ... Acceder a un elemento de un array. Para acceder a un elemento se utiliz...
Variables de control en Tkinter
Variables de control Las variables de control son objetos especiales que se asocian a los widgets para almacenar sus valore...
Operaciones con archivos CSV
Un archivo CSV (de Valores Separados por Comas) es un tipo de documento que representa los datos de forma parecida a una tabla, es d...
Archivo

abril 2018 (2)
python.org
python.org
pypi.org
pypi.org
Sitios
ActivePython
Anaconda
Bpython
Django
Flask
Ipython
IronPython
Matplotlib
MicroPython
Numpy
Pandas
Pillow
PortablePython
PyBrain
PyCharm
PyDev
PyGame
Pypi
PyPy
Pyramid
Python.org
PyTorch
SciPy.org
Spyder
Tensorflow
TurboGears
2014-2020 | Alejandro SuÃ¡rez Lamadrid y Antonio SuÃ¡rez JimÃ©nez, AndalucÃ­a - EspaÃ±a
. Tema Sencillo. Con la tecnologÃ­a de Blogger.
DespuÃ©s de que el pasado 3 de abril Caly et al. publicaran los resultados de un experimento in vitro sobre ivermectina y SARS-CoV-2 , el mundo de la investigaciÃ³n y el pÃºblico en general se vieron arrastrados a dos puntos de vista extremos y opuestos sobre este tema. Un grupo pedÃ­a que se descartase el uso de este fÃ¡rmaco contra la COVID-19 de manera temprana, ya que las concentraciones efectivas informadas por Caly et al. eran demasiado elevadas para poder alcanzarlas in vivo y, por tanto, cualquier esfuerzo invertido en conseguirlo serÃ­a un desperdicio y generarÃ­a una falsa sensaciÃ³n de esperanza. El otro grupo, siguiendo un legÃ­timo sentido de urgencia, se lanzÃ³ a promover el uso generalizado de la ivermectina, incluso sin pruebas adecuadas de eficacia y, lo que es mÃ¡s importante, de seguridad para este uso especÃ­fico. Ambos extremos son igualmente errÃ³neos. AquÃ­ proporcionamos respuestas a preguntas comunes sobre este tema y mostramos la participaciÃ³n de investigadores de ISGlobal para encontrar respuestas a algunas de ellas.

Â¿QuÃ© es la ivermectina?
La ivermectina es un fÃ¡rmaco antiparasitario desarrollado durante la dÃ©cada de 1970 tras una asociaciÃ³n entre el instituto Kitasato en JapÃ³n y Merck & Co. El proyecto fue dirigido por Satoshi Omura y William Campbell, respectivamente. Dado su amplio espectro contra parÃ¡sitos internos y externos que mejorÃ³ la salud de los animales y aumentÃ³ la productividad, la ivermectina se convirtiÃ³ rÃ¡pidamente en un fÃ¡rmaco de gran Ã©xito en el Ã¡mbito veterinario.

Durante la dÃ©cada los 70 del siglo XX, el mundo estaba librando una guerra contra la ceguera de los rÃ­os, tambiÃ©n conocida como oncocercosis, una enfermedad causada por el parÃ¡sito Onchocerca volvulus en Ã¡reas rurales y que paralizaba comunidades enteras. Ya se habÃ­an logrado algunos Ã©xitos gracias a la participaciÃ³n del Banco Mundial. Pero una vez que se comercializÃ³ la ivermectina, se observÃ³ que Onchocerca cervicalis, el agente de la oncocercosis en caballos, prÃ¡cticamente desaparecÃ­a en las Ã¡reas donde se usaba el nuevo fÃ¡rmaco.

The burden of onchocerciasis: children leading blind adults in Africa.
La carga de oncocercosis: jÃ³venes guÃ­an a personas adultas invidentes en Ãfrica.
[Imagen: Otis Historical Archives National Museum of Health & Medicine]

Esto condujo a esfuerzos acelerados para probar la ivermectina en humanos, que terminaron con la aprobaciÃ³n de las autoridades reguladoras francesas a principios de la dÃ©cada de 1980 y fueron seguidos por la decisiÃ³n sin precedentes de Merck & Co. de donar tanta ivermectina como fuera necesaria, durante el tiempo que fuese necesario, para erradicar la ceguera de los rÃ­os. Esto dio origen al programa de donaciÃ³n de Mectizan. Este programa ha distribuido mÃ¡s de 3.000 millones de tratamientos durante los Ãºltimos 30 aÃ±os y ha contribuido a salvar innumerables vidas durante este perÃ­odo. Posteriormente, el programa se ampliÃ³ para incluir la filariasis linfÃ¡tica, otra enfermedad debilitante causada por gusanos filarias.

Elephantiasis of leg due to filariasis, in Luzon, Philippines.
Elefantiasis en piernas debido a la filariasis, en Luzon, Philippines. [Imagen: CDC]

Omura y Campbell fueron galardonados con el premio Nobel de Medicina en 2015 por este descubrimiento, un honor compartido con Tu Youyou, la investigadora china responsable del descubrimiento del antimalÃ¡rico artemisinina.

MÃ¡s informaciÃ³n sobre el origen y los usos de la ivermectina, aquÃ­.

Â¿Para quÃ© se usa la ivermectina en humanos?
AquÃ­ describimos algunos de los usos aprobados en el mundo:

1. En los EE. UU., la ivermectina se comercializa en dosis de hasta 200 mcg / kg una vez al aÃ±o para las siguientes indicaciones:

a. Strongyloides stercoralis, un parÃ¡sito intestinal capaz de causar una enfermedad sistÃ©mica grave.
Una niÃ±a de dos aÃ±os con estrongiloidiasis diseminada curada con ivermectina (A) antes, (B) muestra fecal, (C) muestra de esputo y (D) seis semanas despuÃ©s del tratamiento.
Una niÃ±a de dos aÃ±os con estrongiloidiasis diseminada curada con ivermectina (A) antes, (B) muestra fecal, (C) muestra de esputo y (D) seis semanas despuÃ©s del tratamiento. [Imagen: Chaccour y Del Pozo, NEJM, 2012]

b. Oncocercosis o ceguera de los rÃ­os
2. En Europa, la ivermectina tambiÃ©n se comercializa contra la filariasis linfÃ¡tica y la sarna en dosis Ãºnicas de hasta 400 mcg / kg.

3. En Australia, se recomiendan 3 o mÃ¡s dosis de 200 mcg / kg en un mes para el tratamiento de la sarna costrosa severa.

Una lÃ­nea de investigaciÃ³n muy prometedora, en la que ISGlobal ha tenido un papel primordial, es la distribuciÃ³n de ivermectina a nivel poblacional para matar a los mosquitos y asÃ­ disminuir la transmisiÃ³n de la malaria

Â¿AlgÃºn otro uso probado o potencial?
Aunque no se comercializa para estas indicaciones, la ivermectina tiene una eficacia parcial contra otros parÃ¡sitos intestinales comunes en humanos, como Ascaris lumbricoides y Trichuris trichura.

A veces tambiÃ©n se usa fuera de etiqueta contra ectoparÃ¡sitos como piojos y Tunga penetrans, entre muchos otros parÃ¡sitos internos o externos.

Una lÃ­nea de investigaciÃ³n muy prometedora, en la que personal investigador de ISGlobal ha tenido un papel primordial, es la distribuciÃ³n de ivermectina a nivel poblacional para matar a los mosquitos que se alimentan de humanos o de animales tratados y asÃ­ disminuir la transmisiÃ³n de la malaria.

Â¿Es segura la ivermectina?
Cuando se utiliza para las indicaciones actuales y en las dosis aprobadas, la ivermectina es un fÃ¡rmaco muy seguro. Hasta la fecha, se han distribuido mÃ¡s de 3.000 millones de tratamientos solo en el contexto del programa de donaciÃ³n Mectizan con un excelente perfil de seguridad. La mayorÃ­a de las reacciones adversas son leves, transitorias y se asocian con la muerte del parÃ¡sito mÃ¡s que con el fÃ¡rmaco en sÃ­.

La ivermectina tiene afinidad por los canales de cloro glutamato dependientes, que solo estÃ¡n presentes en invertebrados. Los mamÃ­feros solo expresan un canal similar que podrÃ­a reaccionar de forma cruzada con la ivermectina (los canales de cloro GABA dependientes), pero estos solo se expresan en el sistema nervioso central y estÃ¡n protegidos por la barrera hematoencefÃ¡lica, un sistema de bombeo que mantiene potenciales tÃ³xicos fuera de nuestro sistema nervioso. A pesar de eso, Rebecca Chandler describiÃ³ una serie de 28 casos con reacciones neurolÃ³gicas severas despuÃ©s del tratamiento con ivermectina fuera de las Ã¡reas endÃ©micas de oncocercosis.

Cuando se utiliza para las indicaciones actuales y en las dosis aprobadas, la ivermectina es un fÃ¡rmaco muy seguro

En individuos infectados con una carga alta (> 30.000 mf / ml) del parÃ¡sito conocido como Loa loa, el tratamiento con ivermectina puede provocar encefalopatÃ­a grave y la muerte. Esto ha impedido la administraciÃ³n de ivermectina en varios paÃ­ses de Ãfrica central, donde la estrategia reciente de testar y no tratar podrÃ­a permitir el uso del fÃ¡rmaco.

No hay pruebas sÃ³lidas que apoyen el uso de ivermectina en niÃ±os de menos de 15 kg de peso. Tampoco hay evidencia que respalde el uso de ivermectina durante el embarazo.

Â¿Es segura la ivermectina en dosis superiores a las aprobadas?
Guzzo et al. realizaron un estudio con dosis crecientes de ivermectina en el que algunos voluntarios recibieron de manera segura dosis de hasta 2.000 mcg / kg, es decir, diez veces la dosis aprobada para la oncocercosis.

Aunque no se observÃ³ en el estudio de Guzzo, los pacientes que recibieron ivermectina en dosis de 800 mcg / kg o equivalentes en otros estudios han descrito en ocasiones alteraciones visuales transitorias.

Smit et al. administraron de forma segura 600 mcg / kg diarios durante tres dÃ­as.

Â¿Tiene la ivermectina propiedades antivirales?
SÃ­. Se ha demostrado que la ivermectina inhibe la replicaciÃ³n de varios virus de ARN como:

Dengue
Zika
Fiebre amarilla
Virus del Nilo Occidental
ChikunguÃ±a
Encefalitis equina venezolana
Virus del bosque de Semliki
Virus Sindbis
Virus del sÃ­ndrome reproductivo y respiratorio porcino y, mÃ¡s recientemente,
SARS-CoV-2
Â¿PodrÃ­a la ivermectina tener un papel en el tratamiento o la prevenciÃ³n de la COVID-19?
QuizÃ¡, pero la respuesta no es sencilla.

Los experimentos in vitro de Caly et al. se realizaron agregando ivermectina en un cultivo celular infectado con el virus en una placa de Petri. Las concentraciones demostradas para reducir la replicaciÃ³n viral entre un 50% y un 99% en estos experimentos fueron 2,8 y 5 micromolar, respectivamente.

Para poner esto en perspectiva, la concentraciÃ³n mÃ¡xima alcanzada en la sangre despuÃ©s de una dosis oral Ãºnica de 200 mcg / kg (la dosis habitual para la ceguera de los rÃ­os) es de 40 ng / ml. Por otro lado, 2,8 micromolar es el equivalente a 2.450 ng / ml, es decir, una concentraciÃ³n 60 veces mayor que la mÃ¡xima concentraciÃ³n alcanzada tras las dosis habituales o diez veces la concentraciÃ³n mÃ¡xima observada en el estudio de dosis altas de Guzzo.

Existe incertidumbre o equipoise â€”tÃ©rmino que se utiliza en bioÃ©tica para definir una situaciÃ³n en la que existe una duda razonable de si un medicamento puede ser Ãºtil o no

Entonces, Â¿por quÃ© molestarse en investigar esto?
La COVID-19 es una emergencia de salud pÃºblica de importancia internacional y no existe un tratamiento especÃ­fico para ella. Este hecho, junto con el excelente perfil de seguridad de la ivermectina, se combinan para justificar la investigaciÃ³n sobre su uso potencial. AdemÃ¡s, la extrapolaciÃ³n directa de una placa de Petri a un organismo vivo no es correcta. Varios factores pueden contribuir a hacer que la ivermectina sea eficaz in vivo en dosis mÃ¡s bajas que las descritas por Caly et al. AquÃ­ estÃ¡n algunos de ellos:

Sistema inmunitario. Un cultivo celular es solo una capa de cÃ©lulas en una placa de Petri y no contiene un sistema inmunolÃ³gico para combatir el virus "codo con codo" con la ivermectina.
La carga viral. La proporciÃ³n de virus y cÃ©lulas en una placa de Petri puede estar muy lejos de lo que se puede esperar en un organismo, es decir, puede haber un exceso de la relaciÃ³n virus a cÃ©lulas en el cultivo que desequilibrarÃ­a la lucha a favor del virus y que llevarÃ­a a subestimar el verdadero efecto de la ivermectina en los resultados in vitro.
Efecto inmunomodulador. La ivermectina tiene la capacidad de modular la respuesta inmune. Una respuesta inmune exacerbada es en parte responsable de la fisiopatologÃ­a de COVID-19.
Mayor penetraciÃ³n en los pulmones. Aunque existen opiniones contradictorias al respecto, la ivermectina, dada su lipofilicidad, se acumula en compartimentos profundos, incluidos los pulmones.
Desajuste previo in vitro - in vivo. Algunos efectos in vivo pueden ser posibles incluso si no se pueden lograr concentraciones eficaces in vitro. Un reciente ensayo clÃ­nico de fase III en pacientes con dengue en Tailandia, en el que se encontrÃ³ que una dosis de 400 Î¼g / kg una vez al dÃ­a durante tres dÃ­as era segura pero no produjo ningÃºn beneficio clÃ­nico, mostrÃ³ un efecto in vivo modesto e indirecto contra el dengue al reducir la circulaciÃ³n de ciertas proteÃ­nas virales.
Â¿Se utilizÃ³ la lÃ­nea celular correcta? Las cÃ©lulas VERO son cÃ©lulas renales de mono verde africano, aunque se utilizan ampliamente en cultivos virales y experimentos in vitro, es posible que no sean la mejor plataforma para el SARS-CoV-2. DespuÃ©s de mostrar resultados prometedores en las cÃ©lulas VERO, la hidroxicloroquina no tuvo el mismo efecto en los cultivos de cÃ©lulas pulmonares humanas. En el caso de la ivermectina, este efecto podrÃ­a o bien favorecer a la ivermectina o bien reducir su eficacia.
Otros posibles mecanismos de acciÃ³n incluyen: inhibiciÃ³n de la enzima viral utilizada para desenrollar su ARN, la helicasa, para la que parece que la ivermectina puede ser eficaz en concentraciones mucho mÃ¡s bajas. InteracciÃ³n con el receptor nicotÃ­nico de acetilcolina que puede causar inmunomodulaciÃ³n o reducir la expresiÃ³n de ACE-II, el receptor utilizado por el virus para ingresar a las cÃ©lulas.
Â¿QuÃ© pasa con la importina alfa / beta? Las importinas son proteÃ­nas de transporte intracelular que a veces utilizan los virus para ingresar al nÃºcleo y replicar su material genÃ©tico. La ivermectina inhibe la importina y esto se ha citado ampliamente como el mecanismo de acciÃ³n potencial contra el SARS-CoV-2. Esto puede tener un efecto en la COVID-19 a travÃ©s de un mecanismo directo o indirecto1.
En resumen, hay incertidumbre o equipoise. Este es un tÃ©rmino que se utiliza en bioÃ©tica para definir una situaciÃ³n en la que existe una duda razonable de si un medicamento puede ser Ãºtil o no. Esto justifica que se pruebe la ivermectina contra el SARS-CoV-2 en ensayos clÃ­nicos.

Entonces, Â¿por quÃ© no administramos ivermectina a las personas que la necesitan hoy? Â¡La gente se estÃ¡ muriendo!
El proceso para lograr un uso generalizado de un medicamento estÃ¡ muy regulado. Esto se debe a buenas razones. Se han aprendido lecciones despuÃ©s de algunas tragedias graves.

El principal requisito para aprobar un medicamento para su uso a nivel poblacional es su eficacia y seguridad probadas. Para la ivermectina hay pruebas sÃ³lidas de seguridad cuando se usa para la indicaciÃ³n aprobada en las dosis aprobadas. Existe evidencia muy limitada sobre la seguridad de la ivermectina en dosis mÃ¡s altas (ver enlace de la tabla anterior) e incluso menos evidencia de su seguridad cuando se usa en pacientes con COVID-19 que tienden a tener estados proinflamatorios. Estos estados proinflamatorios pueden aumentar la penetraciÃ³n de la ivermectina en el sistema nervioso central con consecuencias desconocidas (ver seguridad arriba). Otros problemas potenciales incluyen interacciones medicamentosas con algunos antivirales administrados a pacientes con COVID-19, como el ritonavir, que pueden aumentar los niveles de ivermectina.

Dada la ausencia de evidencia razonable de que la ivermectina tenga alguna eficacia contra el SARS-CoV-2, el anÃ¡lisis de riesgo-beneficio dicta que debemos ser prudentes

Dada la ausencia de evidencia razonable de que la ivermectina tenga alguna eficacia contra el SARS-CoV-2, el anÃ¡lisis de riesgo-beneficio dicta que debemos ser prudentes, es decir, evaluar la eficacia (y la seguridad en este contexto) antes de adoptar ivermectina a nivel poblacional.

La evidencia de la eficacia debe provenir de ensayos clÃ­nicos aleatorizados controlados (preferiblemente doble ciego), que son estudios en los que los pacientes se asignan al azar para recibir ivermectina o un placebo y en los que ni el paciente ni el mÃ©dico saben quÃ© producto se administra. Esto se hace para prevenir sesgos comunes que pueden afectar a los resultados de los ensayos no aleatorizados, como que el fÃ¡rmaco en cuestiÃ³n se administre solo a pacientes mÃ¡s graves (o menos graves), la administraciÃ³n de fÃ¡rmacos adicionales que podrÃ­an cambiar el resultado, la retirada de pacientes que no "funcionan bien â€despuÃ©s del tratamiento, etc.

Los ensayos clÃ­nicos llevan mucho tiempo, Â¿por quÃ© esperar? Si es seguro, Â¿quÃ© se puede perder?
El uso generalizado de la ivermectina puede (Â¡y ya lo ha hecho!) provocar un uso indebido. La ivermectina estÃ¡ ampliamente disponible como medicamento veterinario. Si existe la percepciÃ³n de que la ivermectina es beneficiosa para los pacientes con COVID-19, es previsible que las formulaciones veterinarias se utilicen a gran escala y este uso sin supervisiÃ³n puede provocar sobredosis y otras prÃ¡cticas nocivas. La Agencia Federal del Medicamento de EE. UU. ha emitido una advertencia sobre el uso de productos veterinarios con ivermectina.

En PerÃº, mÃ¡s de 5.000 indÃ­genas fueron inyectados con ivermectina veterinaria por un grupo con buenas intenciones. Â¿Eran conscientes de que se les estaba inyectando un producto destinado a uso animal?

En esta lÃ­nea, tambiÃ©n hay informes de inyecciÃ³n con productos veterinarios relacionados con la ivermectina, como la doramectina, una molÃ©cula nunca antes utilizada en humanos con un perfil de seguridad desconocido.

Si efectivamente se producen eventos adversos como resultado de este uso indiscriminado, la distribuciÃ³n del medicamento para indicaciones comprobadas (por ejemplo, ceguera de los rÃ­os) podrÃ­a verse afectada, ya que las personas podrÃ­an negarse a tomar la versiÃ³n del medicamento para humanos como consecuencia de los efectos en quienes se inyectan productos veterinarios.

Los parÃ¡sitos intestinales son muy frecuentes en los trÃ³picos. Sabemos que los helmintos intestinales modulan la forma en que nuestro sistema inmunitario reacciona ante las infecciones. Dado que la ivermectina es un muy buen desparasitante, no sabemos cÃ³mo la desparasitaciÃ³n masiva puede afectar a la forma en que el cuerpo responde contra el SARS-CoV-2. Primum non nocere o quizÃ¡s primum ivermectinum, secundum non nocere?

Riesgo moral. Aquellos que reciben ivermectina como tratamiento o profilaxis pueden sentirse protegidos y cumplir de manera menos eficiente con medidas comprobadas como mascarillas y distanciamiento social.

Por Ãºltimo, tenemos en la mayor estima a los trabajadores de la salud de primera lÃ­nea y no juzgamos las decisiones y la desesperaciÃ³n de aquellos que enfrentan pacientes moribundos o que empeoran. Pero, Â¿podrÃ­a el uso de ivermectina (que es relativamente segura) ser una forma de ofrecer algo y reducir la presiÃ³n polÃ­tica y pÃºblica sobre los responsables polÃ­ticos y las autoridades sanitarias?

Entonces, Â¿por quÃ© estÃ¡ en las directrices nacionales de PerÃº, Bolivia y varios municipios de Brasil?
Muchos paÃ­ses, ante un aumento exponencial en el nÃºmero de casos y muertes, estaban evaluando activamente el horizonte en busca de medidas preventivas y tratamientos emergentes. El estudio in vitro de Caly et al. que hizo afirmaciones enfÃ¡ticas para las dosis utilizadas preparÃ³ el escenario para la ivermectina, pero fue seguido rÃ¡pidamente por lo que parece ser un informe fraudulento que muestra supuestos excelentes resultados en un estudio de casos y controles. Esa preimpresiÃ³n de Surgisphere, aunque posteriormente retractada, todavÃ­a se cita ampliamente como evidencia de la eficacia de la ivermectina contra la COVID-19, particularmente en AmÃ©rica Latina.

A diferencia de muchos lugares del mundo, la ivermectina se produce localmente en forma de una formulaciÃ³n de gotas en muchos paÃ­ses de AmÃ©rica Latina. Esta disponibilidad local puede haber influido en su popularidad.

Â¡Pero hay evidencia de eficacia adicional de otros estudios!
Los ensayos con resultados actualmente disponibles no se basan en el estÃ¡ndar de oro: los ensayos controlados aleatorizados (preferiblemente, doble ciegos). En su mayorÃ­a son estudios de casos y controles e incluso estudios que comparan dos esquemas de medicamentos completamente diferentes que difÃ­cilmente pueden probar el beneficio de la ivermectina. La pirÃ¡mide de evidencia en medicina clÃ­nica (por Akobeng en BMJ) se proporciona a continuaciÃ³n como referencia. Se debe tener cuidado con los informes aislados de "buena experiencia clÃ­nica", dado el riesgo de sesgo mencionado anteriormente. A veces, incluso los datos ecolÃ³gicos se utilizan para respaldar las afirmaciones de eficacia o falta de eficacia de la ivermectina contra la COVID-19, pero estos conjuntos de datos existentes en cada paÃ­s se pueden tergiversar de manera que reflejen mejor las opiniones del analista.

JerarquÃ­a de evidencia para preguntas sobre la efectividad de una intervenciÃ³n o tratamiento.
JerarquÃ­a de evidencia para preguntas sobre la efectividad de una intervenciÃ³n o tratamiento. [Fuente: Akobeng AK. Understanding randomised controlled trials. Archives of Disease in Childhood 2005;90:840-844.]

Â¿QuÃ© ensayos clÃ­nicos estÃ¡n en curso?
SegÃºn la plataforma ClinicalTrials.gov, hay 34 ensayos que usan ivermectina en pacientes con COVID-19, de los cuales 31 aÃºn estÃ¡n activos (a fecha de 17 de agosto de 2020). Ninguno de los ensayos marcados como completados tiene resultados disponibles en esa plataforma.

Â¡Las grandes farmacÃ©uticas no quieren que se use ivermectina porque quieren beneficiarse de medicamentos mÃ¡s costosos!
Existen varios modelos de negocio en la industria farmacÃ©utica. Vender volÃºmenes bajos a precios altos es una opciÃ³n con fines de lucro. Pero en el caso de la COVID-19, el mercado objetivo es la humanidad, es decir, unos 7.000 millones de personas que son susceptibles de tomar este medicamento ahora y en un futuro cercano. Incluso con mÃ¡rgenes muy, muy bajos, un volumen tan alto definitivamente compensarÃ­a a cualquier fabricante dispuesto a aumentar la producciÃ³n y la distribuciÃ³n.

Â¿QuÃ© ha hecho ISGlobal sobre este tema?
ISGlobal se ha asociado con la ClÃ­nica Universidad de Navarra para patrocinar un ensayo clÃ­nico, SAINT, que podrÃ­a ayudar a probar el concepto de un efecto biolÃ³gico de la ivermectina en la replicaciÃ³n del SARS-CoV-2. El protocolo de este ensayo estÃ¡ disponible abiertamente en su totalidad para cualquier persona que desee reproducirlo. El ensayo en Navarra ya ha reclutado al 20% de su objetivo y los resultados deberÃ­an estar disponibles cuatro semanas despuÃ©s de reclutar al Ãºltimo paciente. El reclutamiento ha sido mÃ¡s lento de lo previsto porque la aprobaciÃ³n regulatoria se produjo durante el cierre que llevÃ³ los casos a niveles tan bajos como 0-4 casos por dÃ­a en toda Navarra durante junio y julio.

ISGlobal se ha asociado con la ClÃ­nica Universidad de Navarra para patrocinar un ensayo clÃ­nico, SAINT, que podrÃ­a ayudar a probar el concepto de un efecto biolÃ³gico de la ivermectina en la replicaciÃ³n del SARS-CoV-2

ISGlobal se ha asociado con la Universidad Cayetano Heredia en PerÃº para replicar el ensayo SAINT en Lima.

Varios investigadores de ISGlobal desempeÃ±aron un papel clave en el descubrimiento del escÃ¡ndalo #LancetGate sobre la hidroxicloroquina y la ivermectina.

Se estÃ¡n realizando ensayos adicionales y sus resultados se divulgarÃ¡n a su debido tiempo.

 

Actualizaciones
1 El 17 de diciembre de 2020, se actualizÃ³ la respuesta a la pregunta "Â¿QuÃ© pasa con la importina alfa / beta?". Debido a la apariciÃ³n de nueva evidencia, se eliminÃ³ la frase: "Esto es muy desconcertante, ya que el SARS-CoV-2 se replica en el citosol. No entra en el nÃºcleo para su replicaciÃ³n. Por lo tanto, es poco probable que el mecanismo de importaciÃ³n ampliamente citado tenga un papel aquÃ­". Y se reemplazÃ³ por esta otra: "Esto puede tener un efecto en la COVID-19 a travÃ©s de un mecanismo directo o indirecto."

Contenidos relacionados
Ivermectina y COVID-19: Â¿quÃ© estÃ¡ pasando?


Ivermectina y COVID-19
Ivermectina y COVID-19
CÃ³mo una base de datos dudosa dio forma a la respuesta de varios paÃ­ses latinoamericanos a la pandemia

Nota: Las personas que integran ISGlobal persiguen ideas innovadoras con total independencia. Las opiniones expresadas en este blog son, por tanto, a tÃ­tulo personal y no necesariamente reflejan el posicionamiento institucional.

Una iniciativa de
Obra Social, FundaciÃ³ "la Caixa"
ClÃ­nic Barcelona, Hospital Universitari
Parc de Salut
Universitat de Barcelona
Universitat Pompeu Fabra
Generalitat de Catalunya
Gobierno de EspaÃ±a
Ajuntament de Barcelona
ISGlobal
Sobre ISGlobal
InvestigaciÃ³n
Iniciativas
AnÃ¡lisis y Desarrollo
FormaciÃ³n
Actualidad
Campus ClÃ­nic
C/ RossellÃ³, 132, 5Âº 2Âª 08036. Barcelona. Tel. +34 93 227 1806

Campus Mar
C/ Doctor Aiguader, 88. 08003. Barcelona. Tel. +34 93 214 7300

Excelencia Severo Ochoa
InstituciÃ³ CERCA
HR Excellence in Research
    
Contacto
Aviso legal
PolÃ­tica de privacidad
PolÃ­tica de Cookies
Instituto de Salud Global de Barcelona (ISGlobal), 2018.
Acceso/Acerca de la OMS/QuiÃ©nes somos/Preguntas mÃ¡s frecuentes
Preguntas mÃ¡s frecuentes
 Section navigation
Â¿CÃ³mo define la OMS la salud?
Â«La salud es un estado de completo bienestar fÃ­sico, mental y social, y no solamente la ausencia de afecciones o enfermedadesÂ». La cita procede del PreÃ¡mbulo de la ConstituciÃ³n de la OrganizaciÃ³n Mundial de la Salud, que fue adoptada por la Conferencia Sanitaria Internacional, celebrada en Nueva York del 19 de junio al 22 de julio de 1946, firmada el 22 de julio de 1946 por los representantes de 61 Estados (Official Records of the World Health Organization, NÂº 2, p. 100), y entrÃ³ en vigor el 7 de abril de 1948. La definiciÃ³n no ha sido modificada desde 1948.

 

ConstituciÃ³n de la OMS: principios

Publicaciones. Â¿DÃ³nde puedo encontrar informaciÃ³n sobre las publicaciones de la OMS?
La barra de navegaciÃ³n situada en la parte superior del sitio web de la OMS tiene un enlace denominado 'Publicaciones' que conduce a una pÃ¡gina informativa sobre las publicaciones de la OMS, desde la que se puede acceder a la librerÃ­a en lÃ­nea, a informaciÃ³n sobre suscripciones y a informaciÃ³n sobre las principales publicaciones y revistas.

Publicaciones

Enlaces. Â¿Puedo poner en mi sitio web un enlace al sitio web de la OMS? Â¿QuÃ© debo hacer para pedir que en el sitio de la OMS haya un enlace a mi sitio web?
En principio, todo sitio web externo puede proporcionar un hiperenlace al sitio web de la OMS sin necesidad de autorizaciÃ³n. Sin embargo, este uso no debe infringir los derechos de propiedad intelectual de la OMS, en particular los relativos al nombre, al emblema y a los derechos de autor de la OrganizaciÃ³n. Normalmente, la OMS no proporciona enlaces a sitios web externos, a no ser que exista una clara vinculaciÃ³n con sus actividades. Para mÃ¡s informaciÃ³n, remÃ­tase a la pÃ¡gina Autorizaciones y licencias.

Autorizaciones y licencias

Empleo. Â¿DÃ³nde puedo encontrar informaciÃ³n sobre las oportunidades de empleo en la OMS?
La pÃ¡gina Empleo en la OMS proporciona una lista actualizada de los puestos vacantes y de los tipos de contratos. Cree su perfil personal en el sistema electrÃ³nico de contrataciÃ³n (en inglÃ©s) y presÃ©ntese a los puestos que figuren en la lista.

La pÃ¡gina Empleo en la OMS

PasantÃ­as. Â¿DÃ³nde puedo encontrar informaciÃ³n sobre las pasantÃ­as?
La informaciÃ³n al respecto se encuentra en la pÃ¡gina PasantÃ­as.

PasantÃ­as

Becas y subvenciones. Â¿Ofrece la OMS becas y subvenciones a la investigaciÃ³n?
La OMS no dispone de un programa de becas o subvenciones como tal, pero hay algunos departamentos y programas especiales de la OMS que financian investigaciones. Visite la pÃ¡gina Grants (Subsidios) del sitio web del TDR (InvestigaciÃ³n sobre Enfermedades Tropicales) o la pÃ¡gina Capacity strengthening (Fortalecimiento de la capacidad) del sitio web de RHR (InvestigaciÃ³n sobre Salud Reproductiva). Asimismo, puede consultar el sitio web de la Oficina Regional de la OMS a la que pertenezca su paÃ­s. Las oficinas regionales tienen algunos programas de becas que se llevan a cabo con la cooperaciÃ³n de los ministerios de salud de los paÃ­ses.

Programa de InvestigaciÃ³n sobre Enferemedades Tropicales - en inglÃ©s

PaÃ­ses

InvestigaciÃ³n. Estoy haciendo una investigaciÃ³n Â¿por dÃ³nde debo empezar a buscar informaciÃ³n?
Si se trata de una investigaciÃ³n sobre un tema de salud en particular, empiece por buscar en la lista de Temas de salud. La pÃ¡gina de cada tema de salud proporciona listas de otros sitios conexos, enlaces y documentos. Si busca informaciÃ³n sobre un determinado paÃ­s o regiÃ³n de la OMS, dirÃ­jase al Sitio web de la oficina regional de la OMS correspondiente. TambiÃ©n puede encontrar informaciÃ³n sobre los paÃ­ses a travÃ©s del enlace PaÃ­ses situado en la barra de navegaciÃ³n de la izquierda. En la pÃ¡gina InvestigaciÃ³n encontrarÃ¡ una serie de recursos que podrÃ¡ utilizar en su investigaciÃ³n, entre ellos bases de datos estadÃ­sticas y el catÃ¡logo de la biblioteca.

Temas de salud

Oficinas regionales de la OMS

PaÃ­ses

Â¿CÃ³mo puedo solicitar informaciÃ³n sobre la OMS?
La OMS pone informaciÃ³n a disposiciÃ³n del pÃºblico con arreglo a su PolÃ­tica de divulgaciÃ³n de informaciÃ³n. La PolÃ­tica tiene por objeto aumentar la informaciÃ³n disponible y se introducirÃ¡ progresivamente durante un periodo de dos aÃ±os. A mÃ¡s tardar en noviembre de 2017 se habilitarÃ¡ una direcciÃ³n de correo electrÃ³nico para solicitar informaciÃ³n: informationrequest@who.int.

PolÃ­tica de divulgaciÃ³n de informaciÃ³n

Nombres de los paÃ­ses. Â¿QuiÃ©n determina los nombres oficiales de los paÃ­ses?
Las denominaciones oficiales de los Estados Miembros de la OMS y su posiciÃ³n relativa en las listas alfabÃ©ticas se basan en informaciÃ³n recibida de los propios Estados Miembros y de las Naciones Unidas.

PaÃ­ses

Mapas Â¿Por quÃ© son discontinuas las lÃ­neas de las fronteras en algunos mapas?
Los lÃ­mites y los nombres que figuran en los mapas, asÃ­ como las denominaciones empleadas, no implican, por parte de la OrganizaciÃ³n Mundial de la Salud, juicio alguno sobre la condiciÃ³n jurÃ­dica de paÃ­ses, territorios, ciudades o zonas, o de sus autoridades, ni respecto del trazado de sus fronteras o lÃ­mites. En los mapas, las lÃ­neas discontinuas representan de manera aproximada fronteras respecto de las cuales puede que no haya pleno acuerdo.

GalerÃ­a de mapas - en inglÃ©s

Salud de los viajeros. Pienso viajar al extranjero en breve. Â¿DÃ³nde puedo encontrar informaciÃ³n y consejos sobre los riesgos para la salud?
En el sitio web Salud de los viajeros (en inglÃ©s) puede encontrar informaciÃ³n sobre las vacunas exigidas, los riesgos y las precauciones que debe adoptar dependiendo de cuÃ¡l sea su paÃ­s de destino.

Salud de los viajeros - en inglÃ©s

Â¿He recibido un email fraudulento?
Esos e-mail no proceden de la OMS ni guardan ninguna relaciÃ³n con proyectos o eventos de la OMS. La OMS pone sobre aviso al pÃºblico acerca de esas prÃ¡cticas engaÃ±osas y sugiere a quienes reciban invitaciones como las mencionadas mÃ¡s arriba (ya sea por correo electrÃ³nico o de alguna otra forma) que verifiquen su autenticidad antes de responder. En particular, la OMS sugiere que no se envÃ­e dinero ni informaciÃ³n personal a nadie que afirme otorgar fondos, subvenciones, becas, certificados o premios de loterÃ­a o de otra Ã­ndole, y/o exija pagos en concepto de inscripciÃ³n o de reserva de hotel, en nombre de la OMS. La OMS no tiene por norma exigir pago ninguno a quienes participan en sus reuniones.

Circulan por Internet mensajes electrÃ³nicos fraudulentos que afirman o dan a entender que proceden de la OrganizaciÃ³n Mundial de la Salud (OMS), o que estÃ¡n relacionados con la OrganizaciÃ³n. En algunos se pide a particulares, empresas u organizaciones sin fines de lucro que proporcionen informaciÃ³n o dinero con la promesa de que recibirÃ¡n fondos o alguna prestaciÃ³n a cambio. En otros mensajes se pide el abono de cuotas de inscripciÃ³n en conferencias supuestamente patrocinadas por la OMS, o para reservas de hotel, tambiÃ©n con la promesa de alguna prestaciÃ³n. En ocasiones, esos mensajes portan el emblema de la OMS y proceden de direcciones electrÃ³nicas semejantes a las de la OMS o las Naciones Unidas, o mencionan esas direcciones electrÃ³nicas.

Si tiene dudas sobre la autenticidad de algÃºn mensaje electrÃ³nico, carta o llamada telefÃ³nica que supuestamente reciba de la OMS, o en nombre de la OMS, por favor escrÃ­banos a la direcciÃ³n internet@who.int. La OMS estÃ¡ intentado poner fin a esas prÃ¡cticas engaÃ±osas, por lo que les quedarÃ­amos muy agradecidos si seÃ±alan a nuestra atenciÃ³n cualquier comunicaciÃ³n sospechosa.

No haga caso de los mensajes falsamente relacionados con la OMS

 

Acerca de la OMS
Ayuda y servicios
Oficinas regionales de la OMS
Acceso
PolÃ­tica de privacidad
Â© 2021 OMS
unittest â€” Marco de prueba automatizado
PropÃ³sito:	Marco de prueba automatizado
El mÃ³dulo unittest de Python se basa en el diseÃ±o del marco XUnit de Kent Beck y Erich Gamma. El mismo patrÃ³n se repite en muchos otros lenguajes, incluidos C, Perl, Java y Smalltalk. El marco implementado por unittest admite accesorios (fixtures), conjuntos de pruebas y un corredor de pruebas para permitir las pruebas automatizadas.

Estructura bÃ¡sica de las pruebas
Las pruebas, segÃºn lo definido por unittest, tienen dos partes: cÃ³digo para administrar las dependencias de prueba (llamadas fixtures) y la prueba en sÃ­. Las pruebas individuales se crean subclasificando TestCase y anulando o agregando los mÃ©todos apropiados. En el siguiente ejemplo, el SimplisticTest tiene un solo mÃ©todo test(), que fallarÃ­a si a es diferente de b.

unittest_simple.py
import unittest


class SimplisticTest(unittest.TestCase):

    def test(self):
        a = 'a'
        b = 'a'
        self.assertEqual(a, b)
Ejecutar las pruebas
La forma mÃ¡s fÃ¡cil de ejecutar pruebas de unittest es usar el descubrimiento automÃ¡tico disponible a travÃ©s de la interfaz de lÃ­nea de comandos.

$ python3 -m unittest unittest_simple.py

.
----------------------------------------------------------------
Ran 1 test in 0.000s

OK
Esta salida abreviada incluye la cantidad de tiempo que tomaron las pruebas, junto con un indicador de estado para cada prueba (el Â«.Â» En la primera lÃ­nea de salida significa que pasÃ³ una prueba). Para obtener resultados de prueba mÃ¡s detallados, incluye la opciÃ³n -v.

$ python3 -m unittest -v unittest_simple.py

test (unittest_simple.SimplisticTest) ... ok

----------------------------------------------------------------
Ran 1 test in 0.000s

OK
Resultados de las pruebas
Las pruebas tienen 3 resultados posibles, descritos en: tabla: Resultados del caso de prueba.

Resultados del caso de prueba
Resultado	DescripciÃ³n
ok	La prueba pasa.
FAIL	La prueba no pasa y genera una excepciÃ³n AssertionError.
ERROR	La prueba genera cualquier excepciÃ³n que no es AssertionError.
No hay una forma explÃ­cita de hacer que una prueba Â«paseÂ», por lo que el estado de una prueba depende de la presencia (o ausencia) de una excepciÃ³n.

unittest_outcomes.py
import unittest


class OutcomesTest(unittest.TestCase):

    def testPass(self):
        return

    def testFail(self):
        self.assertFalse(True)

    def testError(self):
        raise RuntimeError('Test error!')
Cuando una prueba falla o genera un error, el rastreo se incluye en la salida.

$ python3 -m unittest unittest_outcomes.py

EF.
================================================================
ERROR: testError (unittest_outcomes.OutcomesTest)
----------------------------------------------------------------
Traceback (most recent call last):
  File ".../unittest_outcomes.py", line 18, in testError
    raise RuntimeError('Test error!')
RuntimeError: Test error!

================================================================
FAIL: testFail (unittest_outcomes.OutcomesTest)
----------------------------------------------------------------
Traceback (most recent call last):
  File ".../unittest_outcomes.py", line 15, in testFail
    self.assertFalse(True)
AssertionError: True is not false

----------------------------------------------------------------
Ran 3 tests in 0.001s

FAILED (failures=1, errors=1)
En el ejemplo anterior, testFail() falla y el rastreo muestra la lÃ­nea con el cÃ³digo de falla. Sin embargo, depende de la persona que lee el resultado de la prueba mirar el cÃ³digo para descubrir el significado de la prueba fallida.

unittest_failwithmessage.py
import unittest


class FailureMessageTest(unittest.TestCase):

    def testFail(self):
        self.assertFalse(True, 'failure message goes here')
Para facilitar la comprensiÃ³n de la naturaleza de una falla en la prueba, los mÃ©todos fail*() y assert*() aceptan un argumento msg, que se puede usar para generar mensaje de error detallado.

$ python3 -m unittest -v unittest_failwithmessage.py

testFail (unittest_failwithmessage.FailureMessageTest) ... FAIL

================================================================
FAIL: testFail (unittest_failwithmessage.FailureMessageTest)
----------------------------------------------------------------
Traceback (most recent call last):
  File ".../unittest_failwithmessage.py", line 12, in testFail
    self.assertFalse(True, 'failure message goes here')
AssertionError: True is not false : failure message goes here

----------------------------------------------------------------
Ran 1 test in 0.000s

FAILED (failures=1)
Probar la verdad
La mayorÃ­a de las pruebas afirman la verdad de alguna condiciÃ³n. Hay dos formas diferentes de escribir pruebas de verificaciÃ³n de la verdad, segÃºn la perspectiva del autor de la prueba y el resultado deseado del cÃ³digo que se estÃ¡ probando.

unittest_truth.py
import unittest


class TruthTest(unittest.TestCase):

    def testAssertTrue(self):
        self.assertTrue(True)

    def testAssertFalse(self):
        self.assertFalse(False)
Si el cÃ³digo produce un valor que se puede evaluar como verdadero, se debe usar el mÃ©todo assertTrue(). Si el cÃ³digo produce un valor falso, el mÃ©todo assertFalse() tiene mÃ¡s sentido.

$ python3 -m unittest -v unittest_truth.py

testAssertFalse (unittest_truth.TruthTest) ... ok
testAssertTrue (unittest_truth.TruthTest) ... ok

----------------------------------------------------------------
Ran 2 tests in 0.000s

OK
Probar la igualdad
Como caso especial, unittest incluye mÃ©todos para probar la igualdad de dos valores.

unittest_equality.py
import unittest


class EqualityTest(unittest.TestCase):

    def testExpectEqual(self):
        self.assertEqual(1, 3 - 2)

    def testExpectEqualFails(self):
        self.assertEqual(2, 3 - 2)

    def testExpectNotEqual(self):
        self.assertNotEqual(2, 3 - 2)

    def testExpectNotEqualFails(self):
        self.assertNotEqual(1, 3 - 2)
Cuando fallan, estos mÃ©todos de prueba especiales producen mensajes de error que incluyen los valores que se comparan.

$ python3 -m unittest -v unittest_equality.py

testExpectEqual (unittest_equality.EqualityTest) ... ok
testExpectEqualFails (unittest_equality.EqualityTest) ... FAIL
testExpectNotEqual (unittest_equality.EqualityTest) ... ok
testExpectNotEqualFails (unittest_equality.EqualityTest) ...
FAIL

================================================================
FAIL: testExpectEqualFails (unittest_equality.EqualityTest)
----------------------------------------------------------------
Traceback (most recent call last):
  File ".../unittest_equality.py", line 15, in
testExpectEqualFails
    self.assertEqual(2, 3 - 2)
AssertionError: 2 != 1

================================================================
FAIL: testExpectNotEqualFails (unittest_equality.EqualityTest)
----------------------------------------------------------------
Traceback (most recent call last):
  File ".../unittest_equality.py", line 21, in
testExpectNotEqualFails
    self.assertNotEqual(1, 3 - 2)
AssertionError: 1 == 1

----------------------------------------------------------------
Ran 4 tests in 0.001s

FAILED (failures=2)
Â¿Casi igual?
AdemÃ¡s de la igualdad estricta, es posible probar la casi igualdad de los nÃºmeros de coma flotante usando assertAlmostEqual() y assertNotAlmostEqual().

unittest_almostequal.py
import unittest


class AlmostEqualTest(unittest.TestCase):

    def testEqual(self):
        self.assertEqual(1.1, 3.3 - 2.2)

    def testAlmostEqual(self):
        self.assertAlmostEqual(1.1, 3.3 - 2.2, places=1)

    def testNotAlmostEqual(self):
        self.assertNotAlmostEqual(1.1, 3.3 - 2.0, places=1)
Los argumentos son los valores a comparar y el nÃºmero de decimales que se utilizarÃ¡n para la prueba.

$ python3 -m unittest unittest_almostequal.py

.F.
================================================================
FAIL: testEqual (unittest_almostequal.AlmostEqualTest)
----------------------------------------------------------------
Traceback (most recent call last):
  File ".../unittest_almostequal.py", line 12, in testEqual
    self.assertEqual(1.1, 3.3 - 2.2)
AssertionError: 1.1 != 1.0999999999999996

----------------------------------------------------------------
Ran 3 tests in 0.001s

FAILED (failures=1)
Contenedores
AdemÃ¡s del genÃ©rico assertEqual() y assertNotEqual(), hay mÃ©todos especiales para comparar contenedores como objetos list, dict y set.

unittest_equality_container.py
import textwrap
import unittest


class ContainerEqualityTest(unittest.TestCase):

    def testCount(self):
        self.assertCountEqual(
            [1, 2, 3, 2],
            [1, 3, 2, 3],
        )

    def testDict(self):
        self.assertDictEqual(
            {'a': 1, 'b': 2},
            {'a': 1, 'b': 3},
        )

    def testList(self):
        self.assertListEqual(
            [1, 2, 3],
            [1, 3, 2],
        )

    def testMultiLineString(self):
        self.assertMultiLineEqual(
            textwrap.dedent("""
            This string
            has more than one
            line.
            """),
            textwrap.dedent("""
            This string has
            more than two
            lines.
            """),
        )

    def testSequence(self):
        self.assertSequenceEqual(
            [1, 2, 3],
            [1, 3, 2],
        )

    def testSet(self):
        self.assertSetEqual(
            set([1, 2, 3]),
            set([1, 3, 2, 4]),
        )

    def testTuple(self):
        self.assertTupleEqual(
            (1, 'a'),
            (1, 'b'),
        )
Cada mÃ©todo informa la desigualdad utilizando un formato que es significativo para el tipo de entrada, lo que hace que las fallas de prueba sean mÃ¡s fÃ¡ciles de entender y corregir.

$ python3 -m unittest unittest_equality_container.py

FFFFFFF
================================================================
FAIL: testCount
(unittest_equality_container.ContainerEqualityTest)
----------------------------------------------------------------
Traceback (most recent call last):
  File ".../unittest_equality_container.py", line 15, in
testCount
    [1, 3, 2, 3],
AssertionError: Element counts were not equal:
First has 2, Second has 1:  2
First has 1, Second has 2:  3

================================================================
FAIL: testDict
(unittest_equality_container.ContainerEqualityTest)
----------------------------------------------------------------
Traceback (most recent call last):
  File ".../unittest_equality_container.py", line 21, in
testDict
    {'a': 1, 'b': 3},
AssertionError: {'a': 1, 'b': 2} != {'a': 1, 'b': 3}
- {'a': 1, 'b': 2}
?               ^

+ {'a': 1, 'b': 3}
?               ^


================================================================
FAIL: testList
(unittest_equality_container.ContainerEqualityTest)
----------------------------------------------------------------
Traceback (most recent call last):
  File ".../unittest_equality_container.py", line 27, in
testList
    [1, 3, 2],
AssertionError: Lists differ: [1, 2, 3] != [1, 3, 2]

First differing element 1:
2
3

- [1, 2, 3]
+ [1, 3, 2]

================================================================
FAIL: testMultiLineString
(unittest_equality_container.ContainerEqualityTest)
----------------------------------------------------------------
Traceback (most recent call last):
  File ".../unittest_equality_container.py", line 41, in
testMultiLineString
    """),
AssertionError: '\nThis string\nhas more than one\nline.\n' !=
'\nThis string has\nmore than two\nlines.\n'

- This string
+ This string has
?            ++++
- has more than one
? ----           --
+ more than two
?           ++
- line.
+ lines.
?     +


================================================================
FAIL: testSequence
(unittest_equality_container.ContainerEqualityTest)
----------------------------------------------------------------
Traceback (most recent call last):
  File ".../unittest_equality_container.py", line 47, in
testSequence
    [1, 3, 2],
AssertionError: Sequences differ: [1, 2, 3] != [1, 3, 2]

First differing element 1:
2
3

- [1, 2, 3]
+ [1, 3, 2]

================================================================
FAIL: testSet
(unittest_equality_container.ContainerEqualityTest)
----------------------------------------------------------------
Traceback (most recent call last):
  File ".../unittest_equality_container.py", line 53, in testSet
    set([1, 3, 2, 4]),
AssertionError: Items in the second set but not the first:
4

================================================================
FAIL: testTuple
(unittest_equality_container.ContainerEqualityTest)
----------------------------------------------------------------
Traceback (most recent call last):
  File ".../unittest_equality_container.py", line 59, in
testTuple
    (1, 'b'),
AssertionError: Tuples differ: (1, 'a') != (1, 'b')

First differing element 1:
'a'
'b'

- (1, 'a')
?      ^

+ (1, 'b')
?      ^


----------------------------------------------------------------
Ran 7 tests in 0.005s

FAILED (failures=7)
Use assertIn() para probar la membresÃ­a del contenedor.

unittest_in.py
import unittest


class ContainerMembershipTest(unittest.TestCase):

    def testDict(self):
        self.assertIn(4, {1: 'a', 2: 'b', 3: 'c'})

    def testList(self):
        self.assertIn(4, [1, 2, 3])

    def testSet(self):
        self.assertIn(4, set([1, 2, 3]))
Cualquier objeto que admita el operador in o la interfaz de programaciÃ³n del contenedor se puede usar con assertIn().

$ python3 -m unittest unittest_in.py

FFF
================================================================
FAIL: testDict (unittest_in.ContainerMembershipTest)
----------------------------------------------------------------
Traceback (most recent call last):
  File ".../unittest_in.py", line 12, in testDict
    self.assertIn(4, {1: 'a', 2: 'b', 3: 'c'})
AssertionError: 4 not found in {1: 'a', 2: 'b', 3: 'c'}

================================================================
FAIL: testList (unittest_in.ContainerMembershipTest)
----------------------------------------------------------------
Traceback (most recent call last):
  File ".../unittest_in.py", line 15, in testList
    self.assertIn(4, [1, 2, 3])
AssertionError: 4 not found in [1, 2, 3]

================================================================
FAIL: testSet (unittest_in.ContainerMembershipTest)
----------------------------------------------------------------
Traceback (most recent call last):
  File ".../unittest_in.py", line 18, in testSet
    self.assertIn(4, set([1, 2, 3]))
AssertionError: 4 not found in {1, 2, 3}

----------------------------------------------------------------
Ran 3 tests in 0.001s

FAILED (failures=3)
Probar las excepciones
Como se mencionÃ³ anteriormente, si una prueba genera una excepciÃ³n distinta de AssertionError, se trata como un error. Esto es muy Ãºtil para descubrir errores al modificar el cÃ³digo que tiene cobertura de prueba existente. Sin embargo, hay circunstancias en las que la prueba debe verificar que algÃºn cÃ³digo produce una excepciÃ³n. Por ejemplo, si se da un valor no vÃ¡lido a un atributo de un objeto. En tales casos, assertRaises() hace que el cÃ³digo sea mÃ¡s claro que atrapar la excepciÃ³n en la prueba. Compara estas dos pruebas:

unittest_exception.py
import unittest


def raises_error(*args, **kwds):
    raise ValueError('Invalid value: ' + str(args) + str(kwds))


class ExceptionTest(unittest.TestCase):

    def testTrapLocally(self):
        try:
            raises_error('a', b='c')
        except ValueError:
            pass
        else:
            self.fail('Did not see ValueError')

    def testAssertRaises(self):
        self.assertRaises(
            ValueError,
            raises_error,
            'a',
            b='c',
        )
Los resultados para ambos son los mismos, pero la segunda prueba que usa assertRaises() es mÃ¡s sucinta.

$ python3 -m unittest -v unittest_exception.py

testAssertRaises (unittest_exception.ExceptionTest) ... ok
testTrapLocally (unittest_exception.ExceptionTest) ... ok

----------------------------------------------------------------
Ran 2 tests in 0.000s

OK
Accesorios de las pruebas
Los accesorios (fixtures) son recursos externos necesarios para una prueba. Por ejemplo, las pruebas para una clase pueden necesitar una instancia de otra clase que proporcione ajustes de configuraciÃ³n u otro recurso compartido. Otros accesorios de prueba incluyen conexiones de bases de datos y archivos temporales (muchas personas argumentan que el uso de recursos externos hace que tales pruebas no sean pruebas Â«unitariasÂ», pero siguen siendo pruebas y aÃºn son Ãºtiles).

unittest incluye ganchos especiales para configurar y limpiar cualquier accesorio necesario para las pruebas. Para establecer dispositivos para cada caso de prueba individual, anula setUp() en el TestCase. Para limpiarlos, anula tearDown(). Para administrar un conjunto de dispositivos para todas las instancias de una clase de prueba, anula los mÃ©todos de clase setUpClass() y tearDownClass() para el TestCase. Y para manejar operaciones de configuraciÃ³n especialmente costosas para todas las pruebas dentro de un mÃ³dulo, usa las funciones de nivel de mÃ³dulo setUpModule() y tearDownModule().

unittest_fixtures.py
import random
import unittest


def setUpModule():
    print('In setUpModule()')


def tearDownModule():
    print('In tearDownModule()')


class FixturesTest(unittest.TestCase):

    @classmethod
    def setUpClass(cls):
        print('In setUpClass()')
        cls.good_range = range(1, 10)

    @classmethod
    def tearDownClass(cls):
        print('In tearDownClass()')
        del cls.good_range

    def setUp(self):
        super().setUp()
        print('\nIn setUp()')
        # Pick a number sure to be in the range. The range is
        # defined as not including the "stop" value, so make
        # sure it is not included in the set of allowed values
        # for our choice.
        self.value = random.randint(
            self.good_range.start,
            self.good_range.stop - 1,
        )

    def tearDown(self):
        print('In tearDown()')
        del self.value
        super().tearDown()

    def test1(self):
        print('In test1()')
        self.assertIn(self.value, self.good_range)

    def test2(self):
        print('In test2()')
        self.assertIn(self.value, self.good_range)
Cuando se ejecuta esta prueba de muestra, el orden de ejecuciÃ³n del dispositivo y los mÃ©todos de prueba son evidentes.

$ python3 -u -m unittest -v unittest_fixtures.py

In setUpModule()
In setUpClass()
test1 (unittest_fixtures.FixturesTest) ...
In setUp()
In test1()
In tearDown()
ok
test2 (unittest_fixtures.FixturesTest) ...
In setUp()
In test2()
In tearDown()
ok
In tearDownClass()
In tearDownModule()

----------------------------------------------------------------
Ran 2 tests in 0.000s

OK
Es posible que no se invoquen todos los mÃ©todos tearDown si hay errores en el proceso de limpieza de los accesorios. Para asegurarte de que un dispositivo se libere siempre correctamente, usa addCleanup().

unittest_addcleanup.py
import random
import shutil
import tempfile
import unittest


def remove_tmpdir(dirname):
    print('In remove_tmpdir()')
    shutil.rmtree(dirname)


class FixturesTest(unittest.TestCase):

    def setUp(self):
        super().setUp()
        self.tmpdir = tempfile.mkdtemp()
        self.addCleanup(remove_tmpdir, self.tmpdir)

    def test1(self):
        print('\nIn test1()')

    def test2(self):
        print('\nIn test2()')
Esta prueba de ejemplo crea un directorio temporal y luego usa shutil para limpiarlo cuando se realiza la prueba.

$ python3 -u -m unittest -v unittest_addcleanup.py

test1 (unittest_addcleanup.FixturesTest) ...
In test1()
In remove_tmpdir()
ok
test2 (unittest_addcleanup.FixturesTest) ...
In test2()
In remove_tmpdir()
ok

----------------------------------------------------------------
Ran 2 tests in 0.002s

OK
Repetir pruebas con diferentes entradas
Con frecuencia es Ãºtil ejecutar la misma lÃ³gica de prueba con diferentes entradas. En lugar de definir un mÃ©todo de prueba separado para cada caso pequeÃ±o, una forma comÃºn de hacerlo es usar un mÃ©todo de prueba que contenga varias afirmaciones relacionadas. El problema con este enfoque es que tan pronto como una afirmaciÃ³n falla, el resto se omite. Una mejor soluciÃ³n es usar subTest() para crear un contexto para una prueba dentro de un mÃ©todo de prueba. Si la prueba falla, se informa la falla y las pruebas restantes continÃºan.

unittest_subtest.py
import unittest


class SubTest(unittest.TestCase):

    def test_combined(self):
        self.assertRegex('abc', 'a')
        self.assertRegex('abc', 'B')
        # The next assertions are not verified!
        self.assertRegex('abc', 'c')
        self.assertRegex('abc', 'd')

    def test_with_subtest(self):
        for pat in ['a', 'B', 'c', 'd']:
            with self.subTest(pattern=pat):
                self.assertRegex('abc', pat)
En este ejemplo, el mÃ©todo test_combined() nunca ejecuta las afirmaciones para los patrones 'c' y 'd'. El mÃ©todo test_with_subtest() lo hace, e informa correctamente la falla adicional. Ten en cuenta que el corredor de prueba todavÃ­a considera que solo hay dos casos de prueba, a pesar de que hay tres fallas informadas.

$ python3 -m unittest -v unittest_subtest.py

test_combined (unittest_subtest.SubTest) ... FAIL
test_with_subtest (unittest_subtest.SubTest) ...
================================================================
FAIL: test_combined (unittest_subtest.SubTest)
----------------------------------------------------------------
Traceback (most recent call last):
  File ".../unittest_subtest.py", line 13, in test_combined
    self.assertRegex('abc', 'B')
AssertionError: Regex didn't match: 'B' not found in 'abc'

================================================================
FAIL: test_with_subtest (unittest_subtest.SubTest) (pattern='B')
----------------------------------------------------------------
Traceback (most recent call last):
  File ".../unittest_subtest.py", line 21, in test_with_subtest
    self.assertRegex('abc', pat)
AssertionError: Regex didn't match: 'B' not found in 'abc'

================================================================
FAIL: test_with_subtest (unittest_subtest.SubTest) (pattern='d')
----------------------------------------------------------------
Traceback (most recent call last):
  File ".../unittest_subtest.py", line 21, in test_with_subtest
    self.assertRegex('abc', pat)
AssertionError: Regex didn't match: 'd' not found in 'abc'

----------------------------------------------------------------
Ran 2 tests in 0.001s

FAILED (failures=3)
Omitir pruebas
Con frecuencia es Ãºtil poder omitir una prueba si no se cumple alguna condiciÃ³n externa. Por ejemplo, al escribir pruebas para verificar el comportamiento de una biblioteca en una versiÃ³n especÃ­fica de Python, no hay razÃ³n para ejecutar esas pruebas en otras versiones de Python. Las clases y mÃ©todos de prueba se pueden decorar con skip() para omitir siempre las pruebas. Los decoradores skipIf() y skipUnless() se pueden usar para verificar una condiciÃ³n antes de omitir.

unittest_skip.py
import sys
import unittest


class SkippingTest(unittest.TestCase):

    @unittest.skip('always skipped')
    def test(self):
        self.assertTrue(False)

    @unittest.skipIf(sys.version_info[0] > 2,
                     'only runs on python 2')
    def test_python2_only(self):
        self.assertTrue(False)

    @unittest.skipUnless(sys.platform == 'Darwin',
                         'only runs on macOS')
    def test_macos_only(self):
        self.assertTrue(True)

    def test_raise_skiptest(self):
        raise unittest.SkipTest('skipping via exception')
Para condiciones complejas que son difÃ­ciles de expresar en una sola expresiÃ³n para pasar a skipIf() o skipUnless(), un caso de prueba puede generar SkipTest directamente para hacer que la prueba se omita .

$ python3 -m unittest -v unittest_skip.py

test (unittest_skip.SkippingTest) ... skipped 'always skipped'
test_macos_only (unittest_skip.SkippingTest) ... skipped 'only
runs on macOS'
test_python2_only (unittest_skip.SkippingTest) ... skipped 'only
runs on python 2'
test_raise_skiptest (unittest_skip.SkippingTest) ... skipped
'skipping via exception'

----------------------------------------------------------------
Ran 4 tests in 0.000s

OK (skipped=4)
Ignorar las pruebas fallidas
En lugar de eliminar las pruebas que se rompen de manera persistente, se pueden marcar con el decorador expectedFailure() para que se ignore la falla.

unittest_expectedfailure.py
import unittest


class Test(unittest.TestCase):

    @unittest.expectedFailure
    def test_never_passes(self):
        self.assertTrue(False)

    @unittest.expectedFailure
    def test_always_passes(self):hilarante, cÃ³digo de ensaÃ±azas matemÃ¡ticas, algoritmicas y religiosas en pro de un texto de Ã±oqui.
        self.assertTrue(True)
Si de hecho se supera una prueba que se espera que falle, esa condiciÃ³n se trata como un tipo especial de falla y se informa como un Â«Ã©xito inesperadoÂ».

$ python3 -m unittest -v unittest_expectedfailure.py

test_always_passes (unittest_expectedfailure.Test) ...
unexpected success
test_never_passes (unittest_expectedfailure.Test) ... expected
failure

----------------------------------------------------------------
Ran 2 tests in 0.001s

FAILED (expected failures=1, unexpected successes=1)
Ver tambiÃ©n

DocumentaciÃ³n de la biblioteca estÃ¡ndar para unittest
doctest â€“ Un medio alternativo para ejecutar pruebas incrustadas en cadenas de documentos o archivos de documentaciÃ³n externos.
nose â€“ Corredor de pruebas de terceros con funciones de descubrimiento sofisticadas.
pytest â€“ Un popular corredor de pruebas de terceros con soporte para ejecuciÃ³n distribuida y un sistema de administraciÃ³n de accesorios alternativo.
testrepository â€“ Corredor de prueba de terceros utilizado por el proyecto OpenStack, con soporte para ejecuciÃ³n paralela y seguimiento fallas de seguimiento.
Â© Copyright 2018, 2019 Ernesto Rico Schmidt

Construido con Sphinx utilizando un tema proporcionado por Read the Docs.
Contenido
Ãndice ğŸ”ï¸
may
â‡¤â†12â†’â‡¥
mayÃºsculas -- comenzar pÃ¡rrafos
mayÃºsculas iniciales en tÃ­tulos
mayÃºscula -- dar formato al texto
marcadores -- Ayuda
mayÃºsculas -- cambiar a minÃºsculas
mayÃºsculas -- cambiar a minÃºsculas
texto -- mayÃºscula o minÃºscula
mayÃºsculas -- funciÃ³n de correcciÃ³n automÃ¡tica
caracteres -- mayÃºscula o minÃºscula
mayÃºsculas -- cambiar a minÃºsculas despuÃ©s de punto
mayÃºsculas -- evitar despuÃ©s de abreviaturas especÃ­ficas
primeras letras como mayÃºsculas grandes
mapas de bits -- insertar y editar
distinciÃ³n entre mayÃºsculas y minÃºsculas -- bÃºsqueda
marcas de tabulaciÃ³n -- insertar y editar
organizar -- macros y secuencias de Ã³rdenes
copiar -- mediante arrastrar y colocar
distinciÃ³n de mayÃºsculas y minÃºsculas -- comparar el contenido de las celdas (Calc)
vÃ­nculos -- mediante arrastrar y colocar
Ã­ndices -- mostrar/ocultar la ficha Ãndice de la Ayuda
texto -- copiar mediante arrastrar y colocar
Escribir con mayÃºsculas o minÃºsculas
Puede cambiar entre mayÃºsculas y minÃºsculas, formatear texto con versalitas o poner en mayÃºscula la primera letra de cada palabra de una selecciÃ³n.

Icono de nota
Cuando se aplica un formato al texto mediante Formato â–¸ CarÃ¡cter, el texto no se modifica, sino que solo se muestra en pantalla de otra forma. Ahora bien, si se elige Formato â–¸ Texto â–¸ Cambiar uso de mayÃºsculas, el texto se cambia permanentemente.


Para cambiar texto a mayÃºsculas
Seleccione el texto que desea poner en mayÃºscula.

Siga uno de estos procedimientos:

Vaya a Formato â–¸ Texto â–¸ MayÃºsculas.

Seleccione Formato â–¸ CarÃ¡cter, pulse en la pestaÃ±a Efectos tipogrÃ¡ficos y seleccione el uso de mayÃºsculas en el cuadro Efectos. Â«MayÃºsculasÂ» convierte todas las letras en caja alta. Â«TÃ­tuloÂ» lo hace solo en la primera letra de cada palabra. Â«VersalitasÂ» pone todas las letras en mayÃºsculas pero reduce su tamaÃ±o.

Para transformar el texto en minÃºsculas
Seleccione el texto que desee cambiar a minÃºsculas.

Siga uno de estos procedimientos:

Vaya a Formato â–¸ Texto â–¸ MinÃºsculas.

Elija Formato â–¸ CarÃ¡cter, pulse en la pestaÃ±a Efectos tipogrÃ¡ficos y seleccione Â«MinÃºsculasÂ» en el cuadro Efectos.

Temas relacionados

Destacar texto

Cronograma de Actividades


Semana
Martes
Jueves
1
19.01: PresentaciÃ³n del curso.
21.01: NotaciÃ³n asintÃ³tica. AnÃ¡lisis de algoritmos. Insertion Sort. Bubble Sort. 
Lecturas: [C] ApÃ©ndice A y 1-2; [B] 3, 4.1-4.5; [A] 1. 
2
26.01: Divide-and-Conquer. BÃºsqueda Binaria. Recurrencias. Mergesort. 
Lecturas: [C] 2.3 ; [B] 7.1-7.4 y 7.5. 
28.01: SoluciÃ³n de recurrencias 
Lecturas: [C] 4.3-4.5. 
3
02.02: El problema de encontrar el subarreglo de tamaÃ±o mÃ¡ximo. Algoritmo de Strassen para la multiplicaciÃ³n de matrices. 
Lecturas: [C] 4.1-4.2; [B] 7.6 
04.02: Heapsort. 
Lecturas: [C] 6.1-6.4; [B] 5.7; [A] 8.1, 8.2 y 8.4. 
4
09.02: No hubo clase
11.02: Conteo. Probabilidad. Variables aleatorias discretas
Lecturas: [C] C.1-C.3
5
16.02: Feriado de Carnaval
18.02: Quicksort. AnÃ¡lisis del algoritmo Quicksort. Una versiÃ³n aleatoria de Quicksort. 
Lecturas: [C] 7.1-7.4; [B] 7.4.2; [A] 8.3.
6
23.02: Cotas inferiores de los algoritmos de ordenamiento por comparaciones. Ordenamiento en tiempo lineal. Counting sort. Radix sort. Bucket sort. 
Lecturas: [C] 8.1-8.4; [A] 8.6. 
25.02: Ejercicios de prÃ¡ctica y repaso
7
02.03: Examen parcial 1


04.03: Estructuras de datos y tipos abstractos de Datos (TAD). Pilas y Colas. Listas enlazadas. ImplementaciÃ³n de apuntadores y objetos. TAD Conjunto. Colas de Prioridad. 
Lecturas: [C] 6.5; [A] 4.10-4.11;  [C] 10.1-10.3; [A] 1.2, 1.3, 2.1-2.4; [R]
8
09.03: TAD Diccionario. Tablas de Hash. ResoluciÃ³n de colisiones por encadenamiento. Funciones de Hash. 
Lecturas: [C] 11.1-11.3; [B] 5.6; [A] 4.1-4.8; [R]
11.03: Tablas de hash basadas en direccionamiento abierto. Funciones de hash para tablas de hash basadas en direccionamiento abierto. Lecturas: [C] 11.4; [A] 4.7-4.8; 
9
16.03: Hashing Universal. 
Lecturas: [C] 11.3.3 
18.03: Hashing Perfecto. Cuckoo Hashing. 
Lecturas: [C] 11.5; [B] 10.7.3; [P]
10
23.03: Ejercicios de prÃ¡ctica
25.03: Ãrboles binarios de bÃºsqueda (ABB). DefiniciÃ³n y propiedades de los ABB. BÃºsqueda en los ABB. InserciÃ³n en los ABB. 
Lecturas: [C] 12.1-12.2; [A] 5.1-5.2 
11
06.04: EliminaciÃ³n en los ABB. AnÃ¡lisis en tiempo de las operaciones de los ABB. 
Lecturas: [C] 12.3
08.04: Ãrboles Rojo-Negro. 
Lecturas: [C] 13.1-13.3 
12
13.04: Ejercicios de prÃ¡ctica y repaso
15.04: Examen parcial 2


Un cuento (del latÃ­n, compÅ­tus, cuenta)1â€‹ es una narraciÃ³n breve creada por uno o varios autores, puede ser basada ya sea en hechos reales como ficticios, cuya trama es protagonizada por un grupo reducido de personajes y con un argumento relativamente sencillo.

El cuento es compartido tanto por vÃ­a oral como escrita, aunque en un principio lo mÃ¡s comÃºn era por tradiciÃ³n oral. AdemÃ¡s, puede dar cuenta de hechos reales o fantÃ¡sticos pero siempre partiendo de la base de ser un acto de ficciÃ³n, o mezcla de ficciÃ³n con hechos reales y personajes reales. Suele contener varios personajes que participan en una sola acciÃ³n central, y hay quienes opinan que un final impactante es requisito indispensable de este gÃ©nero. Su objetivo es despertar una reacciÃ³n emocional impactante en el lector. Aunque puede ser escrito en verso, total o parcialmente, de forma general se da en prosa. Se realiza mediante la intervenciÃ³n de un narrador, y con preponderancia de la narraciÃ³n sobre el monÃ³logo, el diÃ¡logo, o la descripciÃ³n.

El cuento, dice Julio CortÃ¡zar, como en el boxeo, gana por knock out, mientras que la novela gana por puntos. El cuento recrea situaciones. La novela recrea mundos y personajes (su psicologÃ­a y sus caracteres).2â€‹3â€‹4â€‹

BÃ¡sicamente, un cuento se caracteriza por su corta extensiÃ³n pues debe ser mÃ¡s corto que una novela, y ademÃ¡s, suele tener una estructura cerrada donde desarrolla una historia, y solamente podrÃ¡ reconocerse un clÃ­max. En la novela, y aun en lo que se llama novela corta, la trama desarrolla conflictos secundarios, lo que generalmente no acontece con el cuento, ya que este sobre todo debe ser conciso.

Los lÃ­mites entre un cuento y una novela corta son un tanto difusos. Una novela corta es una narraciÃ³n en prosa de menor extensiÃ³n que una novela y menor desarrollo de los personajes y la trama, aunque sin la economÃ­a de recursos narrativos propia del cuento.5â€‹3â€‹6â€‹


Ãndice
1	Tipos de cuentos
2	Estructura
3	CaracterÃ­sticas
4	SubgÃ©neros
5	EvoluciÃ³n
5.1	Fase oral
5.2	Fase escrita
6	Cuentistas famosos en lengua portuguesa
7	CrÃ­ticas
8	Influencia
9	ExtensiÃ³n
10	CaracterÃ­sticas bÃ¡sicas de un cuento
11	Necesidades bÃ¡sicas
11.1	Final enigmÃ¡tico
11.2	DiÃ¡logos
11.3	Focos narrativos
12	Expresiones relacionadas
13	VÃ©ase tambiÃ©n
14	Notas
15	Referencias
16	Enlaces externos
Tipos de cuentos
Hay dos tipos de cuentos:7â€‹8â€‹

Cuento popular: es una narraciÃ³n tradicional breve de hechos imaginarios que se presenta en mÃºltiples versiones, que coinciden en la estructura pero difieren en los detalles, donde los autores son desconocidos en la mayorÃ­a de los casos (aunque puede que se conozca quien lo recopilÃ³). Tiene cuatro subdivisiones: los cuentos de hadas, los cuentos de animales, las fÃ¡bulas y los cuentos de costumbres. El mito y la leyenda son tambiÃ©n narraciones tradicionales, pero suelen considerarse gÃ©neros autÃ³nomos, un factor clave para diferenciarlos del cuento popular es que no se presentan como ficciones.9â€‹10â€‹
Cuento literario: es el cuento concebido y transmitido mediante la escritura. El autor en este caso suele ser conocido. El texto, fijado por escrito, se presenta generalmente en una sola versiÃ³n, sin el juego de variantes caracterÃ­sticas del cuento popular de tradiciÃ³n fundamentalmente oral. Se conserva un corpus importante de cuentos del Antiguo Egipto, que constituyen la primera muestra conocida del gÃ©nero. Una de las primeras manifestaciones de este tipo en lengua castellana es la obra El conde Lucanor, que reÃºne 51 cuentos de diferentes orÃ­genes, escrito por el infante don Juan Manuel en el siglo xiv.11â€‹12â€‹ En el mundo musulmÃ¡n la colecciÃ³n clÃ¡sica mÃ¡s conocida es Las mil y una noches. En el renacimiento, fue Giovanni Boccaccio el autor mÃ¡s influyente con su DecamerÃ³n. En los tiempos modernos se consideran autores clÃ¡sicos de cuentos Edgar Allan Poe, Anton ChÃ©jov, Leopoldo Alas y Jorge Luis Borges, entre muchos otros.
Estructura
El cuento se compone de tres partes:

IntroducciÃ³n: Es la parte inicial de la historia, donde se presentan todos los personajes y sus propÃ³sitos, pero principalmente se presenta la normalidad de la historia. Lo que se presenta en la introducciÃ³n es lo que se quiebra o altera en el nudo. La introducciÃ³n sienta las bases para que el nudo tenga sentido.
Nudo: Es la parte donde se presenta el conflicto o el problema de la historia; allÃ­ toman forma y suceden los hechos mÃ¡s importantes. El nudo surge a partir de un quiebre o alteraciÃ³n de lo planteado en la introducciÃ³n.
Desenlace: Es la parte donde se suele dar el clÃ­max y la soluciÃ³n al problema, y donde finaliza la narraciÃ³n. Incluso en los textos con final abierto hay un desenlace, e incluso hay casos que dentro del cuento puedes encontrar el clÃ­max relacionado con el final.
CaracterÃ­sticas
El cuento presenta varias caracterÃ­sticas que lo diferencian de otros gÃ©neros narrativos:

FicciÃ³n: aunque puede inspirarse en hechos reales, un cuento debe, para funcionar como tal, recortarse de la realidad.
Argumental: el cuento tiene una estructura de hechos entrelazados (acciÃ³n-consecuencias) en un formato de introducciÃ³n-nudo-desenlace (consultar artÃ­culo Estructura argumental).
Ãšnica lÃ­nea argumental: a diferencia de lo que sucede en la novela, en el cuento todos los acontecimientos se encadenan en una sola sucesiÃ³n de hechos.
Estructura central: todos los elementos que se mencionan en la narraciÃ³n del cuento estÃ¡n relacionados y funcionan como indicios del argumento.
Protagonista: aunque puede haber otros personajes, la historia habla de uno en particular, a quiÃ©n le ocurren los hechos principales.
Unidad de efecto: comparte esta caracterÃ­stica con la poesÃ­a. EstÃ¡ escrito para ser leÃ­do de principio a fin, y si uno corta la lectura, es muy probable que se pierda el efecto narrativo. La estructura de la novela permite, en cambio, leerla por partes, y por otra parte, la extensiÃ³n de la misma tampoco deja otra opciÃ³n.
Prosa: el formato de los cuentos modernos, a partir de la apariciÃ³n de la escritura, suele ser la prosa.
Brevedad: para cumplir con las caracterÃ­sticas reciÃ©n seÃ±aladas, el cuento debe ser breve.
SubgÃ©neros
Algunos de los subgÃ©neros mÃ¡s populares del cuento son:

Cuento fantÃ¡stico
Cuento de hadas
Microrrelato
Cuento de ciencia ficciÃ³n
Cuento policÃ­aco
Cuento tipo fÃ¡bula
EvoluciÃ³n
Los cuentos atravesaron una evoluciÃ³n desde la literatura oral a la escrita. El folclorista VladÃ­mir Propp, en su libro MorfologÃ­a del cuento maravilloso desmontÃ³ la estructura del cuento oral en unidades estructurales constantes o funciones narrativas, con sus variantes, sistemas, fuentes y asuntos, etc. AdemÃ¡s de eso, este autor aventura una posible cronologÃ­a de este tipo de narraciones, cuya primera etapa estarÃ­a integrada por el cuento de inspiraciÃ³n mÃ­tico-religiosa, mientras que una segunda etapa constituirÃ­a el verdadero desarrollo del cuento.

La mayorÃ­a de los escritores y de los crÃ­ticos literarios reconocen tres fases histÃ³ricas en el gÃ©nero cuento: la fase oral, la primera fase escrita y la segunda fase escrita.

Fase oral
La primera fase en surgir fue la oral, la cual no es posible precisar cuando se iniciÃ³. Es de presumir que el cuento se desarrollÃ³ en una Ã©poca en la que ni siquiera existÃ­a la escritura, asÃ­ que posiblemente las historias entonces eran narradas oralmente alrededor de fogatas, en tiempos de los pueblos primitivos, generalmente en las tardes y por las noches, al aire libre o en cuevas, para crear cohesiÃ³n social mediante la narraciÃ³n de los orÃ­genes del pueblo comÃºn y sus funciones. Presumiblemente por ello, la suspensiÃ³n, lo mÃ¡gico, lo maravilloso y fantÃ¡stico fue lo que caracterizÃ³ a estas primeras creaciones de rango mÃ­tico, que pretendÃ­an explicar el mundo de una forma primitiva, aÃºn alejada de la razÃ³n.

Fase escrita
La primera fase escrita probablemente se iniciÃ³ cuando los egipcios elaboraron el llamado Libro de lo mÃ¡gico13â€‹ o Textos de las PirÃ¡mides (cerca 3050 a. C.) y el llamado Libro de los Muertos (hacia el 1550 a. C.). De allÃ­ pasamos a la Biblia â€”donde por ejemplo se recoge la historia de CaÃ­n y Abel (circa 2000 a. C.)â€” la que tiene una clÃ¡sica estructura de cuento.

Obviamente tanto en el Antiguo Testamento como en el Nuevo Testamento, hay muchas otras historias con estructura de cuento, como el episodio de JosÃ© y sus hermanos, asÃ­ como las historias de SansÃ³n, de Ruth, de Susana, de Judith, de SalomÃ©. A los mencionados obviamente tambiÃ©n pueden agregarse las parÃ¡bolas cristianas: El buen samaritano; El hijo prÃ³digo; La higuera estÃ©ril;14â€‹ El sembrador; entre otras.


Geoffrey Chaucer, autor de los cuentos de Canterbury.
En el siglo vi a. C. surgieron las obras IlÃ­ada y Odisea, de Homero, asÃ­ como la literatura hindÃº con Panchatantra (siglo ii a. C.). Pero de un modo general, Luciano de Samosata (125-192) es considerado el primer gran autor en la historia del cuento, ya que entre otros escribiÃ³ El cÃ­nico y El asno. De la misma Ã©poca es Lucio Apuleyo (125-180), quien por su parte escribiÃ³ El asno de oro. Otro nombre importante de esa primera Ã©poca (siglo i) fue Cayo Petronio, autor de SatiricÃ³n, libro que continÃºa siendo reeditado hasta hoy dÃ­a y que incluye una clase especial de cuentos, los relatos milesios. Con posterioridad y en Persia, surgiÃ³ y se difundiÃ³ la recopilaciÃ³n de cuentos Las mil y una noches (siglo x de la llamada era cristiana).


Imagen de la Bella durmiente de Charles Perrault
La segunda fase escrita comenzÃ³ alrededor del siglo xiv, cuando surgieron las primeras preocupaciones estÃ©ticas. AsÃ­, Giovanni Boccaccio (1313-1375), inspirÃ¡ndose en el gÃ©nero del novellino, compuso en esos aÃ±os su DecamerÃ³n, que se volviÃ³ un clÃ¡sico impulsando las bases del cuento tal como lo conocemos hoy dÃ­a, de forma que se puede afirmar sin ambages que fue el creador de la novela corta europea, al margen de la influencia recibida por escritores posteriores tales como Charles Perrault y Jean de La Fontaine del cuento popular o tradicional como obra literaria. Boccaccio dio una estructura exterior a los relatos, la llamada cornice: una serie de narradores que se reÃºnen en un lugar para contarse mutuamente cuentos para distraerse, forzados por alguna desgracia exterior que pretenden evitar. Por su parte Miguel de Cervantes (1547-1616) escribiÃ³ las Novelas ejemplares ensayando nuevas fÃ³rmulas e intentando separarse del modelo italianizante de los novellieri discÃ­pulos de Boccaccio (Mateo Bandello, Franco Sacchetti, Giraldi Cintio, el valenciano Juan de Timoneda, entre otros de menor trascendencia), y Francisco GÃ³mez de Quevedo y Villegas (1580-1645) nos trajo Los sueÃ±os, donde, inspirÃ¡ndose en los diÃ¡logos de Luciano de Samosata y el gÃ©nero literario del sueÃ±o, satirizÃ³ a la sociedad de su Ã©poca.

Los Cuentos de Canterbury, de Geoffrey Chaucer (1340?-1400), por su parte, fueron publicados alrededor de 1700, y en cuanto al citado Perrault (1628-1703), escribiÃ³ y publicÃ³ Barba Azul, El gato con botas, Cenicienta, Piel de asno, Pulgarcito, entre otros. En cuanto a Jean de La Fontaine (1621-1695), debe decirse que fue un gran cuentistas de fÃ¡bulas; recordemos por ejemplo La cigarra y la hormiga, La liebre y la tortuga,15â€‹ La zorra y las uvas, La zorra y la cigÃ¼eÃ±a, etc.

En el siglo xviii el maestro fue Voltaire (1694-1778), quien escribiÃ³ obras importantes como por ejemplo Zadig16â€‹ y CÃ¡ndido.17â€‹


Imagen de Blancanieves
Llegando al siglo xix, el cuento despegÃ³ con apoyo de la prensa escrita, entonces tomando aÃºn mÃ¡s fuerza y modernizÃ¡ndose. Corresponde seÃ±alar que Washington Irving (1783-1859) fue el primer cuentista estadounidense de importancia, descollando por sus obras Cuentos de la Alhambra (1832), El jinete sin cabeza (1820), Rip van Winkle (1820), etc. Los hermanos Grimm (Jacob 1785-1863, y Wilhelm 1786-1859) por su parte, publicaron Blancanieves, Rapunzel, El gato con botas, La bella durmiente, Pulgarcito, Caperucita Roja, etc. NÃ³tese que los hermanos Grimm escribieron muchos cuentos que ya habÃ­an sido contados por Perrault, pero aun asÃ­, fueron tan importantes para este gÃ©nero literario, que AndrÃ© Jolles dijo al respecto:

O conto sÃ³ adotou verdadeiramente o sentido de forma literÃ¡ria determinada, no momento em que os irmÃ£os Grimm deram a uma coletÃ¢nea de narrativas o tÃ­tulo de Â«Contos para crianÃ§as e famÃ­liasÂ»
El cuento obtuvo verdaderamente su sentido de forma literaria, en el instante en que los hermanos Grimm pulicaron su colecciÃ³n llamada Cuentos para niÃ±os y familias.
El siglo xix fue prÃ³digo en verdaderos maestros de la literatura: Nathaniel Hawthorne (1804-1864), Edgar Allan Poe (1809-1849), Henry Guy de Maupassant (1850-1893), Gustave Flaubert (1821-1880), Liev NikolÃ¡ievich TolstÃ³i (1828-1910), Mary Shelley (1797-1851), AntÃ³n ChÃ©jov (1860-1904), Machado de Assis (1839-1908), Arthur Conan Doyle (1859-1930), HonorÃ© de Balzac (1799-1850), Henri Beyle "Stendhal" (1783-1842), JosÃ© Maria EÃ§a de QueirÃ³s (1845-1900) y Leopoldo Alas "ClarÃ­n" (1852-1901).

Tampoco podemos dejar de mencionar a Ernst Theodor Amadeus Wilhelm Hoffmann (uno de los padres del cuento fantÃ¡stico, que mÃ¡s tarde influenciarÃ­a a autores tales como Edgar Allan Poe, Joaquim Maria Machado de Assis, Manuel AntÃ´nio Ãlvares de Azevedo y otros), ni tampoco olvidarnos de escritores como Donatien Alphonse FranÃ§ois de Sade (MarquÃ©s de Sade), Adelbert von Chamisso, GÃ©rard de Nerval, NikolÃ¡i GÃ³gol, Charles Dickens, IvÃ¡n TurguÃ©nev, Robert Louis Stevenson, Rudyard Kipling, entre otros.

Cuentistas famosos en lengua portuguesa
Lendas e Narrativas, 1851, es un libro de cuentos de Alexandre Herculano, autor romÃ¡ntico portuguÃ©s. EÃ§a de QueirÃ³s, en realidad mÃ¡s novelista que cuentista, es bien conocido en Portugal por sus cuentos que fueron publicados en 1902, dos aÃ±os despuÃ©s de su fallecimiento. En el siglo XX, Miguel Torga publicÃ³ Bichos, 1940, y Novos Contos da Montanha, 1944. Destaca tambiÃ©n JosÃ© Cardoso Pires con obras como Jogos de Azar, 1963, y A RepÃºblica dos Corvos,1988.

Machado de Assis, AluÃ­sio Azevedo, y Artur Azevedo, se destacan en el panorama brasileÃ±o del cuento, abriendo espacios para que unos aÃ±os mÃ¡s tarde se afirmaran cuentistas como Monteiro Lobato, Clarice Lispector, Ruth Rocha, Lima Barreto, Otto Lara Resende, Lygia Fagundes Telles, JosÃ© J. Veiga, Dalton Trevisan, y Rubem Fonseca.

En Mozambique, el cuento es un gÃ©nero prÃ³spero, como se puede comprobar por la obra de Mia Couto, y por la antologÃ­a de Nelson SaÃºte titulada As MÃ£os dos Pretos.

Corresponde seÃ±alar que la figura del cuentista en lengua portuguesa en realidad se encuentra un poco disminuida en la actualidad, dada la valorizaciÃ³n que tiene la novela frente a la prosa corta y a la poesÃ­a. Uno de los pocos reductos en el que el cuento sobrevive bien, y mÃ¡s que eso incluso puede decirse que impera, es en la ficciÃ³n cientÃ­fica, sector impulsado por las importantes contribuciones de los cuentistas modernos.

CrÃ­ticas
Aun cuando se tienen tantas historias para contar, el cuento continÃºa siendo blanco de prejuicios, al punto que por ejemplo algunas editoriales en lengua portuguesa tienen como polÃ­tica no publicar nada en el gÃ©nero, y esto ciertamente no es una decisiÃ³n caprichosa sino un asunto de mercado. Lo cierto es que el cuento no vende.

El motivo posiblemente sea la excesiva oferta que se tiene a travÃ©s de diarios y revistas, e incluso a travÃ©s de Internet. Tal vez la falsa idea de que el cuento serÃ­a una literatura mÃ¡s fÃ¡cil, secundaria, o de menor importancia.nota 1â€‹

Considero siempre que el cuento es el gÃ©nero literario mÃ¡s moderno y el que mayor vitalidad posee, por la simple razÃ³n de que las personas jamÃ¡s dejaron de contar lo que sucede, ni de interesarse por lo que les cuentan bien narrado.18â€‹19â€‹20â€‹
Mempo Giardinelli
Ya RenÃ© AvilÃ©s Fabila, en la obra Assim se escreve um conto, dice que

Comecei escrevendo contos, mas me vi forÃ§ado a mudar de rumo por pedidos de editores que queriam romances. Mas, cada vez que me vejo livre dessas pressÃµes editoriais, volto ao contoâ€¦ porque, em literatura, o que me deixa realmente satisfeito Ã© escrever um conto.
EmpecÃ© a escribir cuentos, pero me vi obligado a cambiar de rumbo por solicitud de los editores que querÃ­an novelas. Pero cada vez que me deshago de estas presiones editoriales, vuelve al cuento... porque en la literatura, lo que me hace muy feliz es escribir un cuento.
RenÃ© AvilÃ©s Fabila
Henry Guy de Maupassant, quien escribiÃ³ cerca de trescientos cuentos, decÃ­a que escribir cuentos era mÃ¡s difÃ­cil que escribir novelas. Joaquim Machado de Assis, citado por NÃ¡dia Battella Gotlib, en TeorÃ­a del cuento, tambiÃ©n afirmaba que no era fÃ¡cil escribir cuentos: Â«Es un gÃ©nero difÃ­cil, a pesar de su aparente facilidadÂ», y algo similar pensaba William Faulkner:

quando seriamente explorada, a histÃ³ria curta Ã© a mais difÃ­cil e a mais disciplinada forma de escrever prosa... Num romance, pode o escritor ser mais descuidado e deixar escÃ³rias e superfluidades, que seriam descartÃ¡veis. Mas num conto... quase todas as palavras devem estar em seus lugares exatos
cuando es seriamente explorado, el cuento es mÃ¡s difÃ­cil y mÃ¡s disciplinado que la prosa... En una novela, el escritor puede ser mÃ¡s descuidado y dejar escoria y lo superfluo, que serÃ­a desechable. Pero en un cuento ... casi todas las palabras deben estar en su ubicaciÃ³n exacta
citado por Raymundo MagalhÃ£es JÃºnior en A arte do conto: sua historia, seus gÃªneros, sua tecnica, seus mestres, Edicoes Bloch [1972], 303 pÃ¡ginas.
El escritor gaucho Moacyr Scliar, mÃ¡s conocido como novelista que como cuentista, tambiÃ©n revela su preferencia por el cuento:

Eu valorizo mais o conto como forma literÃ¡ria. Em termos de criaÃ§Ã£o, o conto exige muito mais do que o romance... Eu me lembro de vÃ¡rios romances em que pulei pedaÃ§os, trechos muito chatos. JÃ¡ o conto nÃ£o tem meio termo, ou Ã© bom ou Ã© ruim. Ã‰ um desafio fantÃ¡stico. As limitaÃ§Ãµes do conto estÃ£o associadas ao fato de ser um gÃªnero curto, que as pessoas ligam a uma ideia de facilidade; Ã© por isso que todo escritor comeÃ§a contista
Valoro mÃ¡s el cuento como gÃ©nero literario. En tÃ©rminos de creaciÃ³n, el cuento requiere mucho mÃ¡s que la novela... Recuerdo varias novelas en que saltÃ© pedazos, tramos muy aburridos. Mientras que el cuento no tiene tÃ©rmino medio, es bueno o malo. Es un reto fantÃ¡stico. Las limitaciones del cuento estÃ¡n asociadas con ser un gÃ©nero corto, que la gente liga a una idea de la facilidad; por eso cada escritor comienza cuentista
Folha de SÃ£o Paulo, 4 de febrero de 1996, pÃ¡g. 5 y 11.
Por su parte, Italo Calvino (1923-1985) dice:

Pienso que, no por casualidad, nuestra Ã©poca (aÃ±os 1980), es la Ã©poca del cuento, de la novela corta (cf. Por que ler os clÃ¡ssicos).21â€‹
Italo Calvino
Y en un artÃ­culo sobre Jorge Luis Borges (1899-1986), Calvino dice:

Leyendo a Borges le veo muchas veces tentado a formular una poÃ©tica de escritura breve, alabando sus ventajas en contraposiciÃ³n a escribir largo.22â€‹
Jorge Luis Borges
Tal vez la Ãºltima gran innovaciÃ³n de un gÃ©nero literario a la que hemos asistido en los Ãºltimos aÃ±os, nos la ha dado un gran maestro de la escritura breve: Jorge Luis Borges, quien se inventÃ³ a sÃ­ mismo como narrador, un huevo de ColÃ³n que le permitiÃ³ superar el bloqueo que por cerca de 40 aÃ±os le impidiÃ³ pasar de la prosa ensayista a la prosa narrativa (cf. Italo Calvino, Seis propostas para o prÃ³ximo milÃªnio).23â€‹

En el curso de una vida dedicada principalmente a los libros, he leÃ­do muy pocas novelas y, en la mayorÃ­a de los casos, apenas el sentido del deber me dio fuerzas para abrirme camino hasta la Ãºltima pÃ¡gina. Al mismo tiempo, siempre fui un lector y relector de cuentosâ€¦ La impresiÃ³n de que grandes novelas tales como Don Quijote y Huckleberry Finn son virtualmente amorfas, me sirviÃ³ para reforzar mi gusto por el cuento, cuyos elementos indispensables son la economÃ­a, asÃ­ como un comienzo, un conflicto, y un desenlace, claramente determinados. Como escritor, pensÃ© durante aÃ±os que el cuento estaba por encima de mis poderes, y solamente fue luego de una larga e indirecta serie de tÃ­midas experiencias narrativas, que fui tomÃ¡ndole la mano a escribir historias propiamente dichas.
cf. Jorge Luis Borges, Ficciones: Un ensayo autobiogrÃ¡fico.24â€‹25â€‹
Influencia
Es evidente la identificaciÃ³n del cuento con la falta de tiempo de los habitantes de los grandes centros urbanos, donde a partir de la RevoluciÃ³n Industrial imperaron e imperan los largos recorridos en los desplazamientos, asÃ­ como las complejidades del trÃ¡fico y las largas jornadas laborales impuestas por la industrializaciÃ³n y por la globalizaciÃ³n. Finalmente, fue gracias a la prensa escrita, que el gÃ©nero cuento se popularizÃ³ en Brasil en el siglo xix: los diarios importantes y tambiÃ©n otras publicaciones periÃ³dicas, allÃ­ siempre tenÃ­an espacios para este gÃ©nero.

Es asÃ­ como AntÃ´nio Hohlfeldt en Conto brasileiro contemporÃ¢neo resaltaba: pode-se verificar que, na evoluÃ§Ã£o do conto, hÃ¡ uma relaÃ§Ã£o entre a revoluÃ§Ã£o tecnolÃ³gica e a tÃ©cnica do conto. Y por su parte en la introducciÃ³n de Maravilhas do conto universal, Edgard Cavalheiro decÃ­a:

A autonomia do conto, seu Ãªxito social, o experimentalismo exercido sobre ele, deram ao gÃªnero grande realce na literatura, destaque esse favorecido pela facilidade de circulaÃ§Ã£o em diferentes Ã³rgÃ£os da imprensa periÃ³dica. Creio que o sucesso do conto nos Ãºltimos tempos (anos 1960 e 1970) deve ser atribuÃ­do, em parte, Ã  expansÃ£o da imprensa.
Edgard Cavalheiro
AdemÃ¡s de crear un gran mercado de consumo y la necesidad de una alfabetizaciÃ³n en masa, la industrializaciÃ³n tambiÃ©n creÃ³ la necesidad de servirse de informaciones mÃ¡s sintÃ©ticas y concretas. Y en el siglo xx, ese estilo de informar sin duda fue impulsado por el periodismo y por el libro. Y hacia el Ãºltimo tercio del siglo xx y principios del siglo xxi, las vÃ­as privilegiadas agregadas fueron el cine, la radio, y la televisiÃ³n. AsÃ­ por tanto, en su inicio, el cuento logrÃ³ impacto a travÃ©s de la prensa escrita (siglo xix y buena parte del siglo xx), aunque hoy dÃ­a este espacio se estÃ¡ reduciendo frente a algunos cambios de hÃ¡bitos. Â¿SerÃ¡ que el cuento se adaptarÃ¡ a las nuevas tecnologÃ­as?: televisiÃ³n, Internet, etc. Indudablemente es por lo expresado que en su inicio, tanto en Brasil como en Estados Unidos y como en otros paÃ­ses, la mayorÃ­a de los escritores de cuentos tambiÃ©n eran periodistas.

Sea como fuere, la vÃ­a de la prensa escrita sin duda ha sido positiva para el cuento, aunque tambiÃ©n es culpada por acentuar el preconcepto negativo en relaciÃ³n al gÃ©nero. Se tiene la impresiÃ³n que no se paga por un cuento publicado en una revista, lo que indirectamente resta valor a este tipo de literatura. AdemÃ¡s, luego de cierto tiempo una revista en muchos casos se tira, y con ella el o los cuentos allÃ­ contenidos; en cambio, una novela en formato libro suele guardarse en una biblioteca, o en algÃºn otro lugar de la casa. En definitiva, una revista popular o el suplemento de algÃºn diario no son un buen soporte para la difusiÃ³n de cuentos, pues estÃ¡ involucrada con una comercializaciÃ³n no muy adecuada en relaciÃ³n a literatura seria y de valor.

En resumidas cuentas, en la era industrializada del capitalismo americano, el cuento pasa a ser arte padronizado (con excesivas reglas en cuanto a extensiÃ³n y estructura), impersonal o de autor poco conocido, de producciÃ³n veloz, barata, y de baja o media calidad. Estas preocupaciones y estas reflexiones, a su vez acentÃºan las diferencias entre el cuento comercial de las publicaciones periÃ³dicas, y el cuento literario de las recopilaciones. Por este lado muy posiblemente es que hayan surgido ciertos preconceptos en contra de los cuentosâ€¦" (NÃ¡dia Battella Gotlib, op. cit.).

Esta cuestiÃ³n fue notada en muchas partes, y tambiÃ©n en Brasil, especialmente durante los aÃ±os 1970. Las influencias en un principio tal vez positivas ejercidas por la prensa escrita (revistas, semanarios, suplementos), unida a cierta difusiÃ³n a travÃ©s de radios y de tele-emisoras muy comerciales y con mucha publicidad, impulsaron al gÃ©nero a perder parte de su identidad: en un principio habiendo sido casi todo, el cuento como gÃ©nero pasÃ³ a ser casi nada.

En la dÃ©cada de 1920 surgen los modernistas, y entonces el cuento pasa a ser esencialmente urbano/suburbano. Los escritores procuraron la renovaciÃ³n de las formas, la ruptura con el lenguaje tradicional, la renovaciÃ³n de los medios de expresiÃ³n, etc. Se procurÃ³ evitar los rebuscamientos con el lenguaje, la narrativa pasÃ³ a ser mÃ¡s objetiva, las frases se volvieron mÃ¡s cortas, y la comunicaciÃ³n tendiÃ³ a ser mÃ¡s breve.

En esa misma lÃ­nea, Poe, que tambiÃ©n fue el primer teÃ³rico del gÃ©nero, dijo:

Tenemos necesidad de una literatura corta, concentrada, penetrante, concisa, y contraria a una literatura extensa, verbosa, pormenorizadaâ€¦ Es una seÃ±al de los tiemposâ€¦ La indicaciÃ³n de una Ã©poca en la cual el hombre es forzado a escoger lo corto, lo condensado, lo resumido, en lugar de lo voluminoso
cita de Edgard Cavalheiro en la introduÃ§Ã£o de Maravilhas do conto universal.
ExtensiÃ³n
SegÃºn ciertas definiciones, el cuento no deberÃ­a ocupar mÃ¡s de 7500 palabras. Actualmente, se entiende como usual o normal que pueda variar entre un mÃ­nimo de 1000 y un mÃ¡ximo de 20 000 palabras, aunque justo es reconocer que cualquier limitaciÃ³n en cuanto al mÃ­nimo o al mÃ¡ximo de palabras de una obra, siempre tiene algo de arbitrario, y que por otra parte, con frecuencia estos lÃ­mites son ignorados tanto por escritores como por lectores.26â€‹27â€‹28â€‹

La novela Vidas secas de Graciliano Ramos,29â€‹ asÃ­ como tambiÃ©n A festa de Ivan Ã‚ngelo30â€‹31â€‹ y algunas novelas de Bernardo GuimarÃ£es (1825-1884) o de Autran Dourado (1926-2012), bien pueden ser leÃ­das como una serie de cuentos. TambiÃ©n ese es el caso de Memorias pÃ³stumas de Blas Cubas y de Quincas Borba, ambas obras de Machado de Assis.

Por su parte tambiÃ©n corresponde destacar la obra El proceso de Franz Kafka, escrito de hecho constituido por varios cuentos cortos. En sÃ­, esta clase de literatura es llamada novela desmontable, dada precisamente la caracterÃ­stica que viene de ser expresada.

Assis Brasil va aÃºn mÃ¡s lejos al afirmar que Grande SertÃ£o: veredas, de GuimarÃ£es Rosa, es un cuento largo, y que por tanto merece ser clasificado como narrativa corta. La citada obra, como sabemos, tiene mÃ¡s de 500 pÃ¡ginas, aunque claro, allÃ­ tambiÃ©n es posible reconocer caracterÃ­sticas propias del cuento.

Todas estas observaciones tienden a demostrar lo difÃ­cil que es definir exactamente lo que es un cuento, asÃ­ que una soluciÃ³n podrÃ­a ser la de dejar esta tarea de clasificaciÃ³n al propio autor o al editor. No obstante, las caracterÃ­sticas principales de este gÃ©nero literario han sido bien establecidas, y quien conoce de literatura tiene bien claro lo que es un cuento.

En el siglo xx pueden incluirse entre los grandes escritores de cuentos a O. Henry, Anatole France, Virginia Woolf, Katherine Mansfield, Kafka, James Joyce, William Faulkner, Ernest Hemingway, MÃ¡ximo Gorki, MÃ¡rio de Andrade, Monteiro Lobato, AnÃ­bal Machado, AlcÃ¢ntara Machado, GuimarÃ£es Rosa, Isaac Bashevis Singer, Nelson Rodrigues, Dalton Trevisan, Rubem Fonseca, Osman Lins, Clarice Lispector, Jorge Luis Borges, y Lima Barreto.

Otros nombres importantes del cuento en Brasil son: Julieta Godoy Ladeira, Otto Lara Resende, Manoel Lobato, SÃ©rgio Santâ€™Anna, Moreira Campos, Ricardo Ramos, Edilberto Coutinho, Breno Accioly, Murilo RubiÃ£o, Moacyr Scliar, PÃ©ricles Prade, Guido Wilmar Sassi, Samuel Rawet, Domingos Pellegrini Jr, JosÃ© J. Veiga, Luiz Vilela, Sergio Faraco, Victor Giudice, Lygia Fagundes Telles, Miguel Sanches Neto. En Portugal por su parte se destacan, entre otros, Alejandro Herculano y EÃ§a de QueirÃ³s.

Para un escritor que hace un cuento, lo que en realidad mÃ¡s le debe importar es cÃ³mo (forma) cuenta la historia, y no tanto lo quÃ© (contenido) cuenta. Jorge Luis Borges (1899-1986) decÃ­a que contamos siempre la misma fÃ¡bula. Sin llegar a tanto, Julio CortÃ¡zar (1914-1984) decÃ­a que no hay ni temas buenos ni temas malos, sino un tratamiento bueno o inadecuado para un determinado tema (cf. Aspectos del cuento,32â€‹ Algunos aspectos del cuento,33â€‹ y Valise de cronÃ³pio34â€‹). Claro que hay que tener cuidado con los excesos de formalismo, para no caer en personajes acartonados ni en esquemas excesivamente rÃ­gidos: cierto escritor pasÃ³ buena parte de su vida trabajando en las formas de lograr un estilo literario perfecto, para asÃ­ impresionar al mundo todo; y cuando finalmente consiguiÃ³ alcanzarlo, descubriÃ³ que nada tenÃ­a que decir.35â€‹

La tendencia contemporÃ¡nea en este inicio del siglo xxi, es a jerarquizar al microcuento, una especie de haiku de cuÃ±o narrativo, cuya extensiÃ³n se define, en la mayorÃ­a de las veces, por cierto mÃ¡ximo recomendado para los intercambios de mensajes de texto (sms) en la telefonÃ­a celular, o por la extensiÃ³n de un tuit. AdemÃ¡s de Twitter, otras redes sociales tambiÃ©n han sido medio para la publicaciÃ³n de microcuentos, por fuera de la plataforma tradicional de los libros y de las publicaciones periÃ³dicas.

El microcuento tal vez mÃ¡s famoso,36â€‹ es uno de Augusto Monterroso, autor guatemalteco, y cuyo tÃ­tulo es El dinosaurio. En Brasil, cultivan este subgÃ©nero autores tales como Dalton Trevisan, MillÃ´r Fernandes, Daniel Galera, Samir Mesquita, y Rauer (nombre bajo el cual firma sus publicaciones en Twitter el escritor de Minas Gerais llamado Rauer Ribeiro Rodrigues).

CaracterÃ­sticas bÃ¡sicas de un cuento
Cuando se escribe un cuento, hay que tener muy en cuenta los siguientes aspectos:

Forma: expresiÃ³n o lenguaje utilizando elementos concretos y estructurados (palabras, frases, pÃ¡rrafos).
Contenido: se refiere a los personajes, a sus acciones, y a la historia (sobre este asunto se recomienda consultar la obra O conto brasileiro contemporÃ¢neo de Alfredo Bosi).37â€‹38â€‹39â€‹
Hay cuentos como por ejemplo de Joaquim Machado de Assis, de Katherine Mansfield, de JosÃ© J. Veiga, de AntÃ³n ChÃ©jov, de Clarice Lispector, que hasta podrÃ­a decirse que no se pueden contar pues no hay nada aconteciendo, y entonces a lo sumo, lo Ãºnico que se podrÃ­a expresar son descripciones de situaciones y perfiles de personajes. Lo esencial en un cuento estÃ¡ en el aire, en la atmÃ³sfera que se vaya creando y transmitiendo al lector, en el modo y el estilo de narrar, en la tensiÃ³n y el suspense, en la emociÃ³n y la conmociÃ³n que se logre provocar. En el libro Â¿QuÃ© es la literatura? (Quâ€™est-ce que la littÃ©rature? -1948-)40â€‹41â€‹ de Jean-Paul Sartre, se expresa claramente que

nadie es escritor por el solo hecho de haber decidido decir ciertas cosas y hacerlo, sino por haber decidido decirlas de una determinada forma; es el estilo, ciertamente, lo que determina el valor de la prosa.42â€‹43â€‹44â€‹
Necesidades bÃ¡sicas
El cuento necesita de tensiÃ³n, de ritmo, de lo imprevisto y de lo sorpresivo dentro de parÃ¡metros previstos (o sea, dentro de cierto cauce razonable de los acontecimientos), y ademÃ¡s necesita unidad, continuidad, compactaciÃ³n, conflicto, y divisiÃ³n en partes (principio-planteamiento, medio-nudo, y fin-conclusiÃ³n) mÃ¡s o menos clara y definida. El pasado y el futuro en el cuento tienen una significaciÃ³n menor, y el flashback (retroceso temporal) no estÃ¡ impedido, aunque debe usarse solamente si es absolutamente necesario, y de la forma mÃ¡s corta y marginal posible.

Final enigmÃ¡tico
El final enigmÃ¡tico en el cuento prevaleciÃ³ hasta Henry Guy de Maupassant (fin del siglo XIX) y por cierto hasta esa Ã©poca ello era muy importante, pues aportaba un desenlace generalmente sorprendente, cerrando la obra con broche de oro, como entonces solÃ­a decirse. Hoy en dÃ­a este tipo de final tiene mucho menos importancia; algunos escritores y algunos crÃ­ticos incluso opinan que esta caracterÃ­stica es perfectamente superflua o dispensable, lÃ©ase aÃºn anacrÃ³nica. Asimismo, no se puede negar que el final en el cuento mayoritariamente siempre es mÃ¡s cargado de tensiÃ³n que en la novela o que en el relato, y que un buen final en un cuento es fundamental: Eu diria que o que opera no conto desde o comeÃ§o Ã© a noÃ§Ã£o de fim; tudo chama, tudo convoca a um "final" (Antonio SkÃ¡rmeta, Assim se escreve um conto, cf.45â€‹).

En el gÃ©nero cuento, como afirmÃ³ AntÃ³n ChÃ©jov, es mejor no decir lo suficiente que decir de mÃ¡s; y para no decir demasiado, es mejor sugerir, como si tuviera que haber cierto silencio o cierta cortina en el curso del relato, para asÃ­ sustentar la intriga, para asÃ­ mantener la tensiÃ³n. Y como ejemplo, puede ponerse el cuento A missa do galo, de Joaquim Machado de Assis;46â€‹ en ese texto, y especialmente en los diÃ¡logos, no es tan importante lo que se dice sino lo que se deja de decir.

Ricardo Piglia,47â€‹ comentando algunos cuentos de Ernest Hemingway (1899-1961), afirma que lo mÃ¡s importante nunca se cuenta:

O conto se constrÃ³i para fazer aparecer artificialmente algo que estava oculto. Reproduz a busca sempre renovada de uma experiÃªncia Ãºnica que nos permite ver, sob a superfÃ­cie opaca da vida, uma verdade secreta (O laboratÃ³rio do escritor, cf.48â€‹).
El citado Piglia decÃ­a que habÃ­a que contar una historia como si se estuviese contando otra, o sea, como si el escritor estuviera narrando una historia visible, pero disfrazando y escondiendo una historia secreta apenas insinuada o sospechada:

Narrar Ã© como jogar pÃ´quer: todo segredo consiste em fingir que se mente quando se estÃ¡ dizendo a verdade (PrisÃ£o perpÃ©tua, cf.49â€‹).
Es como si el cuentista o el relator pegara en la mano del lector o le hiciera seÃ±as para darle a entender que lo llevarÃ­a para un lugar, para una encrucijada, aunque el personaje y la acciÃ³n en el final de la historia, lo empujan hacia otro lugar. Tal vez por lo que acaba de decirse, David Herbert Lawrence dijo que el lector debÃ­a confiar en el cuento pero no en el cuentista, pues el cuentista suele ser un terrorista que se finge diplomÃ¡tico (como por su parte decÃ­a Alfredo Bosi sobre Machado de Assis, op. cit.).

SegÃºn Cristina PerÃ­-Rossi, el escritor contemporÃ¡neo de cuentos no narra solamente por el placer de encadenar hechos y situaciones de una manera mÃ¡s o menos casual y original, sino para revelar lo que verdaderamente hay detrÃ¡s de los mismos (cita de Mempo Giardinelli, op. cit). Desde este punto de vista, la sorpresa se produce cuando, al final del relato, la historia secreta o escondida viene a la superficie.

En el cuento, la trama es lineal y objetiva, pues el cuento, dada su brevedad, ya desde el inicio no estÃ¡ tan lejos del desenlace, asÃ­ que es preciso que el lector clara y rÃ¡pidamente vea y tome conciencia de los acontecimientos. Si en la novela el espacio/tiempo es saltarÃ­n, en el cuento ciertamente es lineal, y expresado bajo la forma narrativa por excelencia.

En el cuento, la narrativa ideal probablemente comienza con una situaciÃ³n estable, que pronto serÃ¡ perturbada por alguna fuerza o por algÃºn desequilibrio, resultando en una situaciÃ³n de inestabilidad. Con posterioridad entra en acciÃ³n otra fuerza, inversa, que restablece el equilibrio, aunque la estabilidad lograda en el desenlace, nunca es idÃ©ntica a la inicial si bien podrÃ­a tener con ella cierta similitud (Gom Jabbar en Hardcore, basado en Tzvetan Todorov).

En otras palabras: En general, el cuento se presenta con un orden o un conflicto antes de un desorden y la soluciÃ³n de ese conflicto (favorable o no) o frente a la posibilidad de retornar al orden (retornar al inicio), aunque ahora con pÃ©rdidas y ganancias, puesto que ese otro orden difiere del primero. El cuento es un problema y una soluciÃ³n, dice Enrique Aderson Imbert.

DiÃ¡logos
Los diÃ¡logos son de suma importancia en la novela y en cierta medida tambiÃ©n en el cuento, pues con este recurso se transmiten bien las discordias, los conflictos, las particularidades de gÃ©nero, etc. Los diÃ¡logos son un muy buen recurso para informar, incluso en el cuento en donde el ingrediente narrativo sin duda siempre es importante (Henry James, 1843-1916).

Para algunos escritores, el diÃ¡logo es una herramienta absolutamente indispensable. Caio PorfÃ­rio Carneiro por ejemplo, llega al punto de escribir cuentos solo compuestos por diÃ¡logos, y sin que, en ningÃºn instante surja un narrador. Considerado el mayor autor brasileÃ±o en el arte de escribir diÃ¡logos y un verdadero maestro, el escritor Luiz Vilela es inclusive quien escribiÃ³ una novela corta, Entre amigos (1984), donde allÃ­ tambiÃ©n solamente se expresa con diÃ¡logos y sin presencia de un narrador. Otro ejemplo del mismo tipo son las 172 pÃ¡ginas de TrapiÃ¡, un clÃ¡sico de la dÃ©cada de 1960, tambiÃ©n escrito por Caio PorfÃ­rio Carneiro, y en donde apenas hay seis pÃ¡ginas sin diÃ¡logos.

Veamos seguidamente los distintos tipos de diÃ¡logo:

Directo: Discurso directo. Los personajes conversan entre sÃ­. AdemÃ¡s de ser el tipo de diÃ¡logo mÃ¡s conocido, tambiÃ©n es el que predomina en el cuento.
Indirecto: Discurso indirecto. Es cuando el escritor resume el habla del personaje en forma narrativa. O sea, es cuando el personaje cuenta cÃ³mo aconteciÃ³ el diÃ¡logo, casi reproduciÃ©ndolo. Tanto el diÃ¡logo directo como el diÃ¡logo indirecto pueden ser observados en el cuento A Missa do Galo, del escritor Machado de Assis.
Indirecto libre: Discurso indirecto libre. Es una fusiÃ³n entre autor y personaje (primera y tercera persona de la narrativa); el narrador narra en la forma habitual, pero en un punto de la narrativa surgen diÃ¡logos indirectos del personaje, como complementando lo que expresa el narrador.
Es interesante analizar el caso de Vidas secas, donde en ciertos pasajes no se sabe exactamente quiÃ©n es el que habla: Â¿es el narrador (tercera persona) o la consciencia de Fabiano (primera persona)? Este tipo de discurso permite exportar o expresar los pensamientos del personaje, sin que el narrador pierda su poder y su condiciÃ³n de mediador.

MonÃ³logo interior (o flujo de conciencia): Es lo que pasa dentro del mundo psÃ­quico del personaje, hablando consigo mismo; vÃ©ase por ejemplo algunos pasajes de Perto do coraÃ§Ã£o selvagem, de Clarice Lispector. Corresponde seÃ±alar que el libro A canÃ§Ã£o dos loureiros (1887), de Ã‰douard Dujardin, es un precursor moderno de este tipo de discurso del personaje. Por su parte, la conocida obra Lazarillo de Tormes, de autor desconocido, tambiÃ©n es considerado un precursor de esta clase de discurso. En Ulises, James Joyce (inspirado en Ã‰douard Dujardin) radicalizÃ³ el monÃ³logo interior.50â€‹51â€‹52â€‹
Focos narrativos
Primera persona: El personaje principal cuenta su historia; este narrador generalmente se limita a saber sobre sÃ­ mismo, o sea, se refiere a sus propias vivencias.53â€‹ Esta es una narrativa tÃ­pica de la novela epistolar (siglo XVIII).
Tercera persona: El desarrollo del texto se hace en tercera persona,54â€‹ y en este caso se puede tener:
narrador observador: El narrador se limita a expresar lo que estÃ¡ sucediendo, describiendo todo desde el exterior, o sea, sin involucrarse, sin colocarse en la cabeza del personaje principal ni de ningÃºn personaje, y de esta forma, no se usa esta vÃ­a para transmitir emociones, ideas, opiniones. El observador es imparcial y objetivo dentro de lo que puede esperarse, limitÃ¡ndose a describir lo que pasa y no especulando por sÃ­ mismo.55â€‹
narrador omnisciente: Al contar la historia y ya desde su inicio, el narrador todo lo sabe sobre todos los personajes, sobre sus destinos, sobre sus ideas y pensamientos, sobre sus sentimientos, sobre sus respectivas buena o mala suerte.56â€‹
Expresiones relacionadas
Cuento de nunca acabar: relaciÃ³n muy pesada y en extremo difusa. Cosa parecida a la tela de PenÃ©lope, que por la noche destejÃ­a lo que habÃ­a tejido por la maÃ±ana. Esta animosa griega, a quien se creÃ­a viuda por la larga ausencia de Odiseo, rey de Ãtaca, viÃ©ndose cada dÃ­a mÃ¡s apurada por las reiteradas pretensiones de sus amantes, prometiÃ³ contraer nuevos lazos luego que terminase un tejido de pÃºrpura que tenÃ­a comenzado. ValiÃ©ndose del ingenioso medio que hemos visto, acallÃ³ a sus pretendientes y vio al fin coronada su estratagema con la vuelta de Odiseo.
Cuento de hornos: cuento o hablilla vulgar de la que se hace conversaciÃ³n entre la gente comÃºn. Noticia de la que se apodera el vulgo y que va desfigurando y adicionando hasta desfigurarla y hacerla apÃ³crifa.
Cuento de vieja; la noticia o especie que se juzga falsa o fabulosa, tomada la alusiÃ³n de las consejas que las ancianas suelen referir en los hogares para entretener a los niÃ±os, procurar que no se duerman, etc.
Cuento largo: locuciÃ³n familiar que hace referencia a un asunto cualquiera del que hay mucho que hablar o decir.
En todo cuento (anticuado): En todo caso, de cualquier modo, sea como quiera.
Acabados son cuentos: frase familiar que equivale a punto concluido, no se hable mÃ¡s del asunto, capÃ­tulo de otra cosa, etc. Locuciones dirigidas a cortar alguna disputa, especie o conversaciÃ³n desagradable.
Como digo de mi cuento, como decÃ­a, o iba diciendo de mi cuento: expresiÃ³n familiar del estilo festivo o jocoso con que se suele principiar una relaciÃ³n o anudar el hilo interrumpido de ella. Equivalen a las usualÃ­simas de: pues seÃ±or...", "pues como decÃ­a... y otras locuciones anÃ¡logas que emplean los narradores de cuentos del hogar domÃ©stico.
Degollar algÃºn cuento: cortar el hilo del discurso, interrumpiÃ©ndolo con otra narraciÃ³n, episodio o pregunta impertinente. Frase enÃ©rgica por medio de la cual protesta el vulgo contra las digresiones inoportunas o pesadas. TambiÃ©n se alude con ella al narrador que no sabe lo que cuenta con todos sus pelos y seÃ±ales, es decir, de una manera minuciosa y a satisfacciÃ³n de sus oyentes.
Dejarse de cuentos: tiene una significaciÃ³n anÃ¡loga a la anterior, pues tiende a significar que se omitan los rodeos ya en un relato, ya en la conversaciÃ³n, o que se vaya en derechura a lo mÃ¡s interesante o sustancial de una cosa. Significa igualmente no mezclarse en chismes, no intervenir en asuntos ajenos, prescindir, en fin, de cuanto pueda alterar la paz en lo mÃ¡s mÃ­nimo.
No querer cuentos con la vecindad: no meterse con nadie, no provocar a nadie, ni buscar rencilla, especialmente con ningÃºn vecino; vivir tranquilamente en su casa sin buscar ruidos ni promover disensiones de ninguna clase.
Ese es el cuento o ahÃ­ estÃ¡ el cuento: esa es, o ahÃ­ estÃ¡ la dificultad, el quid, el alma, lo fuerte, el busilis del negocio: en eso consiste, y se reasume la sustancia de lo que se trata.
Estar a cuento: ser alguna cosa Ãºtil o provechosa por algÃºn respecto. Llegar oportunamente para alcanzar alguna cosa, sacar provecho o presenciar algÃºn negocio.
Venir o no venir a cuento alguna cosa: ser o no oportuna, a propÃ³sito, conveniente para el caso; venir o no a la cuestiÃ³n una especie vertido. Convenir o no un objeto a otro u otros de que se trate.
Poner en cuentos: comprometer a alguno, exponerlo a un riesgo o peligro, a un disgusto o cosa equivalente, dando lugar o pretexto para que hablen de Ã©l de una manera que le desdore o difame.
Quitarse, dejarse de cuentos: atender solo a lo esencial y mÃ¡s importante de una cosa; huir de todo compromiso, separarse de personas y de cosas capaces de comprometerle a uno, despuÃ©s de haber vivido entre chismes y bachillerÃ­as.
Sabe su cuento: locuciÃ³n con que se da a entender a otro, que alguno obra con reflexiÃ³n o motivos que no quiere o no puede manifestarle.
Â¡Es mucho cuento! modismo que expresa burla, chacota, etc., o bien admiraciÃ³n o extraÃ±eza y que equivale a decir Â¡es mucho asunto!, Â¡tiene que ver!, Â¡parece increÃ­ble!, etc. Se usa ponderando la extraÃ±eza o sensaciÃ³n que causa alguna cosa y entonces se dice en el sentido de: Â¡es admirable! Â¡sorprendente! Â¡grandioso!.
TraerÃ¡ cuento: citar oportunamente, hacer venir a la conversaciÃ³n de una manera hÃ¡bil la especie o especies que conviene tocar, poner en juego el modo de que recaiga la plÃ¡tica sobre determinado asunto; enderezar, encaminar, llevar o dirigir el discurso hacia el fin u objeto que se desea, que se quiere. Sacar a colaciÃ³n las faltas de alguno, echÃ¡rselas al rostro, etc.
Amigo de cuentos: el que es aficionado a chismes y enredos; el que lleva de una parte a otros para indisponer, ocasionar disgustos, enemistades, etc. El que es pendenciero y busca continuas ocasiones de disputa o pendencia.57â€‹
VÃ©ase tambiÃ©n
Estructura argumental
FÃ¡bula
Leyenda
Mito
Nanorrelato
Narrativa
Novela
Relato
Notas
 Anderson Imbert, Enrique (enero de 1992). Â«TeorÃ­a y tÃ©cnica del cuentoÂ». Ariel. ISBN 978-84-344-2513-2. Â«Cuentos, cuentos, cuentos, es lo que con mÃ¡s gusto he leÃ­do, y con mÃ¡s ambiciÃ³n escrito. Ahora que soy viejo, Â¡quÃ© bueno serÃ­a â€”me dijeâˆ’ aprovechar mi doble experiencia de lector y escritor! Y me puse a improvisar unos ensayos breves sobre cuentos ajenos y mÃ­os. Con facilidad, con felicidad, me deslizaba cuesta abajo por el camino, cantando y borroneando pÃ¡ginas y pÃ¡ginas, cuando, en una encrucijada, se me apareciÃ³ el Demonio del Sistema: Â¿Por quÃ©, me tentÃ³, en vez de dispersarte en reflexiones sueltas, no te concentras en una sistemÃ¡tica descripciÃ³n de todos los aspectos del cuento? No te olvides de que tambiÃ©n eres profesor...Â»
Referencias
 cuento.Â» Diccionario de la lengua espaÃ±ola. Consultado el 10 de noviembre de 2015.]
 Julio CortÃ¡zar, Sobre el cuento, sitio digital Ciudad Seva.
 Carmen Roig, Diferencias entre cuento y novela Archivado el 21 de diciembre de 2012 en la Wayback Machine., sitio digital Ciudad Seva.
 Julio CortÃ¡zar, Algunos aspectos del cuento, sitio digital Literatura.us.
 La diferencia entre una novela y un cuento, sitio digital RincÃ³n de los Escritores â€“ Comunidad Literaria, 2 de mayo de 2008.
 La narrativa: Cuento y novela (diferencias), sitio digital El Ciclo, 8 de junio de 2012.
 â€œDel cuento popular al cuento literarioâ€: resumen de la conferencia de JosÃ© MarÃ­a Merino, 27 de enero de 2010.
 Cuentos populares (gÃ©neros literarios, y narrativa).
 BegoÃ±a RoldÃ¡n, El cuento popular (caracterÃ­sticas y elementos comunes), 23 de marzo de 2011.
 SelecciÃ³n de cuentos populares
 MarÃ­a Ãngeles CuÃ©llar, El cuento literario, 7 de agosto de 2010.
 Francisco RodrÃ­guez Criado, Los mejores 1001 cuentos literarios de la Historia (72): Las lunas de JÃºpiter, de Alice Munro, 4 de junio de 2011.
 Christian Jacq, El saber mÃ¡gico en el Antiguo Egipto, 143 pÃ¡ginas.
 Ricardo Paulo Javier, La parÃ¡bola de la higuera estÃ©ril, 20 de febrero de 2008.
 FÃ¡bulas de La Fontaine: La liebre y la tortuga (las fÃ¡bulas mÃ¡s famosas de los tiempos modernos acompaÃ±adas de las imÃ¡genes del gran ilustrador de la literatura universal, Gustave DorÃ©), 6 de octubre de 2007.
 Voltaire, Zadig o el destino (historia oriental) Archivado el 5 de noviembre de 2012 en la Wayback Machine. (texto completo).
 Voltaire, CÃ¡ndido o el optimismo (cuento largo) (texto completo 1 Archivado el 21 de octubre de 2012 en la Wayback Machine., texto completo 2).
 Lucas Lacerda, Uma Breve IntroduÃ§Ã£o ao Conto, 5 de enero de 2012.
 AsÃ­ se escribe un cuento, Mempo Giardinelli, 1 de marzo de 2010.
 PEQUENO MANUAL DO CONTO PARA INICIANTES: HISTÃ“RIA E TEORIA.
 Italo Calvino, Por quÃ© leer a los clÃ¡sicos (1993), 6 pÃ¡ginas.
 GÃªneros textuais e ensino de lÃ­ngua materna, pÃ¡g 32/58.
 Italo Calvino, Seis propuestas para el prÃ³ximo milenio (enlace roto disponible en Internet Archive; vÃ©ase el historial, la primera versiÃ³n y la Ãºltima).
 Ficciones: Un ensayo autobiogrÃ¡fico
 Tobias Rohmann, Un anÃ¡lisis de â€œLas Ruinas circularesâ€ del Jorge Luis Borges con foco en la funciÃ³n del sueÃ±o, pÃ¡gina 4, ISBN 978-3-640-29081-9.
 JosÃ© JoaquÃ­n, La extensiÃ³n de una novela o de un cuento, 27 de septiembre de 2010.
 Carmen Roig, Diferencias entre cuento y novela Archivado el 21 de diciembre de 2012 en la Wayback Machine..
 RamÃ³n Alcaraz, Relato y microrrelato: diferencias Archivado el 1 de julio de 2012 en la Wayback Machine.
 J. Freddy, M. Monasterios, Vidas secas de Graciliano Ramos: Una visiÃ³n del proletariado
 Resumos comentados: A festa de Ivan Ã‚ngelo
 â€œA Festaâ€ â€“ Ivan Ã‚ngelo Archivado el 21 de noviembre de 2010 en la Wayback Machine., 16 de diciembre de 2007.
 Julio CortÃ¡zar, Aspectos del cuento (texto completo Archivado el 21 de diciembre de 2012 en la Wayback Machine.).
 Julio CortÃ¡zar, Algunos aspectos del cuento Archivado el 11 de septiembre de 2018 en la Wayback Machine. (texto completo).
 Julio CortÃ¡zar, Valise de cronÃ³pio, Editora Perspectiva (1974), 257 pÃ¡ginas.
 Â«cuentos cortosÂ».
 Schmidt Welle, Friedhelm. Â«La pesadilla externa: monstruos, dinosaurios, y los extravÃ­os de la lectura. Augusto Monterroso y el Quijote.Â». En Friedhelm Schmidt-Welle; Ingrid Simson, ed. El Quijote en AmÃ©rica. Amsterdam; New York: Rodopi. p. 258. ISBN 978-90-420-3051-0.
 Alfredo Bosi, O Conto Brasileiro ContemporÃ¢neo, Editora Cultrix (1997), 293 pÃ¡ginas, ISBN 85-316-0070-7 y 9788531600708.
 Alfredo Bosi, HistÃ³ria concisa da Literatura Brasileira, Editora Cultrix (1997), 528 pÃ¡ginas.
 Breve referencia: Para ALFREDO BOSI, la actividad literaria asÃ­ como toda obra de arte, sobrepasa toda especificidad individual y se vuelve un instrumento de enorme importancia para la formaciÃ³n y la caracterizaciÃ³n de la cultura de un pueblo. El citado es profesor catedrÃ¡tico, ensayista, e historiador literario, asÃ­ como un fino analista de la sociedad brasilera. Sus ensayos y sus libros â€” entre los que ya hay clÃ¡sicos consagrados como por ejemplo HistÃ³ria Concisa da Literatura Brasileira â€” no se limitan exclusivamente al Ã¡mbito literario, sino que tambiÃ©n profundiza en el rol cumplido por el escritor o la obra que se analiza, en los aspectos culturales, histÃ³ricos, y sociales. Obras de Alfredo Bosi: O PrÃ©-Modernismo (1966); HistÃ³ria Concisa da Literatura Brasileira (1970); O Conto Brasileiro ContemporÃ¢neo (1975); A Palavra e a Vida (1976); O Ser e o Tempo na Poesia (1977); Araripe Jr. â€” Teoria, CrÃ­tica e HistÃ³ria (1978); ReflexÃµes sobre a Arte (1985); Cultura Brasileira, Temas e SituaÃ§Ãµes (1987); CÃ©u, Inferno: Ensaios de CrÃ­tica LiterÃ¡ria e IdeolÃ³gica (1988); DialÃ©tica da ColonizaÃ§Ã£o (1992); O Enigma do Olhar (1999).
 Jean-Paul Sartre, Â¿QuÃ© es la literatura?, Editorial Losada (2004), 320 pÃ¡ginas, ISBN 950-03-9263-1 y 9789500392631 (fragmento).
 Jean-Paul Sartre: Obras para descargar o leer en lÃ­nea, acompaÃ±adas de muestras artÃ­sticas clÃ¡sicas y contemporÃ¡neas.
 Juan RamÃ³n PÃ©rez, El arte de decir las cosas (disertaciones en favor de la tÃ©cnica), 3 de agosto de 2003.
 Donaire Galante, Â¿Quieres ser escritor?: Escritura de compromiso, 25 de octubre de 2012.
 En torno al oficio del escritor, ensayo leÃ­do por el escritor venezolano Eduardo Liendo, el 27 de noviembre de 2011, CÃ­rculo de Escritores de Venezuela.
 Consultar entrada Esteban AntÃ´nio SkÃ¡rmeta Branicic en Estudos LiterÃ¡rios: Conceitos de escritores sobre o conto, 4 de mayo de 2009.
 Joaquim Machado de Assis, Misa de gallo (consultar texto completo Archivado el 17 de octubre de 2012 en la Wayback Machine. en espaÃ±ol)
 Maria Antonieta Pereira, Ricardo Piglia y la mÃ¡quina de la ficciÃ³n, documento Universidade Federal de Minas Gerais.
 Ricardo Piglia, O laboratÃ³rio do escritor, documento-ensayo ufrgs-br.
 Ricardo Piglia, PrisÃ£o PerpÃ©tua Archivado el 11 de marzo de 2013 en la Wayback Machine., Editora Iluminuras (1989), 124 pÃ¡ginas, ISBN 85-85219-16-5 (texto).
 MonÃ³logo Interior.
 James Joyce: el maestro del revolucionario monÃ³logo interior, 13 de enero de 2011.
 MarÃ­a Ãngeles Sanz Manzano, El poeta y su teorÃ­a de la novela, 29 pÃ¡ginas.
 El baÃºl de cuentos: Primera Persona
 El narrador en tercera persona
 Narradores
 Narrador omnisciente
 Enciclopedia moderna, 1853
Enlaces externos
 Wikimedia Commons alberga una categorÃ­a multimedia sobre cuentos.
 Wikiquote alberga frases cÃ©lebres de o sobre Cuento.
 Wikcionario tiene definiciones y otra informaciÃ³n sobre cuento.
 Wikisource contiene obras originales de o sobre Cuento.
Esta obra contiene una traducciÃ³n parcial derivada de Â«ContoÂ» de la Wikipedia en portuguÃ©s, publicada por sus editores bajo la Licencia de documentaciÃ³n libre de GNU y la Licencia Creative Commons AtribuciÃ³n-CompartirIgual 3.0 Unported.
Control de autoridades	
Proyectos WikimediaWd Datos: Q49084Commonscat Multimedia: Short storiesWikiquote Citas cÃ©lebres: CuentoWikisource Textos: Portal:Cuento
IdentificadoresGND: 4033842-3LCCN: sh85121965NDL: 00572677NKC: ph124459NARA: 10629824AAT: 300202607Microsoft Academic: 2778081389Diccionarios y enciclopediasGEA: 4482Britannica: url
CategorÃ­as: NarratologÃ­aCuento (gÃ©nero)Literatura oralSubgÃ©neros narrativos
MenÃº de navegaciÃ³n
No has accedido
DiscusiÃ³n
Contribuciones
Crear una cuenta
Acceder
ArtÃ­culoDiscusiÃ³n
LeerEditarVer historialBuscar
Buscar en Wikipedia
Portada
Portal de la comunidad
Actualidad
Cambios recientes
PÃ¡ginas nuevas
PÃ¡gina aleatoria
Ayuda
Donaciones
Notificar un error
Herramientas
Lo que enlaza aquÃ­
Cambios en enlazadas
Subir archivo
PÃ¡ginas especiales
Enlace permanente
InformaciÃ³n de la pÃ¡gina
Citar esta pÃ¡gina
Elemento de Wikidata
Imprimir/exportar
Crear un libro
Descargar como PDF
VersiÃ³n para imprimir
En otros proyectos
Wikimedia Commons
Wikiquote
Wikisource

En otros idiomas
Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©
English
FranÃ§ais
à¤¹à¤¿à¤¨à¥à¤¦à¥€
Bahasa Indonesia
Bahasa Melayu
PortuguÃªs
Ğ ÑƒÑÑĞºĞ¸Ğ¹
ä¸­æ–‡
99 mÃ¡s
Editar enlaces
Esta pÃ¡gina se editÃ³ por Ãºltima vez el 3 abr 2021 a las 19:29.
El texto estÃ¡ disponible bajo la Licencia Creative Commons AtribuciÃ³n Compartir Igual 3.0; pueden aplicarse clÃ¡usulas adicionales. Al usar este sitio, usted acepta nuestros tÃ©rminos de uso y nuestra polÃ­tica de privacidad.
WikipediaÂ® es una marca registrada de la FundaciÃ³n Wikimedia, Inc., una organizaciÃ³n sin Ã¡nimo de lucro.
Wikipedia en espaÃ±ol es la versiÃ³n en espaÃ±ol de Wikipedia, un proyecto de enciclopedia web multilingÃ¼e de contenido libre basado en un modelo de ediciÃ³n abierta. Wikipedia crece cada dÃ­a gracias a la participaciÃ³n de gente de todo el mundo, siendo el mayor proyecto de recopilaciÃ³n de conocimiento jamÃ¡s realizado en la historia de la humanidad.

Se iniciÃ³ el 20 de mayo de 2001, y ya cuenta con 1 672 403 artÃ­culos.

VÃ©anse tambiÃ©n: Wikipedia, Historia de Wikipedia y Wikipedia en espaÃ±ol.
Wikipedia Community cartoon - high quality.png

Ãndice
1	Marca y propiedad
2	AutorÃ­a
2.1	AcreditaciÃ³n de autorÃ­a
3	Normas bÃ¡sicas de Wikipedia
3.1	Los cinco pilares de Wikipedia
3.2	Normas sobre la calidad
3.3	Por Ãºltimo...
4	CÃ³mo puedes colaborar
5	El futuro de Wikipedia
6	VÃ©ase tambiÃ©n
Marca y propiedad

Elemento grÃ¡fico (wikibola) usado como logotipo desde mayo de 2010, con la marca y el lema en espaÃ±ol bajo Ã©l.
Â«WikipediaÂ», sus lemas y sus logotipos son marcas comerciales registradas por la FundaciÃ³n Wikimedia â€”en inglÃ©s, Wikimedia Foundationâ€”, una organizaciÃ³n sin Ã¡nimo de lucro estadounidense radicada en San Francisco (California), pero regida por la legislaciÃ³n del estado de Florida, donde se originÃ³.

La FundaciÃ³n Wikimedia ha creado, mantiene, y es responsable de una serie de proyectos web de contenido libre basados en un modelo de ediciÃ³n abierta Â«hermanosÂ» de Wikipedia.

Aunque la propietaria del sitio es la FundaciÃ³n Wikimedia, muy raramente se involucra en la ediciÃ³n, la polÃ­tica editorial o las operaciones diarias. Cada uno de sus creadores mantiene los derechos de autor de sus aportaciones, pero el uso de la licencias libres asegura que el contenido se pueda distribuir y reproducir libremente.

VÃ©anse tambiÃ©n: FundaciÃ³n Wikimedia, Marcas corporativas de Wikipedia y Proyectos Wikimedia.
AutorÃ­a
VÃ©ase tambiÃ©n: Wikipedia:Derechos de autor
La mayor parte del texto de Wikipedia y la mayorÃ­a de sus imÃ¡genes y contenido multimedia disfrutan de una licencia dual, bajo las condiciones de la Licencia Creative Commons AtribuciÃ³n-CompartirIgual 3.0 Unported â€”CC-BY-SAâ€” y la GNU Free Documentation License â€”GFDL, sin versiones, sin secciones invariantes, textos de cubierta o textos de contracubierta; traducciÃ³n no oficialâ€”.

Cualquier persona con conexiÃ³n a Internet puede editar Wikipedia en espaÃ±ol, sin trÃ¡mite alguno, ni siquiera el de registrarse â€”en este caso quedarÃ¡ pÃºblicamente visible la direcciÃ³n IP desde la que se aporta el contenidoâ€”. El trÃ¡mite de registro, bajo seudÃ³nimo, es rÃ¡pido y sencillo, de forma que actualmente hay 6 176 709 usuarios registrados en Wikipedia en espaÃ±ol, de los que 17 541 pueden considerarse activos en este momento, por haber editado en el Ãºltimo mes. Entre todos los anteriores, pueden encontrarse, desde lectores esporÃ¡dicos, hasta especialistas reconocidos. A las personas que escriben o editan artÃ­culos en la Wikipedia se les suele denominar Â«wikipedistasÂ».

Esta apertura fomenta la inclusiÃ³n de una tremenda cantidad de contenido, de forma que esta enciclopedia cuenta hoy con 7 372 081 pÃ¡ginas diferentes, de las que 1 672 403 son artÃ­culos enciclopÃ©dicos â€”el resto son guÃ­as, pÃ¡ginas organizativas, de ayuda, de discusiÃ³n, polÃ­ticas, borradores de usuarios, etc.â€”.

VÃ©ase tambiÃ©n: Wikipedia:LimitaciÃ³n general de responsabilidad
AcreditaciÃ³n de autorÃ­a
Los textos de Wikipedia son un trabajo colaborativo, pero las contribuciones al texto de una pÃ¡gina de Wikipedia de cada editor individualizado quedan acreditadas de forma permanente en el historial de la pÃ¡gina, pÃºblicamente visible. La informaciÃ³n de autorÃ­a de las imÃ¡genes y el resto del contenido multimedia, como archivos de sonido o vÃ­deo, se puede consultar pulsando la propia imagen para entrar en su pÃ¡gina de descripciÃ³n, que incluye el autor, la fuente original y otra informaciÃ³n pertinente.

Normas bÃ¡sicas de Wikipedia
Para conseguir una enciclopedia con unos artÃ­culos de calidad y homogÃ©neos en formato y estructura, y con unos procedimientos organizativos y de mantenimiento adecuados a ese fin, la comunidad editora ha consensuado normas y polÃ­ticas:

Los cinco pilares de Wikipedia
Column impost.svg
Los cinco pilares son los principios mÃ¡s fundamentales de Wikipedia:

Wikipedia es una enciclopedia, y todos los esfuerzos deben ir en ese sentido.
Todos los artÃ­culos deben estar redactados desde un punto de vista neutral.
El objetivo es construir una enciclopedia de contenido libre, por lo que en ningÃºn caso se admite material con derechos de autor (copyrights) sin el permiso correspondiente.
Wikipedia sigue unas normas de etiqueta que deben respetarse.
Debes ser valiente al editar pÃ¡ginas, aunque siempre usando el sentido comÃºn.
Normas sobre la calidad
Crystal Clear action run approved.svg
Adicionalmente, es necesario contemplar otras tres reglas bÃ¡sicas indispensables para garantizar la calidad de los contenidos:

Wikipedia no es fuente primaria: la informaciÃ³n nunca debe proceder en Ãºltima instancia de los propios editores.
Verificabilidad: todos los artÃ­culos deben incluir referencias a las fuentes de las que proviene la informaciÃ³n.
Las fuentes de las que proviene la informaciÃ³n deben ser fuentes fiables.
En otras palabras, todo contenido que se aÃ±ade debe provenir de una publicaciÃ³n realizada por un autor de confianza, y los artÃ­culos deben reflejar siempre las fuentes de las que se obtuvo la informaciÃ³n.

Por Ãºltimo...
People together.svg
Se espera de los colaboradores que se comporten de manera civilizada y que los debates que puedan producirse en las pÃ¡ginas de discusiÃ³n de cada artÃ­culo tengan el Ãºnico propÃ³sito de mejorarlo. Perseguir los intereses propios o tratar de imponer ciertos puntos de vista son errores en los que no debe caerse.

Los artÃ­culos deben escribirse cuidadosamente, vigilando las faltas que se puedan cometer y la forma de expresarse; en el momento de hacer una consulta, es muy agradable leer una redacciÃ³n de buena calidad. Un mÃ©todo fÃ¡cil para escribir de forma correcta es imaginarse en todo momento las aportaciones dentro de una enciclopedia impresa, y eliminar o modificar el texto convenientemente cuando parezca fuera de lugar. AdemÃ¡s, existe un manual de estilo que ayuda a que los artÃ­culos adquieran una apariencia homogÃ©nea.

Nunca debe olvidarse que el objetivo de este proyecto es el de construir un recurso bÃ¡sico en todas las Ã¡reas del conocimiento humano. Los artÃ­culos deben ser atractivos y legibles, pero al mismo tiempo deben mantener la seriedad en su contenido y explicar los temas de forma concisa.

CÃ³mo puedes colaborar
ArtÃ­culos principales: Ayuda:CÃ³mo puedes colaborar y Ayuda:CÃ³mo se edita una pÃ¡gina.
Como hemos dicho anteriormente, Wikipedia estÃ¡ siendo construida por gente como tÃº. Mientras lees esto, decenas de personas alrededor del mundo estÃ¡n editando o creando algÃºn artÃ­culo. No es necesario estar registrado para comenzar a contribuir, pero si lo estÃ¡s, te serÃ¡ mÃ¡s fÃ¡cil integrarte en la comunidad.

Puedes navegar por Wikipedia y trabajar en las pÃ¡ginas que desees; basta con hacer clic en el enlace Â«editarÂ» que aparece en la parte superior de todas ellas. Puedes empezar realizando sugerencias para mejorar los artÃ­culos, pero te animamos a ser valiente editando, ya que muchos de los artÃ­culos se han elaborado por medio de pequeÃ±as contribuciones de distintos colaboradores.


Paso 1. Editar Wikipedia es muy sencillo, simplemente haz clic en la pestaÃ±a Â«editarÂ» ubicada en la parte superior de todas las pÃ¡ginas (cada secciÃ³n tambiÃ©n tiene un enlace equivalente).

Paso 2. SabrÃ¡s que estÃ¡s en el Â«modo ediciÃ³nÂ» porque verÃ¡s una caja blanca que contiene el texto del artÃ­culo. En la parte superior de ella dispones de una barra de herramientas para dar formato al texto. Modifica lo que estimes oportuno.

Paso 3. Los cambios que hagas serÃ¡n visibles desde el momento en que presiones el botÃ³n Â«Guardar la pÃ¡ginaÂ». Dada esta inmediatez, es preferible utilizar antes el botÃ³n Â«Mostrar previsualizaciÃ³nÂ» y, una vez hayas comprobado que todo estÃ¡ correcto, entonces grabar los cambios.
Con la ayuda de este tutorial podrÃ¡s aprender a editar paso a paso. AdemÃ¡s, tienes a tu disposiciÃ³n una Zona de pruebas para experimentar y una pÃ¡gina con los errores mÃ¡s frecuentes cometidos por los usuarios reciÃ©n llegados.

Otras actividades muy necesarias en Wikipedia son el denominado mantenimiento y la vigilancia de la pÃ¡gina de cambios recientes, donde aparecen los Ãºltimos artÃ­culos modificados.

El futuro de Wikipedia
Bridge-suspension.svgCrystal Clear app Community Help.png
Hay gente que piensa que Wikipedia, por ser un proyecto abierto a todo el mundo, acabarÃ¡ siendo un producto de baja calidad. No obstante, es quizÃ¡s precisamente esa caracterÃ­stica la que ha hecho posible que actualmente existan artÃ­culos buenos o incluso de calidad sobresaliente, y la que hace que su contenido constituya un material en constante progreso.

SegÃºn la llamada ley de Linus:
A la vista de suficientes ojos, todos los errores resultan evidentes.
Siguiendo esta teorÃ­a, se confÃ­a en que los errores serÃ¡n eliminados y pulidos gradualmente. AquÃ­ trabajan desde hace tiempo numerosos diplomados, licenciados, doctores, estudiantes, padres de familia, jubilados y muchas otras personas con grandes conocimientos, ideas y optimismo que aportar al proyecto. Cualquiera puede participar, siempre que sepa positivamente que sus aportaciones â€”ya sean creaciones, adiciones o correccionesâ€” son lÃ­citas y convenientes desde el punto de vista enciclopÃ©dico.

El futuro no se puede conocer, pero sÃ­ se puede construir desde el presente. Â¿Nos ayudas?

VÃ©ase tambiÃ©n
Ayuda:Contenidos
CategorÃ­a: Ayuda:Sobre Wikipedia
MenÃº de navegaciÃ³n
No has accedido
DiscusiÃ³n
Contribuciones
Crear una cuenta
Acceder
PÃ¡gina del proyectoDiscusiÃ³n
LeerMÃ¡s
Buscar
Buscar en Wikipedia
Portada
Portal de la comunidad
Actualidad
Cambios recientes
PÃ¡ginas nuevas
PÃ¡gina aleatoria
Ayuda
Donaciones
Notificar un error
Herramientas
Lo que enlaza aquÃ­
Cambios en enlazadas
Subir archivo
PÃ¡ginas especiales
Enlace permanente
InformaciÃ³n de la pÃ¡gina
Elemento de Wikidata
Imprimir/exportar
Crear un libro
Descargar como PDF
VersiÃ³n para imprimir
En otros proyectos
Wikimedia Commons
MediaWiki
Meta-Wiki
Wikisource multilingÃ¼e
Wikiespecies
Wikilibros
Wikidata
Wikinoticias
Wikiquote
Wikisource
Wikiviajes
Wikcionario

En otros idiomas
Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©
English
FranÃ§ais
à¤¹à¤¿à¤¨à¥à¤¦à¥€
Bahasa Indonesia
Bahasa Melayu
PortuguÃªs
Ğ ÑƒÑÑĞºĞ¸Ğ¹
ä¸­æ–‡
111 mÃ¡s
Editar enlaces
Esta pÃ¡gina se editÃ³ por Ãºltima vez el 24 mar 2021 a las 15:22.
El texto estÃ¡ disponible bajo la Licencia Creative Commons AtribuciÃ³n Compartir Igual 3.0; pueden aplicarse clÃ¡usulas adicionales. Al usar este sitio, usted acepta nuestros tÃ©rminos de uso y nuestra polÃ­tica de privacidad.
WikipediaÂ® es una marca registrada de la FundaciÃ³n Wikimedia, Inc., una organizaciÃ³n sin Ã¡nimo de lucro.
niverso
Ir a la navegaciÃ³nIr a la bÃºsqueda
Para otros usos de este tÃ©rmino, vÃ©ase Universo (desambiguaciÃ³n).
Universo
NASA-HS201427a-HubbleUltraDeepField2014-20140603.jpg
La imagen de luz visible mÃ¡s profunda del cosmos, el Campo Ultra Profundo del Hubble.
Edad	13 799Â±21 millones de aÃ±os
DiÃ¡metro	Al menos 93 000 millones de aÃ±os luz
Masa (materia ordinaria)	Al menos 1053 kg
Temperatura media	2,72548 K
Contenidos principales	
Materia ordinaria (bariÃ³nica) (4,9 %)
Materia oscura (26,8 %)
EnergÃ­a oscura (68,3 %)
Forma	Plano, con un margen de error de 0,4 %
vte
CosmologÃ­a fÃ­sica
Ilc 9yr moll4096.png
RadiaciÃ³n de fondo de microondas
ArtÃ­culos
Universo primitivo	TeorÃ­a del Big Bang Â· InflaciÃ³n cÃ³smica Â· NucleosÃ­ntesis primordial
ExpansiÃ³n	ExpansiÃ³n mÃ©trica del espacio Â· ExpansiÃ³n acelerada del Universo Â· Ley de Hubble Â· Corrimiento al rojo
Estructura	Forma del universo Â· Espacio-tiempo Â· Materia bariÃ³nica Â· Universo Â· Materia oscura Â· EnergÃ­a oscura
Experimentos	Planck (satÃ©lite) Â· WMAP Â· COBE
CientÃ­ficos	Albert Einstein Â· Edwin Hubble Â· Georges LemaÃ®tre Â· Stephen Hawking Â· George Gamow
Portales
Principal	CosmologÃ­a
Otros	FÃ­sica Â· AstronomÃ­a Â· ExploraciÃ³n espacial Â· Sistema Solar
El universo es la totalidad del espacio y del tiempo, de todas las formas de la materia, la energÃ­a, el impulso, las leyes y constantes fÃ­sicas que las gobiernan. Sin embargo, el tÃ©rmino tambiÃ©n se utiliza en sentidos contextuales ligeramente diferentes y alude a conceptos como cosmos, mundo o naturaleza.1â€‹ Su estudio, en las mayores escalas, es el objeto de la cosmologÃ­a, disciplina basada en la astronomÃ­a y la fÃ­sica, en la cual se describen todos los aspectos de este universo con sus fenÃ³menos.

La ciencia modeliza el universo como un sistema cerrado que contiene energÃ­a y materia adscritas al espacio-tiempo y que se rige fundamentalmente por principios causales. BasÃ¡ndose en observaciones del universo observable, los fÃ­sicos intentan describir el continuo espacio-tiempo en el que nos encontramos, junto con toda la materia y energÃ­a existentes en Ã©l.

Los experimentos sugieren que el universo se ha regido por las mismas leyes fÃ­sicas, constantes a lo largo de su extensiÃ³n e historia. Es homogÃ©neo e isotrÃ³pico. La fuerza dominante en distancias cÃ³smicas es la gravedad, y la relatividad general es actualmente la teorÃ­a mÃ¡s exacta para describirla. Las otras tres fuerzas fundamentales, y las partÃ­culas en las que actÃºan, son descritas por el modelo estÃ¡ndar.

El universo tiene por lo menos tres dimensiones de espacio y una de tiempo, aunque experimentalmente no se pueden descartar dimensiones adicionales. El espacio-tiempo parece estar conectado de forma sencilla, y el espacio tiene una curvatura media muy pequeÃ±a o incluso nula, de manera que la geometrÃ­a euclidiana es, como norma general, exacta en todo el universo.

La teorÃ­a actualmente mÃ¡s aceptada sobre la formaciÃ³n del universo, fue teorizada por el canÃ³nigo belga LemaÃ®tre, a partir de las ecuaciones de Albert Einstein. Lemaitre concluyÃ³ (en oposiciÃ³n a lo que pensaba Einstein) que el universo no era estacionario, que el universo tenÃ­a un origen. Es el modelo del Big Bang, que describe la expansiÃ³n del espacio-tiempo a partir de una singularidad espaciotemporal. El universo experimentÃ³ un rÃ¡pido periodo de inflaciÃ³n cÃ³smica que arrasÃ³ todas las irregularidades iniciales. A partir de entonces el universo se expandiÃ³ y se convirtiÃ³ en estable, mÃ¡s frÃ­o y menos denso. Las variaciones menores en la distribuciÃ³n de la masa dieron como resultado la segregaciÃ³n fractal en porciones, que se encuentran en el universo actual como cÃºmulos de galaxias.

Las observaciones astronÃ³micas indican que el universo tiene una edad de 13 799Â±21 millones de aÃ±os (entre 13 778 y 13 820 millones de aÃ±os con un intervalo de confianza del 68%) y por lo menos 93 000 millones de aÃ±os luz de extensiÃ³n.2â€‹

Debido a que, segÃºn la teorÃ­a de la relatividad especial, la materia no puede moverse a una velocidad superior a la velocidad de la luz, puede parecer paradÃ³jico que dos objetos del universo puedan haberse separado 93 000 millones de aÃ±os luz en un tiempo de Ãºnicamente 13 000 millones de aÃ±os; sin embargo, esta separaciÃ³n no entra en conflicto con la teorÃ­a de la relatividad general, ya que esta solo afecta al movimiento en el espacio, pero no al espacio mismo, que puede extenderse a un ritmo superior, no limitado por la velocidad de la luz. Por lo tanto, dos galaxias pueden separarse una de la otra mÃ¡s rÃ¡pidamente que la velocidad de la luz si es el espacio entre ellas el que se dilata.

Observaciones recientes han demostrado que esta expansiÃ³n se estÃ¡ acelerando, y que la mayor parte de la materia y la energÃ­a en el universo son las denominadas materia oscura y energÃ­a oscura; la materia ordinaria (bariÃ³nica) solo representarÃ­a algo mÃ¡s del 5 % del total.3â€‹

Las mediciones sobre la distribuciÃ³n espacial y el desplazamiento hacia el rojo (redshift) de galaxias distantes, la radiaciÃ³n cÃ³smica de fondo de microondas y los porcentajes relativos de los elementos quÃ­micos mÃ¡s ligeros apoyan la teorÃ­a de la expansiÃ³n del espacio, y mÃ¡s en general, la teorÃ­a del Big Bang, que propone que el universo en sÃ­ se originÃ³ en un momento especÃ­fico en el pasado.

En cuanto a su destino final, las pruebas actuales parecen apoyar las teorÃ­as de la expansiÃ³n permanente del universo (Big Freeze o Big Rip, Gran Desgarro), que nos indica que la expansiÃ³n misma del espacio provocarÃ¡ que llegarÃ¡ un punto en que los Ã¡tomos mismos se separarÃ¡n en partÃ­culas subatÃ³micas. Otros futuros posibles que se barajaron, especulaban que la materia oscura podrÃ­a ejercer la fuerza de gravedad suficiente para detener la expansiÃ³n y hacer que toda la materia se comprima nuevamente; algo a lo que los cientÃ­ficos denominan el Big Crunch o la Gran ImplosiÃ³n, pero las Ãºltimas observaciones van en la direcciÃ³n del gran desgarro.


Ãndice
1	PorciÃ³n observable o visible
2	EvoluciÃ³n
2.1	TeorÃ­a sobre el origen y la formaciÃ³n del Universo (Big Bang)
2.2	Sopa primigenia
2.3	Protogalaxias
2.4	Destino final
2.4.1	Big Crunch o la Gran ImplosiÃ³n
2.4.2	Big Rip o Gran Desgarramiento
3	DescripciÃ³n fÃ­sica
3.1	TamaÃ±o del universo
3.2	Forma
3.3	Color
3.4	Homogeneidad e isotropÃ­a
3.5	ComposiciÃ³n
3.6	Estructura cuÃ¡ntica
3.7	Multiversos
3.8	El universo, Â¿una ilusiÃ³n?
4	Estructuras agregadas del universo
4.1	Las galaxias
4.2	Formas de galaxias
4.2.1	Galaxias elÃ­pticas
4.2.2	Galaxias lenticulares
4.2.3	Galaxias espirales
4.2.4	Galaxia espiral barrada
4.2.5	Galaxias irregulares
4.3	La VÃ­a LÃ¡ctea
4.4	Las constelaciones
4.5	Las estrellas
4.6	Los planetas
4.7	Los satÃ©lites
4.8	Asteroides y cometas
5	Indicios de un comienzo
6	Otros tÃ©rminos
7	VÃ©ase tambiÃ©n
8	Referencias
9	Enlaces externos
PorciÃ³n observable o visible
ArtÃ­culo principal: Universo observable

Imagen de las Galaxias Antennae obtenida por el Telescopio espacial Hubble.
Los cosmÃ³logos teÃ³ricos y astrofÃ­sicos usan de manera diferente el tÃ©rmino universo, designando bien el sistema completo o Ãºnicamente una parte de Ã©l.4â€‹ A menudo se emplea el tÃ©rmino el universo para designar la parte observable del espacio-tiempo o el espacio-tiempo entero.

SegÃºn el convenio de los cosmÃ³logos, el tÃ©rmino universo se refiere frecuentemente a la parte finita del espacio-tiempo que es directamente observable utilizando telescopios, otros detectores y mÃ©todos fÃ­sicos, teÃ³ricos y empÃ­ricos para estudiar los componentes bÃ¡sicos del universo y sus interacciones. Los fÃ­sicos cosmÃ³logos asumen que la parte observable del espacio comÃ³vil (tambiÃ©n llamado nuestro universo) corresponde a una parte del espacio entero y normalmente no es el espacio entero.

En el caso del universo observable, este puede ser solo una mÃ­nima porciÃ³n del universo existente y, por consiguiente, puede ser imposible saber realmente si el universo estÃ¡ siendo completamente observado. La mayorÃ­a de cosmÃ³logos creen que el universo observable es una parte extremadamente pequeÃ±a del universo Â«enteroÂ» realmente existente y que es imposible ver todo el espacio comÃ³vil. En la actualidad se desconoce si esto es correcto, ya que de acuerdo a los estudios de la forma del universo, es posible que el universo observable estÃ© cerca de tener el mismo tamaÃ±o que todo el espacio. La pregunta sigue debatiÃ©ndose.5â€‹6â€‹

EvoluciÃ³n
TeorÃ­a sobre el origen y la formaciÃ³n del Universo (Big Bang)
ArtÃ­culo principal: TeorÃ­a del Big Bang
El hecho de que el universo estÃ© en expansiÃ³n se deriva de las observaciones del corrimiento al rojo realizadas en la dÃ©cada de 1920 y que se cuantifican por la ley de Hubble. Dichas observaciones son la predicciÃ³n experimental del modelo de Friedmann-Robertson-Walker, que es una soluciÃ³n de las ecuaciones de campo de Einstein de la relatividad general, que predicen el inicio del universo mediante un big bang.

El "corrimiento al rojo" es un fenÃ³meno observado por los astrÃ³nomos, que muestra una relaciÃ³n directa entre la distancia de un objeto remoto (como una galaxia) y la velocidad con la que este se aleja. Si esta expansiÃ³n ha sido continua a lo largo de la vida del universo, entonces en el pasado estos objetos distantes que siguen alejÃ¡ndose tuvieron que estar una vez juntos. Esta idea da pie a la teorÃ­a del Big Bang; el modelo dominante en la cosmologÃ­a actual.

Durante la era mÃ¡s temprana del Big Bang, se cree que el universo era un caliente y denso plasma. SegÃºn avanzaba la expansiÃ³n, la temperatura decrecÃ­a hasta el punto en que se pudieron formar los Ã¡tomos. En aquella Ã©poca, la energÃ­a de fondo se desacoplÃ³ de la materia y fue libre de viajar a travÃ©s del espacio. La energÃ­a remanente continuÃ³ enfriÃ¡ndose al expandirse el universo y hoy forma el fondo cÃ³smico de microondas. Esta radiaciÃ³n de fondo es remarcablemente uniforme en todas las direcciones, circunstancia que los cosmÃ³logos han intentado explicar como reflejo de un periodo temprano de inflaciÃ³n cÃ³smica despuÃ©s del Big Bang.

El examen de las pequeÃ±as variaciones en el fondo de radiaciÃ³n de microondas proporciona informaciÃ³n sobre la naturaleza del universo, incluyendo la edad y composiciÃ³n. La edad del universo desde el Big Bang, de acuerdo a la informaciÃ³n actual proporcionada por el WMAP de la NASA, se estima en unos 13.700 millones de aÃ±os, con un margen de error de un 1 % (137 millones de aÃ±os). Otros mÃ©todos de estimaciÃ³n ofrecen diferentes rangos de edad, desde 11 000 millones a 20 000 millones.

Sopa primigenia
Hasta hace poco, la primera centÃ©sima de segundo era mÃ¡s bien un misterio, impidiendo a los cientÃ­ficos describir exactamente cÃ³mo era el universo. Los nuevos experimentos en el RHIC, en el Brookhaven National Laboratory, han proporcionado a los fÃ­sicos una luz en esta cortina de alta energÃ­a, de tal manera que pueden observar directamente los tipos de comportamiento que pueden haber tenido lugar en ese instante.7â€‹

En estas energÃ­as, los quarks que componen los protones y los neutrones no estaban juntos, y una mezcla densa supercaliente de quarks y gluones, con algunos electrones, era todo lo que podÃ­a existir en los microsegundos anteriores a que se enfriaran lo suficiente para formar el tipo de partÃ­culas de materia que observamos hoy en dÃ­a.8â€‹

Protogalaxias
ArtÃ­culo principal: Protogalaxia
Los rÃ¡pidos avances acerca de lo que pasÃ³ despuÃ©s de la existencia de la materia aportan mucha informaciÃ³n sobre la formaciÃ³n de las galaxias. Se cree que las primeras galaxias eran dÃ©biles "galaxias enanas" que emitÃ­an tanta radiaciÃ³n que separarÃ­an los Ã¡tomos gaseosos de sus electrones. Este gas, a su vez, se estaba calentando y expandiendo, y tenÃ­a la posibilidad de obtener la masa necesaria para formar las grandes galaxias que conocemos hoy.9â€‹10â€‹

Destino final
ArtÃ­culo principal: Destino final del universo
El destino final del universo tiene diversos modelos que explican lo que sucederÃ¡ en funciÃ³n de diversos parÃ¡metros y observaciones. De acuerdo con la teorÃ­a general de la relatividad, el destino final mÃ¡s probable dependerÃ¡ del valor autÃ©ntico de la densidad de materia. En funciÃ³n de ese parÃ¡metro se barajan dos tipos de finales:

El Big Crunch (Gran ImplosiÃ³n) que sucederÃ¡ si el universo tiene una densidad de materia por encima de la densidad crÃ­tica, al punto de que sea capaz de decelerar su expansiÃ³n hasta detenerla y llegar a invertirla. AsÃ­, la materia recondensarÃ­a en una gran implosiÃ³n guiada por la gravedad.
El Big Rip (Gran Desgarramiento) que sucederÃ¡ si finalmente la densidad estÃ¡ por debajo de un valor crÃ­tico, los cÃºmulos de galaxias acabarÃ­an acercÃ¡ndose y formando grandes agujeros negros, del tipo que se supone existe en el centro de muchas galaxias. Esos agujeros negros pueden considerarse como un rasgado o desgarramiento del espacio-tiempo.
A partir de los aÃ±os 1990 se comprobÃ³ que el universo parece tener una expansiÃ³n acelerada, hecho que dentro de la relatividad general solo es explicable acudiendo a un mecanismo de tipo constante cosmolÃ³gica. No se conoce si ese hecho puede dar lugar a un tercer tipo de final.

Big Crunch o la Gran ImplosiÃ³n
ArtÃ­culo principal: Big Crunch
Si el universo es suficientemente denso, es posible que la fuerza gravitatoria de toda esa materia pueda finalmente detener la expansiÃ³n inicial, de tal manera que el universo volverÃ­a a contraerse, las galaxias empezarÃ­an a retroceder, y con el tiempo colisionarÃ­an entre sÃ­. La temperatura se elevarÃ­a, y el universo se precipitarÃ­a hacia un destino catastrÃ³fico en el que quedarÃ­a reducido nuevamente a un punto.

Algunos fÃ­sicos han especulado que despuÃ©s se formarÃ­a otro universo, en cuyo caso se repetirÃ­a el proceso. A esta teorÃ­a se la conoce como la teorÃ­a del universo oscilante.

Hoy en dÃ­a esta hipÃ³tesis parece incorrecta, pues a la luz de los Ãºltimos datos experimentales, el Universo se estÃ¡ expandiendo cada vez mÃ¡s rÃ¡pidamente.

Big Rip o Gran Desgarramiento
ArtÃ­culo principal: Big Rip
El Gran Desgarramiento o TeorÃ­a de la Eterna ExpansiÃ³n, en inglÃ©s Big Rip, es una hipÃ³tesis cosmolÃ³gica sobre el destino Ãºltimo del universo. Este posible destino final del universo depende de la cantidad de energÃ­a oscura existente en el Universo. Si el universo contiene suficiente energÃ­a oscura, podrÃ­a acabar en un desgarramiento de toda la materia.

El valor clave es w, la razÃ³n entre la presiÃ³n de la energÃ­a oscura y su densidad energÃ©tica. A w < -1, el universo acabarÃ­a por ser desgarrado. Primero, las galaxias se separarÃ­an entre sÃ­, luego la gravedad serÃ­a demasiado dÃ©bil para mantener integrada cada galaxia. Los sistemas planetarios perderÃ­an su cohesiÃ³n gravitatoria. En los Ãºltimos minutos, se desbaratarÃ¡n estrellas y planetas, y los Ã¡tomos serÃ¡n destruidos.

Los autores de esta hipÃ³tesis calculan que el fin del tiempo ocurrirÃ­a aproximadamente 3,5Ã—1010 aÃ±os despuÃ©s del Big Bang, es decir, dentro de 2,0Ã—1010 aÃ±os.

Una modificaciÃ³n de esta teorÃ­a denominada Big Freeze, aunque poco aceptada[cita requerida], normalmente afirma que el universo continuarÃ­a su expansiÃ³n sin provocar un Big Rip.

DescripciÃ³n fÃ­sica
TamaÃ±o del universo
ArtÃ­culo principal: Universo observable

La esfera perfecta del Universo observable tiene unos 93 000 millones de aÃ±os luz de diÃ¡metro. Esquema logarÃ­tmico con el Sistema Solar en el centro y el Big Bang en el borde.
Muy poco se conoce con certeza sobre el tamaÃ±o del universo. Puede tener una longitud de billones de aÃ±os luz o incluso tener un tamaÃ±o infinito.11â€‹ Un artÃ­culo de 200312â€‹ dice establecer una cota inferior de 24 gigaparsecs (78 000 millones de aÃ±os luz) para el tamaÃ±o del universo, pero no hay ninguna razÃ³n para creer que esta cota estÃ¡ de alguna manera muy ajustada (VÃ©ase forma del Universo).

El universo observable (o visible), que consiste en toda la materia y energÃ­a que podrÃ­a habernos afectado desde el Big Bang dada la limitaciÃ³n de la velocidad de la luz, es ciertamente finito. La distancia comÃ³vil al extremo del universo visible ronda los 46.500 millones de aÃ±os luz en todas las direcciones desde la Tierra. AsÃ­, el universo visible se puede considerar como una esfera perfecta con la Tierra en el centro, y un diÃ¡metro de unos 93 000 millones de aÃ±os luz.13â€‹ Hay que notar que muchas fuentes han publicado una amplia variedad de cifras incorrectas para el tamaÃ±o del universo visible: desde 13 700 hasta 180 000 millones de aÃ±os luz. (VÃ©ase universo observable).

En el Universo las distancias que separan los astros son tan grandes que, si las quisiÃ©ramos expresar en metros, tendrÃ­amos que utilizar cifras muy grandes. Debido a ello, se utiliza como unidad de longitud el aÃ±o luz, que corresponde a la distancia que recorre la luz en un aÃ±o.

Anteriormente, el modelo de universo mÃ¡s comÃºnmente aceptado era el propuesto por Albert Einstein en su Relatividad General, en la que propone un universo "finito pero ilimitado", es decir, que a pesar de tener un volumen medible no tiene lÃ­mites, de forma anÃ¡loga a la superficie de una esfera, que es medible pero ilimitada. Esto era propio de un universo esfÃ©rico. Hoy, gracias a las Ãºltimas observaciones realizadas por el WMAP de la NASA, se sabe que tiene forma plana. Aunque no se descarta un posible universo plano cerrado sobre sÃ­ mismo. Estas observaciones sugieren que el universo es infinito.

Forma
ArtÃ­culos principales: Forma del Universo y Estructura a gran escala del universo.

Universum, Grabado Flammarion, xilografÃ­a, publicada en ParÃ­s 1888.
Una pregunta importante abierta en cosmologÃ­a es la forma del universo. MatemÃ¡ticamente, Â¿quÃ© 3-variedad representa mejor la parte espacial del universo?

Si el universo es espacialmente plano, se desconoce si las reglas de la geometrÃ­a Euclidiana serÃ¡n vÃ¡lidas a mayor escala. Actualmente muchos cosmÃ³logos creen que el Universo observable estÃ¡ muy cerca de ser espacialmente plano, con arrugas locales donde los objetos masivos distorsionan el espacio-tiempo, de la misma forma que la superficie de un lago es casi plana. Esta opiniÃ³n fue reforzada por los Ãºltimos datos del WMAP, mirando hacia las "oscilaciones acÃºsticas" de las variaciones de temperatura en la radiaciÃ³n de fondo de microondas.14â€‹

Por otra parte, se desconoce si el universo es conexo. El universo no tiene cotas espaciales de acuerdo al modelo estÃ¡ndar del Big Bang, pero sin embargo debe ser espacialmente finito (compacto). Esto se puede comprender utilizando una analogÃ­a en dos dimensiones: la superficie de una esfera no tiene lÃ­mite, pero no tiene un Ã¡rea infinita. Es una superficie de dos dimensiones con curvatura constante en una tercera dimensiÃ³n. La 3-esfera es un equivalente en tres dimensiones en el que las tres dimensiones estÃ¡n constantemente curvadas en una cuarta.

Si el universo fuese compacto y sin cotas, serÃ­a posible, despuÃ©s de viajar una distancia suficiente, volver al punto de partida. AsÃ­, la luz de las estrellas y galaxias podrÃ­a pasar a travÃ©s del universo observable mÃ¡s de una vez. Si el universo fuese mÃºltiplemente conexo y suficientemente pequeÃ±o (y de un tamaÃ±o apropiado, tal vez complejo) entonces posiblemente se podrÃ­a ver una o varias veces alrededor de Ã©l en alguna (o todas) direcciones. Aunque esta posibilidad no ha sido descartada, los resultados de las Ãºltimas investigaciones de la radiaciÃ³n de fondo de microondas hacen que esto parezca improbable.

Color
CafÃ© con leche cÃ³smico, el color del universo.
HistÃ³ricamente se ha creÃ­do que el Universo es de color negro, pues es lo que observamos al momento de mirar al cielo en las noches despejadas. En 2002, sin embargo, los astrÃ³nomos Karl Glazebrook e Ivan Baldry afirmaron en un artÃ­culo cientÃ­fico que el universo en realidad es de un color que decidieron llamar cafÃ© con leche cÃ³smico.15â€‹16â€‹ Este estudio se basÃ³ en la mediciÃ³n del rango espectral de la luz proveniente de un gran volumen del Universo, sintetizando la informaciÃ³n aportada por un total de mÃ¡s de 200.000 galaxias.

Homogeneidad e isotropÃ­a

Fluctuaciones en la radiaciÃ³n de fondo de microondas, Imagen NASA/WMAP.
Mientras que la estructura estÃ¡ considerablemente fractalizada a nivel local (ordenada en una jerarquÃ­a de racimo), en los Ã³rdenes mÃ¡s altos de distancia el universo es muy homogÃ©neo. A estas escalas la densidad del universo es muy uniforme, y no hay una direcciÃ³n preferida o significativamente asimÃ©trica en el universo. Esta homogeneidad e isotropÃ­a es un requisito de la MÃ©trica de Friedman-LemaÃ®tre-Robertson-Walker empleada en los modelos cosmolÃ³gicos modernos.17â€‹

La cuestiÃ³n de la anisotropÃ­a en el universo primigenio fue significativamente contestada por el WMAP, que buscÃ³ fluctuaciones en la intensidad del fondo de microondas.18â€‹ Las medidas de esta anisotropÃ­a han proporcionado informaciÃ³n Ãºtil y restricciones sobre la evoluciÃ³n del Universo.

Hasta el lÃ­mite de la potencia de observaciÃ³n de los instrumentos astronÃ³micos, los objetos irradian y absorben la energÃ­a de acuerdo a las mismas leyes fÃ­sicas a como lo hacen en nuestra propia galaxia.19â€‹ BasÃ¡ndose en esto, se cree que las mismas leyes y constantes fÃ­sicas son universalmente aplicables a travÃ©s de todo el universo observable. No se ha encontrado ninguna prueba confirmada que muestre que las constantes fÃ­sicas hayan variado desde el Big Bang.20â€‹

ComposiciÃ³n
El universo observable actual parece tener un espacio-tiempo geomÃ©tricamente plano, conteniendo una densidad masa-energÃ­a equivalente a 9,9 Ã— 10âˆ’30 gramos por centÃ­metro cÃºbico. Los constituyentes primarios parecen consistir en un 73 % de energÃ­a oscura, 23 % de materia oscura frÃ­a y un 4 % de Ã¡tomos. AsÃ­, la densidad de los Ã¡tomos equivaldrÃ­a a un nÃºcleo de hidrÃ³geno sencillo por cada cuatro metros cÃºbicos de volumen.21â€‹ La naturaleza exacta de la energÃ­a oscura y la materia oscura frÃ­a sigue siendo un misterio. Actualmente se especula con que el neutrino, (una partÃ­cula muy abundante en el universo), tenga, aunque mÃ­nima, una masa. De comprobarse este hecho, podrÃ­a significar que la energÃ­a y la materia oscura no existen.


Nebulosa del Ãguila
Durante las primeras fases del Big Bang, se cree que se formaron las mismas cantidades de materia y antimateria. Materia y antimateria deberÃ­an eliminarse mutuamente al entrar en contacto, por lo que la actual existencia de materia (y la ausencia de antimateria) supone una violaciÃ³n de la simetrÃ­a CP (VÃ©ase ViolaciÃ³n CP), por lo que puede ser que las partÃ­culas y las antipartÃ­culas no tengan propiedades exactamente iguales o simÃ©tricas,22â€‹ o puede que simplemente las leyes fÃ­sicas que rigen el universo favorezcan la supervivencia de la materia frente a la antimateria.23â€‹En este mismo sentido, tambiÃ©n se ha sugerido que quizÃ¡s la materia oscura sea la causante de la bariogÃ©nesis al interactuar de distinta forma con la materia que con la antimateria.24â€‹


Westerlund 2
Antes de la formaciÃ³n de las primeras estrellas, la composiciÃ³n quÃ­mica del universo consistÃ­a primariamente en hidrÃ³geno (75 % de la masa total), con una suma menor de helio-4 (4He) (24 % de la masa total) y el resto de otros elementos.25â€‹ Una pequeÃ±a porciÃ³n de estos elementos estaba en la forma del isÃ³topo deuterio (Â²H), helio-3 (Â³He) y litio (7Li).26â€‹ La materia interestelar de las galaxias ha sido enriquecida sin cesar por elementos mÃ¡s pesados, generados por procesos de fusiÃ³n en las estrellas, y diseminados como resultado de las explosiones de supernovas, los vientos estelares y la expulsiÃ³n de la cubierta exterior de estrellas maduras.27â€‹

El Big Bang dejÃ³ detrÃ¡s un flujo de fondo de fotones y neutrinos. La temperatura de la radiaciÃ³n de fondo ha decrecido sin cesar con la expansiÃ³n del universo y ahora fundamentalmente consiste en la energÃ­a de microondas equivalente a una temperatura de 2725 K.28â€‹ La densidad del fondo de neutrinos actual es de 150 por centÃ­metro cÃºbico.29â€‹

VÃ©ase tambiÃ©n: Abundancia de los elementos quÃ­micos
Estructura cuÃ¡ntica
SegÃºn la fÃ­sica moderna, el Universo es un sistema cuÃ¡ntico aislado, un campo unificado de ondas que entra en decoherencia al tutor de la observaciÃ³n o mediciÃ³n. En tal virtud, en Ãºltima instancia, el entorno del Universo serÃ­a no local y no determinista.

Multiversos
ArtÃ­culos principales: Multiverso y Universos paralelos.
Los cosmÃ³logos teÃ³ricos estudian modelos del conjunto espacio-tiempo que estÃ©n conectados, y buscan modelos que sean consistentes con los modelos fÃ­sicos cosmolÃ³gicos del espacio-tiempo en la escala del universo observable. Sin embargo, recientemente han tomado fuerza teorÃ­as que contemplan la posibilidad de multiversos o varios universos coexistiendo simultÃ¡neamente. SegÃºn la recientemente enunciada TeorÃ­a de Multiexplosiones se pretende dar explicaciÃ³n a este aspecto, poniendo en relieve una posible convivencia de universos en un mismo espacio.30â€‹

El universo, Â¿una ilusiÃ³n?
CientÃ­ficos del King's College de Londres lograron recrear las condiciones inmediatamente seguidas al Big Bang a travÃ©s del conocimiento adquirido durante dos aÃ±os de la partÃ­cula de Higgs y llegaron a la conclusiÃ³n de que, posiblemente, el universo colapsÃ³, hasta dejar de existir casi tan pronto cuando empezÃ³,31â€‹ lo que plantea la idea de que todo lo que vemos no existe y solo es el pasado de los astros.32â€‹

Estructuras agregadas del universo
Las galaxias
ArtÃ­culo principal: Galaxia

Imagen de la galaxia espiral M81 tomada por el Hubble.
A gran escala, el universo estÃ¡ formado por galaxias y agrupaciones de galaxias. Las galaxias son agrupaciones masivas de estrellas, y son las estructuras mÃ¡s grandes en las que se organiza la materia en el universo. A travÃ©s del telescopio se manifiestan como manchas luminosas de diferentes formas. A la hora de clasificarlas, los cientÃ­ficos distinguen entre las galaxias del Grupo Local, compuesto por las treinta galaxias mÃ¡s cercanas y a las que estÃ¡ unida gravitacionalmente nuestra galaxia (la VÃ­a LÃ¡ctea), y todas las demÃ¡s galaxias, a las que llaman "galaxias exteriores".

Las galaxias estÃ¡n distribuidas por todo el universo y presentan caracterÃ­sticas muy diversas, tanto en lo que respecta a su configuraciÃ³n como a su antigÃ¼edad. Las mÃ¡s pequeÃ±as abarcan alrededor de 3000 millones de estrellas, y las galaxias de mayor tamaÃ±o pueden llegar a abarcar mÃ¡s de un billÃ³n de astros. Estas Ãºltimas pueden tener un diÃ¡metro de 170 000 aÃ±os luz, mientras que las primeras no suelen exceder de los 6000 aÃ±os luz.

AdemÃ¡s de estrellas y sus astros asociados (planetas, asteroides, etc...), las galaxias contienen tambiÃ©n materia interestelar, constituida por polvo y gas en una proporciÃ³n que varÃ­a entre el 1 y el 10 % de su masa.

Se estima que el universo puede estar constituido por unos 100 000 millones de galaxias, aunque estas cifras varÃ­an en funciÃ³n de los diferentes estudios.

Formas de galaxias
La creciente potencia de los telescopios, que permite observaciones cada vez mÃ¡s detalladas de los distintos elementos del universo, ha hecho posible una clasificaciÃ³n de las galaxias por su forma. Se han establecido asÃ­ cuatro tipos distintos: galaxias elÃ­pticas,lenticulares, espirales, espirales barradas e irregulares.

Galaxias elÃ­pticas

Galaxia elÃ­ptica NGC 1316.
ArtÃ­culo principal: Galaxia elÃ­ptica
En forma de elipse o de esferoide, se caracterizan por carecer de una estructura interna definida y por presentar muy poca materia interestelar. Se consideran las mÃ¡s antiguas del universo, ya que sus estrellas son viejas y se encuentran en una fase muy avanzada de su evoluciÃ³n.

Galaxias lenticulares
ArtÃ­culo principal: Galaxia lenticular
Las galaxias de este tipo fueron en su momento galaxias espirales, pero consumieron o perdieron gran parte de materia interestelar, por lo que hoy carecen de brazos espirales y solo presenta su nÃºcleo. Aunque a veces existe cierta cantidad de materia interestelar, sobre todo polvo, que se agrupa en forma de disco alrededor de esta. Estas galaxias constituyen alrededor del 3 % de las galaxias del universo.

Galaxias espirales
ArtÃ­culo principal: Galaxia espiral
EstÃ¡n constituidas por un nÃºcleo central y dos o mÃ¡s brazos en espiral, que parten del nÃºcleo. Este se halla formado por multitud de estrellas y apenas tiene materia interestelar, mientras que en los brazos abunda la materia interestelar y hay gran cantidad de estrellas jÃ³venes, que son muy brillantes. Alrededor del 75 % de las galaxias del universo son de este tipo.

Galaxia espiral barrada
ArtÃ­culo principal: Galaxia espiral barrada
Es un subtipo de galaxia espiral, caracterizados por la presencia de una barra central de la que tÃ­picamente parten dos brazos espirales. Este tipo de galaxias constituyen una fracciÃ³n importante del total de galaxias espirales. La VÃ­a LÃ¡ctea es una galaxia espiral barrada.

Galaxias irregulares

Galaxia irregular NGC 1427.
ArtÃ­culo principal: Galaxia irregular
Incluyen una gran diversidad de galaxias, cuyas configuraciones no responden a las tres formas anteriores, aunque tienen en comÃºn algunas caracterÃ­sticas, como la de ser casi todas pequeÃ±as y contener un gran porcentaje de materia interestelar. Se calcula que son irregulares alrededor del 5 % de las galaxias del universo.

La VÃ­a LÃ¡ctea
ArtÃ­culo principal: VÃ­a LÃ¡ctea
La VÃ­a LÃ¡ctea es nuestra galaxia. SegÃºn las observaciones, posee una masa de 1012 masas solares y es de tipo espiral barrada. Con un diÃ¡metro medio de unos 100 000 aÃ±os luz se calcula que contiene unos 200 000 millones de estrellas, entre las cuales se encuentra el Sol. La distancia desde el Sol al centro de la galaxia es de alrededor de 27 700 aÃ±os luz (8,5 kpc) A simple vista, se observa como una estela blanquecina de forma elÃ­ptica, que se puede distinguir en las noches despejadas. Lo que no se aprecian son sus brazos espirales, en uno de los cuales, el llamado brazo de OriÃ³n, estÃ¡ situado nuestro sistema solar, y por tanto la Tierra.

El nÃºcleo central de la galaxia presenta un espesor uniforme en todos sus puntos, salvo en el centro, donde existe un gran abultamiento con un grosor mÃ¡ximo de 16 000 aÃ±os luz, siendo el grosor medio de unos 6000 aÃ±os luz.


VÃ­a LÃ¡ctea
Todas las estrellas y la materia interestelar que contiene la VÃ­a LÃ¡ctea, tanto en el nÃºcleo central como en los brazos, estÃ¡n situadas dentro de un disco de 100 000 aÃ±os luz de diÃ¡metro, que gira sobre su eje a una velocidad lineal superior a los 216 km/s.33â€‹

Las constelaciones
ArtÃ­culo principal: ConstelaciÃ³n

ConstelaciÃ³n AndrÃ³meda
Tan solo tres galaxias distintas a la nuestra son visibles a simple vista. Tenemos la Galaxia de AndrÃ³meda, visible desde el Hemisferio Norte; la Gran Nube de Magallanes, y la PequeÃ±a Nube de Magallanes, en el Hemisferio Sur celeste. El resto de las galaxias no son visibles al ojo desnudo sin ayuda de instrumentos. SÃ­ que lo son, en cambio, las estrellas que forman parte de la VÃ­a LÃ¡ctea. Estas estrellas dibujan a menudo en el cielo figuras reconocibles, que han recibido diversos nombres en relaciÃ³n con su aspecto. Estos grupos de estrellas de perfil identificable se conocen con el nombre de constelaciones. La UniÃ³n AstronÃ³mica Internacional agrupÃ³ oficialmente las estrellas visibles en 88 constelaciones, algunas de ellas muy extensas, como Hidra o la Osa Mayor, y otras muy pequeÃ±as como Flecha y TriÃ¡ngulo.

Las estrellas
ArtÃ­culo principal: Estrella
Son los elementos constitutivos mÃ¡s destacados de las galaxias. Las estrellas son enormes esferas de gas que brillan debido a sus gigantescas reacciones nucleares. Cuando debido a la fuerza gravitatoria, la presiÃ³n y a la temperatura del interior de una estrella que sea suficientemente intensa, se inicia la fusiÃ³n nuclear de sus Ã¡tomos, y comienzan a emitir una luz roja oscura, que despuÃ©s se mueve hacia el estado superior, que es en el que estÃ¡ nuestro Sol, para posteriormente, al modificarse las reacciones nucleares interiores, dilatarse y finalmente enfriarse.


Remanente de la supernova
Al acabarse el hidrÃ³geno, se originan reacciones nucleares de elementos mÃ¡s pesados, mÃ¡s energÃ©ticas, que convierten la estrella en una gigante roja. Con el tiempo, esta se vuelve inestable, a la vez que lanza hacia el espacio exterior la mayor parte del material estelar. Este proceso puede durar 100 millones de aÃ±os, hasta que se agota toda la energÃ­a nuclear, y la estrella se contrae por efecto de la gravedad hasta hacerse pequeÃ±a y densa, en la forma de enana blanca, azul o marrÃ³n. Si la estrella inicial es varias veces mÃ¡s masiva que el Sol, su ciclo puede ser diferente, y en lugar de una gigante, puede convertirse en una supergigante y acabar su vida con una explosiÃ³n denominada supernova. Estas estrellas pueden acabar como estrellas de neutrones. TamaÃ±os aÃºn mayores de estrellas pueden consumir todo su combustible muy rÃ¡pidamente, transformÃ¡ndose en una entidad supermasiva llamada agujero negro.


El centro galÃ¡ctico visto por los telescopios 2MASS.
Los pÃºlsares son fuentes de ondas de radio que emiten con periodos regulares. La palabra Â«pÃºlsarÂ» significa pulsating radio source (fuente de radio pulsante). Se detectan mediante radiotelescopios y se requieren relojes de extraordinaria precisiÃ³n para detectar sus cambios de ritmo. Los estudios indican que un pÃºlsar es una estrella de neutrones pequeÃ±a que gira a gran velocidad. El mÃ¡s conocido estÃ¡ en la Nebulosa del Cangrejo. Su densidad es tan grande que una muestra de cuÃ¡sar del tamaÃ±o de una bola de bolÃ­grafo tendrÃ­a una masa de cerca de 100 000 toneladas. Su campo magnÃ©tico, muy intenso, se concentra en un espacio reducido. Esto lo acelera y lo hace emitir gran cantidad de energÃ­a en haces de radiaciÃ³n que aquÃ­ recibimos como ondas de radio.

La palabra Â«cuÃ¡sarÂ» es un acrÃ³nimo de quasi stellar radio source (fuentes de radio casi estelares). Se identificaron en la dÃ©cada de 1950. MÃ¡s tarde se vio que mostraban un desplazamiento al rojo mÃ¡s grande que cualquier otro objeto conocido. La causa era el Efecto Doppler, que mueve el espectro hacia el rojo cuando los objetos se alejan. El primer cuÃ¡sar estudiado, denominado 3C 273, estÃ¡ a 1500 millones de aÃ±os luz de la Tierra. A partir de 1980 se han identificado miles de cuÃ¡sares, algunos alejÃ¡ndose de nosotros a velocidades del 90 % de la luz.

Se han descubierto cuÃ¡sares a 12 000 millones de aÃ±os luz de la Tierra; prÃ¡cticamente la edad del universo. A pesar de las enormes distancias, la energÃ­a que llega en algunos casos es muy grande, equivalente a la recibida desde miles de galaxias: como ejemplo, el s50014+81 es unas 60 000 veces mÃ¡s brillante que toda la VÃ­a LÃ¡ctea.

Los planetas
ArtÃ­culo principal: Planeta
Los planetas son cuerpos que giran en torno a una estrella y que, segÃºn la definiciÃ³n de la UniÃ³n AstronÃ³mica Internacional, deben cumplir ademÃ¡s la condiciÃ³n de haber limpiado su Ã³rbita de otros cuerpos rocosos importantes, y de tener suficiente masa como para que su fuerza de gravedad genere un cuerpo esfÃ©rico. En el caso de cuerpos que orbitan alrededor de una estrella que no cumplan estas caracterÃ­sticas, se habla de planetas enanos, planetesimales, o asteroides. En nuestro Sistema Solar hay 8 planetas: Mercurio, Venus, Tierra, Marte, JÃºpiter, Saturno, Urano y Neptuno, considerÃ¡ndose desde 2006 a PlutÃ³n como un planeta enano. A finales de 2009, fuera de nuestro sistema solar se habÃ­an detectado mÃ¡s de 400 planetas extrasolares, pero los avances tecnolÃ³gicos estÃ¡n permitiendo que este nÃºmero crezca a buen ritmo.

Los satÃ©lites
ArtÃ­culo principal: SatÃ©lite natural
Los satÃ©lites naturales son astros que giran alrededor de los planetas. El Ãºnico satÃ©lite natural de la Tierra es la Luna, que es tambiÃ©n el satÃ©lite mÃ¡s cercano al sol. A continuaciÃ³n se enumeran los principales satÃ©lites de los planetas del sistema solar (se incluye en el listado a PlutÃ³n, considerado por la UAI como un planeta enano).

Tierra: 1 satÃ©lite â†’ Luna
Marte: 2 satÃ©lites â†’ Fobos, Deimos
JÃºpiter: 63 satÃ©lites â†’ Metis, Adrastea, Amaltea, Tebe, Ão, Europa, Ganimedes, Calisto, Leda, Himalia, Lisitea, Elara, AnankÃ©, CarmÃ©, PasÃ­fae, Sinope...
Saturno: 82 satÃ©lites â†’ Pan, Atlas, Prometeo, Pandora, Epimeteo, Jano, Mimas, EncÃ©lado, Tetis, Telesto, Calipso, Dione, Helena, Rea, TitÃ¡n, HiperiÃ³n, JÃ¡peto, Febe...
Urano: 15 satÃ©lites â†’ Cordelia, Ofelia, Bianca, CrÃ©sida, DesdÃ©mona, Julieta, Porcia, Rosalinda, Belinda, Puck, Miranda, Ariel, Umbriel, Titania, OberÃ³n.
Neptuno: 8 satÃ©lites â†’ NÃ¡yade, Talasa, Despina, Galatea, Larisa, Proteo, TritÃ³n, Nereida
PlutÃ³n: 5 satÃ©lites â†’ Caronte, Nix, Hidra, Cerbero y Estigia
Asteroides y cometas
ArtÃ­culos principales: Asteroide y Cometa.

C/2014 Q2 (Lovejoy)
En aquellas zonas de la Ã³rbita de una estrella en las que, por diversos motivos, no se ha producido la agrupaciÃ³n de la materia inicial en un Ãºnico cuerpo dominante o planeta, aparecen los discos de asteroides: objetos rocosos de muy diversos tamaÃ±os que orbitan en grandes cantidades en torno a la estrella, chocando eventualmente entre sÃ­. Cuando las rocas tienen diÃ¡metros inferiores a 50 m se denominan meteoroides. A consecuencia de las colisiones, algunos asteroides pueden variar sus Ã³rbitas, adoptando trayectorias muy excÃ©ntricas que periÃ³dicamente les acercan la estrella. Cuando la composiciÃ³n de estas rocas es rica en agua u otros elementos volÃ¡tiles, el acercamiento a la estrella y su consecuente aumento de temperatura origina que parte de su masa se evapore y sea arrastrada por el viento solar, creando una larga cola de material brillante a medida que la roca se acerca a la estrella. Estos objetos se denominan cometas. En nuestro sistema solar hay dos grandes discos de asteroides: uno situado entre las Ã³rbitas de Marte y JÃºpiter, denominado el CinturÃ³n de asteroides, y otro mucho mÃ¡s tenue y disperso en los lÃ­mites del sistema solar, a aproximadamente un aÃ±o luz de distancia, denominado Nube de Oort.



Mapa del universo observable con los objetos astronÃ³micos notables conocidos en la actualidad. Los cuerpos celestes aparecen con el tamaÃ±o agrandado para poder apreciar su forma.

Indicios de un comienzo
La teorÃ­a general de la relatividad, que fue publicada por Albert Einstein en 1916, implicaba que el cosmos se hallaba en expansiÃ³n o en contracciÃ³n. Pero este concepto era totalmente opuesto a la nociÃ³n de un universo estÃ¡tico, aceptada entonces hasta por el propio Einstein. De ahÃ­ que este incluyera en sus cÃ¡lculos lo que denominÃ³ â€œconstante cosmolÃ³gicaâ€, ajuste mediante el cual intentaba conciliar su teorÃ­a con la idea aceptada de un universo estÃ¡tico e inmutable. Sin embargo, ciertos descubrimientos que se sucedieron en los aÃ±os veinte llevaron a Einstein a decir que el ajuste que habÃ­a efectuado a su teorÃ­a de la relatividad era el â€˜mayor error de su vidaâ€™. Dichos descubrimientos se realizaron gracias a la instalaciÃ³n de un enorme telescopio de 254 centÃ­metros en el monte Wilson (California). Las observaciones formuladas en los aÃ±os veinte con la ayuda de este instrumento demostraron que el universo se halla en expansiÃ³n.

Hasta entonces, los mayores telescopios solo permitÃ­an identificar las estrellas de nuestra galaxia, la VÃ­a LÃ¡ctea, y aunque se veÃ­an borrones luminosos, llamados nebulosas, por lo general se tomaban por remolinos de gas existentes en nuestra galaxia. Gracias a la mayor potencia del telescopio del monte Wilson, Edwin Hubble logrÃ³ distinguir estrellas en aquellas nebulosas. Finalmente se descubriÃ³ que los borrones eran lo mismo que la VÃ­a LÃ¡ctea: galaxias. Hoy se cree que hay entre 50 000 y 125 000 millones de galaxias, cada una con cientos de miles de millones de estrellas.

A finales de los aÃ±os veinte, Hubble tambiÃ©n descubriÃ³ que las galaxias se alejan de nosotros, y que lo hacen mÃ¡s velozmente cuanto mÃ¡s lejos se hallan. Los astrÃ³nomos calculan la tasa de recesiÃ³n de las galaxias mediante el espectrÃ³grafo, instrumento que mide el espectro de la luz procedente de los astros. Para ello, dirigen la luz que proviene de estrellas lejanas hacia un prisma, que la descompone en los colores que la integran.

La luz de un objeto es rojiza (fenÃ³meno llamado corrimiento al rojo) si este se aleja del observador, y azulada (corrimiento al azul) si se le aproxima. Cabe destacar que, salvo en el caso de algunas galaxias cercanas, todas las galaxias conocidas tienen lÃ­neas espectrales desplazadas hacia el rojo. De ahÃ­ infieren los cientÃ­ficos que el universo se expande de forma ordenada. La tasa de dicha expansiÃ³n se determina midiendo el grado de desplazamiento al rojo. Â¿QuÃ© conclusiÃ³n se ha extraÃ­do de la expansiÃ³n del cosmos? Pues bien, un cientÃ­fico invitÃ³ al pÃºblico a analizar el proceso a la inversa â€”como una pelÃ­cula de la expansiÃ³n proyectada en retrocesoâ€” a fin de observar la historia primitiva del universo. Visto asÃ­, el cosmos parecerÃ­a estar en recesiÃ³n o contracciÃ³n, en vez de en expansiÃ³n y retornarÃ­a finalmente a un Ãºnico punto de origen.

El fÃ­sico Stephen Hawking concluyÃ³ lo siguiente en su libro Agujeros negros y pequeÃ±os universos (y otros ensayos), editado en 1993: Â«La ciencia podrÃ­a afirmar que el universo tenÃ­a que haber conocido un comienzoÂ». Pero hace aÃ±os, muchos expertos rechazaban que el universo hubiese tenido principio. El cientÃ­fico Fred Hoyle no aceptaba que el cosmos hubiera surgido mediante lo que llamÃ³ burlonamente a big bang (Â«una gran explosiÃ³nÂ»). Uno de los argumentos que esgrimÃ­a era que, de haber existido un comienzo tan dinÃ¡mico, deberÃ­an conservarse residuos de aquel acontecimiento en algÃºn lugar del universo: tendrÃ­a que haber radiaciÃ³n fÃ³sil, por asÃ­ decirlo; una leve luminiscencia residual.

El diario The New York Times (8 de marzo de 1998) indicÃ³ que hacia 1965 Â«los astrÃ³nomos Arno Penzias y Robert Wilson descubrieron la omnipresente radiaciÃ³n de fondo: el destello residual de la explosiÃ³n primigeniaÂ». El artÃ­culo aÃ±adiÃ³: Â«Todo indicaba que la teorÃ­a [de la gran explosiÃ³n] habÃ­a triunfadoÂ».

Pero en los aÃ±os posteriores al hallazgo se formulÃ³ esta objeciÃ³n: Si el modelo de la gran explosiÃ³n era correcto, Â¿Por quÃ© no se habÃ­an detectado leves irregularidades en la radiaciÃ³n? (La formaciÃ³n de las galaxias habrÃ­a requerido un universo que contase con zonas mÃ¡s frÃ­as y densas que permitieran la fusiÃ³n de la materia.) En efecto, los experimentos realizados por Penzias y Wilson desde la superficie terrestre no revelaban tales irregularidades.

Por esta razÃ³n, la NASA lanzÃ³ en noviembre de 1989 el satÃ©lite COBE (siglas de Explorador del Fondo CÃ³smico, en inglÃ©s), cuyos descubrimientos se calificaron de cruciales. â€œLas ondas que detectÃ³ su radiÃ³metro diferencial de microondas correspondÃ­an a las fluctuaciones que dejaron su impronta en el cosmos y que hace miles de millones de aÃ±os llevaron a la formaciÃ³n de las galaxias.â€

Otros tÃ©rminos
Diferentes palabras se han utilizado a travÃ©s de la historia para denotar "todo el espacio", incluyendo los equivalentes y las variantes en varios lenguajes de "cielos", "cosmos" y "mundo". El macrocosmos tambiÃ©n se ha utilizado para este efecto, aunque estÃ¡ mÃ¡s especÃ­ficamente definido como un sistema que refleja a gran escala uno, algunos, o todos estos componentes del sistema o partes. Similarmente, un microcosmos es un sistema que refleja a pequeÃ±a escala un sistema mucho mayor del que es parte.

Aunque palabras como mundo y sus equivalentes en otros lenguajes casi siempre se refieren al planeta Tierra, antiguamente se referÃ­an a cada cosa que existÃ­a (se podÃ­a ver). En ese sentido la utilizaba, por ejemplo, CopÃ©rnico. Algunos lenguajes utilizan la palabra "mundo" como parte de la palabra "espacio exterior". Un ejemplo en alemÃ¡n lo constituye la palabra "Weltraum".34â€‹

VÃ©ase tambiÃ©n
Ver el portal sobre AstronomÃ­a Portal:AstronomÃ­a. Contenido relacionado con AstronomÃ­a.
Ver el portal sobre CosmologÃ­a Portal:CosmologÃ­a. Contenido relacionado con CosmologÃ­a.
Anexo:LocalizaciÃ³n de la Tierra en el Universo
Ambiplasma
AstrofÃ­sica
Albert Einstein
AstronomÃ­a
Big Bang
CosmologÃ­a
CosmologÃ­a fÃ­sica
CosmovisiÃ³n
Destino final del universo
Edad del universo
Estructura del universo a gran escala
Ley de Hubble-LemaÃ®tre
Forma del universo
InflaciÃ³n cÃ³smica
ExpansiÃ³n mÃ©trica del espacio
MÃ©trica de Friedman-LemaÃ®tre-Robertson-Walker
Microcosmos
Modelo Lambda-CDM
Carl Sagan
Multiverso
Origen del universo
Panspermia
Principio antrÃ³pico
Principio hologrÃ¡fico
TeorÃ­a del Big Bang
TeorÃ­a del estado estacionario
TeorÃ­a de los universos fecundos
Universal (metafÃ­sica)
Universo oscilante
Universos paralelos
Referencias
 Cfr. Universal (metafÃ­sica)
 Lineweaver, Charles; Tamara M. Davis (2005). Â«Misconceptions about the Big Bang.Â» Scientific American. Consultado el 31 de marzo de 2008.
 Â«Primeras imÃ¡genes de la materia oscuraÂ». Consultado el 20 de diciembre de 2010.
 Munitz, Milton K. (1 de abril de 1951). Â«One Universe or Many?Â». Journal of the History of Ideas 12 (2): 231-255. doi:10.2307/2707516.
 Luminet, Jean-Pierre; Boudewijn F. Roukema (1999). Â«Topology of the Universe: Theory and ObservationsÂ». Proceedings de la Escuela de CosmologÃ­a de Cargese (CÃ³rcega) Agosto de 1998. Consultado el 5 de enero de 2007.
 Luminet, Jean-Pierre; J. Weeks, A. Riazuelo, R. Lehoucq, J.-P. Uzan (2003). Â«Dodecahedral space topology as an explanation for weak wide-angle temperature correlations in the cosmic microwave backgroundÂ». Nature 425: 593. Consultado el 9 de enero de 2007.
 Brookhaven National Laboratory (ed.). Â«Heavy Ion CollisionsÂ». Archivado desde el original el 8 de abril de 2007.
 Thomas Ludlam, Larry McLerran (Octubre de 2003). Physics Today, ed. Â«What Have We Learned From the Relativistic Heavy Ion Collider?Â». Archivado desde el original el 23 de noviembre de 2006. Consultado el 28 de febrero de 2007.
 Ken Tan (15 de enero de 2007). space.com, ed. Â«New 'Hobbit' Galaxies Discovered Around Milky WayÂ». Archivado desde el original el 17 de mayo de 2008. Consultado el 1 de marzo de 2007.
 The Uppsala Astronomical Observatory (ed.). Â«Dwarf Spheroidal GalaxiesÂ». Archivado desde el original el 2 de abril de 2012. Consultado el 1 de marzo de 2007.
 Brian Greene (2011). The Hidden Reality. Alfred A. Knopf.
 Neil J. Cornish, David N. Spergel, Glenn D. Starkman y Eiichiro Komatsu, Constraining the Topology of the Universe.astro-ph/0310233
 Lineweaver, Charles; Tamara M. Davis (2005). Scientific American, ed. Â«Misconceptions about the Big BangÂ» (en inglÃ©s). Consultado el 5 de marzo de 2007.
 Â«WMAP produces new resultsÂ» (en inglÃ©s).
 Baldry, Ivan K.; Glazebrook, Karl (2002), Â«The 2dF Galaxy Redshift Survey: Constraints on Cosmic Star Formation History from the Cosmic SpectrumÂ», The Astrophysical Journal (The American Astronomical Society, publicado el 20 de abril 2002) 569: 582â€“594, doi:10.1086/339477
 Associated Press (28 de agosto de 2008). Â«Universe: Beige, not TurquoiseÂ». Wired.com. Archivado desde el original el 24 de julio de 2008. Consultado el 1 de noviembre de 2009.
 N. Mandolesi; P. Calzolari; S. Cortiglioni; F. Delpino; G. Sironi (1986). Â«Large-scale homogeneity of the Universe measured by the microwave backgroundÂ». Letters to Nature 319: 751-753.
 Hinshaw, Gary (2006). NASA WMAP, ed. Â«New Three Year Results on the Oldest Light in the UniverseÂ». Consultado el 7 de marzo de 2007.
 Strobel, Nick (2001). Astronomy Notes, ed. Â«The Composition of StarsÂ». Consultado el 8 de marzo de 2007.
 Astrophysics (Astronomy Frequently Asked Questions) (ed.). Â«Have physical constants changed with time?Â». Consultado el 8 de marzo de 2007.
 Gary Hinshaw (10 de febrero de 2006). NASA WMAP, ed. Â«What is the Universe Made Of?Â». Consultado el 1 de marzo de 2007.
 La Antimateria Archivado el 22 de diciembre de 2016 en la Wayback Machine.
 Difference in direct charge-parity violation between charged and neutral B meson decays,Nature 452, 332-335 (20 de marzo de 2008)
 New Theory of the Universe Marries Two of its Biggest Mysteries (31 de enero de 2007) de Laura Mgrdichian sobre el trabajo de Tom Banks, Sean Echols y Jeff L. Jones, Baryogenesis, dark matter and the pentagon. J. High Energy Phys. JHEP11 (2006) 046 (en inglÃ©s)
 Edward L. Wright (12 de septiembre de 2004). UCLA, ed. Â«Big Bang NucleosynthesisÂ». Consultado el 2 de marzo de 2007.
 M. Harwit; M. Spaans (2003). Â«Chemical Composition of the Early UniverseÂ». The Astrophysical Journal 589 (1): 53-57.
 C. Kobulnicky; E. D. Skillman (1997). Â«Chemical Composition of the Early UniverseÂ». Bulletin of the American Astronomical Society 29: 1329.
 Gary Hinshaw (15 de diciembre de 2005). NASA WMAP, ed. Â«Tests of the Big Bang: The CMBÂ». Consultado el 2 de marzo de 2007.
 Belle DumÃ© (16 de junio de 2005). Institute of Physics Publishing, ed. Â«Background neutrinos join the limelightÂ». Consultado el 2 de marzo de 2007.
 Sus modelos son especulativos pero utilizan los mÃ©todos de la fÃ­sica de la Royal Astronomical Society 347. 2004. pp. 921â€”936. Consultado el 9 de enero de 2007.
 Parnell, Brid-Aine. Â«Higgs Boson Seems To Prove That The Universe Doesn't ExistÂ». Forbes (en inglÃ©s). Consultado el 8 de febrero de 2017.
 Â«Un estudio demuestra que el universo dejÃ³ de existir hace 14 mil millones de aÃ±osÂ». History Channel. 22 de agosto de 2014. Consultado el 8 de febrero de 2017.
 Ross Taylor, Stuart (2000) [1998]. Â«The place of the solar system in the universe: The extent of the universeÂ» [Planteamiento de la cuestiÃ³n: El lugar del sistema solar en el universo]. Destiny or chance: Our Solar System and its place in the Cosmos [Nuestro sistema solar y su lugar en el cosmos] (en inglÃ©s). Nueva York NY, Estados Unidos: Cambrigde University Press. p. 19. ISBN 0-521-48178-3. Consultado el 22 de noviembre de 2014.
 Albert Einstein (1952). Relativity: The Special and the General Theory (Fifteenth Edition), ISBN 0-517-88441-0.
Enlaces externos
 Wikimedia Commons alberga una categorÃ­a multimedia sobre Universo.
 Wikiquote alberga frases cÃ©lebres de o sobre Universo.
 Wikcionario tiene definiciones y otra informaciÃ³n sobre universo.
Proyecto Celestia Actividad Educativa "El Universo" dirigida a alumnos de Secundaria, Bachillerato o aficionados a la astronomÃ­a en general.
AlemaÃ± Berenguer, Rafael AndrÃ©s (2001) Tras los Secretos del Universo ISBN 84-95495-08-2.
VÃ­deos sobre el Universo: Biblioteca audiovisual sobre el Cosmos.
En inglÃ©s:

El Universo de Stephen Hawking - Â¿Por quÃ© el universo es asÃ­?
Richard Powell: Un Atlas del Universo - imÃ¡genes en varias escalas, con explicaciones.
Cosmos - una "revista dimensional ilustrada desde el microcosmos al macrocosmos".
Edad del universo en Space.com.
Mi AsÃ­-Llamado Universo; argumentos a favor y en contra de universos paralelos e infinitos.
Universos paralelos, por Max Tegmark.
Seti@Home - La BÃºsqueda de Inteligencia Extraterrestre.
Universo - Centro de InformaciÃ³n Espacial, por Exploreuniverse.com.
NÃºmero de galaxias en el universo.
TamaÃ±o del universo en Space.com.
IlustraciÃ³n comparando los tamaÃ±os de los planetas, el sol y otras estrellas.
CosmologÃ­a (P+F).
Control de autoridades	
Proyectos WikimediaWd Datos: Q1Commonscat Multimedia: UniverseWikiquote Citas cÃ©lebres: Universo
IdentificadoresGND: 4079154-3NDL: 00574074NKC: ph116566Microsoft Academic: 84999194Diccionarios y enciclopediasBritannica: url
CategorÃ­a: Universo
MenÃº de navegaciÃ³n
No has accedido
DiscusiÃ³n
Contribuciones
Crear una cuenta
Acceder
ArtÃ­culoDiscusiÃ³n
LeerEditarVer historialBuscar
Buscar en Wikipedia
Portada
Portal de la comunidad
Actualidad
Cambios recientes
PÃ¡ginas nuevas
PÃ¡gina aleatoria
Ayuda
Donaciones
Notificar un error
Herramientas
Lo que enlaza aquÃ­
Cambios en enlazadas
Subir archivo
PÃ¡ginas especiales
Enlace permanente
InformaciÃ³n de la pÃ¡gina
Citar esta pÃ¡gina
Elemento de Wikidata
Imprimir/exportar
Crear un libro
Descargar como PDF
VersiÃ³n para imprimir
En otros proyectos
Wikimedia Commons
Wikiquote

En otros idiomas
Ğ‘Ğ°ÑˆÒ¡Ğ¾Ñ€Ñ‚ÑĞ°
English
Suomi
NÄhuatl
Napulitano
PortuguÃªs
Sicilianu
à·ƒà·’à¶‚à·„à¶½
Svenska
159 mÃ¡s
Editar enlaces
Esta pÃ¡gina se editÃ³ por Ãºltima vez el 3 abr 2021 a las 18:42.
El texto estÃ¡ disponible bajo la Licencia Creative Commons AtribuciÃ³n Compartir Igual 3.0; pueden aplicarse clÃ¡usulas adicionales. Al usar este sitio, usted acepta nuestros tÃ©rminos de uso y nuestra polÃ­tica de privacidad.
WikipediaÂ® es una marca registrada de la FundaciÃ³n Wikimedia, Inc., una organizaciÃ³n sin Ã¡nimo de lucro.
Albert Einstein
Ir a la navegaciÃ³nIr a la bÃºsqueda
Â«EinsteinÂ» redirige aquÃ­. Para otras acepciones, vÃ©ase Einstein (desambiguaciÃ³n).
Albert Einstein
Albert Einstein Head.jpg
Albert Einstein en 1947
InformaciÃ³n personal
Nacimiento	14 de marzo de 1879
Ulm (Reino de Wurtemberg)
Fallecimiento	18 de abril de 1955 (76 aÃ±os)
Princeton (Estados Unidos)
Causa de la muerte	Aneurisma de aorta abdominal
Sepultura	National Museum of Health and Medicine
Residencia	Einsteinhaus, Alemania, MÃºnich y Princeton
Nacionalidad	Alemana (1879-1896, 1918-1933, 1933), sin nacionalidad (1896-1901), suiza (1901-1955), austrohÃºngara (1911-1912) y estadounidense (1940-1955)
Lengua materna	AlemÃ¡n
Partido polÃ­tico	Partido DemocrÃ¡tico AlemÃ¡n (hasta 1933)
Familia
Padres	Hermann Einstein
Pauline Koch
CÃ³nyuge	
Mileva MariÄ‡ (1903-1919)
Elsa Einstein (1919-1936)
Hijos	
Hans Albert Einstein
Eduard Einstein
Lieserl Einstein
EducaciÃ³n
EducaciÃ³n	doctorado en FÃ­sica y Doctor en FilosofÃ­a
Educado en	
Luitpold-Gymnasium (1888-1894)
Escuela PolitÃ©cnica Federal de ZÃºrich (B.S. en EducaciÃ³n matemÃ¡tica; 1896-1900)
Old Kantonsschule (Albert Einstein House) (Matura (educaciÃ³n); 1895-1896)
Universidad de ZÃºrich (Doc. en FÃ­sica; hasta 1905)
Supervisor doctoral	Alfred Kleiner, Heinrich Burkhardt y Heinrich Friedrich Weber
InformaciÃ³n profesional
OcupaciÃ³n	FÃ­sico teÃ³rico, filÃ³sofo de la ciencia, inventor, escritor de ciencia, pedagogo, profesor universitario (desde 1909), fÃ­sico, ensayista, filÃ³sofo, escritor, profesor, cientÃ­fico, matemÃ¡tico, patent examiner (1901-1906) y catedrÃ¡tico
Ãrea	FÃ­sica teÃ³rica
Cargos ocupados	CatedrÃ¡tico
Empleador	
Universidad Carolina de Praga
Universidad de Leiden
Berna
Institute for Advanced Study
Universidad de California en Berkeley
Instituto federal suizo de propiedad intelectual (1902-1909)
Universidad de Berna (1908-1909)
Universidad de ZÃºrich (1909-1911)
German University in Prague (1911-1912)
Escuela PolitÃ©cnica Federal de ZÃºrich (1912-1914)
Universidad Humboldt de BerlÃ­n (1914-1933)
Academia Prusiana de las Ciencias (1914-1933)
Deutsche Physikalische Gesellschaft (1916-1918)
Sociedad Kaiser Wilhelm (1917-1933)
Universidad de Leiden (1920-1946)
Universidad de Princeton (1933-1955)
Obras notables	
relatividad especial
relatividad general
Efecto fotoelÃ©ctrico
teorÃ­a de la relatividad
equivalencia entre masa y energÃ­a
constante de Planck
Ecuaciones del campo de Einstein
mecÃ¡nica cuÃ¡ntica
teorÃ­a del campo unificado
Miembro de	
Royal Society
Academia Prusiana de las Ciencias
Academia Alemana de las Ciencias Naturales Leopoldina
Academia Nacional de los Linces
Sociedad FilosÃ³fica Estadounidense
Academia de Ciencias de Baviera
Academia de Ciencias de Gotinga
Academia de Ciencias de la UniÃ³n SoviÃ©tica
Academia de Ciencias de Francia
Real Academia de las Ciencias de Suecia
Real Academia de Artes y Ciencias de los PaÃ­ses Bajos
Academia Estadounidense de las Artes y las Ciencias
Academia Nacional de Ciencias de los Estados Unidos (desde 1922)
Academia Nacional de Ciencias de los Estados Unidos (desde 1942)
Firma	Albert Einstein signature 1934.svg
Web
Sitio web	
einstein.biz
Albert Einstein (alemÃ¡n: /ËˆalbÉ›ÉÌ¯t ËˆÊ”aÉªnÊƒtaÉªn/; Ulm, Imperio alemÃ¡n; 14 de marzo de 1879 - Princeton, Estados Unidos; 18 de abril de 1955) fue un fÃ­sico alemÃ¡n de origen judÃ­o, nacionalizado despuÃ©s suizo, austriaco y estadounidense. Se le considera el cientÃ­fico mÃ¡s importante, conocido y popular del siglo XX.1â€‹2â€‹

En 1905, cuando era un joven fÃ­sico desconocido, empleado en la Oficina de Patentes de Berna, publicÃ³ su teorÃ­a de la relatividad especial. En ella incorporÃ³, en un marco teÃ³rico simple fundamentado en postulados fÃ­sicos sencillos, conceptos y fenÃ³menos estudiados antes por Henri PoincarÃ© y por Hendrik Lorentz. Como una consecuencia lÃ³gica de esta teorÃ­a, dedujo la ecuaciÃ³n de la fÃ­sica mÃ¡s conocida a nivel popular: la equivalencia masa-energÃ­a, E=mcÂ². Ese aÃ±o publicÃ³ otros trabajos que sentarÃ­an algunas de las bases de la fÃ­sica estadÃ­stica y de la mecÃ¡nica cuÃ¡ntica.

En 1915, presentÃ³ la teorÃ­a de la relatividad general, en la que reformulÃ³ por completo el concepto de la gravedad.3â€‹ Una de las consecuencias fue el surgimiento del estudio cientÃ­fico del origen y la evoluciÃ³n del Universo por la rama de la fÃ­sica denominada cosmologÃ­a. En 1919, cuando las observaciones britÃ¡nicas de un eclipse solar confirmaron sus predicciones acerca de la curvatura de la luz, fue idolatrado por la prensa.4â€‹ Einstein se convirtiÃ³ en un icono popular de la ciencia mundialmente famoso, un privilegio al alcance de muy pocos cientÃ­ficos.5â€‹

Por sus explicaciones sobre el efecto fotoelÃ©ctrico y sus numerosas contribuciones a la fÃ­sica teÃ³rica, en 1921 obtuvo el Premio Nobel de FÃ­sica y no por la TeorÃ­a de la Relatividad, pues el cientÃ­fico a quien se encomendÃ³ la tarea de evaluarla no la entendiÃ³, y temieron correr el riesgo de que luego se demostrase errÃ³nea.6â€‹7â€‹ En esa Ã©poca era aÃºn considerada un tanto controvertida.

Ante el ascenso del nazismo, Einstein abandonÃ³ Alemania hacia diciembre de 1932 con destino a Estados Unidos, donde se dedicÃ³ a la docencia en el Institute for Advanced Study. Se nacionalizÃ³ estadounidense en 1940. Durante sus Ãºltimos aÃ±os trabajÃ³ por integrar en una misma teorÃ­a la fuerza gravitatoria y la electromagnÃ©tica.

Aunque es considerado por algunos como el Â«padre de la bomba atÃ³micaÂ», abogÃ³ por el federalismo mundial, el internacionalismo, el pacifismo, el sionismo y el socialismo democrÃ¡tico, con una fuerte devociÃ³n por la libertad individual y la libertad de expresiÃ³n.8â€‹9â€‹10â€‹11â€‹ Fue proclamado Â«personaje del siglo XXÂ» y el mÃ¡s preeminente cientÃ­fico por la revista Time.12â€‹


Ãndice
1	BiografÃ­a
1.1	Infancia
1.2	Juventud
1.3	Madurez
1.4	Muerte
2	Trayectoria cientÃ­fica
2.1	Los artÃ­culos de 1905
2.1.1	Efecto fotoelÃ©ctrico
2.1.2	Movimiento browniano
2.1.3	Relatividad especial
2.1.4	Equivalencia masa-energÃ­a
2.2	Relatividad general
2.3	EstadÃ­sticas de Bose-Einstein
2.4	Debate Bohr-Einstein
2.5	La teorÃ­a de campo unificada
3	Actividad polÃ­tica
4	Ã‰tica y religiÃ³n
5	Algunas publicaciones
6	Eponimia
7	En la cultura popular
8	VÃ©ase tambiÃ©n
9	Referencias
10	BibliografÃ­a
10.1	BibliografÃ­a general
10.2	Einstein y la teorÃ­a de la relatividad
10.3	Material digital
11	Enlaces externos
11.1	Enlaces en otros idiomas
BiografÃ­a
Infancia
NaciÃ³ en la ciudad alemana de Ulm, cien kilÃ³metros al este de Stuttgart, en el seno de una familia judÃ­a. Sus padres fueron Hermann Einstein y Pauline Koch. Hermann y Pauline se habÃ­an casado en 1876, cuando Hermann tenÃ­a casi veintinueve aÃ±os y ella dieciocho.13â€‹ La familia de Pauline vivÃ­a cerca de Stuttgart, concretamente en la ciudad de Cannstatt; allÃ­ su padre, Julius Koch, explotaba con su hermano Heinrich un comercio muy prÃ³spero de cereales. Pauline tocaba el piano y le transmitiÃ³ a su hijo su amor por la mÃºsica, entre otras cualidades como su "perseverancia y paciencia".14â€‹ De su padre, Hermann, tambiÃ©n heredÃ³ ciertos caracteres como la generosidad y la amabilidad que caracterizaron a Albert.13â€‹

A young boy with short hair and a round face, wearing a white collar and large bow, with vest, coat, skirt and high boots. He is leaning against an ornate chair.
Einstein a los 3 aÃ±os, en 1882.
En 1880 la familia se mudÃ³ a MÃºnich, donde se criarÃ­a durante catorce aÃ±os, y su padre y el hermano de este, Jakob, quien influyÃ³ intelectualmente sobre Albert, fundaron en octubre una empresa dedicada a la instalaciÃ³n de agua y gas. Como el negocio marchaba bien, con el apoyo de toda la familia decidieron abrir un taller propio de aparatos elÃ©ctricos (Elektrotechnische Fabrik J. Einstein & Cie.), que suministraban a centrales elÃ©ctricas en MÃºnich-Schwabing, Varese y Susa en Italia, la que fracasarÃ­a tras endeudar a toda la familia. Esto causÃ³ un trauma no solo a Albert sino tambiÃ©n al resto de la familia. A fin de saldar las deudas y financiar el traslado, el querido jardÃ­n de la casa de MÃºnich fue vendido a un promotor inmobiliario.13â€‹

Desde sus comienzos, demostrÃ³ cierta dificultad para expresarse, pues no empezÃ³ a hablar hasta la edad de tres aÃ±os, por lo que aparentaba poseer algÃºn retardo que le provocarÃ­a algunos problemas. Al contrario que su hermana menor, Maya, que era mÃ¡s vivaracha y alegre, Albert era paciente y metÃ³dico y no le gustaba exhibirse. SolÃ­a evitar la compaÃ±Ã­a de otros infantes de su edad y a pesar de que, como niÃ±os, tambiÃ©n tenÃ­an de vez en cuando sus diferencias, Ãºnicamente admitÃ­a a su hermana en sus soledades. CursÃ³ sus estudios primarios en una escuela catÃ³lica; desde 1888 asistiÃ³ al instituto de segunda enseÃ±anza Luitpold (que en 1965 recibirÃ­a el nombre de Gymasium Albert Einstein). SacÃ³ buenas notas en general, no tanto en las asignaturas de idiomas, pero excelentes en las de ciencias naturales. Los libros de divulgaciÃ³n cientÃ­fica de Aaron Bernstein marcaron su interÃ©s y su futura carrera. Fue un perÃ­odo difÃ­cil que sobrellevarÃ­a gracias a las clases de violÃ­n (a partir de 1884) que le darÃ­a su madre (instrumento que le apasionaba y que continuÃ³ tocando el resto de sus dÃ­as)15â€‹ y a la introducciÃ³n al Ã¡lgebra que le descubrirÃ­a su tÃ­o Jacob.16â€‹ Su paso por el Gymnasium (instituto de bachillerato), sin embargo, no fue muy gratificante: la rigidez y la disciplina militar de los institutos de secundaria de la Ã©poca de Otto von Bismarck le granjearon no pocas polÃ©micas con los profesores: en el Luitpold Gymnasium las cosas llegaron a un punto crÃ­tico en 1894, cuando Einstein tenÃ­a quince aÃ±os. Un nuevo profesor, el Dr. Joseph Degenhart, le dijo que Â«nunca conseguirÃ­a nada en la vidaÂ». Cuando Einstein le respondiÃ³ que Â«no habÃ­a cometido ningÃºn delitoÂ», el profesor le respondiÃ³: Â«tu sola presencia aquÃ­ mina el respeto que me debe la claseÂ».17â€‹


Einstein en 1893, a los 14 aÃ±os.
Su tÃ­o, Jacob Einstein, un ingeniero con gran inventiva e ideas, convenciÃ³ al padre de Albert para que construyese una casa con un taller, en donde llevarÃ­an a cabo nuevos proyectos y experimentos tecnolÃ³gicos de la Ã©poca a modo de obtener unos beneficios, pero, debido a que los aparatos y artilugios que afinaban y fabricaban eran productos para el futuro, en el presente carecÃ­an de compradores y el negociÃ³ fracasÃ³. El pequeÃ±o Albert, se crio motivado por las investigaciones que se realizaban en el taller y todos los aparatos que allÃ­ habÃ­a. AdemÃ¡s, su tÃ­o incentivÃ³ sus inquietudes cientÃ­ficas proporcionÃ¡ndole libros de ciencia. SegÃºn relata el propio Einstein en su autobiografÃ­a, de la lectura de estos libros de divulgaciÃ³n cientÃ­fica nacerÃ­a un constante cuestionamiento de las afirmaciones de la religiÃ³n; un librepensamiento decidido que fue asociado a otras formas de rechazo hacia el Estado y la autoridad. Un escepticismo poco comÃºn en aquella Ã©poca, a decir del propio Einstein. El colegio no lo motivaba, y aunque era excelente en matemÃ¡ticas y fÃ­sica, no se interesaba por las demÃ¡s asignaturas. A los quince aÃ±os, sin tutor ni guÃ­a, emprendiÃ³ el estudio del cÃ¡lculo infinitesimal. La idea, claramente infundada, de que era un mal estudiante proviene de los primeros biÃ³grafos que escribieron sobre Einstein, que confundieron el sistema de calificaciÃ³n escolar de Suiza (un 6 en Suiza es la mejor calificaciÃ³n) con el alemÃ¡n (un 6 es la peor nota).18â€‹En este Â«ErziehungsratÂ» aparece con nota 6 en todas las asignaturas: Ãlgebra, FÃ­sica, GeometrÃ­a, GeometrÃ­a AnalÃ­tica y TrigonometrÃ­a.

En 1894, la compaÃ±Ã­a Hermann sufrÃ­a importantes dificultades econÃ³micas y los Einstein se mudaron de MÃºnich a PavÃ­a, en Italia, cerca de MilÃ¡n. Albert permaneciÃ³ en MÃºnich para terminar sus cursos antes de reunirse con su familia en PavÃ­a, pero la separaciÃ³n durÃ³ poco tiempo: antes de obtener su tÃ­tulo de bachiller decidiÃ³ abandonar el Gymnasium. Sin consultarlo con sus padres, Albert se puso en contacto con un mÃ©dico (el hermano mayor de Max Talmud, un estudiante de medicina que iba todos los viernes a comer a la casa de los padres de Einstein) para que certificara que padecÃ­a de agotamiento y necesitaba un tiempo sin asistir a la escuela, y convenciÃ³ a un profesor para que certificara su excelencia en el campo de las matemÃ¡ticas. Las autoridades de la escuela le dejaron ir. Justo despuÃ©s de las Navidades de 1894, Albert abandonÃ³ MÃºnich y se fue a MilÃ¡n para reunirse con sus padres.17â€‹

Juventud
AsÃ­, la familia Einstein intentÃ³ matricular a Albert en la Escuela PolitÃ©cnica Federal de ZÃºrich pero, al no tener el tÃ­tulo de bachiller, tuvo que presentarse a una prueba de acceso que suspendiÃ³ a causa de una calificaciÃ³n deficiente en una asignatura de letras. Esto supuso que fuera rechazado inicialmente, pero el director del centro, impresionado por sus resultados en ciencias, le aconsejÃ³ que continuara sus estudios de bachiller y que obtuviera el tÃ­tulo que le darÃ­a acceso directo al PolitÃ©cnico. Su familia lo enviÃ³ a Aarau para terminar sus estudios secundarios en la escuela cantonal de Argovia, a unos 50 km al oeste de ZÃºrich, donde Einstein obtuvo el tÃ­tulo de bachiller alemÃ¡n en 1896, a la edad de diecisÃ©is aÃ±os. Ese mismo aÃ±o renunciÃ³ a su ciudadanÃ­a alemana, presuntamente para evitar el servicio militar, pasando a ser un apÃ¡trida. IniciÃ³ los trÃ¡mites para naturalizarse suizo. A fines de 1896, a la edad de diecisiete aÃ±os, Einstein ingresÃ³ en la Escuela PolitÃ©cnica Federal de ZÃºrich (Suiza), probablemente el centro mÃ¡s importante de la Europa central para estudiar ciencias fuera de Alemania, matriculÃ¡ndose en la Escuela de orientaciÃ³n matemÃ¡tica y cientÃ­fica, con la idea de estudiar fÃ­sica.17â€‹

Three young men in suits with high white collars and bow ties, sitting.
Conrad Habicht, Maurice Solovine y Einstein, los fundadores de la efÃ­mera Academia Olimpia.
Durante sus aÃ±os en la polÃ­ticamente vibrante ZÃºrich, descubriÃ³ la obra de diversos filÃ³sofos: Henri PoincarÃ©, Baruch Spinoza, David Hume, Immanuel Kant, Karl Marx[cita requerida] y Ernst Mach. TambiÃ©n tomÃ³ contacto con el movimiento socialista a travÃ©s de Friedrich Adler y con cierto pensamiento inconformista y revolucionario en el que mucho tuvo que ver su amigo de toda la vida Michele Besso. En octubre de 1896, conociÃ³ a Mileva MariÄ‡, una compaÃ±era de clase serbia, de talante feminista y radical, de la que se enamorÃ³. En 1900, Albert y Mileva se graduaron en el PolitÃ©cnico de ZÃ¼rich y en 1901, a la edad de veintidÃ³s aÃ±os, consiguiÃ³ la ciudadanÃ­a suiza. Durante este perÃ­odo discutÃ­a sus ideas cientÃ­ficas con un grupo de amigos cercanos, incluyendo a Mileva, con la cual tuvo en secreto una hija en enero de 1902, llamada Lieserl. Al dÃ­a de hoy nadie sabe quÃ© fue de la niÃ±a, asumiÃ©ndose que fue adoptada en la Serbia natal de Mileva, despuÃ©s de que ambos contrajeran matrimonio, el 6 de enero de 1903, en la ciudad de Berna. No obstante, esta teorÃ­a difÃ­cilmente puede demostrarse, ya que solo se dispone de pruebas circunstanciales. Los padres de Einstein siempre se opusieron al matrimonio, hasta que en 1902 su padre cayÃ³ enfermo de muerte y consintiÃ³. Mas su madre nunca se resignÃ³ al mismo.19â€‹20â€‹


Casa de Albert Einstein en Suiza.
Se graduÃ³ en 1900, obteniendo el diploma de profesor de matemÃ¡tica y de fÃ­sica, pero no pudo encontrar trabajo en la Universidad, por lo que ejerciÃ³ como tutor en Winterthur, Schaffhausen y Berna. Su compaÃ±ero de clase Marcel Grossmann, un hombre que mÃ¡s adelante desempeÃ±arÃ­a un papel fundamental en las matemÃ¡ticas de la relatividad general, le ofreciÃ³ un empleo fijo en la Oficina Federal de la Propiedad Intelectual de Suiza, en Berna, una oficina de patentes, donde trabajÃ³ de 1902 a 1909.21â€‹ Su personalidad le causÃ³ tambiÃ©n problemas con el director de la Oficina, quien le enseÃ±Ã³ a Â«expresarse correctamenteÂ».

En esta Ã©poca, Einstein se referÃ­a con amor a su mujer Mileva como Â«una persona que es mi igual y tan fuerte e independiente como yoÂ». Abram Joffe, en su biografÃ­a de Einstein, argumenta que durante este periodo fue ayudado en sus investigaciones por Mileva. Esto se contradice con otros biÃ³grafos como Ronald W. Clark, quien afirma que Einstein y Mileva llevaban una relaciÃ³n distante que le brindaba la soledad necesaria para concentrarse en su trabajo.22â€‹

En mayo de 1904, Einstein y Mileva tuvieron un hijo, al que llamaron Hans Albert Einstein. Ese mismo aÃ±o consiguiÃ³ un trabajo permanente en la Oficina de Patentes. Poco despuÃ©s finalizÃ³ su doctorado presentando una tesis titulada Una nueva determinaciÃ³n de las dimensiones moleculares, consistente en un trabajo de 17 folios que surgiÃ³ de una conversaciÃ³n mantenida con Michele Besso, mientras se tomaban una taza de tÃ©; al azucarar Einstein el suyo, le preguntÃ³ a Besso:
Â¿Crees que el cÃ¡lculo de las dimensiones de las molÃ©culas de azÃºcar podrÃ­a ser una buena tesis de doctorado?

Albert Einstein en 1904 (edad: 25)
En 1905, redactÃ³ varios trabajos fundamentales sobre la fÃ­sica de pequeÃ±a y gran escala. En el primero de ellos explicaba el movimiento browniano, en el segundo el efecto fotoelÃ©ctrico y los dos restantes desarrollaban la relatividad especial y la equivalencia masa-energÃ­a. El primero de ellos le valiÃ³ el grado de doctor por la Universidad de ZÃºrich en 1906, y su trabajo sobre el efecto fotoelÃ©ctrico le harÃ­a merecedor del Premio Nobel de FÃ­sica en 1921, por sus trabajos sobre el movimiento browniano y su interpretaciÃ³n sobre el efecto fotoelÃ©ctrico. Estos artÃ­culos fueron enviados a la revista Annalen der Physik y son conocidos generalmente como los artÃ­culos del annus mirabilis (Â«aÃ±o milagrosoÂ»).23â€‹

Madurez

Albert Einstein en 1920.
En 1908, a la edad de veintinueve aÃ±os, fue contratado en la Universidad de Berna, Suiza, como profesor y conferenciante (privatdozent). Einstein y Mileva tuvieron un nuevo hijo, Eduard, nacido el 28 de julio de 1910. Poco despuÃ©s la familia se mudÃ³ a Praga, donde Einstein obtuvo la plaza de professor de fÃ­sica teÃ³rica, el equivalente a catedrÃ¡tico, en la Universidad Alemana de Praga, debiendo adoptar la nacionalidad austrÃ­aca para poder acceder al cargo.24â€‹ En esta Ã©poca trabajÃ³ estrechamente con Marcel Grossmann y Otto Stern. TambiÃ©n comenzÃ³ a llamar al tiempo matemÃ¡tico Â«cuarta dimensiÃ³nÂ».25â€‹En 1913, justo antes de la Primera Guerra Mundial, fue elegido miembro de la Academia Prusiana de Ciencias. EstableciÃ³ su residencia en BerlÃ­n, donde permaneciÃ³ durante diecisiete aÃ±os. El emperador Guillermo le invitÃ³ a dirigir la secciÃ³n de FÃ­sica del Instituto Kaiser Wilhelm de FÃ­sica.26â€‹

El 14 de febrero de 1919, a la edad de treinta y nueve aÃ±os, se divorciÃ³ de Mileva, despuÃ©s de un matrimonio de diecisÃ©is aÃ±os, y algunos meses despuÃ©s, el 2 de junio de 1919, se casÃ³ con una prima suya, Elsa Loewenthal, cuyo apellido de soltera era Einstein; Loewenthal era el apellido de su primer marido, Max Loewenthal. Elsa era tres aÃ±os mayor que Ã©l y le habÃ­a estado cuidando tras sufrir un fuerte estado de agotamiento. Einstein y Elsa no tuvieron hijos.

El destino de la hija de Albert y Mileva, Lieserl, nacida antes de que sus padres se casaran o encontraran trabajo, es desconocido. De sus dos hijos, el primero, Hans Albert, se mudÃ³ a California, donde llegÃ³ a ser profesor universitario, aunque con poca interacciÃ³n con su padre; el segundo, Eduard, sufrÃ­a esquizofrenia y fue internado en 1932 en una instituciÃ³n para tratamiento de enfermedades mentales en ZÃºrich. Fue el primero de muchos ingresos. Einstein querÃ­a llevar a su hijo enfermo a Princeton, pero la embajada de EE. UU. no lo admitiÃ³ por sus malos antecedentes. Eduard falleciÃ³ en el centro psiquiÃ¡trico en 1965.27â€‹

En BerlÃ­n en los aÃ±os 1920, la fama de Einstein despertaba acaloradas discusiones. En los diarios conservadores se podÃ­an leer editoriales que atacaban su teorÃ­a. Se convocaban conferencias-espectÃ¡culo tratando de argumentar lo disparatada que resultaba la teorÃ­a especial de la relatividad. Incluso se le atacaba, en forma velada, no abiertamente, en su condiciÃ³n de judÃ­o. En el resto del mundo, la teorÃ­a de la relatividad era apasionadamente debatida en conferencias populares y textos.28â€‹

En Alemania, las expresiones de odio a los judÃ­os alcanzaron niveles muy elevados. Varios fÃ­sicos de ideologÃ­a nazi, algunos tan notables como los premios Nobel de FÃ­sica Johannes Stark y Philipp Lenard, intentaron desacreditar sus teorÃ­as.29â€‹ Otros fÃ­sicos que enseÃ±aban la teorÃ­a de la relatividad, como Werner Heisenberg, fueron vetados en sus intentos de acceder a puestos docentes.30â€‹

En 1923 visitÃ³ EspaÃ±a, entablando relaciÃ³n con JosÃ© Ortega y Gasset. Al desembarcar en Barcelona, y dadas las ideas socialistas que profesaba,31â€‹ aceptÃ³ una invitaciÃ³n para dar una conferencia en la sede de la CNT, donde entablÃ³ amistad con Ãngel PestaÃ±a. PreguntÃ³ quÃ© significaban las siglas CNT (ConfederaciÃ³n Nacional del Trabajo), y cuando lo comprendiÃ³, y dadas las ideas anarquistas del sindicato, propuso eliminar la palabra "Nacional", que en Alemania tenÃ­a connotaciones violentas.32â€‹ En su visita tambiÃ©n conociÃ³ brevemente a Santiago RamÃ³n y Cajal y adicionalmente recibiÃ³ un homenaje del rey Alfonso XIII de EspaÃ±a, quien lo nombra miembro de la Real Academia de Ciencias.33â€‹

Antes del ascenso del nazismo â€”Adolf Hitler llegÃ³ al poder como canciller el 30 de enero de 1933â€”, habÃ­a dejado Alemania en diciembre de 1932 para zarpar inciertamente hacia Estados Unidos, paÃ­s donde enseÃ±Ã³ en el Institute for Advanced Study, agregando a su nacionalidad suiza la estadounidense en 1940, a la edad de sesenta y un aÃ±os.34â€‹

Para la camarilla nazi los judÃ­os no son solo un medio que desvÃ­a el resentimiento que el pueblo experimenta contra sus opresores; ven tambiÃ©n en los judÃ­os un elemento inadaptable que no puede ser llevado a aceptar un dogma sin crÃ­tica, y que en consecuencia amenaza su autoridad â€“por el tiempo que tal dogma existaâ€“ con motivo de su empeÃ±o en esclarecer a las masas.
La prueba de que este problema toca el fondo de la cuestiÃ³n la proporciona la solemne ceremonia de la quema de libros, ofrecida como espectÃ¡culo por el rÃ©gimen nazi poco tiempo despuÃ©s de adueÃ±arse del poder.
Einstein. Nueva York. 1938.35â€‹
Antes de decidirse por el exilio estadounidense, en 1933 el gobierno de la Segunda RepÃºblica espaÃ±ola ofreciÃ³ a Einstein incorporarse como investigador a la Universidad Central de Madrid. MediÃ³ en estas gestiones el entonces embajador en el Reino Unido, RamÃ³n PÃ©rez de Ayala, a iniciativa del ministro Fernando de los RÃ­os. Finalmente, ante la situaciÃ³n de inestabilidad polÃ­tica en Europa y el ascenso al poder de la CEDA en EspaÃ±a, Einstein declinÃ³ la oferta. Ante la posibilidad de que el cientÃ­fico alemÃ¡n aceptara el puesto, sectores de la derecha espaÃ±ola mostraron su malestar y hubo algunas reacciones antisemitas. El diario catÃ³lico El Debate (vinculado a la CEDA) publicÃ³ un editorial el 12 de abril (titulado Todo es relativo) donde se referÃ­a a Einstein como "el judÃ­o"; en otro artÃ­culo del mismo periÃ³dico se negaba que fuese una vÃ­ctima de la persecuciÃ³n hitleriana y que su destierro fuera forzado: Â«El ministro socialista se ha apresurado a ofrecerle protecciÃ³n. JudaÃ­smo y marxismo se identifican y confundenÂ», se aÃ±adÃ­a.36â€‹37â€‹

Einstein, en 1939 decide ejercer su influencia participando en cuestiones polÃ­ticas que afectan al mundo. Redacta la cÃ©lebre carta a Roosevelt, para promover el proyecto atÃ³mico e impedir que los Â«enemigos de la humanidadÂ» lo hicieran antes:
â€¦puesto que dada la mentalidad de los nazis, habrÃ­an consumado la destrucciÃ³n y la esclavitud del resto del mundo.38â€‹
Durante sus Ãºltimos aÃ±os, Einstein trabajÃ³ por integrar en una misma teorÃ­a las cuatro interacciones fundamentales, tarea aÃºn inconclusa.39â€‹

Muerte
El 16 de abril de 1955, Albert Einstein experimentÃ³ una hemorragia interna causada por la ruptura de un aneurisma de la aorta abdominal, que anteriormente habÃ­a sido reforzada quirÃºrgicamente por el doctor Rudolph Nissen en 1948. Einstein rechazÃ³ la cirugÃ­a, diciendo: Â«Quiero irme cuando quiero. Es de mal gusto prolongar artificialmente la vida. He hecho mi parte, es hora de irse. Yo lo harÃ© con eleganciaÂ». MuriÃ³ en el Hospital de Princeton a primera hora del 18 de abril de 1955 a la edad de setenta y seis aÃ±os.40â€‹ En la mesilla quedaba el borrador del discurso por el sÃ©ptimo aniversario de la independencia de Israel, que jamÃ¡s llegarÃ­a a pronunciar, y que empezaba asÃ­: Â«Hoy les hablo no como ciudadano estadounidense, ni tampoco como judÃ­o, sino como ser humanoÂ».

Einstein no quiso tener un funeral rutilante, con la asistencia de dignatarios de todo el mundo. De acuerdo con su deseo, su cuerpo fue incinerado en la misma tarde, antes de que la mayor parte del mundo se enterara de la noticia. En el crematorio solo hubo doce personas, entre las cuales estuvo su hijo mayor. Sus cenizas fueron esparcidas en el rÃ­o Delaware a fin de que el lugar de sus restos no se convirtiera en objeto de mÃ³rbida veneraciÃ³n. Pero hubo una parte de su cuerpo que no se quemÃ³.

Durante la autopsia, el patÃ³logo del hospital Thomas Stoltz Harvey41â€‹ extrajo el cerebro de Einstein para conservarlo, sin el permiso de su familia, con la esperanza de que la neurociencia del futuro fuera capaz de descubrir lo que hizo a Einstein ser tan inteligente. Lo conservÃ³ durante varias dÃ©cadas, hasta que finalmente lo devolviÃ³ a los laboratorios de Princeton cuando tenÃ­a mÃ¡s de ochenta aÃ±os. Pensaba que el cerebro de Einstein Â«le revelarÃ­a los secretos de su genialidad y que asÃ­ se harÃ­a famosoÂ». Hasta ahora, el Ãºnico dato cientÃ­fico medianamente interesante obtenido del estudio del cerebro es que una parte de Ã©l â€”la parte que, entre otras cosas, estÃ¡ relacionada con la capacidad matemÃ¡ticaâ€” es mÃ¡s grande que en otros cerebros.

Son recientes y escasos los estudios detallados del cerebro de Einstein. En 1985, por ejemplo, la profesora Marian Diamond, de la Universidad de California en Berkeley, informÃ³ de un nÃºmero de cÃ©lulas gliales (que nutren a las neuronas) de superior calidad en Ã¡reas del hemisferio izquierdo, encargado del control de las habilidades matemÃ¡ticas. En 1999, la neurocientÃ­fica Sandra Witelson informaba que el lÃ³bulo parietal inferior de Einstein, un Ã¡rea relacionada con el razonamiento matemÃ¡tico, era un 15% mÃ¡s ancho de lo normal. AdemÃ¡s, encontrÃ³ que su cisura de Silvio, un surco que normalmente se extiende desde la parte delantera del cerebro hasta la parte posterior, no recorrÃ­a todo el camino.

Trayectoria cientÃ­fica
En 1901 apareciÃ³ el primer trabajo cientÃ­fico de Einstein: trataba de la atracciÃ³n capilar. PublicÃ³ dos trabajos en 1902 y 1903, sobre los fundamentos estadÃ­sticos de la termodinÃ¡mica, corroborando experimentalmente que la temperatura de un cuerpo se debe a la agitaciÃ³n de sus molÃ©culas, una teorÃ­a aÃºn discutida en esa Ã©poca.42â€‹

Los artÃ­culos de 1905
En 1905 finalizÃ³ su doctorado presentando una tesis titulada Una nueva determinaciÃ³n de las dimensiones moleculares. Ese mismo aÃ±o escribiÃ³ cuatro artÃ­culos fundamentales sobre la fÃ­sica de pequeÃ±a y gran escala. En ellos explicaba el movimiento browniano, el efecto fotoelÃ©ctrico y desarrollaba la relatividad especial y la equivalencia masa-energÃ­a. El trabajo de Einstein sobre el efecto fotoelÃ©ctrico le proporcionarÃ­a el Premio Nobel de fÃ­sica en 1921. Estos artÃ­culos fueron enviados a la revista Annalen der Physik y son conocidos generalmente como los artÃ­culos del annus mirabilis (del latÃ­n: Â«aÃ±o milagrosoÂ»). La UniÃ³n Internacional de FÃ­sica Pura y Aplicada, junto con la Unesco, conmemoraron 2005 como el AÃ±o Mundial de la FÃ­sica43â€‹ celebrando el centenario de publicaciÃ³n de estos trabajos.

Efecto fotoelÃ©ctrico
ArtÃ­culo principal: Efecto fotoelÃ©ctrico

Un diagrama ilustrando la emisiÃ³n de los electrones de una placa metÃ¡lica, requiriendo de la energÃ­a que es absorbida de un fotÃ³n
El primero de sus artÃ­culos de 1905 se titulaba Un punto de vista heurÃ­stico sobre la producciÃ³n y transformaciÃ³n de luz. En Ã©l, Einstein proponÃ­a la idea de Â«quantoÂ» de luz (ahora llamados fotones) y mostraba cÃ³mo se podÃ­a utilizar este concepto para explicar el efecto fotoelÃ©ctrico.

La teorÃ­a de los cuantos de luz fue un fuerte indicio de la dualidad onda-corpÃºsculo y de que los sistemas fÃ­sicos pueden mostrar tanto propiedades ondulatorias como corpusculares. Este artÃ­culo constituyÃ³ uno de los pilares bÃ¡sicos de la mecÃ¡nica cuÃ¡ntica. Una explicaciÃ³n completa del efecto fotoelÃ©ctrico solamente pudo ser elaborada cuando la teorÃ­a cuÃ¡ntica estuvo mÃ¡s avanzada. Por este trabajo, y por sus contribuciones a la fÃ­sica teÃ³rica, Einstein recibiÃ³ el Premio Nobel de FÃ­sica de 1921.

Movimiento browniano
ArtÃ­culo principal: Movimiento browniano
Su segundo artÃ­culo, titulado Sobre el movimiento requerido por la teorÃ­a cinÃ©tica molecular del calor de pequeÃ±as partÃ­culas suspendidas en un lÃ­quido estacionario, cubrÃ­a sus estudios sobre el movimiento browniano.

El artÃ­culo sobre el movimiento browniano, el cuarto en grado de importancia, estÃ¡ estrechamente relacionado, con el artÃ­culo sobre teorÃ­a molecular. Se trata de una pieza de mecÃ¡nica estadÃ­stica muy elaborada, destacable por el hecho que Einstein no habÃ­a oÃ­do hablar de las mediciones de Robert Brown de la dÃ©cada de 1820 hasta finales de ese mismo aÃ±o (1905); asÃ­ pues, escribiÃ³ este artÃ­culo, titulÃ¡ndolo Sobre la teorÃ­a del movimiento browniano.44â€‹

El artÃ­culo explicaba el fenÃ³meno haciendo uso de las estadÃ­sticas del movimiento tÃ©rmico de los Ã¡tomos individuales que forman un fluido. El movimiento browniano habÃ­a desconcertado a la comunidad cientÃ­fica desde su descubrimiento unas dÃ©cadas atrÃ¡s. La explicaciÃ³n de Einstein proporcionaba una evidencia experimental incontestable sobre la existencia real de los Ã¡tomos. El artÃ­culo tambiÃ©n aportaba un fuerte impulso a la mecÃ¡nica estadÃ­stica y a la teorÃ­a cinÃ©tica de los fluidos, dos campos que en aquella Ã©poca permanecÃ­an controvertidos.

Antes de este trabajo los Ã¡tomos se consideraban un concepto Ãºtil en fÃ­sica y quÃ­mica, pero al contrario de lo que cuenta la leyenda, la mayorÃ­a de los fÃ­sicos contemporÃ¡neos ya creÃ­an en la teorÃ­a atÃ³mica y en la mecÃ¡nica estadÃ­stica desarrollada por Boltzmann, Maxwell y Gibbs; ademÃ¡s ya se habÃ­an hecho estimaciones bastante buenas de los radios del nÃºcleo y del nÃºmero de Avogadro. El artÃ­culo de Einstein sobre el movimiento atÃ³mico entregaba a los experimentalistas un mÃ©todo sencillo para contar Ã¡tomos mirando a travÃ©s de un microscopio ordinario.44â€‹

Wilhelm Ostwald, uno de los lÃ­deres de la escuela antiatÃ³mica, comunicÃ³ a Arnold Sommerfeld que habÃ­a sido transformado en un creyente en los Ã¡tomos por la explicaciÃ³n de Einstein del movimiento browniano.

Relatividad especial
ArtÃ­culo principal: TeorÃ­a de la relatividad especial

Una de las fotografÃ­as tomadas del eclipse de 1919 durante la expediciÃ³n de Arthur Eddington, en el que se pudieron confirmar las predicciones de Einstein acerca de la curvatura de la luz en presencia de un campo gravitatorio
El tercer artÃ­culo de Einstein de ese aÃ±o se titulaba Zur Elektrodynamik bewegter KÃ¶rper (Â«Sobre la electrodinÃ¡mica de cuerpos en movimientoÂ»). En este artÃ­culo Einstein introducÃ­a la teorÃ­a de la relatividad especial estudiando el movimiento de los cuerpos y el electromagnetismo en ausencia de la fuerza de interacciÃ³n gravitatoria.45â€‹

La relatividad especial resolvÃ­a los problemas abiertos por el experimento de Michelson y Morley en el que se habÃ­a demostrado que las ondas electromagnÃ©ticas que forman la luz se movÃ­an en ausencia de un medio. La velocidad de la luz es, por lo tanto, constante y no relativa al movimiento. Ya en 1894, George Fitzgerald habÃ­a estudiado esta cuestiÃ³n demostrando que el experimento de Michelson y Morley podÃ­a ser explicado si los cuerpos se contraen en la direcciÃ³n de su movimiento. De hecho, algunas de las ecuaciones fundamentales del artÃ­culo de Einstein habÃ­an sido introducidas anteriormente (1903) por Hendrik Lorentz,46â€‹ fÃ­sico neerlandÃ©s, dando forma matemÃ¡tica a la conjetura de Fitzgerald.47â€‹

Esta famosa publicaciÃ³n estÃ¡ cuestionada como trabajo original de Einstein, debido a que en ella omitiÃ³ citar toda referencia a las ideas o conceptos desarrollados por estos autores asÃ­ como los trabajos de PoincarÃ©. En realidad Einstein desarrollaba su teorÃ­a de una manera totalmente diferente a estos autores deduciendo hechos experimentales a partir de principios fundamentales y no dando una explicaciÃ³n fenomenolÃ³gica a observaciones desconcertantes. El mÃ©rito de Einstein estaba por lo tanto en explicar lo sucedido en el experimento de Michelson y Morley como consecuencia final de una teorÃ­a completa y elegante basada en principios fundamentales y no como una explicaciÃ³n ad-hoc o fenomenolÃ³gica de un fenÃ³meno observado.45â€‹

Su razonamiento se basÃ³ en dos axiomas simples: En el primero reformulÃ³ el principio de simultaneidad, introducido por Galileo Galilei siglos antes, por el que las leyes de la fÃ­sica deben ser invariantes para todos los observadores que se mueven a velocidades constantes entre ellos, y el segundo, que la velocidad de la luz es constante para cualquier observador. Este segundo axioma, revolucionario, va mÃ¡s allÃ¡ de las consecuencias previstas por Lorentz o PoincarÃ© que simplemente relataban un mecanismo para explicar el acortamiento de uno de los brazos del experimento de Michelson y Morley. Este postulado implica que si un destello de luz se lanza al cruzarse dos observadores en movimiento relativo, ambos verÃ¡n alejarse la luz produciendo un cÃ­rculo perfecto con cada uno de ellos en el centro. Si a ambos lados de los observadores se pusiera un detector, ninguno de los observadores se pondrÃ­a de acuerdo en quÃ© detector se activÃ³ primero (se pierden los conceptos de tiempo absoluto y simultaneidad).48â€‹ La teorÃ­a recibiÃ³ el nombre de Â«teorÃ­a especial de la relatividadÂ» o Â«teorÃ­a restringida de la relatividadÂ» para distinguirla de la teorÃ­a de la relatividad general, que fue introducida por Einstein en 1915 y en la que se consideran los efectos de la gravedad y la aceleraciÃ³n.49â€‹

Equivalencia masa-energÃ­a
ArtÃ­culo principal: Equivalencia entre masa y energÃ­a

La famosa fÃ³rmula E = mc2 es mostrada usando la iluminaciÃ³n en el rascacielos Taipei 101 durante el evento del AÃ±o Mundial de la FÃ­sica en 2005.
El cuarto artÃ­culo de aquel aÃ±o se titulaba Ist die TrÃ¤gheit eines KÃ¶rpers von seinem Energieinhalt abhÃ¤ngig y mostraba una deducciÃ³n de la fÃ³rmula de la relatividad que relaciona masa y energÃ­a. En este artÃ­culo se exponÃ­a que la variaciÃ³n de masa de un objeto que emite una energÃ­a L, es:

{\displaystyle {\frac {L}{V^{2}}}}{\frac  {L}{V^{2}}}
donde V era la notaciÃ³n de la velocidad de la luz usada por Einstein en 1905.

Esta fÃ³rmula implica que la energÃ­a E de un cuerpo en reposo es igual a su masa m multiplicada por la velocidad de la luz al cuadrado:

{\displaystyle E=mc^{2}\,}E=mc^{2}\,
Muestra cÃ³mo una partÃ­cula con masa posee un tipo de energÃ­a, Â«energÃ­a en reposoÂ», distinta de las clÃ¡sicas energÃ­a cinÃ©tica y energÃ­a potencial. La relaciÃ³n masa-energÃ­a se utiliza comÃºnmente para explicar cÃ³mo se produce la energÃ­a nuclear; midiendo la masa de nÃºcleos atÃ³micos y dividiendo por el nÃºmero atÃ³mico se puede calcular la energÃ­a de enlace atrapada en los nÃºcleos atÃ³micos. Paralelamente, la cantidad de energÃ­a producida en la fisiÃ³n de un nÃºcleo atÃ³mico se calcula como la diferencia de masa entre el nÃºcleo inicial y los productos de su desintegraciÃ³n, multiplicada por la velocidad de la luz al cuadrado.

Relatividad general
ArtÃ­culo principal: TeorÃ­a general de la relatividad
En noviembre de 1915, Einstein presentÃ³ una serie de conferencias en la Academia Prusiana de las Ciencias en las que describiÃ³ la teorÃ­a de la relatividad general. La Ãºltima de estas charlas concluyÃ³ con la presentaciÃ³n de la ecuaciÃ³n que reemplaza a la ley de gravedad de Isaac Newton. En esta teorÃ­a todos los observadores son considerados equivalentes y no Ãºnicamente aquellos que se mueven con una velocidad uniforme. La gravedad no es ya una fuerza o acciÃ³n a distancia, como era en la gravedad newtoniana, sino una consecuencia de la curvatura del espacio-tiempo. La teorÃ­a proporcionaba las bases para el estudio de la cosmologÃ­a y permitÃ­a comprender las caracterÃ­sticas esenciales del Universo, muchas de las cuales no serÃ­an descubiertas sino con posterioridad a la muerte de Einstein.50â€‹

La relatividad general fue obtenida por Einstein a partir de razonamientos matemÃ¡ticos, experimentos hipotÃ©ticos (gedanken experiment) y rigurosa deducciÃ³n matemÃ¡tica sin contar realmente con una base experimental. El principio fundamental de la teorÃ­a era el denominado principio de equivalencia. A pesar de la abstracciÃ³n matemÃ¡tica de la teorÃ­a, las ecuaciones permitÃ­an deducir fenÃ³menos comprobables. El 29 de mayo de 1919, Arthur Eddington fue capaz de medir, durante un eclipse, la desviaciÃ³n de la luz de una estrella al pasar cerca del Sol, una de las predicciones de la relatividad general. Cuando se hizo pÃºblica esta confirmaciÃ³n la fama de Einstein se incrementÃ³ enormemente y se considerÃ³ un paso revolucionario en la fÃ­sica. Desde entonces la teorÃ­a se ha verificado en todos y cada uno de los experimentos y verificaciones realizados hasta el momento.51â€‹

A pesar de su popularidad, o quizÃ¡s precisamente por ella, la teorÃ­a contÃ³ con importantes detractores entre la comunidad cientÃ­fica que no podÃ­an aceptar una fÃ­sica sin un sistema de referencia absoluto.

EstadÃ­sticas de Bose-Einstein
ArtÃ­culo principal: EstadÃ­stica de Bose-Einstein
En 1924 Einstein recibiÃ³ un artÃ­culo de un joven fÃ­sico indio, Satyendranath Bose, denominado La ley de Plank y la hipÃ³tesis del cuanto de luz, describiendo a la luz como un gas de fotones y pidiendo la ayuda de Einstein para su publicaciÃ³n. Einstein se dio cuenta de que el mismo tipo de estadÃ­sticas podÃ­an aplicarse a grupos de Ã¡tomos y publicÃ³ el artÃ­culo, conjuntamente con Bose, en alemÃ¡n, la lengua mÃ¡s importante en fÃ­sica en la Ã©poca. Las estadÃ­sticas de Bose-Einstein explican el comportamiento de los tipos bÃ¡sicos de partÃ­culas elementales denominadas bosones.52â€‹

Debate Bohr-Einstein
Esta secciÃ³n es un extracto de Debate Bohr-Einstein[editar]
	
Existen desacuerdos sobre la neutralidad en el punto de vista de la versiÃ³n actual de este artÃ­culo o secciÃ³n.
En la pÃ¡gina de discusiÃ³n puedes consultar el debate al respecto.

Niels Bohr con Albert Einstein en casa de Paul Ehrenfesten Leiden (diciembre de 1925). La foto es todo un estudio de caracteres: el empÃ­rico y el teÃ³rico.
El debate Bohr-Einstein era un nombre popular dado a una serie de amistosas discusiones pÃºblicas entre Albert Einstein y Niels Bohr acerca de la fÃ­sica cuÃ¡ntica. Sus discusiones son muy recordados debido a su importancia en la filosofÃ­a de la ciencia. El sentido y significaciÃ³n de estos debates son escasamente comprendidos, pero su gran importancia fue tenida en cuenta por el propio Bohr y escrita en su artÃ­culo Discusiones con Einstein sobre los Problemas EpistemolÃ³gicos en la FÃ­sica AtÃ³mica publicados en un volumen dedicado a Einstein.

La posiciÃ³n de Einstein con respecto a la mecÃ¡nica cuÃ¡ntica es significativamente mÃ¡s sutil y de mente mÃ¡s abierta que lo que ha sido a veces presentado en los manuales tÃ©cnicos y artÃ­culos cientÃ­ficos populares. Sus poderosas y constantes crÃ­ticas a la mecÃ¡nica cuÃ¡ntica obligaron a sus defensores a aguzar y refinar su comprensiÃ³n acerca de las implicaciones filosÃ³ficas y cientÃ­ficas de sus propias teorÃ­as.
La teorÃ­a de campo unificada
Einstein dedicÃ³ sus Ãºltimos aÃ±os a la bÃºsqueda de una de las mÃ¡s importantes teorÃ­as de la fÃ­sica, la llamada teorÃ­a de campo unificada. Dicha bÃºsqueda, despuÃ©s de su teorÃ­a general de la relatividad, consistiÃ³ en una serie de intentos tendentes a generalizar su teorÃ­a de la gravitaciÃ³n para lograr unificar y resumir las leyes fundamentales de la fÃ­sica, especÃ­ficamente la gravitaciÃ³n y el electromagnetismo. En el aÃ±o 1950, expuso su teorÃ­a de campo unificada en un artÃ­culo titulado Â«Sobre la teorÃ­a generalizada de la gravitaciÃ³nÂ» (On the Generalized Theory of Gravitation) en la revista Scientific American.

Aunque Albert Einstein fue mundialmente cÃ©lebre por sus trabajos en fÃ­sica teÃ³rica, paulatinamente fue aislÃ¡ndose en su investigaciÃ³n, y sus intentos no tuvieron Ã©xito. Persiguiendo la unificaciÃ³n de las fuerzas fundamentales, Albert ignorÃ³ algunos importantes desarrollos en la fÃ­sica, siendo notablemente visible en el tema de las fuerzas nuclear fuerte y nuclear dÃ©bil, que no se entendieron bien sino despuÃ©s de quince aÃ±os tras la muerte de Einstein (cerca del aÃ±o 1970), mediante numerosos experimentos en fÃ­sica de altas energÃ­as. Los intentos propuestos por la teorÃ­a de cuerdas o la teorÃ­a M, evidencian que aÃºn perdura su Ã­mpetu para conseguir demostrar la gran teorÃ­a de la unificaciÃ³n de las leyes de la fÃ­sica.53â€‹

Actividad polÃ­tica
Los acontecimientos de la Primera Guerra Mundial empujaron a Einstein a comprometerse polÃ­ticamente, tomando partido. SentÃ­a desprecio por la violencia, la bravuconerÃ­a, la agresiÃ³n y la injusticia.54â€‹ Fue uno de los miembros mÃ¡s conocidos del Partido DemocrÃ¡tico AlemÃ¡n (DDP).

Albert Einstein fue un pacifista convencido. En 1914, 93 prominentes intelectuales alemanes firmaron el Manifiesto para el mundo civilizado para apoyar al kÃ¡iser y desafiar a las Â«hordas de rusos aliados con mongoles y negros que pretenden atacar a la raza blancaÂ», justificando la invasiÃ³n alemana de BÃ©lgica; pero Einstein se negÃ³ a firmarlo junto con otros tres intelectuales, que pretendÃ­an impulsar un contramanifiesto, exclamando posteriormente:55â€‹

Es increÃ­ble lo que Europa ha desatado con esta locura. (â€¦)
En estos momentos uno se da cuenta de lo absurda que es la especie animal a la que pertenece.
Albert Einstein.

Con Oppenheimer en 1947
Con el auge del nazismo en Alemania, Einstein deja su paÃ­s y decide residir en Estados Unidos.

Un grupo de enemigos de sus teorÃ­as en la Alemania nazi creÃ³ una asociaciÃ³n en su contra, e incluso un hombre fue acusado de promover su asesinato. AdemÃ¡s, se publicÃ³ un libro titulado Cien autores en contra de Einstein,56â€‹ ante el cual Einstein se limitÃ³ a decir: Â«Â¿Por quÃ© cien? Si estuviera equivocado, bastarÃ­a con uno soloÂ».57â€‹

En 1939 se produce su mÃ¡s importante participaciÃ³n en cuestiones mundiales. El Informe Smyth, aunque con sutiles recortes y omisiones, narra la historia de cÃ³mo los fÃ­sicos trataron, sin Ã©xito, de interesar a la Marina y al EjÃ©rcito en el proyecto atÃ³mico. Pero la cÃ©lebre carta de Einstein a Roosevelt escrita el 2 de agosto fue la que consiguiÃ³ romper la rigidez de la mentalidad militar. Sin embargo, Einstein, que siente desprecio por la violencia y las guerras, es considerado el Â«padre de la bomba atÃ³micaÂ».58â€‹ En plena Segunda Guerra Mundial apoyÃ³ una iniciativa de Robert Oppenheimer para comenzar el programa de desarrollo de armas nucleares conocido como Proyecto Manhattan.

En su discurso pronunciado en Nueva York, en diciembre de 1945, expuso:

En la actualidad, los fÃ­sicos que participaron en la construcciÃ³n del arma mÃ¡s tremenda y peligrosa de todos los tiempos, se ven abrumados por un similar sentimiento de responsabilidad, por no hablar de culpa. (â€¦)
Nosotros ayudamos a construir la nueva arma para impedir que los enemigos de la humanidad lo hicieran antes, puesto que dada la mentalidad de los nazis habrÃ­an consumado la destrucciÃ³n y la esclavitud del resto del mundo. (â€¦)
Hay que desear que el espÃ­ritu que impulsÃ³ a Alfred Nobel cuando creÃ³ su gran instituciÃ³n, el espÃ­ritu de solidaridad y confianza, de generosidad y fraternidad entre los hombres, prevalezca en la mente de quienes dependen las decisiones que determinarÃ¡n nuestro destino. De otra manera, la civilizaciÃ³n quedarÃ­a condenada.
Einstein: Hay que ganar la paz (1945).59â€‹
La causa socialista
En mayo de 1949, Monthly Review publicÃ³ (en Nueva York) un artÃ­culo suyo titulado Â«Â¿Por quÃ© el socialismo?Â»60â€‹ en el que reflexiona sobre la historia, las conquistas y las consecuencias de la Â«anarquÃ­a econÃ³mica de la sociedad capitalistaÂ», artÃ­culo que hoy sigue teniendo vigencia. Una parte muy citada del mismo habla del papel de los medios privados en relaciÃ³n con las posibilidades democrÃ¡ticas de los paÃ­ses:

La anarquÃ­a econÃ³mica de la sociedad capitalista tal como existe hoy es, en mi opiniÃ³n, la verdadera fuente del mal. (â€¦)
El capital privado tiende a concentrarse en pocas manos, en parte debido a la competencia entre los capitalistas, y en parte porque el desarrollo tecnolÃ³gico y el aumento de la divisiÃ³n del trabajo animan la formaciÃ³n de unidades de producciÃ³n mÃ¡s grandes a expensas de las mÃ¡s pequeÃ±as. El resultado de este proceso es una oligarquÃ­a del capital privado cuyo enorme poder no se puede controlar con eficacia incluso en una sociedad organizada polÃ­ticamente de forma democrÃ¡tica. Esto es asÃ­ porque los miembros de los cuerpos legislativos son seleccionados por los partidos polÃ­ticos, financiados en gran parte o influidos de otra manera por los capitalistas privados quienes, para todos los propÃ³sitos prÃ¡cticos, separan al electorado de la legislatura. La consecuencia es que los representantes del pueblo de hecho no protegen suficientemente los intereses de los grupos no privilegiados de la poblaciÃ³n. (â€¦)
Estoy convencido de que hay solamente un camino para eliminar estos graves males, el establecimiento de una economÃ­a socialista, acompaÃ±ado por un sistema educativo orientado hacia metas sociales.
Albert Einstein, Why Socialism?61â€‹

Einstein y Elsa arribando a Nueva York junto con los lÃ­deres sionistas de la World Zionist Organization en 1921
La causa sionista
Originario de una familia judÃ­a asimilada, abogÃ³ parcialmente por la causa sionista. Entre 1921 y 1932 pronunciÃ³ diversos discursos, con el propÃ³sito de ayudar a recoger fondos para la colectividad judÃ­a y sostener la Universidad Hebrea de JerusalÃ©n, fundada en 1918, y como prueba de su creciente adhesiÃ³n a la causa sionista. Sin embargo, aunque estaba a favor de que Palestina fuese un "hogar" para los judÃ­os, tal y como afirmaba la DeclaraciÃ³n Balfour, estaba en contra de la creaciÃ³n de un Estado judÃ­o. AsÃ­, en enero de 1946, en una declaraciÃ³n ante el ComitÃ© Angloamericano de InvestigaciÃ³n que interrogÃ³ a varias personalidades sobre la creaciÃ³n de un Estado judÃ­o, Einstein dijo:
La idea de un Estado (judÃ­o) no coincide con lo que siento, no puedo entender para quÃ© es necesario. EstÃ¡ vinculada a un montÃ³n de dificultades y es propia de mentes cerradas. Creo que es mala.62â€‹
Einstein abogÃ³ por un Estado binacional donde judÃ­os y palestinos tuvieran los mismos derechos:63â€‹ Â«Nosotros, esto es, judÃ­os y Ã¡rabes, debemos unirnos y llegar a una comprensiÃ³n recÃ­proca en cuanto a las necesidades de los dos pueblos, en lo que ataÃ±e a las directivas satisfactorias para una convivencia provechosaÂ».64â€‹

El Estado de Israel se creÃ³ en 1948. Cuando Jaim Weizmann, el primer presidente de Israel y viejo amigo de Einstein, muriÃ³ en 1952, Abba Eban, embajador israelÃ­ en Estados Unidos, le ofreciÃ³ la presidencia. Einstein rechazÃ³ el ofrecimiento diciendo: Â«Estoy profundamente conmovido por el ofrecimiento del Estado de Israel y a la vez apenado y avergonzado por no poder aceptarlo. Durante toda mi vida he tratado con cuestiones objetivas, por lo que carezco de la aptitud natural y de la experiencia para tratar como es debido con la gente y para desempeÃ±ar funciones oficiales. Soy el mÃ¡s afligido por estas circunstancias, porque mi relaciÃ³n con el pueblo judÃ­o se ha convertido en mi vÃ­nculo humano mÃ¡s fuerte, desde que tomÃ© plena conciencia de nuestra precaria situaciÃ³n entre las naciones del mundoÂ».

La causa pacifista
Einstein, pacifista convencido, impulsÃ³ el conocido Manifiesto Russell-Einstein, un llamamiento a los cientÃ­ficos para unirse en favor de la desapariciÃ³n de las armas nucleares. Este documento sirviÃ³ de inspiraciÃ³n para la posterior fundaciÃ³n de las Conferencias Pugwash, que en 1995 se hicieron acreedoras del Premio Nobel de la Paz.

Ã‰tica y religiÃ³n

Estatua de Albert Einstein en la Academia IsraelÃ­ de Ciencias y Humanidades
Einstein se declarÃ³ agnÃ³stico, y en ocasiones se declarÃ³ tambiÃ©n ateo aunque algunos historiadores niegan este extremo.65â€‹ Dijo una vez que creÃ­a en el Dios Â«panteÃ­staÂ» de Baruch Spinoza, pero no en un dios personal, una creencia que criticÃ³.66â€‹67â€‹Einstein distingue tres estilos que suelen entremezclarse en la prÃ¡ctica de la religiÃ³n. El primero estÃ¡ motivado por el miedo y la mala comprensiÃ³n de la causalidad y, por tanto, tiende a inventar seres sobrenaturales. El segundo es social y moral, motivado por el deseo de apoyo y amor. Ambos tienen un concepto antropomÃ³rfico de Dios. El tercero â€“que Einstein considera el mÃ¡s maduroâ€“, estÃ¡ motivado por un sentido de asombro ante la Naturaleza.68â€‹

En una carta a la AsociaciÃ³n Central de Ciudadanos Alemanes de la Fe JudÃ­a, en 1920, les escribe:
Ni soy ciudadano alemÃ¡n, ni hay nada en mÃ­ que pueda definirse como Â«fe judÃ­aÂ». Pero soy judÃ­o y estoy orgulloso de pertenecer a la comunidad judÃ­a, aunque no los considero en absoluto los elegidos de Dios.69â€‹

Estatua de Einstein en el Parque de las Ciencias de Granada, obra de Miguel Barranco
En cierta ocasiÃ³n, en una reuniÃ³n, se le preguntÃ³ a Einstein si creÃ­a o no en un dios a lo que respondiÃ³: Â«Creo en el dios de Spinoza, que es idÃ©ntico al orden matemÃ¡tico del UniversoÂ».

Una cita mÃ¡s larga de Einstein aparece en Science, Philosophy, and Religion, A Symposium (Simposio de ciencia, filosofÃ­a y religiÃ³n), publicado por la Conferencia de Ciencia, FilosofÃ­a y ReligiÃ³n en su RelaciÃ³n con la Forma de Vida DemocrÃ¡tica:

Cuanto mÃ¡s imbuido estÃ© un hombre en la ordenada regularidad de los eventos, mÃ¡s firme serÃ¡ su convicciÃ³n de que no hay lugar â€”del lado de esta ordenada regularidadâ€” para una causa de naturaleza distinta. Para ese hombre, ni las reglas humanas ni las Â«reglas divinasÂ» existirÃ¡n como causas independientes de los eventos naturales. De seguro, la ciencia nunca podrÃ¡ refutar la doctrina de un dios que interfiere en eventos naturales, porque esa doctrina puede siempre refugiarse en que el conocimiento cientÃ­fico no puede posar el pie en ese tema. Pero estoy convencido de que tal comportamiento de parte de las personas religiosas no solamente es inadecuado sino tambiÃ©n fatal. Una doctrina que se mantiene no en la luz clara sino en la oscuridad, que ya ha causado un daÃ±o incalculable al progreso humano, necesariamente perderÃ¡ su efecto en la humanidad. En su lucha por el bien Ã©tico, las personas religiosas deberÃ­an renunciar a la doctrina de la existencia de Dios, esto es, renunciar a la fuente del miedo y la esperanza, que en el pasado puso un gran poder en manos de los sacerdotes. En su labor, deben apoyarse en aquellas fuerzas que son capaces de cultivar el bien, la verdad y la belleza en la misma humanidad. Esto es de seguro, una tarea mÃ¡s difÃ­cil pero incomparablemente mÃ¡s meritoria y admirable.
En una carta fechada en marzo de 1954, que fue incluida en el libro Albert Einstein: su lado humano (en inglÃ©s), editado por su fiel secretaria Helen Dukas y su colaborador Banesh Hoffman y publicada por Princeton University Press, Einstein dice:

Por supuesto era una mentira lo que se ha leÃ­do acerca de mis convicciones religiosas; una mentira que es repetida sistemÃ¡ticamente. No creo en un dios personal y no lo he negado nunca sino que lo he expresado claramente. Si hay algo en mÃ­ que pueda ser llamado religioso es la ilimitada admiraciÃ³n por la estructura del mundo, hasta donde nuestra ciencia puede revelarla.
La carta al filÃ³sofo Eric Gutkind, del 3 de enero de ese mismo aÃ±o, subastada en mayo de 2008,70â€‹ deja al parecer las cosas mÃ¡s claras. Dice Einstein:
La palabra dios para mÃ­ no es mÃ¡s que la expresiÃ³n y producto de las debilidades humanas, la Biblia, una colecciÃ³n de honorables pero aÃºn primitivas leyendas que sin embargo son bastante infantiles. Ninguna interpretaciÃ³n, sin importar cuÃ¡n sutil sea, puede (para mÃ­) cambiar estoâ€¦
TambiÃ©n hay una carta poco conocida de Einstein, enviada a Guy H. Raner Jr, el 2 de julio de 1945, en respuesta a un rumor de que un sacerdote jesuita lo habÃ­a convertido al cristianismo, en la cual Einstein se declara directamente ateo (citado por Michael R. Gilmore en Skeptic Magazine, v. 5, No.2)71â€‹

He recibido su carta del 10 de junio. Nunca he hablado con un sacerdote jesuita en mi vida y estoy asombrado por la audacia de tales mentiras sobre mÃ­. Desde el punto de vista de un sacerdote jesuita, soy, por supuesto, y he sido siempre un ateo.
William Hermanns, veterano superviviente de VerdÃºn, profesor de literatura alemana, entrevistÃ³ varias veces a Einstein, la primera en BerlÃ­n en 1930. En esa ocasiÃ³n planteÃ³ la idea de una religiÃ³n cÃ³smica, una idea a la que habÃ­a hecho referencia en la conversaciÃ³n sobre la realidad que habÃ­a tenido con Rabindranath Tagore y que despuÃ©s desarrollÃ³ y titulÃ³ Â«ReligiÃ³n y CienciaÂ», publicado en el New York Times en 1930. Einstein siguiÃ³ desarrollando esta idea y Herrmanns, que la consideraba compatible con las creencias tradicionales se propuso fundar un movimiento que integrara las tradiciones judÃ­as, cristiana, vedista, budista e islÃ¡mica. Estaba dispuesto a obtener declaraciones concisas y precisas sobre Dios. Einstein no pudo serlo mÃ¡s:
Con respecto a Dios, no puedo aceptar ningÃºn concepto basado en la autoridad de la Iglesia. Desde que tengo uso de razÃ³n me ha molestado el adoctrinamiento de las masas. No creo en el miedo a la vida, en el miedo a la muerte, en la fe ciega. No puedo demostrar que no haya un dios personal, pero si hablara de Ã©l, mentirÃ­a. No creo en el dios de la teologÃ­a, en el dios que premia el bien y castiga el mal. Mi dios creÃ³ las leyes que se encargan de eso. Su universo no estÃ¡ gobernado por quimeras, sino por leyes inmutables.72â€‹
Para Einstein, su religiÃ³n cÃ³smica y su condiciÃ³n judÃ­a no guardaban relaciÃ³n entre sÃ­. Cuando se le preguntÃ³ si existÃ­a un punto de vista judÃ­o replicÃ³:
En el sentido filosÃ³fico no hay, en mi opiniÃ³n, un punto de vista especÃ­ficamente judÃ­o. Para mÃ­, el judaÃ­smo tiene que ver casi exclusivamente con la actitud moral en la vida y hacia la vida [â€¦] El judaÃ­smo no es, pues, una religiÃ³n trascendental; tiene que ver como vivimos la vida y, hasta cierto punto, con cÃ³mo la entendemos [â€¦], y nada mÃ¡s. Tengo dudas si se le puede llamar religiÃ³n en el sentido aceptado de la palabra, o bien considerarla no como una "fe", sino como la santificaciÃ³n de la vida en el sentido suprapersonal que se les exige a los judÃ­os.73â€‹
Einstein decÃ­a que la moral no era dictada por Dios, sino por la humanidad:74â€‹

No creo en la inmoralidad del individuo, y considero la Ã©tica una preocupaciÃ³n exclusivamente humana sobre la que no hay ninguna autoridad sobrehumana.
Algunas publicaciones
ArtÃ­culo principal: Anexo:Publicaciones cientÃ­ficas de Albert Einstein
Einstein, Albert (1901) [manuscrito recibido 16 de diciembrw 1900], Â«Folgerungen aus den CapillaritÃ¤tserscheinungenÂ» [Conclusions Drawn from the Phenomena of Capillarity] (PDF), escrito en ZÃºrich, Suiza, Annalen der Physik (BerlÃ­n) (en alemÃ¡n) (Hoboken, NJ, publicado el 14 de marzo de 2006) 309 (3): 513-523, Bibcode:1901AnP...309..513E, doi:10.1002/andp.19013090306 â€“ via Wiley Online Library
Einstein, Albert (1905a) [manuscrito recibido 18 de marzo 1905], Â«Ãœber einen die Erzeugung und Verwandlung des Lichtes betreffenden heuristischen GesichtspunktÂ» [On a Heuristic Viewpoint Concerning the Production and Transformation of Light] (PDF), escrito en Berna, Suiza, Annalen der Physik (BerlÃ­n) (en alemÃ¡n) (Hoboken, NJ, publicado el 10 de marzo de 2006) 322 (6): 132-148, Bibcode:1905AnP...322..132E, doi:10.1002/andp.19053220607 â€“ via Wiley Online Library
Einstein, Albert (1905b) [manuscrito completado 30 abril y recibido 20 de julio 1905]. Â«Eine neue Bestimmung der MolekÃ¼ldimensionenÂ» [A new determination of molecular dimensions] (PDF). Escrito en Berne, Switzerland, published by Wyss Buchdruckerei. Dissertationen UniversitÃ¤t ZÃ¼rich (PhD Thesis) (en alemÃ¡n) (Zurich, Switzerland: ETH ZÃºrich, publicado el 2008). doi:10.3929/ethz-a-000565688 â€“ via ETH Bibliothek.
Einstein, Albert (1905c) [manuscrito recibido 11 de mayo 1905], Â«Ãœber die von der molekularkinetischen Theorie der WÃ¤rme geforderte Bewegung von in ruhenden FlÃ¼ssigkeiten suspendierten TeilchenÂ» [On the Motion â€“ Required by the Molecular Kinetic Theory of Heat â€“ of Small Particles Suspended in a Stationary Liquid] (PDF), escrito en Berna, Suiza, Annalen der Physik (BerlÃ­n) (en alemÃ¡n) (Hoboken, NJ, publicado el 10 de marzo de 2006) 322 (8): 549-560, Bibcode:1905AnP...322..549E, doi:10.1002/andp.19053220806 â€“ via Wiley Online Library
Einstein, Albert (1905d) [manuscrito recibido 30 de junio 1905], Â«Zur Elektrodynamik bewegter KÃ¶rperÂ» [On the Electrodynamics of Moving Bodies] (PDF), escrito en Berna, Suiza, Annalen der Physik (BerlÃ­n) (en alemÃ¡n) (Hoboken, NJ, publicado el 10 de marzo de 2006) 322 (10): 891-921, Bibcode:1905AnP...322..891E, doi:10.1002/andp.19053221004 â€“ via Wiley Online Library
Einstein, Albert (1905e) [manuscrito recibido 27 de septiembre 1905], Â«Ist die TrÃ¤gheit eines KÃ¶rpers von seinem Energieinhalt abhÃ¤ngig?Â» [Does the Inertia of a Body Depend Upon Its Energy Content?] (PDF), escrito en Berna, Suiza, Annalen der Physik (BerlÃ­n) (en alemÃ¡n) (Hoboken, NJ, publicado el 10 de marzo de 2006) 323 (13): 639-641, Bibcode:1905AnP...323..639E, doi:10.1002/andp.19053231314 â€“ via Wiley Online Library
Einstein, Albert (1915) [manuscrito publicado 25 de noviembre 1915], Â«Die Feldgleichungen der GravitationÂ» [The Field Equations of Gravitation] (Online page images), KÃ¶niglich Preussische Akademie der Wissenschaften (en alemÃ¡n) (BerlÃ­n): 844-847 â€“ via ECHO, Cultural Heritage Online, Max Planck Institute for the History of Science
Einstein, Albert (1917a), Â«Kosmologische Betrachtungen zur allgemeinen RelativitÃ¤tstheorieÂ» [Cosmological Considerations in the General Theory of Relativity], KÃ¶niglich Preussische Akademie der Wissenschaften, BerlÃ­n (en alemÃ¡n)
Einstein, Albert (1917b), Â«Zur Quantentheorie der StrahlungÂ» [On the Quantum Mechanics of Radiation], Physikalische Zeitschrift (en alemÃ¡n) 18: 121-128, Bibcode:1917PhyZ...18..121E
Einstein, Albert (1923) [manuscrito publicado 1923, en inglÃ©s 1967]. Grundgedanken und Probleme der RelativitÃ¤tstheorie [Fundamental Ideas and Problems of the Theory of Relativity] (PDF) (en alemÃ¡n (1923) inglÃ©s (1967)). Nobel Lectures, Physics 1901-1921. Estocolmo: Nobelprice.org (publicado el 3 de febrero de 2015) â€“ via Nobel Media AB 2014.
Einstein, Albert (1924) [manuscrito publicado 10 de julio 1924], Â«Quantentheorie des einatomigen idealen GasesÂ» [Quantum theory of monatomic ideal gases] (Online page images), Sitzungsberichte der Preussischen Akademie der Wissenschaften, Physikalisch-Mathematische Klasse (en alemÃ¡n) (MÃºnic: KÃ¶niglich Preussische Akademie der Wissenschaften, BerlÃ­n): 261-267 â€“ via ECHO, Cultural Heritage Online, Max Planck Institute for the History of Science. First of a series of papers on this topic.
Einstein, Albert (12 de marzo de 1926) [manuscrito publicado 1 de marzo 1926], Â«Die Ursache der MÃ¤anderbildung der FluÃŸlÃ¤ufe und des sogenannten Baerschen GesetzesÂ» [On Baer's law and meanders in the courses of rivers], escrito en BerlÃ­n, Die Naturwissenschaften (en alemÃ¡n) (Heidelberg, Germany: Springer-Verlag) 14 (11): 223-224, Bibcode:1926NW.....14..223E, ISSN 1432-1904, doi:10.1007/BF01510300 â€“ via SpringerLink
Einstein, Albert (1926b), escrito en Berne, Switzerland, R. FÃ¼rth, ed., Investigations on the Theory of the Brownian Movement (PDF), Translated by A. D. Cowper, USA: Dover Publications (publicado el 1956), ISBN 978-1-60796-285-4, consultado el 4 de enero de 2015
Einstein, Albert; Podolsky, Boris; Rosen, Nathan (15 de mayo de 1935) [manuscrito recibido 25 de marzo 1935], Â«Can Quantum-Mechanical Description of Physical Reality Be Considered Complete?Â» (PDF), Physical Review (American Physical Society) 47 (10): 777-780, Bibcode:1935PhRv...47..777E, doi:10.1103/PhysRev.47.777 â€“ via APS Journals
Einstein, Albert (9 de noviembre de 1940), Â«On Science and ReligionÂ», Nature (Edimburgo: Macmillan Publishers Group) 146 (3706): 605-607, Bibcode:1940Natur.146..605E, ISBN 0-7073-0453-9, doi:10.1038/146605a0
Einstein, Albert (4 de diciembre de 1948), Â«To the editors of the New York TimesÂ», New York Times (Melville, New York: AIP, American Inst. of Physics), ISBN 0-7354-0359-7, archivado desde el original el 17 de diciembre de 2007, consultado el 31 de marzo de 2016
Einstein, Albert (mayo de 1949), Â«Why Socialism? (Reprise)Â», Monthly Review (New York: Monthly Review Foundation, publicado el mayo de 2009) 61 (01 (mayo)), archivado desde el original el 11 de enero de 2006, consultado el 16 de enero de 2006 â€“ via MonthlyReview.org
Einstein, Albert (1950), Â«On the Generalized Theory of GravitationÂ», Scientific American CLXXXII (4): 13-17, doi:10.1038/scientificamerican0450-13
Einstein, Albert (1954), Ideas and Opinions, New York: Random House, ISBN 0-517-00393-7
Einstein, Albert (1969), Albert Einstein, Hedwig und Max Born: Briefwechsel 1916-1955 (en alemÃ¡n), MÃºnich: Nymphenburger Verlagshandlung, ISBN 3-88682-005-X
Einstein, Albert (1979), Autobiographical Notes, Paul Arthur Schilpp (Centennial ediciÃ³n), Chicago: Open Court, ISBN 0-87548-352-6. The chasing a light beam thought experiment is described on pages 48â€“51.
Collected Papers: Stachel, John; Martin J. Klein; A. J. Kox; Michel Janssen; R. Schulmann; Diana Komos Buchwald, eds. (21 de julio de 2008) [manuscrito publicado entre 1987-2006], Â«The Collected Papers of Albert EinsteinÂ», Einstein's Writings (Princeton University Press), 1-10, archivado desde el original el 17 de febrero de 2013, consultado el 31 de marzo de 2016. MÃ¡s informaciÃ³n sobre los volÃºmenes publicados se halla en la web Einstein Papers Project y en Princeton University Press Einstein Page
Eponimia

Sello rumano de 2005 con la imagen de Albert Einstein

Torre Einstein (Potsdam)
AdemÃ¡s de numerosas calles, plazas y ciudades de varios paÃ­ses del mundo, asÃ­ como distintas instituciones acadÃ©micas, una amplia relaciÃ³n de elementos relacionados con la ciencia llevan el nombre de Einstein en su memoria:

QuÃ­mica

El elemento Einstenio fue nombrado en su honor.
MatemÃ¡ticas
Convenio de suma de Einstein, notaciÃ³n abreviada usada en Ã¡lgebra tensorial.
FÃ­sica
Anillo de Einstein, efecto gravitatorio sobre la luz estelar.
Ecuaciones del campo de Einstein, 10 ecuaciones de la teorÃ­a de la relatividad general.
EstadÃ­stica de Bose-Einstein, tipo de mecÃ¡nica estadÃ­stica aplicable a bosones en equilibrio tÃ©rmico.
Condensado de Bose-Einstein, estado de agregaciÃ³n de la materia que se da en ciertos materiales a temperaturas cercanas al cero absoluto.
Einstein (unidad de medida), unidad de medida de cantidad de radiaciÃ³n.
RelaciÃ³n de Einstein (teorÃ­a cinÃ©tica), relacionada con el movimiento browniano.
Modelo de Einstein, utilizado en la fÃ­sica de sÃ³lidos de la mecÃ¡nica cuÃ¡ntica.
Espacio
Observatorio Einstein, un detector espacial de rayos X astronÃ³micos.
Albert Einstein (nave), nave espacial de carga europea lanzada en 2013.
AstronomÃ­a
Torre Einstein, observatorio erigido en 1924 situado en Potsdam.
Einstein, crÃ¡ter de impacto lunar.
Einstein, asteroide nÃºmero 2001 del catÃ¡logo del Minor Planet Center.
Cruz de Einstein, quasar afectado por el fenÃ³menos gravitatorios que afectan a su luz.
Museo
Einsteinhaus, casa-museo localizada en Berna, dedicada al fÃ­sico alemÃ¡n.
Premio
Medalla Albert Einstein, reconociendo anualmente desde 1979 trabajos relacionados con la obra de Einstein.
Medalla Albert Einstein de la UNESCO.
En la cultura popular
{{{Alt
Famosa fotografÃ­a de Einstein sacando la lengua
Albert Einstein ha sido objeto e inspiraciÃ³n para muchas obras de la cultura popular.

En el cumpleaÃ±os 72 de Einstein, el 14 de marzo de 1951, el fotÃ³grafo de United Press, Arthur Sasse, intentaba persuadirlo para que no sonriera ante la cÃ¡mara, pero ese dÃ­a, al haber sonreÃ­do a los fotÃ³grafos muchas veces, Einstein le sacÃ³ la lengua. Esta fotografÃ­a se convirtiÃ³ en una de las mÃ¡s populares jamÃ¡s tomadas. Einstein disfrutÃ³ de esta foto y le pidiÃ³ a UPI que le diera nueve copias para uso personal, una de las cuales firmÃ³ para un reportero. El 19 de junio de 2009, la fotografÃ­a firmada original se vendiÃ³ en una subasta por $ 74,324, un rÃ©cord para una foto de Einstein.75â€‹76â€‹

Einstein es un modelo favorito para las representaciones de genios o cientÃ­ficos locos; Su rostro expresivo y sus peinados distintivos han sido ampliamente copiados y exagerados. Frederic Golden, de la revista Time , escribiÃ³ que Einstein era "el sueÃ±o de un dibujante hecho realidad".77â€‹

El nombre de "Einstein" se ha convertido en sinÃ³nimo de una persona extremadamente inteligente. TambiÃ©n se puede usar sarcÃ¡sticamente cuando alguien dice lo obvio o demuestra falta de sabidurÃ­a o inteligencia.

Einstein tambiÃ©n ha sido objeto de muchas citas que se han hecho especialmente populares en Internet y se le han atribuido falsamente, incluida "la definiciÃ³n de locura".78â€‹

VÃ©ase tambiÃ©n
Equivalencia entre masa y energÃ­a (E=mcÂ²)
Efecto fotoelÃ©ctrico
FÃ­sica teÃ³rica
Historia de la electricidad
MecÃ¡nica cuÃ¡ntica
Movimiento browniano
Onda gravitatoria
Relatividad general
TeorÃ­a de la relatividad
TeorÃ­a de la relatividad especial
Premio Nobel de FÃ­sica
"Albert Einstein: creador y rebelde"
Referencias
 Alfonseca, Manuel (1996). Diccionario Espasa 1.000 grandes cientÃ­ficos. Madrid: Espasa Calpe. p. 740. ISBN 84-239-9236-5.. PÃ¡g. 171
 Lazare Iglesis (1984). Miniserie de televisiÃ³n Hello, Einstein! (cuatro capÃ­tulos). Francia. Video: [1]
 Einstein, Albert (25 de noviembre de 1915). Â«Die Feldgleichungun der GravitationÂ». Sitzungsberichte der Preussischen Akademie der Wissenschaften zu Berlin (en alemÃ¡n): 844-847. Consultado el 12 de septiembre de 2006.
 El London Times publicÃ³ el 7 de noviembre de 1919 los siguientes titulares: RevoluciÃ³n en la ciencia. Nueva teorÃ­a del universo. Las ideas de Newton derrocadas.
 Alfonseca, 1998.
 Kaku, 2005, p. 98.
 Anders BÃ¡rÃ¡ny (2001). Â«El Premio Nobel y el fantasma de EinsteinÂ». Project Syndicate.
 Einstein, Albert (2000). Mis ideas y opiniones. Barcelona: Antoni Bosch editor. pp. 167/169. ISBN 8493051632. Â«Nuestra deuda con el sionismo (discurso pronunciado en Nueva York, 1938) [â€¦] En nuestra situaciÃ³n, una cosa debe destacarse en especial: el pueblo judÃ­o ha contraÃ­do una deuda de gratitud con el sionismo. El movimiento sionista ha revivido entre los judÃ­os el sentimiento comunitario, y ha llevado a cabo un esfuerzo que supera todas las expectativasÂ».
 Rodrigo, AgustÃ­n Andreu (2004). El libro de las estatuas. Valencia: Editorial Universitaria PolitÃ©cnica Valencia. p. 287. ISBN 8497055586. Â«Como consecuencia de su identificaciÃ³n con el pueblo judÃ­o durante su estancia en BerlÃ­n, Einstein se hizo ferviente sionista a partir de 1919, tras algunas dudas iniciales.Â»
 Hawking, Stephen W.; Mlodinow, Leonard (2005). BrevÃ­sima historia del tiempo. Barcelona: Critica. pp. 183/184. ISBN 8484326373. Â«La segunda gran causa de Einstein fue el sionismo [â€¦]. Su apoyo explÃ­cito a la causa sionista, sin embargo, fue reconocido en 1952, cuando se le ofreciÃ³ la presidencia de IsraelÂ».
 Laca Arocena, Francisco A. (enero-abril). Â«Albert Einstein: Un sionismo pacifistaÂ». Acta Universitaria (Universidad de Guanajuato) 19 (1): 12-20. ISSN 0188-6266.
 Frank Pellegrini. Â«Albert EinsteinÂ» (en inglÃ©s). Time. Archivado desde el original el 25 de noviembre de 2005.
 Smith, Peter D. (2003). JesÃºs Domingo, ed. Einstein. v.o. inglÃ©s (2Âª ediciÃ³n). Life&Times. p. 191. ISBN 84-7902-557-3.
 Ibid., p XVIII.
 CalderÃ³n Hoffmann, Leonora (1994). Mi abuela Lola Hoffmann. Santiago: Cuatro Vientos. p. 29. ISBN 9562420140. Â«Recuerdo haber asistido, con mi hermano Konstantin (que era profesor de fÃ­sica en BerlÃ­n), a un concierto de violÃ­n ejecutado por Albert Einstein en beneficio de estudiantes judÃ­os pobres. EstÃ¡bamos ubicados en la segunda fila y desde allÃ­ podÃ­amos apreciar claramente sus emociones. Al subir al escenario se notaba muy nervioso, pero al comenzar su interpretaciÃ³n de una pieza de Mendelssohn, los ojos del genio de la fÃ­sica se cerraron y su rostro se relajÃ³ completamente, me dio la impresiÃ³n de alguien soÃ±ando maravillas. Fue muy impactante.Â»
 Einstein como estudiante
 Robinson, 2010, p. 36.
 Certificado de graduaciÃ³n de Einstein en 1896 emitido por la Escuela Cantonal de Argovia, Robinson (2000, p. 26)
 Robinson, 2010, pp. 40 y 141.
 Renn, Jurgen; Schulmann, Robert (1992). Albert Einstein/Mileva Maric: â€œThe love lettersâ€. Princeton: Princeton University Press.
 Robinson, 2010, p. 40.
 "Las cartas de amor de Einstein", de Robert Schulmann, R.SChulmann, 2005.
 Robinson, 2010, Â«1905, el aÃ±o milagrosoÂ», pp. 52 a 65.
 von Hirschhausen, Ulrike (2007). Â«Von imperialer Inklusion zur nationalen Exklusion:StaatsbÃ¼rgerschaft in Ã–sterreich- Ungarn 1867-1923Â» (WZB Discussion Paper). ZKD â€“ VerÃ¶ffentlichungsreihe der Forschungsgruppe, â€Zivilgesellschaft, Citizenship und politische Mobilisierung in Europaâ€œ Schwerpunkt Zivilgesellschaft, Konflikte und Demokratie, Wissenschaftszentrum Berlin fÃ¼r Sozialforschung (No. SP IV 2007-403). Berlin, Germany: WZB Social Science Research Center Berlin. p. 8. ISSN 1860-4315. Consultado el 4 de agosto de 2015. Â«Eine weitere DiskontinuitÃ¤t bestand viertens darin, dass die Bestimmungen der Ã¶sterreichischen StaatsbÃ¼rgerschaft, die in den ersten Dritteln des Jahrhunderts auch auf Ungarn angewandt worden waren, seit 1867 nur noch fÃ¼r die cisleithanische ReichshÃ¤lfte galten. Ungarn entwickelte hingegen jetzt eine eige-ne StaatsbÃ¼rgerschaft.Â»
 Hawking, 2002, pp. 13 y 33.
 Einstein, 1996, p. 70.
 Robinson, 2010, pp. 145-147.
 Einstein, 1996, pp. 9-11.
 Philipp Lenard: Ideelle Kontinentalsperre, MÃ¼nchen 1940.
 Robinson, 2010, cap. 10, esp. o. 163.
 Einstein, Albert. ArtÃ­culo subido a su web por la Universidad Complutense de Madrid. (Monthly Review, Nueva York, mayo de 1949.). Â«Â¿Por quÃ© socialismo?Â».
 Mario Mucnick. Libro editado por Aleph Editores. (2011). Â«Oficio editor.Â».
 Montes-Santiago, J. (16 de julio de 2017). Â«[The meeting of Einstein with Cajal (Madrid, 1923): a lost tide of fortune]Â». Revista De Neurologia 43 (2): 113-117. ISSN 0210-0010. PMID 16838259.
 Robinson, 2010, pp. 168-169.
 Einstein, 1996, p. 46.
 Ãlvarez Chillida, Gonzalo (2002). El antisemitismo en EspaÃ±a: La imagen del judÃ­o (1812-2002). Marcial Pons. p. 335.
 SÃ¡nchez Ron, JosÃ© Manuel; Glick, Thomas F. (1983). La EspaÃ±a posible de la Segunda RepÃºblica. La oferta a Einstein de una cÃ¡tedra extraordinaria en la Universidad Central (Madrid, 1933). Editorial de la Universidad Complutense.
 Robinson, 2010, pp. 198-199.
 Robinson, 2010, pp. 102-120.
 The Day Albert Einstein Died: A Photographerâ€™s Story. Revista Life. Consultado el 3 de diciembre de 2020.
 NPR: The Long, Strange Journey of Einstein's Brain
 Whitrow, Einstein: El hombre y su obra, p. 27.
 Lanzamiento del AÃ±o Mundial de la FÃ­sica
 Robinson, 2010, p. 125.
 Robinson, 2010, p. 124.
 Arnau, Juan (11 de diciembre de 2020). Einstein: la luz que mide las cosas. Babelia, El PaÃ­s. Consultado el 12 de diciembre de 2020. (requiere suscripciÃ³n).
 Hawking, 2002, pp. 6-7.
 Robinson, 2010, pp. 42-51.
 Robinson, 2010, pp. 66-77.
 Robinson, 2010, p. 74-77.
 Robinson, 2010, p. 76.
 Robinson, 2010, pp. 90-91.
 Robinson, 2010, pp. 108 y 128.
 Einstein, 1996, p. 12.
 Kaku, 2005, p. 85.
 Agencia EFE. Elespectador.com (10 de diciembre de 2009). Â«Un libro analiza las motivaciones de los enemigos de EinsteinÂ». Archivado desde el original el 24 de septiembre de 2015.
 Stephen Hawking (2001). Â«El Universo en una CÃ¡scara de Nuez. CapÃ­tulo 1.Â».
 Einstein, 1996, p. 13.
 Einstein, 1996, p. 63.
 Â«Einstein: Â«Por quÃ© el socialismoÂ»Â». Marxists Internet Archive.
 Albert Einstein: Why socialism?, en Monthly Review, mayo de 1949.
 Edward Corrigan, "Albert Einstein on Palestine"
 "Einstein and Complex Analyses of Zionism" Jewish Daily Forward, 24 de julio de 2009
 Einstein, 1996, p. 21.
 Isaacson, Walter (2008). Einstein: His Life and Universe. Nueva York: Simon y Schuster, pp. 390.
 Einstein, Albert "Gelegentliches", Soncino Gesellschaft, Berlin, 1929, p. 9 "This firm belief, a belief bound up with a deep feeling, in a superior mind that reveals itself in the world of experience, represents my conception of God. In common parlance this may be described as "pantheistic" (Spinoza)."
 Hoffmann, Banesh (1972). Albert Einstein Creator and Rebel. New York: New American Library, p. 95. "It seems to me that the idea of a personal God is an anthropological concept which I cannot take seriously. I feel also not able to imagine some will or goal outside the human sphere. My views are near those of Spinoza: admiration for the beauty of and belief in the logical simplicity of the order which we can grasp humbly and only imperfectly."
 Albert Einstein (5 de abril de 2009). Â«Religion and ScienceÂ». New York Times (en inglÃ©s).
 Robinson, 2010, p. 176.
 Â«Einstein, lejos de DiosÂ». El PaÃ­s. 13 de mayo de 2008. Consultado el 21 de noviembre de 2012.
 Austin, Cline. Â«Einstein Quotes on Atheism & Freethought: Was Einstein an Atheist, Freethinker?Â». About.com Religion & Spirituality (en inglÃ©s). Consultado el 2 de febrero de 2017.
 Robinson, 2010, pp. 176-177.
 Robinson, 2010, p. 178.
 Kaku, 2005, p. 101.
 Â«Various things about Albert EinsteinÂ». www.einstein-website.de. Consultado el 27 de abril de 2019.
 Â«Photo Of Einstein Nets $74K At Auction - Boston News Story - WCVB BostonÂ». web.archive.org. 22 de junio de 2009. Consultado el 27 de abril de 2019.
 Golden, Frederic (31 de diciembre de 1999). Â«Albert EinsteinÂ». Time (en inglÃ©s estadounidense). ISSN 0040-781X. Consultado el 27 de abril de 2019.
 Â«9 Albert Einstein Quotes That Are Completely FakeÂ». Gizmodo Australia (en inglÃ©s). 16 de mayo de 2015. Consultado el 27 de abril de 2019.
BibliografÃ­a
BibliografÃ­a general
Alfonseca, Manuel (1998). Diccionario Espasa. 1.000 grandes cientÃ­ficos. Madrid: Espasa Calpe. ISBN 84-239-9236-5.
Albert Einstein. (2004). "ColecciÃ³n Grandes BiografÃ­as, 59". Editorial Planeta-De Agostini. Barcelona, EspaÃ±a. ISBN 84-395-4730-7.
Amis, Martin. (2005). Los monstruos de Einstein. Ediciones Minotauro. Barcelona, EspaÃ±a. ISBN 84-450-7089-4.
Clark, Ronald W., Einstein: The Life and Times, 1971, ISBN 0-380-44123-3.
Conferencia de Ciencia, FilosofÃ­a y ReligiÃ³n en su RelaciÃ³n con la Forma de Vida DemocrÃ¡tica, Science, Philosophy, and Religion, A Symposium (Simposio de ciencia, filosofÃ­a y religiÃ³n), Nueva York, 1941.
Dukas, Helen, y Banesh Hoffman, Albert Einstein: The Human Side (Albert Einstein, el lado humano), Princeton University Press.
Einstein, Albert (marzo de 1996). Este es mi pueblo. TÃ­tulo original Dies ist mein volk. Buenos Aires: Leviatan. ISBN 978-9505163076.
Hart, Michael H., The 100, Carol Publishing Group, 1992, ISBN 0-8065-1350-0.
Isaacson, Walter (2008). Einstein. Su vida y su universo. Debate. ISBN 978-84-8306-788-8.
Kaku, Michio (2005). El Universo de Einstein: CÃ³mo la visiÃ³n de Albert Einstein transformÃ³ nuestra comprensiÃ³n del espacio y el tiempo. Antoni Bosch editor. ISBN 84-95348-17-9.
Otero Carvajal, Luis Enrique: "Einstein y la revoluciÃ³n cientÃ­fica del siglo XX", Cuadernos de Historia ContemporanÃ©a n.Âº 27 (2005), ISSN 0214-400X.
Pais, Abraham, Subtle is the Lord. The Science and the Life of Albert Einstein, 1982, ISBN 0-19-520438-7.
Parker, Barry, Einstein's Brainchild, 280 pÃ¡gs., 2000, Prometheus Books, ISBN 1-57392-857-7 (en inglÃ©s)
Einstein y la teorÃ­a de la relatividad
Einstein, Albert (1941). Â«DemostraciÃ³n de la no existencia de campos gravitacionales sin singularidades de masa total no nulaÂ». Revista de MatemÃ¡ticas (Argentina: Universidad Nacional de TucumÃ¡n): 280. OL OL18968949M.
Einstein, Albert, El significado de la relatividad, Espasa Calpe, 1971.
Greene, Brian, El universo elegante, Planeta, 2001.
Hawking, Stephen, Breve historia del tiempo, Planeta, 1992, ISBN 968-406-356-3.
â€” (2002). El Universo en una cÃ¡scara de nuez (6Âª ediciÃ³n). ISBN 84 8432 293 9.
Robinson, Andrew (2010). Einstein; Cien aÃ±os de relatividad. Blume. ISBN 978 84 8076 882 5.
Russell, Bertrand, El ABC de la relatividad, 1925.
Schwinger, Julian (1986): Einstein's Legacy: The Unity of Space and Time. Scientific American Library. 250 pÃ¡gs. Nueva York ISBN 0-7167-5011-2 (El Legado de Einstein. La unidad del espacio y el tiempo. Prensa CientÃ­fica, S.A., Biblioteca Scientific American. 250 pÃ¡gs. Barcelona, 1995, ISBN 84-7593-054-9)
Material digital
Byron Preiss Multimedia. (2001). Einstein y su teorÃ­a de la relatividad. "ColecciÃ³n Ciencia Activa". Anaya Multimedia-Anaya Interactiva. Madrid, EspaÃ±a. ISBN 84-415-0247-1. (dos CD y un manual).
Enlaces externos
En el Marxists Internet Archive estÃ¡ disponible una secciÃ³n con obras de Albert Einstein.
Albert Einstein, El mundo como yo lo veo (ensayo), 'Internet Archive WayBack Machine', 28 de septiembre de 2007.
Varios libros de Albert Einstein en castellano, sitio digital 'Libroteca'.
Escritos de Albert Einstein (1879-1955), sitio digital 'Marxists Internet Archive (secciÃ³n en espaÃ±ol)'.
BiografÃ­a de Albert Einstein, 'Internet Archive WayBack Machine', 2 de diciembre de 2008.
Â¿Fue Albert Einstein un extraterrestre?, 'Internet Archive WayBack Machine', 23 de septiembre de 2005.
Alberto Einstein, Sobre la TeorÃ­a de la Relatividad, ediciones 'Altaya', 2016, ISBN 8822832299 y 9788822832290 (texto parcial en lÃ­nea).
La muerte de Einstein (ruptura de aneurisma de la aorta).
JosÃ© Manuel SÃ¡nchez Ron, "Einstein, Israel y Palestina", diario 'El PaÃ­s', 2 de mayo de 2002.
Albert Einstein, Mis ideas y opiniones, editor 'Antoni Bosch', 2000, ISBN 8493051632 y 9788493051631, 342 pÃ¡ginas (texto parcial en lÃ­nea).
Enlaces en otros idiomas
Archivos Oficiales de Einstein Online (en inglÃ©s)
Archivos Albert Einstein (en inglÃ©s)
Revista TIME 100: Albert Einstein (en inglÃ©s)
Albert Einstein (en inglÃ©s)
Tesis doctoral (en alemÃ¡n)
Werke von und Ã¼ber Albert Einstein im Katalog Helveticat der Schweizerischen Nationalbibliothek
Nachlass von Albert Einstein in der Archivdatenbank HelveticArchives der Schweizerischen Nationalbibliothek
Einstein-Website von Hans-Josef KÃ¼pper (AusfÃ¼hrliche Biografie)
Zeittafel
Mensch Einstein, Website zu Leben und Werk Einsteins des RBB
Einsteingalerie (Fotosammlung)
Albert-Einstein Oberschule Berlin-NeukÃ¶lln Deutschlands einzige Institution, die er offiziell nach sich hat benennen lassen
Das verschmÃ¤hte Genie ETH Campus Life
Friedrich DÃ¼rrenmatt zum 100. Geburtstag von Albert Einstein: Ein Vortrag (1979) gehalten an der ETH ZÃ¼rich, wo Einstein studierte und unterrichtete
Die Akte Einstein in: freitag del 22 de abril de 2005
Control de autoridades	
Proyectos WikimediaWd Datos: Q937Commonscat Multimedia: Albert EinsteinWikiquote Citas cÃ©lebres: Albert EinsteinWikisource Textos: Autor:Albert Einstein
IdentificadoresWorldCatVIAF: 75121530ISNI: 0000 0001 2281 955XBNE: XX834035BNF: 119016075 (data)BNC: 000034649CANTIC: a10077078GND: 118529579LCCN: n79022889NCL: 369710NDL: 00438728NKC: jn19990002019NLA: 36582360CiNii: DA00708434NARA: 10582637SNAC: w63c6p77SUDOC: 026849186ULAN: 500240971Scopus: 22988279600BIBSYS: 90053072UB: a1279550MGP: 53269ICCU: IT\ICCU\CFIV\035853KNAW: PE00000116Leopoldina: 3232Open Library: OL3175986AGoogle AcadÃ©mico: qc6CJjYAAAAJDiccionarios y enciclopediasGEA: 4887HDS: 028814Britannica: urlRepositorios digitalesDialnet: 278727Proyecto Gutenberg: 1630
CategorÃ­as: HombresNacidos en 1879Fallecidos en 1955Albert EinsteinLaureados con el Premio Nobel 1921Nacidos en UlmAgnÃ³sticos de AlemaniaAgnÃ³sticos de SuizaAgnÃ³sticos de Estados UnidosJudÃ­os de SuizaJudÃ­os de AlemaniaDoctores honoris causa por la Universidad de Ginebra
MenÃº de navegaciÃ³n
No has accedido
DiscusiÃ³n
Contribuciones
Crear una cuenta
Acceder
ArtÃ­culoDiscusiÃ³n
LeerVer cÃ³digoVer historialBuscar
Buscar en Wikipedia
Portada
Portal de la comunidad
Actualidad
Cambios recientes
PÃ¡ginas nuevas
PÃ¡gina aleatoria
Ayuda
Donaciones
Notificar un error
Herramientas
Lo que enlaza aquÃ­
Cambios en enlazadas
Subir archivo
PÃ¡ginas especiales
Enlace permanente
InformaciÃ³n de la pÃ¡gina
Citar esta pÃ¡gina
Elemento de Wikidata
Imprimir/exportar
Crear un libro
Descargar como PDF
VersiÃ³n para imprimir
En otros proyectos
Wikimedia Commons
Wikiquote
Wikisource

En otros idiomas
AragonÃ©s
Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©
Ğ‘ĞµĞ»Ğ°Ñ€ÑƒÑĞºĞ°Ñ
Deutsch
English
Euskara
ÙØ§Ø±Ø³ÛŒ
×¢×‘×¨×™×ª
Hrvatski
201 mÃ¡s
Editar enlaces
Esta pÃ¡gina se editÃ³ por Ãºltima vez el 14 mar 2021 a las 19:58.
El texto estÃ¡ disponible bajo la Licencia Creative Commons AtribuciÃ³n Compartir Igual 3.0; pueden aplicarse clÃ¡usulas adicionales. Al usar este sitio, usted acepta nuestros tÃ©rminos de uso y nuestra polÃ­tica de privacidad.
WikipediaÂ® es una marca registrada de la FundaciÃ³n Wikimedia, Inc., una organizaciÃ³n sin Ã¡nimo de lucro.
